---
title: 'Kubernetes The Hard Way'
date: 
draft: true
tags: ['DigitalOcean', 'Docker', 'Kubernetes', 'Ubuntu']
---

[Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way) を DigitalOcean で試してみる

1.  [Cloud Infrastructure Provisioning](#cloud_infrastructure) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/01-infrastructure-gcp.md))
2.  [Setting up a Certificate Authority and Creating TLS Certificates](#setting_up_a_crtificate_authority_and_creating_tls_certificates) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/02-certificate-authority.md))
3.  [Setting up Authentication](#setting_up_authentication) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/03-auth-configs.md))
4.  [Bootstrapping a H/A etcd cluster](#bootstrapping_a_ha_etcd_cluster) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-etcd.md))
5.  [Bootstrapping a H/A Kubernetes Control Plane](#bootstrapping_a_ha_kubernetes) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/05-kubernetes-controller.md))
6.  [Bootstrapping Kubernetes Workers](#bootstrapping_kubernetes_workers) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/06-kubernetes-worker.md))
7.  [Configuring the Kubernetes Client - Remote Access](#configuring_the_kubernetes_client_remote_access) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/07-kubectl.md))
8.  [Managing the Container Network Routes](#managing_the_container_network_routes) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/08-network.md))
9.  [Deploying the Cluster DNS Add-on](#deploying_the_cluster_dns_addon) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/09-dns-addon.md))
10.  [Smoke Test](#smoke_test) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/10-smoke-test.md))
11.  [Cleaning Up](#cleaning_up) ([GitHub](https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/11-cleanup.md))

### Cloud Infrastructure Provisioning

DigitalOcean で controller 3台、worker 3台を用意する。DigitalOcean のはなんちゃって Private Network だけど。 doctl は `source <(doctl completion bash)` で bash の補完が効くようになります。zsh もあります。 controller 用の userdata ファイルを準備します。`xxx.xxx.xxx.xxx` 部分は置換します。

```bash
#!/bin/bash
#
# controller-userdata.sh.tmpl
#
ETCD_VERSION=3.1.4
K8S_VERSION=$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)
K8S_BASE_URL=https://storage.googleapis.com/kubernetes-release/release
CLIENT_IP=xxx.xxx.xxx.xxx

#******************************************************************************
# ufw
#******************************************************************************
ufw enable
ufw allow in proto tcp to any port 22 from $CLIENT_IP
ufw allow in proto tcp to any port 6443 from $CLIENT_IP


#******************************************************************************
# etcd
#******************************************************************************
curl -sLo /tmp/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz https://github.com/coreos/etcd/releases/download/v${ETCD_VERSION}/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz
tar -x -C /tmp -f /tmp/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz
install -o root -g root -m 0755 /tmp/etcd-v${ETCD_VERSION}-linux-amd64/etcd /usr/bin/etcd
install -o root -g root -m 0755 /tmp/etcd-v${ETCD_VERSION}-linux-amd64/etcdctl /usr/bin/etcdctl

cat > /etc/systemd/system/etcd.service <<EOF
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
EnvironmentFile=/etc/default/kubernetes
ExecStart=/usr/bin/etcd \
  --name ${ETCD_NAME} \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \
  --listen-peer-urls https://${INTERNAL_IP}:2380 \
  --listen-client-urls https://${INTERNAL_IP}:2379,http://127.0.0.1:2379 \
  --advertise-client-urls https://${INTERNAL_IP}:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster controller0=https://${CONTROLLER0_IP}:2380,controller1=https://${CONTROLLER1_IP}:2380,controller2=https://${CONTROLLER2_IP}:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF


#******************************************************************************
# install kubernetes binaries
#******************************************************************************
rm -fr /tmp/etcd-v${ETCD_VERSION}-linux-amd64 /tmp/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz
mkdir /etc/etcd /var/lib/etcd
curl -sLo /usr/bin/kube-apiserver \
  ${K8S_BASE_URL}/${K8S_VERSION}/bin/linux/amd64/kube-apiserver
curl -sLo /usr/bin/kube-controller-manager \
  ${K8S_BASE_URL}/${K8S_VERSION}/bin/linux/amd64/kube-controller-manager
curl -sLo /usr/bin/kube-scheduler \
  ${K8S_BASE_URL}/${K8S_VERSION}/bin/linux/amd64/kube-scheduler
curl -sLo /usr/bin/kubectl \
  ${K8S_BASE_URL}/${K8S_VERSION}/bin/linux/amd64/kubectl
chmod 755 /usr/bin/kube*


#******************************************************************************
# kube-apiserver
#******************************************************************************
cat > /etc/systemd/system/kube-apiserver.service <<EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=/etc/default/kubernetes
ExecStart=/usr/bin/kube-apiserver \
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
  --advertise-address=${INTERNAL_IP} \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/lib/audit.log \
  --authorization-mode=RBAC \
  --bind-address=0.0.0.0 \
  --client-ca-file=/var/lib/kubernetes/ca.pem \
  --enable-swagger-ui=true \
  --etcd-cafile=/var/lib/kubernetes/ca.pem \
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \
  --etcd-servers=https://${CONTROLLER0_IP},https://${CONTROLLER1_IP}:2379,https://${CONTROLLER2_IP}:2379 \
  --event-ttl=1h \
  --experimental-bootstrap-token-auth \
  --insecure-bind-address=0.0.0.0 \
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \
  --kubelet-https=true \
  --runtime-config=rbac.authorization.k8s.io/v1alpha1 \
  --service-account-key-file=/var/lib/kubernetes/ca-key.pem \
  --service-cluster-ip-range=${SERVICE_CLUSTER_IP_RANGE} \
  --service-node-port-range=30000-32767 \
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \
  --token-auth-file=/var/lib/kubernetes/token.csv \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF


#******************************************************************************
# kube-controller-manager
#******************************************************************************
cat > /etc/systemd/system/kube-controller-manager.service <<EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=/etc/default/kubernetes
ExecStart=/usr/bin/kube-controller-manager \
  --address=0.0.0.0 \
  --allocate-node-cidrs=true \
  --cluster-cidr=${POD_IP_RANGE} \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \
  --leader-elect=true \
  --master=http://${INTERNAL_IP}:8080 \
  --root-ca-file=/var/lib/kubernetes/ca.pem \
  --service-account-private-key-file=/var/lib/kubernetes/ca-key.pem \
  --service-cluster-ip-range=${SERVICE_CLUSTER_IP_RANGE} \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

#******************************************************************************
# kube-scheduler
#******************************************************************************
cat > /etc/systemd/system/kube-scheduler.service <<EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=/etc/default/kubernetes
ExecStart=/usr/bin/kube-scheduler \
  --leader-elect=true \
  --master=http://${INTERNAL_IP}:8080 \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
```

```bash
for i in 0 1 2
do
  doctl compute droplet create controller${i} \
    --image ubuntu-16-04-x64 \
    --region sgp1 \
    --size 2gb \
    --ssh-keys 11939905 \
    --enable-monitoring \
    --enable-private-networking \
    --tag-names controller \
    --tag-names controller${i} \
    --user-data-file <(sed -e "s/xxx.xxx.xxx.xxx/$(curl -s globalip.me)/" controller-userdata.sh.tmpl)
done
```

worker 用 userdata ファイルの作成、こちらも `xxx.xxx.xxx.xxx` 部分は置換します。

```bash
#!/bin/bash
#
# worker-userdata.sh.tmpl
#

K8S_VERSION=$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)
K8S_BASE_URL=https://storage.googleapis.com/kubernetes-release/release
DOCKER_VERSION=1.12.6
CLIENT_IP=xxx.xxx.xxx.xxx

ufw enable
ufw allow in proto tcp to any port 22 from $CLIENT_IP

curl -sLo /var/tmp/docker-${DOCKER_VERSION}.tgz \
  https://get.docker.com/builds/Linux/x86_64/docker-${DOCKER_VERSION}.tgz
tar -C /var/tmp -xf /var/tmp/docker-${DOCKER_VERSION}.tgz
cp /var/tmp/docker/docker* /usr/bin/
rm -fr /var/tmp/docker*

curl -sLo /usr/bin/kubectl \
  ${K8S_BASE_URL}/${K8S_VERSION}/bin/linux/amd64/kubectl

curl -sLo /usr/bin/kubelet \
  ${K8S_BASE_URL}/${K8S_VERSION}/bin/linux/amd64/kubelet

curl -sLo /usr/bin/kube-proxy \
  ${K8S_BASE_URL}/${K8S_VERSION}/bin/linux/amd64/kube-proxy

chmod 755 /usr/bin/kube*

mkdir /var/lib/{kubelet,kube-proxy,kubernetes} /var/run/kubernetes


#******************************************************************************
# docker
#******************************************************************************
cat > /etc/systemd/system/docker.service <<EOF
[Unit]
Description=Docker Application Container Engine
Documentation=http://docs.docker.com

[Service]
ExecStart=/usr/bin/docker daemon \
  --iptables=false \
  --ip-masq=false \
  --host=unix:///var/run/docker.sock \
  --log-level=error \
  --storage-driver=overlay
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

#******************************************************************************
# kubelet
#******************************************************************************
cat > /etc/systemd/system/kubelet.service <<EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
EnvironmentFile=/etc/default/kubernetes
ExecStart=/usr/bin/kubelet \
  --allow-privileged=true \
  --cluster-dns=${CLUSTER_DNS_ADDRESS} \
  --cluster-domain=cluster.local \
  --container-runtime=docker \
  --bootstrap-kubeconfig=/var/lib/kubelet/bootstrap.kubeconfig \
  --network-plugin=kubenet \
  --kubeconfig=/var/lib/kubelet/kubeconfig \
  --serialize-image-pulls=false \
  --register-node=true \
  --tls-cert-file=/var/lib/kubelet/kubelet-client.crt \
  --tls-private-key-file=/var/lib/kubelet/kubelet-client.key \
  --cert-dir=/var/lib/kubelet \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

#******************************************************************************
# kube-proxy
#******************************************************************************
cat > /etc/systemd/system/kube-proxy.service <<EOF
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=/etc/default/kubernetes
ExecStart=/usr/bin/kube-proxy \
  --cluster-cidr=${POD_IP_RANGE} \
  --masquerade-all=true \
  --kubeconfig=/var/lib/kube-proxy/kube-proxy.kubeconfig \
  --proxy-mode=iptables \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
```

```bash
for i in 0 1 2
do
  doctl compute droplet create worker${i} \
    --image ubuntu-16-04-x64 \
    --region sgp1 \
    --size 2gb \
    --ssh-keys 11939905 \
    --enable-monitoring \
    --enable-private-networking \
    --tag-names worker \
    --tag-names worker${i} \
    --user-data-file <(sed -e "s/xxx.xxx.xxx.xxx/$(curl -s globalip.me)/" worker-userdata.sh.tmpl)
done
```

#### etcd 用 firewall (ufw) 設定

出力されるコマンドを controller{0,1,2} で実行する

```bash
for node in controller{0,1,2} worker{0,1,2}
do
  echo "ufw allow in proto tcp to any port 2379:2380 from $(doctl compute droplet list --tag-name ${node} --format PrivateIPv4 --no-header)"
  echo "ufw allow in proto tcp to any port 8080 from $(doctl compute droplet list --tag-name ${node} --format PrivateIPv4 --no-header)"
  if [[ "$node" =~ "worker" ]] ; then
    echo "ufw allow in proto tcp to any port 6443 from $(doctl compute droplet list --tag-name ${node} --format PrivateIPv4 --no-header)"
    echo "ufw allow in proto tcp to any port 6443 from $(doctl compute droplet list --tag-name ${node} --format PublicIPv4 --no-header)"
  fi
done
```

### Setting up a Certificate Authority and Creating TLS Certificates

クラスタ間通信や API 通信は TLS で証明書での認証を行うため [cfssl](https://github.com/cloudflare/cfssl) (Cloudflare's PKI and TLS toolkit) を使って CA の作成と証明書発行を行う

#### CFSSL のインストール

これはローカルPCなど作業端末へインストールします。私の端末は Ubuntu 17.04 です。

```bash
curl -LO https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -LO https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64

sudo install -o root -g root -m 0755 cfssl_linux-amd64 /usr/local/bin/cfssl
sudo install -o root -g root -m 0755 cfssljson_linux-amd64 /usr/local/bin/cfssljson
```

#### CA のセットアップ

CA 作成用の設定ファイル (ca-config.json) を作成します

```
cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
EOF
```

CSR 作成用のファイル (ca-csr.json) を作成します

```
cat > ca-csr.json <<EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "JP",
      "L": "MyCity",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Tokyo"
    }
  ]
}
EOF

```

CA の証明書作成

```
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

これで `ca-key.pem`, `ca.pem` が生成される

#### 管理者用証明書

クライアントから kubectl でアクセスするために使う証明書を発行します (CommonName = admin)

```
cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "JP",
      "L": "MyCity",
      "O": "system:masters",
      "OU": "Cluster",
      "ST": "Tokyo"
    }
  ]
}
EOF
```

```
cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin

```

`admin-key.pem`, `admin.pem` が作成されます

#### Create the kube-proxy client certificate

kube-proxy が API サーバーにアクセスするための証明書を作成します (CommonName = system:kube-proxy)

```
cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "JP",
      "L": "MyCity",
      "O": "system:node-proxier",
      "OU": "Cluster",
      "ST": "Tokyo"
    }
  ]
}
EOF
```

```
cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy
```

`kube-proxy-key.pem`, `kube-proxy.pem` が作成されます

#### Create the kubernetes server certificate

etcd や kubernetes controller 間の通信、worker から controller へのアクセスに使う証明書の作成

```
cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes",
  "hosts": [
$(doctl compute droplet list --tag-name controller --format PublicIPv4 --no-header | awk '{print "    \""$1"\","}')
$(doctl compute droplet list --tag-name controller --format PrivateIPv4 --no-header | awk '{print "    \""$1"\","}')
    "127.0.0.1",
    "kubernetes.default"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "JP",
      "L": "MyCity",
      "O": "Kubernetes",
      "OU": "Cluster",
      "ST": "Tokyo"
    }
  ]
}
EOF
```

```
cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes
```

`kubernetes-key.pem`, `kubernetes.pem`

#### サーバーに証明書を配る

```
for host in worker{0,1,2}; do
  scp ca.pem kube-proxy.pem kube-proxy-key.pem \
      root@$(doctl compute droplet list | grep $host | awk '{print $3}'):~/
done
```

```
for host in controller{0,1,2}; do
  scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
      root@$(doctl compute droplet list | grep $host | awk '{print $3}'):~/
done
```

### Setting up Authentication

ローカルに kubectl のインストール

```
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

#### Create the TLS Bootstrap Token

```
BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')
```

Generate a token file:

```
cat > token.csv <<EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF
```

Distribute the bootstrap token file to each controller node:

```
for host in controller{0,1,2}; do
  scp token.csv root@$(doctl compute droplet list | grep $host | awk '{print $3}'):~/
done
```

#### クライアント認証の設定

bootstrap.kubeconfig を作成して kubectl のデフォルトとして設定する

```
KUBERNETES_PUBLIC_ADDRESS=$(doctl compute droplet list | grep controller0 | awk '{print $3}')
```

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
  --kubeconfig=bootstrap.kubeconfig
```

```
kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig
```

```
kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig
```

```
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
```

kube-proxy の設定

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
  --kubeconfig=kube-proxy.kubeconfig
```

```
kubectl config set-credentials kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
```

```
kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig
```

```
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

kubeconfig をコピーする

```
for host in worker{0,1,2}; do
  scp bootstrap.kubeconfig kube-proxy.kubeconfig root@$(doctl compute droplet list | grep $host | awk '{print $3}'):~/
done
```

### Bootstrapping a H/A etcd cluster

controller0,1,2 で etcd クラスタを組む バイナリは userdata でインストール済みなので証明書を `/etc/etcd/` にコピーする

```
cp ~/ca.pem ~/kubernetes-key.pem ~/kubernetes.pem /etc/etcd/
```

systemd の service ファイルは userdata で設置済みなので `/etc/default/kubernetes` 用ファイルを作成する ローカルで次のコマンドを実行してコピペする

```
for i in 0 1 2
do
cat <<EOF
# /etc/default/kubernetes on controller${i}
ETCD_NAME=controller${i}
SERVICE_CLUSTER_IP_RANGE=10.10.0.0/16
POD_IP_RANGE=10.200.0.0/16
INTERNAL_IP=$(doctl compute droplet list --tag-name controller${i} --format PrivateIPv4 --no-header)
CONTROLLER0_IP=$(doctl compute droplet list --tag-name controller0 --format PrivateIPv4 --no-header)
CONTROLLER1_IP=$(doctl compute droplet list --tag-name controller1 --format PrivateIPv4 --no-header)
CONTROLLER2_IP=$(doctl compute droplet list --tag-name controller2 --format PrivateIPv4 --no-header)

EOF
done
```

SERVICE_CLUSTER_IP_RANGE は Service に割り当てられるIPアドレス帯、Pod に割り当てられるアドレス帯は kube-controller-manager で指定する

```
systemctl enable etcd
systemctl start etcd
systemctl status etcd --no-pager
```

確認

```
etcdctl \
  --ca-file=/etc/etcd/ca.pem \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  cluster-health
```

### Bootstrapping a H/A Kubernetes Control Plane

#### Kubernetes API Server

```
mkdir /var/lib/kubernetes
cp ~/{ca,ca-key,kubernetes,kubernetes-key}.pem ~/token.csv /var/lib/kubernetes/

systemctl enable kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver --no-pager
```

#### Kubernetes Controller Manager

```
systemctl enable kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager --no-pager
```

#### Kubernetes Scheduler

```
systemctl enable kube-scheduler
systemctl start kube-scheduler
systemctl status kube-scheduler --no-pager
```

#### 確認

```
root@controller0:~# kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-2               Healthy   {"health": "true"}   
etcd-1               Healthy   {"health": "true"}   
etcd-0               Healthy   {"health": "true"}
```

### Bootstrapping Kubernetes Workers

Each worker node will provision a unique TLS client certificate as defined in the [kubelet TLS bootstrapping guide](https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/). The kubelet-bootstrap user must be granted permission to request a client TLS certificate. worker node はそれぞれ個別の TLS 証明書である必要があるため、`kubelet-bootstrap` ユーザーが CSR を出せるようにする controller0 にて

```
kubectl create clusterrolebinding kubelet-bootstrap \
  --clusterrole=system:node-bootstrapper \
  --user=kubelet-bootstrap
```

`clusterrolebinding "kubelet-bootstrap" created` と出力されます

```
mv ~/bootstrap.kubeconfig /var/lib/kubelet
mv ~/kube-proxy.kubeconfig /var/lib/kube-proxy
mv ~/ca.pem /var/lib/kubernetes/
```

```
cat > /etc/default/kubernetes <<EOF
POD_IP_RANGE=10.200.0.0/16
CLUSTER_DNS_ADDRESS=10.10.0.10
EOF
```

#### docker

```
systemctl enable docker
systemctl start docker
systemctl status docker --no-pager
```

#### kubelet

```
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet --no-pager
```

#### kube-proxy

```
systemctl enable kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy --no-pager
```

#### CSR の承認

controller0 にて行う `kubectl get csr` で CSR の一覧を確認し、`Pending` 状態のものを `kubectl certificate approve xxx` で承認する

```
root@controller0:~# kubectl get csr
NAME                                                   AGE       REQUESTOR           CONDITION
node-csr-QT2S48_TeeDBTJqMBl7EmA_dqjrnvfKGON3-4owa2_w   3m        kubelet-bootstrap   Pending

root@controller0:~# kubectl certificate approve node-csr-QT2S48_TeeDBTJqMBl7EmA_dqjrnvfKGON3-4owa2_w
certificatesigningrequest "node-csr-QT2S48_TeeDBTJqMBl7EmA_dqjrnvfKGON3-4owa2_w" approved

root@controller0:~# kubectl get csr
NAME                                                   AGE       REQUESTOR           CONDITION
node-csr-QT2S48_TeeDBTJqMBl7EmA_dqjrnvfKGON3-4owa2_w   5m        kubelet-bootstrap   Approved,Issued
```

### Configuring the Kubernetes Client - Remote Access

### Managing the Container Network Routes

### Deploying the Cluster DNS Add-on

### Smoke Test

### Cleaning Up
