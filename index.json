[{"content":"","date":"2022年8月12日","permalink":"/","section":"1Q77","summary":"","title":"1Q77"},{"content":"GKE (GKE 限定な話ではないけれども) で Preemptible な node を使用していると Graceful Node Shutdown により停止させられた Pod が Failed 状態でどんどん溜まっていって結構邪魔です。\nできれば消えて欲しい。\nということで削除するための cronjob を deploy するための Helm chart を書いてみた。\nhttps://github.com/yteraoka/terminated-pod-cleaner\nkubectl と jq コマンドを使った shell script で bitnami の image を使わせてもらっている。\nPod の中から何も設定せずに kubectl コマンドが実行できる理由については「Kubernetesクラスター内のPodからkubectlを実行する - Qiita」に丁寧な解説があった。ありがとうございます。\ndescheduler でもできるみたい。\n","date":"2022年8月12日","permalink":"/2022/08/delete-failed-pod-periodically/","section":"Posts","summary":"GKE (GKE 限定な話ではないけれども) で Preemptible な node を使用していると Graceful Node Shutdown により停止させられた Pod が Failed 状態でどんどん溜まっていって結構邪魔です。 できれば消え","title":"Graceful Node Shutdown で Terminated 状態で残る Pod を削除する cronjob"},{"content":"","date":"2022年8月12日","permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes"},{"content":"","date":"2022年8月12日","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"Blowfish has full support for Hugo taxonomies and will adapt to any taxonomy set up. Taxonomy listings like this one also support custom content to be displayed above the list of terms.\nThis area could be used to add some extra decriptive text to each taxonomy. Check out the advanced tag below to see how to take this concept even further.\n ","date":"2022年8月12日","permalink":"/tags/","section":"Tags","summary":"Blowfish has full support for Hugo taxonomies and will adapt to any taxonomy set up. Taxonomy listings like this one also support custom content to be displayed above the list of terms.\nThis area could be used to add some extra decriptive text to each taxonomy. Check out the advanced tag below to see how to take this concept even further.\n ","title":"Tags"},{"content":"Istio を導入した環境で Job (CronJob) を実行すると、sidecar としての istio-proxy コンテナを Job 本来の処理が終わった後に istio-proxy コンテナを終了させないといつまで経っても Pod が終了しないという課題があります。 Istio に限らず、Job から生成される Pod の場合、一つでもコンテナが終了せずに残っていれば発生する問題で、例えば Cloud SQL Proxy を sidecar で実行する場合にも同じ問題が発生します。\nIstio 環境で動かすことを前提にしたコンテナイメージを作ったり、Helm Chart などのマニフェストを用意する場合コンテナイメージに仕込んだり、Helm の template で Init Container を追加して wrapper script を仕込んだりすることは可能ですが、Deployment ではそんなことを気にする必要がないのに Job だけ特別な対応が必要になるのはなんだか許せないと思うこともあるでしょう。\nそんな場合に使えるのが envoy-sidecar-helper です。\nenvoy-sidecar-helper を sidecar として追加してやると、Kubernetes の API サーバーに定期的に Pod の情報を問い合わせ、メインの処理をしているコンテナが終了しているかどうかを監視し、終了していれば envoy を停止させてくれます。 でも、これだって Manifest に追加しないとダメじゃん、というわけではあるのですが、istio-proxy がそうであるように、このコンテナを MutatingWebhook で挿入してやれば良いわけですね。ということで Webhook サーバーを書きました、公開してないですけど\u0026hellip;\nそれでも、全く違いがないかと言われるとそうではなくて Job の ServiceAccount に Pod の参照権限、削除権限が必要になります。\nenvoy-sidecar-helper は Istio 専用に書かれたものではなく、envoy 起動するまで他のコンテナが処理を開始するのを待つための仕組みとかを持っていますが、そのために Volume マウントでファイルを共有する必要があり、起動順については Istio が面倒を見てくれるのでその機能は無効にできるようにしてもらいました。\n","date":"2022年8月12日","permalink":"/2022/08/stop-istio-proxy-using-envoy-sidecar-helper/","section":"Posts","summary":"Istio を導入した環境で Job (CronJob) を実行すると、sidecar としての istio-proxy コンテナを Job 本来の処理が終わった後に istio-proxy コンテナを終了させないといつまで経っても Pod","title":"envoy-sidecar-helper で Job の終了後に istio-proxy を停止させる"},{"content":"","date":"2022年8月12日","permalink":"/tags/istio/","section":"Tags","summary":"","title":"Istio"},{"content":"","date":"2022年8月9日","permalink":"/tags/gke/","section":"Tags","summary":"","title":"GKE"},{"content":"GKE の Ingress で Load Balancer を作成すると、同一 namespace 内の Service にしか振り分けられないとか、単一の Cluster でしか使えないとか不都合な場合があります。その場合 Load Balancer 関連のリソースは Terraform で作成したくなりますが、NEG まで作らなければ BackendService を作成できません。 しかし、NEG は GKE のコントローラが作成するため、鶏卵問題で悲しい思いをしたことはないでしょうか。 この NEG は GKE が作るのを待たずに Terraform で作成することも可能だったのでそのメモです。\n答えを先に書いておくと、次のようになります。\ndata \u0026#34;google_compute_zones\u0026#34; \u0026#34;available\u0026#34; {} resource \u0026#34;google_compute_network_endpoint_group\u0026#34; \u0026#34;igw\u0026#34; { for_each = toset([for zone in data.google_compute_zones.available.names : zone if substr(zone, 0,length(var.region)) == var.region]) name = \u0026#34;${var.env}-igw\u0026#34; description = \u0026#34;{\\\u0026#34;cluster-uid\\\u0026#34;:\\\u0026#34;${data.kubernetes_namespace_v1.kube_system.metadata[0].uid}\\\u0026#34;,\\\u0026#34;namespace\\\u0026#34;:\\\u0026#34;istio-ingress\\\u0026#34;,\\\u0026#34;service-name\\\u0026#34;:\\\u0026#34;istio-ingressgateway\\\u0026#34;,\\\u0026#34;port\\\u0026#34;:\\\u0026#34;80\\\u0026#34;}\u0026#34; network = google_compute_network.vpc.id subnetwork = google_compute_subnetwork.tokyo.id zone = each.key depends_on = [ google_container_cluster.cluster, ] } ポイントは description に謎の JSON をセットしている部分です。 GKE のコントローラが作成する場合にこれがセットされるようになっており、それを真似て作ってやると endpoint として GKE の Pod が登録されるようになります。\n{ \u0026#34;cluster-uid\u0026#34;: \u0026#34;5052b0f9-569a-44cc-a407-72554fb21913\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;istio-ingress\u0026#34;, \u0026#34;service-name\u0026#34;: \u0026#34;istio-ingressgateway\u0026#34;, \u0026#34;port\u0026#34;: \u0026#34;80\u0026#34; } 問題はこの cluster-uid というものの値はどこから取得できるのかということですが、kube-system namespace の metadata にある uid が使われるようです。（Google に確認しました)\nNEG は Zonal リソースなので zone ごとに作成しています。 google_compute_zones という data source から対象のリージョンのものだけ取り出すようにしました。\n","date":"2022年8月9日","permalink":"/2022/08/create-gke-service-neg-using-terraform/","section":"Posts","summary":"GKE の Ingress で Load Balancer を作成すると、同一 namespace 内の Service にしか振り分けられないとか、単一の Cluster でしか使えないとか不都合な場合があります。その場合 Load Balancer 関連のリソー","title":"GKE Service の NEG を Terraform で作成する"},{"content":"","date":"2022年8月9日","permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform"},{"content":"Istio でよくわからない通信の問題が発生した際、Envoy の access log だけでは何が起きているのかわからない場合があります。そんなとき、当該 Pod の LogLevel を debug に変更することで得られる情報が増えることがあります。問題が再現しないとダメですが。\nLogLevel は Pod の sidecar.istio.io/logLevel という annotation で指定可能です。\nmetadata:annotations:\u0026#34;sidecar.istio.io/logLevel\u0026#34;: debug選択肢は次の通り\ntrace, debug, info, warning, error, critical, off\nProbe のアクセスが多くてノイズになる場合は readiness.status.sidecar.istio.io/periodSeconds という annotation で感覚を伸ばすこともできます。\nmetadata:annotations:\u0026#34;sidecar.istio.io/logLevel\u0026#34;: debug\u0026#34;readiness.status.sidecar.istio.io/periodSeconds\u0026#34;: \u0026#34;60\u0026#34;参考情報\n https://access.redhat.com/solutions/6303361  ","date":"2022年6月7日","permalink":"/2022/06/istio-proxy-log-level/","section":"Posts","summary":"Istio でよくわからない通信の問題が発生した際、Envoy の access log だけでは何が起きているのかわからない場合があります。そんなとき、当該 Pod の LogLevel を debug に変","title":"istio-proxy の log level を変更する"},{"content":"Mizu - API Traffic viewer for Kubernetes というものの存在を知ったので試してみます。\nサイトには次のように書いてあります。気になります。\n Mizu offers a real-time view of all HTTP requests, REST and gRPC API calls, as well as Kafka, AMQP (activeMQ / RabbitMQ), and Redis.\n HTTP も gRPC も Redis や Kafka とかも通信内容を parse して見やすく表示してくれるそうな。\n使い方を見ても mizu tap と実行するだけになっていて、マジで？？ という感じなので実際にやってみます。\n例えば、Intel Mac なら次のようにして mizu バイナリをダウンロードして実行するだけ。インストール対象の Kubernetes にアクセスできるように KUBECONFIG などは設定されている前提です。kubectl でアクセスできるようになっていれば大丈夫。\ncurl -Lo mizu https://github.com/up9inc/mizu/releases/latest/download/mizu_darwin_amd64 \\ \u0026amp;\u0026amp; chmod 755 mizu ./mizu tap -A これで localhost:8899 がブラウザで開かれてサイトにあるような画面が表示されます。-A は --all-namespaces なので対象が多くなるため外して実行しました。KUBECONFIG の current-context の　namespace が対象となります。\n通信を覗く対象として micoroservices-demo をデプロイしておきました。\nこんなショッピングサイトのやつです。\n HTTP 通信の例です。header や body が見えています。このキャプチャにはないですが、 response も見えます。\n gRPC 通信の例です。proto ファイルがないので message の表示は field 名が連番っぽいものになっていますが、値は見れます。\n 左に表示されている一覧の上部でフィルタリングができます。protocol (http とか grpc とか redis とか) や送信元や送信先などで、 and や or ( ) なども使えます。\n右上にある Service Map というボタンをクリックすると、次のような表示がされます。Distributed Tracing で見るようなやつですね。\n ただし、frontend から線が出ていないのもおかしいし、redis が登場しないのもおかしい。また、frontend に対して loadgenerator からアクセスが行われているはずなのにそれも表示されない。謎\nそれはそうと、mizu コマンドは mizu tap で何をおこなっているのか、ですが、mizu という名前の namespace を作成し、 ServiceAccount と ClusterRole (設定によっては Role) を作成し、紐づける。それから mizu-api-server という Pod と Service を作成する。その後に mizu-tapper-daemon-set という DaemonSet がデプロイされます。\nこの DaemonSet が　packet をキャプチャして mizu-api-server に送り、手元の mizu コマンドで起動されるサーバーが Kubernetes API Server 経由で mizu-api-server にアクセスをしている。\n$ k get all -n mizu NAME READY STATUS RESTARTS AGE pod/mizu-api-server 2/2 Running 0 25m pod/mizu-tapper-daemon-set-6bvbr 1/1 Running 0 25m pod/mizu-tapper-daemon-set-8h454 1/1 Running 0 25m pod/mizu-tapper-daemon-set-v8vnj 1/1 Running 0 25m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mizu-api-server ClusterIP 172.16.6.149 \u0026lt;none\u0026gt; 80/TCP 25m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/mizu-tapper-daemon-set 3 3 3 3 3 \u0026lt;none\u0026gt; 25m mizu コマンドを Ctrl-C で停止すると、これらのリソースの削除も行ってくれます。\nということで、クラスタに対する必要な権限があり、キャプチャ可能な Pod を deploy 可能な環境であれば mizu コマンドをダウンロードして実行するだけで通信内容が見えるのです。使えたら便利なこともあるかな、程度。tcpdump が不要になったりはしない。\nさて、先ほどの Service Map ですが、何かがおかしいです。 frontend へのアクセスは通信内容も確認できているので大きな緑の丸がありますが、frontend からの通信が見えていません。また、redis へのアクセスも見えていません。\nこれとは別に kubernetes/examples にある guestbook-go で試した時は redis サーバーとの通信内容も見えていました。\nこんな Service Map が表示されて欲しかった。\n Istio で mTLS が有効になっていても問題ないよということなので次回試して見ます。\n","date":"2022年6月4日","permalink":"/2022/06/using-mizu-part-1/","section":"Posts","summary":"Mizu - API Traffic viewer for Kubernetes というものの存在を知ったので試してみます。 サイトには次のように書いてあります。気になります。 Mizu offers a real-time view of all HTTP requests, REST and gRPC API calls, as well as","title":"Mizu で kubernetes 内の通信を覗く (part 1)"},{"content":"","date":"2022年2月26日","permalink":"/tags/envoy/","section":"Tags","summary":"","title":"Envoy"},{"content":"新機能 EXIT_ON_ZERO_ACTIVE_CONNECTIONS # 以前、「Istio 導入への道 – sidecar の調整編」という記事で、Istio の sidecar (istio-proxy) が、アプリの終了を待たずに停止してしまってアプリ側が通信できなくなるという問題に対して preStop hook に netstat などを使った wait 処理を入れるというのを紹介しましたが、あれは listen しているプロセスがいなくなるまで待つというもので、nginx などのように処理中のリクエストは完了を待つが、listen している socket は signal を受けるとすぐに close するというサーバーの場合には有効に働きませんでした。これに対して 2021年11月にリリースされた Istio 1.12 では drain モードに変更した後、アクティブなコネクションがなくなるまで待つという設定ができるようになっていました。(drain モードについては後述)\nIstio 1.12 Change Notes に次のように書かれています。\n Added support for envoy to track active connections during drain and quit if active connections become zero instead of waiting for entire drain duration. This is disabled by default and can be enabled by setting EXIT_ON_ZERO_ACTIVE_CONNECTIONS to true. (Issue #34855)\n  Envoy の drain 中にアクティブなコネクションを追跡し、それがゼロになった場合、drain 期間の終了を待たずに envoy を終了させる機能を追加しました。この機能はデフォルトでは無効になっており、EXIT_ON_ZERO_ACTIVE_CONNECTIONS を true にすることで有効化できます。\n envoy の drain とは？ # ここで、Istio における envoy の drain について説明しておきます。通常 drain と聞くと、もう新規の接続を受け付けなくなるんじゃないかと思ってしまいますが、Istio では Envoy の /drain_listeners?inboundonly\u0026amp;graceful という API を使用しており、graceful とうパラメータがついていることで Envoy は起動オプションの --drain-time-s で指定した期間 (Istio のデフォルトは 45 秒) は新規の接続も受け付ける状態を維持します。では、その間は drain の前と何が違うのかというと、HTTP1 の場合はレスポンスヘッダーに Connection: close を設定し keep-alive させないようにします。HTTP2 の場合も GOAWAY を送ってセッションを終わらせるようです(これは試せていない)。また、Istio では --drain-strategy を immediate と指定しているため、即座に全ての connection が　close に向かいます。\n私もちゃんと調べる前は SIGTERM を受け取ってすぐに drain してしまったら意味ないじゃんって思ってましたが、新規接続も受け付けてくれるので Pod の削除開始から実際に新規リクエストが来なくなるまで待つという用途でも使えるんです。ただし、--drain-time-s の 45 秒と terminationGracePeriodSeconds (default 30 秒) には要注意。\nさらに、drain 開始時には Hot restart が行われ、閉じるべき接続を持った古いプロセスの方は --parent-shutdown-time-s で指定された時間が経過すると終了させられてしまいます。Istio ではこの値 (parentShutdownDuration) がデフォルトで 60 秒になっています。\nEXIT_ON_ZERO_ACTIVE_CONNECTIONS が有効の場合の停止処理 # 具体的には istio agent (pilot-agent) に minDrainDuration と exitOnZeroActiveConnections という項目が追加されています。exitOnZeroActiveConnections が ture であれば drain 開始後に minDrainDuration の期間 (default 5秒) sleep した後に1秒おきにアクティブな connection が残っているかどうかを Envoy の stats endpoint (http://localhost:15000/stats?usedonly\u0026amp;filter=downstream_cx_active$) で確認し、0 になったら終了します。\nEXIT_ON_ZERO_ACTIVE_CONNECTIONS が無効の場合の停止処理 # 無効の場合は drain 開始後に terminationDrainDuration (default 5秒) 待って終了します。 この場合、安全のために terminationDrainDuration を長くすると無駄に待つことになってしまうことがあるためあまり長くしたくないということになります。\n設定してみる # Pod に対して設定するには次のような annotation を設定します。動作確認は Istio 1.12.1 で行いました。\nannotations:proxy.istio.io/config:|proxyMetadata: MINIMUM_DRAIN_DURATION: \u0026#39;5s\u0026#39; EXIT_ON_ZERO_ACTIVE_CONNECTIONS: \u0026#39;true\u0026#39; proxyStatsMatcher: inclusionRegexps: - \u0026#34;.*downstream_cx_active\u0026#34;proxyStatusMatcher は本来はここで設定しなくても EXIT_ON_ZERO_ACTIVE_CONNECTIONS　が有効な場合は istio がやってくれそうなコードになっているのですが、なぜか downstream_cx_active メトリクスが取得できずにずっとループ内で待たされ、 terminationGracePeriodSeconds で SIGKILL を受けるということになっていたのでここで指定することで対処しました。annotation での設定か istio が設定してくれるかに関わらず、downstream_cx_active メトリクスを取得可能にすると `/stats/prometheus` で返すメトリクスにも追加され、環境によっては prometheus のストレージへの影響が無視できないことになりそうなので scrpae 時に除外するなどの設定をした方が良さそうです。\nPod の停止時に次のようなログが出ていれば EXIT_ON_ZERO_ACTIVE_CONNECTIONS は有効になっています。\nAgent draining proxy for 5s, then waiting for active connections to terminate... その後 There are no more active connections. terminating proxy... というログが出ないまま終了されていたらそれは metrics が取得できずにずっと待たされているか、本当にコネクションがクローズされなくて terminationGracePeriodSeconds を迎えて SIGKILL で終了させられている可能性があります。\nまとめ # ということで istio の sidecar が inject された Pod の終了について整理してみる。\n istio-proxy (pilot-agent) は SIGTERM を受けるとすぐに Envoy の draining を開始させる  graceful な drain を指定しているので新規の接続も受け入れる 新規接続を受け入れる期間は Envoy の起動オプション --drain-time-s で指定されており (Istio では ProxyConfig.drainDuration で指定可能でデフォルトは 45 秒)、以降の待ち時間の間でもこれを超えると新規の接続は受け入れられなくなる drain 中の HTTP1 のレスポンスには Connection: close ヘッダーを設定したり、HTTP2 の GOAWAY で close したりしてくれる   EXIT_ON_ZERO_ACTIVE_CONNECTIONS が有効でない場合  terminationDrainDuration で指定した期間 (デフォルトは 5 秒) 待って終了   EXIT_ON_ZERO_ACTIVE_CONNECTIONS が有効な場合  minDrainDuration の期間 (デフォルトは 5 秒) 待つ 1秒おきにアクティブな接続の数を確認して、0 になったら終了 0 にならなければそのうち terminationGracePeriodSeconds で SIGKILL を受ける    ProxyConfig.parentShutdownDuration の値が envoy の --parent-shutdown-time-s で指定されており、drain されるコネクションを持っているプロセスは drain 開始からこの時間が経過すると終了させられる。デフォルトは 60 秒。\n多くの時間設定があるので要注意 (DefaultProxyConfig)\n terminationGracePeriodSeconds (default 30s)\nPod の delete 開始から各コンテナに SIGKILL が送られるまでの時間 terminationDrainDuration (default 5s)\nEXIT_ON_ZERO_ACTIVE_CONNECTIONS が無効の場合に drain 状態で待つ時間 minDrainDuration (default 5s)\nEXIT_ON_ZERO_ACTIVE_CONNECTIONS が有効の場合に少なくともこの期間は待つ時間 drainDuration (default 45s)\ndrain 開始から新規接続を受け入れなくなるまでの時間 parentShutdownDuration (default 60s)\ndrain 開始から drain 対象の envoy プロセスを終了させるまでの時間 (Hot restart)  Drain 中も新規接続は受け付けるということが確認できたので、EXIT_ON_ZERO_ACTIVE_CONNECTIONS を有効にせずとも terminationDrainDuration を長めにしておけば Kubernetes でおなじみの preStop の sleep に対応できそうです。\nannotations:proxy.istio.io/config:|terminationDrainDuration:10s思いがけず Envoy の draining についての理解が深まって良かったです。Connection: close ヘッダーの設定までやってくれてただなんて。\nproxyStatsMatcher 設定を追加しなければならな方のは bug だったみたいで、報告したら早速修正の Pull Request を作ってもらえました。\nfix passing exit on zero active connections to metadata #37573\n","date":"2022年2月26日","permalink":"/2022/02/istio-exit-on-zero-active-connections/","section":"Posts","summary":"新機能 EXIT_ON_ZERO_ACTIVE_CONNECTIONS # 以前、「Istio 導入への道 – sidecar の調整編」という記事で、Istio の sidecar (istio-proxy) が、アプリの終了を待たずに停止してしまってアプリ側が通信で","title":"istio sidecar の停止を connection がなくなるまで遅らせる"},{"content":"","date":"2022年1月8日","permalink":"/tags/telepresence/","section":"Tags","summary":"","title":"telepresence"},{"content":"前回の telepresence 入門 (1) の続きです。今回は Kubernetes クラスタの Service へのアクセスをインターセプトして手元の環境に転送することを試します。Kubernetes 側の volume も手元で mount させるし、環境変数も引っ張ってきます。 バージョンなど環境については前回と同じ。\nインターセプト設定 # Kubernetes の Service 宛の通信を手元に転送することを　intercept と呼ぶようです。\nインターセプト前の状態 # 前回使った hello という Service, Deployment を使います。\n$ kubectl create deploy hello --image=k8s.gcr.io/echoserver:1.4 $ kubectl expose deploy hello --port 80 --target-port 8080 Service は port 80 をコンテナの port 8080 に転送するようになっています。\n$ kubectl get svc hello -o jsonpath={.spec.ports} | jq [ { \u0026quot;port\u0026quot;: 80, \u0026quot;protocol\u0026quot;: \u0026quot;TCP\u0026quot;, \u0026quot;targetPort\u0026quot;: 8080 } ] telepresence intercept コマンド実行前の状態です。(traffic-agent not yet installed) ということで agent がまだインストールされていません。\n$ telepresence list hello: ready to intercept (traffic-agent not yet installed) インターセプト # telepresence intercept コマンドを実行します。--port はローカルの転送先 port の指定です。デフォルトが 8080 です。\n$ telepresence intercept hello --port 8080 Using Deployment hello intercepted Intercept name : hello State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:8080 Volume Mount Point: /var/folders/nd/8mk6834s31g8dymd1_9pnqq00000gn/T/telfs-3519194956 Intercepting : all TCP connections これで、手元で port 8080 で Listen するプロセスを実行すれば Kubernetes 内で hello サービスにアクセスするとそこに流れてきます。例えば python で HTTP サーバーを起動させて、Kubernetes の別 Pod から curl -s http://hello.default/ を実行してみたら次のように 127.0.0.1 からのアクセスが行われました。\n$ python3 -m http.server 8080 Serving HTTP on :: port 8080 (http://[::]:8080/) ... ::ffff:127.0.0.1 - - [08/Jan/2022 11:01:18] \u0026quot;GET / HTTP/1.1\u0026quot; 200 - 手元のプロセスは docker などで実行しても問題ありません。\n$ docker run -it --rm -p 8080:80 nginx:latest /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh /docker-entrypoint.sh: Configuration complete; ready for start up 2022/01/08 02:05:31 [notice] 1#1: using the \u0026quot;epoll\u0026quot; event method 2022/01/08 02:05:31 [notice] 1#1: nginx/1.21.5 2022/01/08 02:05:31 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 2022/01/08 02:05:31 [notice] 1#1: OS: Linux 5.13.0-23-generic 2022/01/08 02:05:31 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576 2022/01/08 02:05:31 [notice] 1#1: start worker processes 2022/01/08 02:05:31 [notice] 1#1: start worker process 31 2022/01/08 02:05:31 [notice] 1#1: start worker process 32 2022/01/08 02:05:31 [notice] 1#1: start worker process 33 2022/01/08 02:05:31 [notice] 1#1: start worker process 34 172.17.0.1 - - [08/Jan/2022:02:05:36 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 615 \u0026quot;-\u0026quot; \u0026quot;curl/7.64.0\u0026quot; \u0026quot;-\u0026quot; telepresence intercept コマンドの --docker-run オプションを使うと intercept の有効化と同時に docker run も実行してくれます。\n図解 # こんな感じっぽいです。telepresence intercept を実行すると Service に対応する Deployment などに対して traffic agent コンテナが追加されます(このため　Pod は再作成されます)。また、Service の targetPort が書き換えられます。\n 追加されるコンテナ manifest の例\nspec:template:spec:containers:- args:- agentenv:- name:TELEPRESENCE_CONTAINERvalue:echoserver- name:_TEL_AGENT_LOG_LEVELvalue:info- name:_TEL_AGENT_NAMEvalue:hello- name:_TEL_AGENT_NAMESPACEvalueFrom:fieldRef:apiVersion:v1fieldPath:metadata.namespace- name:_TEL_AGENT_POD_IPvalueFrom:fieldRef:apiVersion:v1fieldPath:status.podIP- name:_TEL_AGENT_APP_PORTvalue:\u0026#34;8080\u0026#34;- name:_TEL_AGENT_PORTvalue:\u0026#34;9900\u0026#34;- name:_TEL_AGENT_MANAGER_HOSTvalue:traffic-manager.ambassadorimage:docker.io/datawire/tel2:2.4.9imagePullPolicy:IfNotPresentname:traffic-agentports:- containerPort:9900name:tx-8080protocol:TCPreadinessProbe:exec:command:- /bin/stat- /tmp/agent/readyfailureThreshold:3periodSeconds:10successThreshold:1timeoutSeconds:1resources:{}terminationMessagePath:/dev/termination-logterminationMessagePolicy:FilevolumeMounts:- mountPath:/tel_pod_infoname:traffic-annotationsインターセプト前の Service\n$ kubectl get svc hello -o jsonpath={.spec.ports} | jq [ { \u0026quot;port\u0026quot;: 80, \u0026quot;protocol\u0026quot;: \u0026quot;TCP\u0026quot;, \u0026quot;targetPort\u0026quot;: 8080 } ] インターセプト後の Service では targetPort が tx-8080 に書き換えられており、これは追加された traffic-agent コンテナの 9900 port を指している。\n$ kubectl get svc hello -o jsonpath={.spec.ports} | jq [ { \u0026quot;port\u0026quot;: 80, \u0026quot;protocol\u0026quot;: \u0026quot;TCP\u0026quot;, \u0026quot;targetPort\u0026quot;: \u0026quot;tx-8080\u0026quot; } ] インターセプトの中断 # telepresence leave コマンドでインターセプトを中断できます。中断では Deployment や Service の書き換えはそのままで、agent の振る舞いが変更されるようです。traffic-agent は元のコンテナの port に転送することになるようです。\n$ telepresence list hello: ready to intercept (traffic-agent already installed) traffic-agent の削除 # telepresence uninstall で書き換えた変更を戻します。また Deployment などが書き換えられるので Pod は再作成されます。\n$ telepresence uninstall --agent hello (traffic-agent not yet installed) に戻りました。\n$ telepresence list hello: ready to intercept (traffic-agent not yet installed) 全ての agent を削除する場合は　telepresence uninstall --all-agents が使えし、traffic-managet ごと削除する --everything というオプションもあります。\nsshfs のインストール # telepresence では転送元 Pod がマウントしている volume に手元からもアクセスできるようにすることができますが、(mac では) これに sshfs が使われるため、手元の環境にインストールしておく必要があります。が、 brew install sshfs すると closed-source な macFUSE が必要だから無効になってるよとインストールできません。\nError: sshfs has been disabled because it requires closed-source macFUSE! よって、まず、github.com/osxfuse/osxfuse/releases にある macFUSE をインストールし、 その後に github.com/gromgit/homebrew-fuse にある Homebrew Formula を使ってインストールします。 このリポジトリは mscFUSE が closed-source になってしまったことで Homebrew の core から削られてしまったものをインストールするために作られたもののようです。\n$ brew install gromgit/fuse/sshfs-mac 環境変数や volume のマウント # telepresence を使うと intercept 対象の Pod に設定されている環境変数を docker 用や json として取得し、使うことができますし、Pod がマウントしている volume を手元でマウントすることが可能で、それをさらに docker コンテナにマウントすることもできます。\n普通に telepresence intercept コマンドを実行するだけでも転送先に docker コンテナを使うことは可能ですが、--docker-run というオプションがあり、これを使うことで telepresence intercept コマンド実行で一緒に docker run までしてくれるようになります。\nテスト用に Wordpress をデプロイする # wordpress と mysql との通信もあるし、volume マウントもあって動作確認に便利そうということで bitnami の helm chart を使って wordpress をデプロイします。この chart は wordpress が NFS などの ReadWriteMany な volume を期待しているのですが、用意するのが面倒なので ReadWriteOne でも使えるように指定しています。(helm3 では --set で null を指定することで default の　key を消すということができないので -f で指定。外部に公開する必要もないので service の type は LoadBalancer から ClusterIP に変更。updateStrategy は Recreate にしないと volume が手放せないので intercept 時の Pod の再作成で詰まる)\n$ helm repo add bitnami https://charts.bitnami.com/bitnami $ helm repo update $ kubectl create namespace wordpress $ kubens wordpress $ helm install wordpress bitnami/wordpress -f - \u0026lt;\u0026lt;EOF updateStrategy: type: Recreate rollingUpdate: null service: type: ClusterIP EOF こんな状態になります。\n$ kubectl get svc,pod,pv NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/wordpress ClusterIP 172.16.1.117 80/TCP,443/TCP 5m52s service/wordpress-mariadb ClusterIP 172.16.7.205 3306/TCP 5m52s NAME READY STATUS RESTARTS AGE pod/wordpress-7c688c7f48-r8mrf 1/1 Running 0 5m45s pod/wordpress-mariadb-0 1/1 Running 0 5m45s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-07259eb1-aa1c-4521-9029-e57f12b2eedc 8Gi RWO Delete Bound wordpress/data-wordpress-mariadb-0 standard 5m49s persistentvolume/pvc-4f85f20c-e376-4489-91df-b126cf029967 10Gi RWO Delete Bound wordpress/wordpress standard 5m50s wordpress で使われているコンテナイメージを確認\n$ kubectl get deploy wordpress -o 'jsonpath={.spec.template.spec.containers[0].image}' docker.io/bitnami/wordpress:5.8.2-debian-10-r47 volume のマウント位置を確認\n$ **kubectl get deploy wordpress -o 'jsonpath={.spec.template.spec.containers[0].volumeMounts}' | jq** [ { \u0026quot;mountPath\u0026quot;: \u0026quot;/bitnami/wordpress\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;wordpress-data\u0026quot;, \u0026quot;subPath\u0026quot;: \u0026quot;wordpress\u0026quot; } ] 環境変数のファイルへの書き出しと volume のマウント # ここで、次のようにすることで環境変数を wordpress.env というファイルに、Pod がマウントしている volume を wordpress.vol ディレクトリ配下にマウントすることができます。 (wordpress Service は http の 8080 と https の 8443 の2つの port があるので --port 8080:http と明示しています)\n$ telepresence intercept wordpress --port 8080:http --env-file ./wordpress.env --mount ./wordpress.vol Using Deployment wordpress intercepted Intercept name : wordpress State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:8080 Service Port Identifier: http Volume Mount Point : ./wordpress.vol Intercepting : all TCP connections wordpress.env の中身\n$ cat wordpress.env ALLOW_EMPTY_PASSWORD=yes APACHE_HTTPS_PORT_NUMBER=8443 APACHE_HTTP_PORT_NUMBER=8080 BITNAMI_DEBUG=false KO_DATA_PATH=/var/run/ko KUBERNETES_PORT=tcp://172.16.0.1:443 KUBERNETES_PORT_443_TCP=tcp://172.16.0.1:443 KUBERNETES_PORT_443_TCP_ADDR=172.16.0.1 KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_SERVICE_HOST=172.16.0.1 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_PORT_HTTPS=443 MARIADB_HOST=wordpress-mariadb MARIADB_PORT_NUMBER=3306 TELEPRESENCE_CONTAINER=wordpress TELEPRESENCE_INTERCEPT_ID=bd126583-7aad-4e8b-b00c-09890e027630:wordpress TELEPRESENCE_MOUNTS=/bitnami/wordpress:/var/run/secrets/kubernetes.io TELEPRESENCE_ROOT=./wordpress.vol WORDPRESS_AUTO_UPDATE_LEVEL=none WORDPRESS_BLOG_NAME=User's Blog! WORDPRESS_DATABASE_NAME=bitnami_wordpress WORDPRESS_DATABASE_PASSWORD=kwSIdkeXEC WORDPRESS_DATABASE_USER=bn_wordpress WORDPRESS_EMAIL=user@example.com WORDPRESS_ENABLE_HTACCESS_PERSISTENCE=no WORDPRESS_EXTRA_WP_CONFIG_CONTENT= WORDPRESS_FIRST_NAME=FirstName WORDPRESS_HTACCESS_OVERRIDE_NONE=no WORDPRESS_LAST_NAME=LastName WORDPRESS_MARIADB_PORT=tcp://172.16.7.205:3306 WORDPRESS_MARIADB_PORT_3306_TCP=tcp://172.16.7.205:3306 WORDPRESS_MARIADB_PORT_3306_TCP_ADDR=172.16.7.205 WORDPRESS_MARIADB_PORT_3306_TCP_PORT=3306 WORDPRESS_MARIADB_PORT_3306_TCP_PROTO=tcp WORDPRESS_MARIADB_SERVICE_HOST=172.16.7.205 WORDPRESS_MARIADB_SERVICE_PORT=3306 WORDPRESS_MARIADB_SERVICE_PORT_MYSQL=3306 WORDPRESS_PASSWORD=CZS97FQnB3 WORDPRESS_PLUGINS=none WORDPRESS_PORT=tcp://172.16.1.117:80 WORDPRESS_PORT_443_TCP=tcp://172.16.1.117:443 WORDPRESS_PORT_443_TCP_ADDR=172.16.1.117 WORDPRESS_PORT_443_TCP_PORT=443 WORDPRESS_PORT_443_TCP_PROTO=tcp WORDPRESS_PORT_80_TCP=tcp://172.16.1.117:80 WORDPRESS_PORT_80_TCP_ADDR=172.16.1.117 WORDPRESS_PORT_80_TCP_PORT=80 WORDPRESS_PORT_80_TCP_PROTO=tcp WORDPRESS_SCHEME=http WORDPRESS_SERVICE_HOST=172.16.1.117 WORDPRESS_SERVICE_PORT=80 WORDPRESS_SERVICE_PORT_HTTP=80 WORDPRESS_SERVICE_PORT_HTTPS=443 WORDPRESS_SKIP_BOOTSTRAP=no WORDPRESS_TABLE_PREFIX=wp_ WORDPRESS_USERNAME=user volume は指定したディレクトリ配下に Pod 内でマウントされていた path でマウントされます。/bitnami/wordpress と /var/run/secrets/kubernetes.io/serviceaccount\n$ find wordpress.vol -maxdepth 3 -type d wordpress.vol wordpress.vol/bitnami wordpress.vol/bitnami/wordpress wordpress.vol/bitnami/wordpress/wp-content wordpress.vol/var wordpress.vol/var/run wordpress.vol/var/run/secrets 元のコンテナはこうなってて\nvolumeMounts:- mountPath:/bitnami/wordpressname:wordpress-datasubPath:wordpress- mountPath:/var/run/secrets/kubernetes.io/serviceaccountname:kube-api-access-cdcc8readOnly:truetraffic-agent コンテナではこうなっています\nvolumeMounts:- mountPath:/tel_app_mounts/bitnami/wordpressname:wordpress-datasubPath:wordpress- mountPath:/tel_pod_infoname:traffic-annotations- mountPath:/var/run/secrets/kubernetes.io/serviceaccountname:kube-api-access-cdcc8readOnly:truetraffic-agent が sftp プロトコルをサポートしてて、sshfs でマウントできるようになっています。\n$ mount | grep wordpress localhost:/tel_app_mounts on /Users/teraoka/wordpress.vol (macfuse, nodev, nosuid, synchronous, mounted by teraoka) 取得した環境変数と　volume を使ってコンテナを起動 # 環境変数と volume が準備できたのでこれを使って手元で docker コンテナを実行します。\n$ docker run --rm -it -p 8080:8080 \\ -v $(pwd)/wordpress.vol/bitnami/wordpress:/bitnami/wordpress \\ --env-file ./wordpress.env \\ docker.io/bitnami/wordpress:5.8.2-debian-10-r47 $ docker run --rm -it -p 8080:8080 \\ -v $(pwd)/wordpress.vol/bitnami/wordpress:/bitnami/wordpress \\ --env-file ./wordpress.env \\ docker.io/bitnami/wordpress:5.8.2-debian-10-r47 wordpress 05:00:16.43 wordpress 05:00:16.44 Welcome to the Bitnami wordpress container wordpress 05:00:16.44 Subscribe to project updates by watching https://github.com/bitnami/bitnami-docker-wordpress wordpress 05:00:16.44 Submit issues and feature requests at https://github.com/bitnami/bitnami-docker-wordpress/issues wordpress 05:00:16.44 wordpress 05:00:16.44 INFO ==\u0026gt; ** Starting WordPress setup ** realpath: /bitnami/apache/conf: No such file or directory wordpress 05:00:16.48 INFO ==\u0026gt; Configuring the HTTP port wordpress 05:00:16.49 INFO ==\u0026gt; Configuring the HTTPS port wordpress 05:00:16.51 INFO ==\u0026gt; Configuring PHP options wordpress 05:00:16.52 INFO ==\u0026gt; Validating settings in MYSQL_CLIENT_* env vars wordpress 05:00:16.57 WARN ==\u0026gt; Hostname wordpress-mariadb could not be resolved, this could lead to connection issues wordpress 05:00:16.58 WARN ==\u0026gt; You set the environment variable ALLOW_EMPTY_PASSWORD=yes. For safety reasons, do not use this flag in a production environment. wordpress 05:00:16.77 INFO ==\u0026gt; Restoring persisted WordPress installation wordpress 05:00:17.63 INFO ==\u0026gt; Trying to connect to the database server wordpress 05:03:00.25 ERROR ==\u0026gt; Could not connect to the database 残念ながら Could not connect to the database というエラーで起動しませんでした\u0026hellip; 原因は DB サーバーのホスト名が wordpress-mariadb と指定されていることで、これは同じ namespace にいる Pod からであればアクセス可能なのですが、telepresence 経由では少なくとも namespace が付いていないと名前解決ができません。 ということなので、当該箇所を書き換えてみます。(mac の sed の -i は GNU sed の -i とは違うので gsed と指定)\n$ grep DB_HOST wordpress.vol/bitnami/wordpress/wp-config.php define( 'DB_HOST', 'wordpress-mariadb:3306' ); $ gsed -i 's/wordpress-mariadb/wordpress-mariadb.wordpress/' ./wordpress.vol/bitnami/wordpress/wp-config.php $ grep DB_HOST wordpress.vol/bitnami/wordpress/wp-config.php define( 'DB_HOST', 'wordpress-mariadb.wordpress:3306' ); これで再度コンテナを起動すれば無事起動しました。(環境変数の MARIADB_HOST も namespace 無しで設定されているので、volume をマウントしないで試す場合はこちらを書き換える必要があります)\n現在の状態は手元のコンテナで wordpress を実行しているが、その wp-content 配下は Kubernetes 内の volume に置かれているし、DB サーバーも Kubernetes 上で実行されている mariadb が使われていることになる。\n手元でブラウザから localhost:8080 で Wordpress にアクセスすることも出来るし、Kubernetes 内から Service 経由でアクセスすることもできる。\n使いたい場面に遭遇したら使えそうです。\n","date":"2022年1月8日","permalink":"/2022/01/telepresence-part-2/","section":"Posts","summary":"前回の telepresence 入門 (1) の続きです。今回は Kubernetes クラスタの Service へのアクセスをインターセプトして手元の環境に転送することを試します。Kubernetes 側の volume","title":"telepresence 入門 (2)"},{"content":"","date":"2022年1月4日","permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker"},{"content":"以前、「Lima で nerdctl」という記事を書きました。その後、lima の VM 上で docker daemon を実行し、ホスト側から docker コマンドでアクセスするという方法があることを知りました。たまたま、brew upgrade を実行していたところ lima が 0.8.0 に更新されたのを見て Github の releases ページを見、試してみようかなと思ったのでメモです。 ちなみに、前回試した時のバージョンは 0.6.4 でした。\nDocker 入りの VM を起動させる # 私は brew のインストール先を $HOME にしていますが、brew で lima をインストールすると [~/.homebrew/Cellar/lima/0.8.0/share/doc/lima/examples/docker.yaml](https://github.com/lima-vm/lima/blob/v0.8.0/examples/docker.yaml) に docker 入りの VM を作成するための設定ファイルも一緒にインストールされています。これを使うことで簡単に VM が作成できます。次のように limactl start に続けてファイルの path を指定するだけです。\n$ limactl start ~/.homebrew/Cellar/lima/0.8.0/share/doc/lima/examples/docker.yaml ファイルは URL でも良いみたいなので次のようにすることもできるようです。\n$ limactl start https://raw.githubusercontent.com/lima-vm/lima/v0.8.0/examples/docker.yaml limactl start を実行すると、指定したファイルの内容そのままでインスタンスを作成するか、編集するかを尋ねられます。その場で編集して起動させられるのは便利ですね。\n? Creating an instance \u0026quot;docker\u0026quot; [Use arrows to move, type to filter] \u0026gt; Proceed with the default configuration Open an editor to override the configuration Exit デフォルトでは $HOME ディレクトリが Read-Only でマウントされるので　Writable　に変更しておくと便利です。(default.yaml のコメントでは writable は false にしておけって書かれてますけど)\nmounts:- location:\u0026#34;~\u0026#34;# CAUTION: `writable` SHOULD be false for the home directory.# Setting `writable` to true is possible, but untested and dangerous.writable:trueCPU, Memory, Disk のデフォルトは次のようになっており、変更はお好みで。\n# CPUs: if you see performance issues, try limiting cpus to 1.# Default: 4cpus:4# Memory size# Default: \u0026#34;4GiB\u0026#34;memory:\u0026#34;4GiB\u0026#34;# Disk size# Default: \u0026#34;100GiB\u0026#34;disk:\u0026#34;100GiB\u0026#34;docker コマンドでアクセス # 起動すると、DOCKER_HOST 環境変数の設定方法が表示されるのでそれをコピペすれば docker コマンドで lima の VM 上の docker daemon を操作できるようになります。この際、Docker Desktop 付属の docker コマンドでは何かで待たされてちょっとイラッとするので brew install docker で別途 docker コマンドをインストールしてそちらを使うのが良いかと思います。(何に引っ掛かってるのか調べたかったけど dtruss もなぜかうまく機能しないので諦め)\nTo run `docker` on the host (assumes docker-cli is installed): $ export DOCKER_HOST=unix://{{.Dir}}/sock/docker.sock $ docker ... lima コマンドの引数で指定したコマンドを　VM 内で実行してくれる便利機能は default　以外の VM で使うためには LIMA_INSTANCE という環境変数を設定する必要があります。\n$ lima hostname lima-default $ LIMA_INSTANCE=docker lima hostname lima-docker VM 内で shell を実行してしまえば良いなら limactl shell docker とすれば環境変数を使わずにすみます。\nその他の変更 # 沢山あると思いますが、Intel VM を M1 mac で、Arm VM を Intel mac で実行可能になっています。\nexamples ディレクトリには docker だけじゃなくていろんな VM 用のテンプレが用意されています。\nport-forward が 1024 未満のポートにも対応しています。(podman の件と混同してしまって、以前どういう状態だったか覚えていない)\n以前は複数の VM を同時に起動させられなかったという記憶があるのですが、同時に起動できるようになってました。\n$ limactl list NAME STATUS SSH ARCH CPUS MEMORY DISK DIR default Running 127.0.0.1:60022 x86_64 4 4GiB 100GiB /Users/teraoka/.lima/default docker Running 127.0.0.1:60006 x86_64 4 4GiB 100GiB /Users/teraoka/.lima/docker docker compose も問題なく使えました。\ndocker.yaml を default.yaml にして limactl start default.yaml とすれば default VM が docker 用になります。\n$ curl -Lo default.yaml https://raw.githubusercontent.com/lima-vm/lima/v0.8.0/examples/docker.yaml $ limactl start default.yaml ","date":"2022年1月4日","permalink":"/2022/01/docker-on-lima/","section":"Posts","summary":"以前、「Lima で nerdctl」という記事を書きました。その後、lima の VM 上で docker daemon を実行し、ホスト側から docker コマンドでアクセスするという方","title":"Docker on Lima"},{"content":"telepresence というツールがあります。手元の端末が Kubernetes クラスタ内にいるかのような通信を可能にし、Kubernetes の Pod の Container への通信をインターセプトして手元の端末に流すことができます。これの仕組みを調べてみます。(以前は Python で書かれていたようですが、v2 は Go で書き直されたみたいです)\n確認に使用した telepresence と mac の version\n$ telepresence version Client: v2.4.9 (api v3) $ sw_vers ProductName:\tmacOS ProductVersion:\t12.1 BuildVersion:\t21C52 Kubernetes クラスタは GKE の v1.21.5-gke.1302\nクラスタへの terffic-manager のデプロイ # helm を使ってインストールすることができます。デフォルトではクラスタワイドな設定になりますが、特定の namespace にのみインストールしたり、namespace 毎に権限を分けてインストールすることも可能です。\n$ helm repo add datawire https://app.getambassador.io $ helm repo update Namespace の作成。デフォルトでは ambassador という namespace を使うことになっているようです。\n$ kubectl create namespace ambassador $ helm install traffic-manager --namespace ambassador datawire/telepresence 次のようなリソースが作成されます。\n   Kind Namespace Name     ServiceAccount ambassador traffic-manager   Secret ambassador mutator-webhook-tls   ClusterRole - traffic-manager-ambassador   ClusterRoleBinding - traffic-manager-ambassador   Role ambassador traffic-manager   RoleBinding ambassador traffic-manager   Service ambassador traffic-manager   Service ambassador agent-injector   Deployment ambassador traffic-manager   MutatingWebhookConfiguration - agent-injector-webhook-ambassador    テスト用 Service / Deployment のデプロイ # default namespace に hello という Deployment をデプロイし、 expose コマンドで Service を作成します。\n$ kubectl create deploy hello --image=k8s.gcr.io/echoserver:1.4 $ kubectl expose deploy hello --port 80 --target-port 8080 $ kubectl get ns,svc,deploy,po NAME STATUS AGE namespace/ambassador Active 104m namespace/default Active 159m namespace/kube-node-lease Active 159m namespace/kube-public Active 159m namespace/kube-system Active 159m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/hello ClusterIP 172.16.1.153 80/TCP 11s service/kubernetes ClusterIP 172.16.0.1 443/TCP 159m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello 1/1 1 1 20s NAME READY STATUS RESTARTS AGE pod/hello-79dbd5fdfb-t59nr 1/1 Running 0 13s telepresence connect # 手元の端末で telepresence connect コマンドを実行することで手元の端末と Kubernetes クラスタ間の tunnel を掘る。root 権限で実行する必要のあるものがあるため、sudo のパスワード入力を求められます。\n$ telepresence connect Launching Telepresence Root Daemon Need root privileges to run: /Users/teraoka/bin/telepresence daemon-foreground /Users/teraoka/Library/Logs/telepresence '/Users/teraoka/Library/Application Support/telepresence' '' Password: Launching Telepresence User Daemon Connected to context gke_MY_PROJECT_ID_asia-northeast1-a_CLUSTER_NAME (https://203.0.113.123) telepresence version コマンドで root で実行する daemon をユーザー権限で実行する daemon の 2 つが実行されているのがわかります。\n$ telepresence version Client: v2.4.9 (api v3) Root Daemon: v2.4.9 (api v3) User Daemon: v2.4.9 (api v3) 手元の curl からクラスタ内へのアクセス # たったこれだけで、あら不思議 curl hello.default とするとクラスタ内の Service に対してアクセスできています。\n$ curl -sv hello.default * Trying 172.16.1.153:80... * Connected to hello.default (172.16.1.153) port 80 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: hello.default \u0026gt; User-Agent: curl/7.79.1 \u0026gt; Accept: */* \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.10.0 \u0026lt; Date: Fri, 31 Dec 2021 08:27:43 GMT \u0026lt; Content-Type: text/plain \u0026lt; Transfer-Encoding: chunked \u0026lt; Connection: keep-alive \u0026lt; CLIENT VALUES: client_address=172.17.0.131 command=GET real path=/ query=nil request_version=1.1 request_uri=http://hello.default:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=hello.default user-agent=curl/7.79.1 BODY: * Connection #0 to host hello.default left intact -no body in request- hello.default が 172.16.1.153 と名前解決されているのがどうしてなのか気になります。答えは下にありますが、これは mac の場合です、Linux や Windows ではまた別の仕組みが使われているのだろうと思われます。(他の環境も含めて DNS resolution に書かれていました)\nサーバー側から見た client_address は 172.17.0.131 となっています、これは ambassador namespace にデプロイされている traffic-manager Deployment の Pod が持つ IP アドレスです。この Pod 経由でアクセスしていることがわかります。\n$ kubectl get pod -n ambassador -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES traffic-manager-5cb99c9fd6-mv6v9 1/1 Running 0 124m 172.17.0.131 gke-teraoka-blue-teraoka-blue-pool1-48fb0873-m4cw \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; telepresence プロセスの役割 # telepresence のぞれぞれのプロセスが何をやっているのかを確認してみます。\nとりあえず ps コマンドで確認。\n$ ps auxww | grep telepresence teraoka 72020 0.0 0.0 34132084 872 s003 R+ 5:18PM 0:00.00 grep telepresence teraoka 71885 0.0 0.3 34956040 46184 s000 S 5:14PM 0:00.65 /Users/teraoka/bin/telepresence connector-foreground root 71881 0.0 0.2 34966080 35296 s000 S 5:14PM 0:00.27 /Users/teraoka/bin/telepresence daemon-foreground /Users/teraoka/Library/Logs/telepresence /Users/teraoka/Library/Application Support/telepresence root 71880 0.0 0.0 34142464 4596 s000 S 5:14PM 0:00.01 sudo --non-interactive --preserve-env /Users/teraoka/bin/telepresence daemon-foreground /Users/teraoka/Library/Logs/telepresence /Users/teraoka/Library/Application Support/telepresence $ pstree -w 71880 -+= 71880 root sudo --non-interactive --preserve-env /Users/teraoka/bin/telepresence daemon-foreground /Users/teraoka/Library/Logs/telepresence /Users/teraoka/Library/Application Support/telepresence \\--- 71881 root /Users/teraoka/bin/telepresence daemon-foreground /Users/teraoka/Library/Logs/telepresence /Users/teraoka/Library/Application Support/telepresence lsof で確認 # root daemon\n$ sudo lsof -nPp 71881 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME teleprese 71881 root cwd DIR 1,9 512 47512210 /Users/teraoka/ghq/github.com/yteraoka/terraform-examples/gcp/gke teleprese 71881 root txt REG 1,9 77231136 74147315 /Users/teraoka/bin/telepresence teleprese 71881 root txt REG 1,9 46096 74099023 /Library/Preferences/Logging/.plist-cache.1VLWA6Q0 teleprese 71881 root txt REG 1,9 2160672 1152921500312811906 /usr/lib/dyld teleprese 71881 root txt REG 1,9 32768 74099068 /private/var/db/mds/messages/se_SecurityMessages teleprese 71881 root txt REG 1,9 114087 74099365 /private/var/db/analyticsd/events.whitelist teleprese 71881 root txt REG 1,9 29638976 1152921500312823656 /usr/share/icu/icudt68l.dat teleprese 71881 root 0r CHR 3,2 0t0 317 /dev/null teleprese 71881 root 1w REG 1,9 2099 74169276 /Users/teraoka/Library/Logs/telepresence/daemon.log teleprese 71881 root 2w REG 1,9 2099 74169276 /Users/teraoka/Library/Logs/telepresence/daemon.log teleprese 71881 root 3u KQUEUE count=0, state=0xa teleprese 71881 root 4 PIPE 0xd904b24160bf46ff 16384 -\u0026gt;0x1ac3420f1be0c501 teleprese 71881 root 5 PIPE 0x1ac3420f1be0c501 16384 -\u0026gt;0xd904b24160bf46ff teleprese 71881 root 6w REG 1,9 2099 74169276 /Users/teraoka/Library/Logs/telepresence/daemon.log teleprese 71881 root 7u systm 0x4715bfb02f5e8bd7 0t0 [ctl com.apple.net.utun_control id 4 unit 4] teleprese 71881 root 8 PIPE 0x77446814497e3b77 16384 -\u0026gt;0x67dbd1c5970420d8 teleprese 71881 root 9u unix 0x4715bfb02f991c3f 0t0 /var/run/telepresence-daemon.socket teleprese 71881 root 10 PIPE 0x67dbd1c5970420d8 16384 -\u0026gt;0x77446814497e3b77 teleprese 71881 root 11 NPOLICY teleprese 71881 root 12u unix 0x4715bfb02f991f5f 0t0 /var/run/telepresence-daemon.socket teleprese 71881 root 13u unix 0x4715bfb02f9944df 0t0 -\u0026gt;0x4715bfb02f9945a7 teleprese 71881 root 14u IPv4 0x4715bfb4fc950c67 0t0 UDP *:51812 teleprese 71881 root 15u unix 0x4715bfb02f993477 0t0 -\u0026gt;0x4715bfb02f99353f root daemon は 51812/udp を listen しています。後でわかりますが、これは DNS サーバーです。クラスタのドメインはここに問い合わせが行われます。utun_control というのは tunnel の制御でしょうか。\nutun3 といネットワークデバイスが作成されており\n$ ifconfig utun3 utun3: flags=8051\u0026lt;UP,POINTOPOINT,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 172.16.0.0 --\u0026gt; 172.16.0.1 netmask 0xfffff000 inet 172.17.0.0 --\u0026gt; 172.17.0.1 netmask 0xffffffc0 inet 172.17.0.64 --\u0026gt; 172.17.0.1 netmask 0xffffffc0 inet 172.17.0.128 --\u0026gt; 172.17.0.1 netmask 0xffffffc0 Pod や Service 用サブネットの route がその utun3 へ向けられています。\n$ netstat -nr | grep utun3 172.16/20 172.16.0.1 UGCS utun3 172.16.0.1 172.16.0.0 UH utun3 172.16.15.255 172.16.0.1 UGHW3I utun3 22 172.17/26 172.17.0.1 UGCS utun3 172.17.0.1 172.17.0.0 UH utun3 172.17.0.63 172.17.0.1 UGHW3I utun3 22 172.17.0.64/26 172.17.0.1 UGCS utun3 172.17.0.127 172.17.0.1 UGHW3I utun3 22 172.17.0.128/26 172.17.0.1 UGCS utun3 172.17.0.191 172.17.0.1 UGHW3I utun3 22 user daemon\n$ lsof -nPp 71885 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME teleprese 71885 teraoka cwd DIR 1,9 512 47512210 /Users/teraoka/ghq/github.com/yteraoka/terraform-examples/gcp/gke teleprese 71885 teraoka txt REG 1,9 77231136 74147315 /Users/teraoka/bin/telepresence teleprese 71885 teraoka txt REG 1,9 46096 74099023 /Library/Preferences/Logging/.plist-cache.1VLWA6Q0 teleprese 71885 teraoka txt REG 1,9 32768 74100907 /private/var/db/mds/messages/501/se_SecurityMessages teleprese 71885 teraoka txt REG 1,9 2160672 1152921500312811906 /usr/lib/dyld teleprese 71885 teraoka txt REG 1,9 114087 74099365 /private/var/db/analyticsd/events.whitelist teleprese 71885 teraoka 0r CHR 3,2 0t0 317 /dev/null teleprese 71885 teraoka 1w REG 1,9 1498 74169289 /Users/teraoka/Library/Logs/telepresence/connector.log teleprese 71885 teraoka 2w REG 1,9 1498 74169289 /Users/teraoka/Library/Logs/telepresence/connector.log teleprese 71885 teraoka 3u KQUEUE count=0, state=0xa teleprese 71885 teraoka 4 PIPE 0xfc30aea6068d98ef 16384 -\u0026gt;0xd5d97fbdd54a6017 teleprese 71885 teraoka 5 PIPE 0xd5d97fbdd54a6017 16384 -\u0026gt;0xfc30aea6068d98ef teleprese 71885 teraoka 6w REG 1,9 1498 74169289 /Users/teraoka/Library/Logs/telepresence/connector.log teleprese 71885 teraoka 7u unix 0x4715bfb02f992347 0t0 /tmp/telepresence-connector.socket teleprese 71885 teraoka 8 PIPE 0xa124ff493681e691 16384 -\u0026gt;0x40a1939f617879e5 teleprese 71885 teraoka 9 PIPE 0x40a1939f617879e5 16384 -\u0026gt;0xa124ff493681e691 teleprese 71885 teraoka 10u IPv6 0x4715bfb9c79fa6f7 0t0 TCP *:53443 (LISTEN) teleprese 71885 teraoka 12u IPv4 0x4715bfbe96aaa46f 0t0 TCP 192.168.210.119:53444-\u0026gt;203.0.113.123:443 (ESTABLISHED) teleprese 71885 teraoka 13u unix 0x4715bfb02f9919e7 0t0 -\u0026gt;0x4715bfb02f991f5f teleprese 71885 teraoka 14 NPOLICY teleprese 71885 teraoka 15u unix 0x4715bfb02f992027 0t0 -\u0026gt;0x4715bfb02f99434f teleprese 71885 teraoka 17u IPv4 0x4715bfbe9eb6546f 0t0 TCP 192.168.210.119:53446-\u0026gt;203.0.113.123:443 (ESTABLISHED) teleprese 71885 teraoka 18u unix 0x4715bfb02f9945a7 0t0 /tmp/telepresence-connector.socket Kubernetes の API Server と通信しているのは user daemon のようです。\nログファイルを確認 # lsof でログファイルらしきファイルが確認できたので中身を見てみます。\nroot daemon のログ\n/Users/teraoka/Library/Logs/telepresence/daemon.log\n2021-12-31 17:14:09.3150 info Logging at this level \u0026quot;info\u0026quot; 2021-12-31 17:14:09.3151 info --- 2021-12-31 17:14:09.3152 info Telepresence daemon v2.4.9 (api v3) starting... 2021-12-31 17:14:09.3152 info PID is 71881 2021-12-31 17:14:09.3152 info 2021-12-31 17:14:09.4008 info daemon/server-grpc : gRPC server started 2021-12-31 17:14:10.6945 info daemon/server-grpc/conn=2 : Adding never-proxy subnet 203.0.113.123/32 2021-12-31 17:14:10.7615 info daemon/watch-cluster-info : Adding service subnet 172.16.0.0/20 2021-12-31 17:14:10.7616 info daemon/watch-cluster-info : Adding pod subnet 172.17.0.0/26 2021-12-31 17:14:10.7616 info daemon/watch-cluster-info : Adding pod subnet 172.17.0.64/26 2021-12-31 17:14:10.7617 info daemon/watch-cluster-info : Adding pod subnet 172.17.0.128/26 2021-12-31 17:14:10.7621 info daemon/watch-cluster-info : Setting cluster DNS to 172.16.0.10 2021-12-31 17:14:10.7621 info daemon/watch-cluster-info : Setting cluster domain to \u0026quot;cluster.local.\u0026quot; 2021-12-31 17:14:10.7778 info daemon/server-router/MGR stream : Connected to Manager 2.4.9 2021-12-31 17:14:10.7983 info daemon/server-dns : Generated new /etc/resolver/telepresence.local 2021-12-31 17:14:10.7985 info daemon/server-dns/SearchPaths : setting search paths ambassador default kube-node-lease kube-public kube-system 2021-12-31 17:14:10.7987 info daemon/server-dns/SearchPaths : Generated new /etc/resolver/telepresence.kube-system.local 2021-12-31 17:14:10.7991 info daemon/server-dns/SearchPaths : Generated new /etc/resolver/telepresence.tel2-search.local 2021-12-31 17:14:10.7994 info daemon/server-dns/SearchPaths : Generated new /etc/resolver/telepresence.ambassador.local 2021-12-31 17:14:10.7996 info daemon/server-dns/SearchPaths : Generated new /etc/resolver/telepresence.default.local 2021-12-31 17:14:10.7999 info daemon/server-dns/SearchPaths : Generated new /etc/resolver/telepresence.kube-node-lease.local 2021-12-31 17:14:10.8001 info daemon/server-dns/SearchPaths : Generated new /etc/resolver/telepresence.kube-public.local root daemon は watch-cluster-info で Service の Subnet や Node 毎に作成される Pod の Subnet をクラスタに合わせて routing 設定しているようですし、DNS の forward 先の管理、そして /etc/resolver 配下に cluster 側に向けるドメイン用のファイルを管理しています。\n例えば /etc/resolver/telepresence.kube-system.local の中身は次のようになっており、kube-system で終わるドメインの DNS の問い合わせは 127.0.0.1:51812 に送られるようになっています。51812/udp は root daemon が listen しており、tunnel 経由で Kubernetes 内の DNS Service に転送されてるようになっているみたいです。mac の /etc/resolver の仕組みを知らず、Linux の LD_PRELOAD 的なものが mac にもあってそれが使われているのだろうと調べてて、そんな設定が見つからずしばらく悩んでしまっていました\u0026hellip;\n$ cat /etc/resolver/telepresence.kube-system.local # Generated by telepresence port 51812 domain kube-system nameserver 127.0.0.1 127.0.0.1:51812 に対して DNS の問い合わせを投げると結果が返ってきます。\nmac のこの設定は scutil --dns でも確認できます。\n$ dig +short @127.0.0.1 -p 51812 hello.default.svc.cluster.local a 172.16.1.153 user daemon のログ\n/Users/teraoka/Library/Logs/telepresence/connector.log\n2021-12-31 17:14:09.5593 info Logging at this level \u0026quot;info\u0026quot; 2021-12-31 17:14:09.5971 info --- 2021-12-31 17:14:09.5972 info Telepresence Connector v2.4.9 (api v3) starting... 2021-12-31 17:14:09.5972 info PID is 71885 2021-12-31 17:14:09.5972 info 2021-12-31 17:14:09.5974 info connector/server-grpc : gRPC server started 2021-12-31 17:14:09.7740 info connector/background-init : Connecting to daemon... 2021-12-31 17:14:09.7747 info connector/background-init : Connecting to k8s cluster... 2021-12-31 17:14:09.8611 info connector/background-init : Server version v1.21.5-gke.1302 2021-12-31 17:14:09.8612 info connector/background-init : Context: gke_MY_PROJECT_ID_asia-northeast1-a_CLUSTER_NAME 2021-12-31 17:14:09.8613 info connector/background-init : Server: https://203.0.113.123 2021-12-31 17:14:09.8613 info connector/background-init : Connected to context gke_MY_PROJECT_ID_asia-northeast1-a_CLUSTER_NAME (https://203.0.113.123) 2021-12-31 17:14:10.0374 info connector/background-init : Connecting to traffic manager... 2021-12-31 17:14:10.0375 info connector/background-init : Waiting for TrafficManager to connect 2021/12/31 17:14:10 Patching synced Namespace 6ed04e58-22c9-4b10-87a2-690867ae2371 2021-12-31 17:14:10.1281 info connector/background-manager : Existing Traffic Manager 2.4.9 not owned by cli or does not need upgrade, will not modify 2021/12/31 17:19:10 Patching synced Namespace 6ed04e58-22c9-4b10-87a2-690867ae2371 telepresence status # telepresence status コマンドで2つの daemon プロセスの状況を確認できます。DNS 周りの仕組みを見てて、誰かが com とか jp とか TLD と同じ namespace 作ってしまうと困るだろうなと思ってたのですが、いくつかはデフォルトで Exclude suffixes に入っているのですね。telepresence connect コマンドの --mapped-namespaces オプションで必要な namespace だけをコンマ区切りで並べることもできます。\n$ telepresence status Root Daemon: Running Version : v2.4.9 (api 3) DNS : Remote IP : 172.16.0.10 Exclude suffixes: [.arpa .com .io .net .org .ru] Include suffixes: [] Timeout : 4s Also Proxy : (0 subnets) Never Proxy: (1 subnets) User Daemon: Running Version : v2.4.9 (api 3) Ambassador Cloud : Logged out Status : Connected Kubernetes server : https://203.0.113.123 Kubernetes context: gke_MY_PROJECT_ID_asia-northeast1-a_CLUSTER_NAME Telepresence proxy: ON (networking to the cluster is enabled) Intercepts : 0 total part 1 はここまで。Kubernetes クラスタ側からのアクセスは次回。\ntelepresence connect で起動した 2 つの daemon プロセスは telepresence quit で終了できます。\n","date":"2021年12月31日","permalink":"/2021/12/telepresence-part-1/","section":"Posts","summary":"telepresence というツールがあります。手元の端末が Kubernetes クラスタ内にいるかのような通信を可能にし、Kubernetes の Pod の Container への通信をインターセプトして手","title":"telepresence 入門 (1)"},{"content":"","date":"2021年12月31日","permalink":"/tags/aws/","section":"Tags","summary":"","title":"aws"},{"content":"「Amazon CloudFront が設定可能な CORS、セキュリティ、およびカスタム HTTP レスポンスヘッダーをサポート」で Lambda@Edge なしで Response にカスタムヘッダーを追加することが可能になりました。 これを使って、このサイトにも Security Headers を追加してみます。\nCloudFront のコンソールで、Policies → Response headers にアクセスすると Managed policies があり、次のポリシーが存在します。\n CORS-and-SecurityHeadersPolicy CORS-With-Preflight CORS-with-preflight-and-SecurityHeadersPolicy SecurityHeadersPolicy SimpleCORS  SecurityHeaders と CORS の組み合わせパターンですね。\n SecurityHeadersPolicy ではレスポンスに次の Header がセットされます。\nStrict-Transport-Security: max-age=31536000 X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block Referrer-Policy: strict-origin-when-cross-origin   これを Distributions → Behaviors の Response headers policy で選択します。\n ","date":"2021年12月31日","permalink":"/2021/12/cloudfront-security-headers/","section":"Posts","summary":"「Amazon CloudFront が設定可能な CORS、セキュリティ、およびカスタム HTTP レスポンスヘッダーをサポート」で Lambda@Edge なしで Response にカスタムヘッダーを追加するこ","title":"CloudFront のレスポンスに Security Headers を追加する"},{"content":"Docker Desktop 代替シリーズ第三部、Podman です。(第一部 minkikube 編、第二部 Lima + nerdctl 編)\nPodman については Red Hat さんのブログが大変参考になります。\n今回の Docker 社によるライセンス周りの変更が発表されるよりだいぶ前に How to replace Docker with Podman on a Mac という記事もありました。Podman は Linux 上で稼働する daemon とそのクライアントという構成となっており、Windows と Mac にはクライアントしかありません。この時の記事では Vagrant を使って Linux 仮想サーバーを起動させるという処理を macOS app にしようという内容でした。現在は podman machine というサブコマンドで仮想サーバーを起動させることができるようになっているようです。それでは試してみましょう。\npodman のインストール # $ brew install podman これにより podman, podman-remote, gvproxy コマンドが $(brew --prefix)/bin/ にリンクされていました。\npodman machine の起動 # podman machine init # podman machine init で初期設定。初回は仮想マシンのイメージのダウンロードも行われます。\n$ podman machine init Downloading VM image: fedora-coreos-34.20210919.1.0-qemu.x86_64.qcow2.xz [=\u0026gt;------------------------] 52.7MiB / 598.3MiB Downloading VM image: fedora-coreos-34.20210919.1.0-qemu.x86_64.qcow2.xz: done Extracting compressed file ホームディレクトリ配下、~/.local/share/containers/podman/machine/qemu/ に仮想マシンのイメージファイル (qcow2) が配置されています。\n$ ls -lh ~/.local/share/containers/podman/machine/qemu total 2.1G -rw-r--r-- 1 teraoka staff 599M Sep 23 18:05 fedora-coreos-34.20210919.1.0-qemu.x86_64.qcow2.xz -rw------- 1 teraoka staff 1.5G Sep 23 18:06 podman-machine-default_fedora-coreos-34.20210919.1.0-qemu.x86_64.qcow2 podman machine start # podman machine start で仮想マシンを起動させます。\n$ podman machine start INFO[0000] waiting for clients... INFO[0000] listening tcp://0.0.0.0:7777 INFO[0000] new connection from to /var/folders/nd/8mk6834s31g8dymd1_9pnqq00000gn/T/podman/qemu_podman-machine-default.sock Waiting for VM ... qemu-system-x86_64: warning: host doesn't support requested feature: CPUID.80000001H:ECX.svm [bit 2] podman machine ls コマンドで仮想マシンの状態を確認できます。\n$ podman machine ls NAME VM TYPE CREATED LAST UP podman-machine-default* qemu 7 minutes ago Currently running podman machine ssh # podman machine ssh で仮想マシンに ssh でログインできます。\n$ podman machine ssh Connecting to vm podman-machine-default. To close connection, use `~.` or `exit` Warning: Permanently added '[localhost]:54552' (ECDSA) to the list of known hosts. Fedora CoreOS 34.20210919.1.0 Tracker: https://github.com/coreos/fedora-coreos-tracker Discuss: https://discussion.fedoraproject.org/c/server/coreos/ [core@localhost ~]$ これまでの流れで hosts は確認しちゃいますね。ホスト(mac)へアクセスするための名前は設定されていないようです。\n[core@localhost ~]$ cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 lima や minikube (virtualbox) の仮想マシンでは default router となっているアドレスにアクセスすることでホストにアクセスできたので、podman machine でも同様かな？と思って 192.168.127.1 にアクセスしてみましたがダメでした。(ホスト(mac)のIPアドレスを指定すればアクセスはできるんですけど、DHCP なので場所とかによって変わってしまうんですよね)\n[core@localhost ~]$ ip a s 1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp0s2: mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 5a:94:ef:e4:0c:ee brd ff:ff:ff:ff:ff:ff inet 192.168.127.2/24 brd 192.168.127.255 scope global dynamic noprefixroute enp0s2 valid_lft 3256sec preferred_lft 3256sec inet6 fe80::7a18:f299:cead:4ecc/64 scope link noprefixroute valid_lft forever preferred_lft forever [core@localhost ~]$ ip r default via 192.168.127.1 dev enp0s2 proto dhcp metric 100 192.168.127.0/24 dev enp0s2 proto kernel scope link src 192.168.127.2 metric 100 ところで、なんでしょう？ この謎のメッセージは？ curl じゃない何かがエラーメッセージを出しています。 tcp の proxy をするやつがいそうです。\n$ curl http://192.168.127.1:8080/ ERRO[6288] net.Dial() = dial tcp 192.168.127.1:8080: connect: operation timed out ERRO[6322] net.Dial() = dial tcp 192.168.127.1:8080: connect: operation timed out curl: (7) Failed to connect to 192.168.127.1 port 8080: Connection refused curl の connect timeout を 1 秒にしてみる\n[core@localhost ~]$ **curl --connect-timeout 1 http://192.168.127.1:8080/** curl: (28) Connection timed out after 1006 milliseconds [core@localhost ~]$ ERRO[6616] net.Dial() = dial tcp 192.168.127.1:8080: connect: operation timed out curl が Go で書かれた別物というわけではなさそうです。\n[core@localhost ~]$ curl --version curl 7.76.1 (x86_64-redhat-linux-gnu) libcurl/7.76.1 OpenSSL/1.1.1l-fips zlib/1.2.11 brotli/1.0.9 libidn2/2.3.2 libpsl/0.21.1 (+libidn2/2.3.0) libssh/0.9.5/openssl/zlib nghttp2/1.43.0 Release-Date: 2021-04-14 Protocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtsp scp sftp smb smbs smtp smtps telnet tftp Features: alt-svc AsynchDNS brotli GSS-API HTTP2 HTTPS-proxy IDN IPv6 Kerberos Largefile libz NTLM NTLM_WB PSL SPNEGO SSL TLS-SRP UnixSockets どうやら podman machine start で起動した際に listening tcp://0.0.0.0:7777 と表示されていたのは gvproxy のログのようですね。これを起動した terminal で podman machine ssh してたのでそこに出てきただけで仮想マシン内ででたメッセージではありませんでした。\nmac 側で gvproxy プロセスを確認してみる。\n$ pgrep -lf gvproxy 52708 /Users/teraoka/.homebrew/bin/gvproxy -listen tcp://0.0.0.0:7777 -listen-qemu unix:///var/folders/nd/8mk6834s31g8dymd1_9pnqq00000gn/T/podman/qemu_podman-machine-default.sock -pid-file /var/folders/nd/8mk6834s31g8dymd1_9pnqq00000gn/T/podman/podman-machine-default.pid -ssh-port 54552 podman machine ssh した時に localhost の 54552 ポートに接続しようとしているのもこの gvproxy が中継しているようですね。podman run の -p で port を expose すると mac から localhost でもアクセスできるようにしてくれているのがこの gvproxy らしいんですが、仮想マシンからの outbound にも絡んでいるとは\u0026hellip; (嬉しくない)\n仮想マシンのリソース設定 # デフォルトでは 2GB のメモリと 1 コアの CPU が割り当てられてました。\n[core@localhost ~]$ free -h total used free shared buff/cache available Mem: 1.9Gi 128Mi 1.6Gi 5.0Mi 208Mi 1.7Gi Swap: 0B 0B 0B [core@localhost ~]$ **lscpu** Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian Address sizes: 40 bits physical, 48 bits virtual CPU(s): 1 On-line CPU(s) list: 0 Thread(s) per core: 1 Core(s) per socket: 1 Socket(s): 1 NUMA node(s): 1 Vendor ID: GenuineIntel CPU family: 15 Model: 107 Model name: QEMU Virtual CPU version 2.5+ Stepping: 1 CPU MHz: 2591.776 BogoMIPS: 5183.55 L1d cache: 32 KiB L1i cache: 32 KiB L2 cache: 4 MiB L3 cache: 16 MiB NUMA node0 CPU(s): 0 Vulnerability Itlb multihit: KVM: Mitigation: VMX unsupported Vulnerability L1tf: Mitigation; PTE Inversion Vulnerability Mds: Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown Vulnerability Meltdown: Mitigation; PTI Vulnerability Spec store bypass: Vulnerable Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization Vulnerability Spectre v2: Mitigation; Full generic retpoline, STIBP disabled, RSB filling Vulnerability Srbds: Not affected Vulnerability Tsx async abort: Not affected Flags: fpu de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr ss e sse2 syscall nx lm constant_tsc nopl xtopology cpuid pni cx16 hypervisor lahf_lm pti CPU やメモリのサイズを調整するには podman machine init 時に設定する必要があるようです。\n$ podman machine init --help Initialize a virtual machine Description: initialize a virtual machine Usage: podman machine init [options] [NAME] Examples: podman machine init myvm Options: --cpus uint Number of CPUs. The default is 1. (default 1) --disk-size uint Disk size in GB (default 10) --ignition-path string Path to ignition file --image-path string Path to qcow image -m, --memory uint Memory (in MB) (default 2048) $ podman machine init --cpus 2 --disk-size 5 --memory 1024 vm2 Extracting compressed file $ podman machine ls NAME VM TYPE CREATED LAST UP podman-machine-default* qemu 4 hours ago Currently running vm2 qemu About a minute ago About a minute ago 仮想マシンを複数作成できるから同時に起動させることもできるのかな？と思ったけど同時に起動させられるのは1つだけのようです。\n$ podman machine start vm2 Error: cannot start VM vm2. VM podman-machine-default is currently running: only one VM can be active at a time 仮想マシンの設定は ~/.config/containers/podman/machine/ 配下に作成されていました。\n$ find .config/containers/podman/machine .config/containers/podman/machine .config/containers/podman/machine/qemu .config/containers/podman/machine/qemu/podman-machine-default.json .config/containers/podman/machine/qemu/podman-machine-default.ign .config/containers/podman/machine/qemu/vm2.json .config/containers/podman/machine/qemu/vm2.ign podman run # コンテナを実行してみます。\nまずは image を pull してみる。\n$ podman pull nginx:alpine Error: failed to parse \u0026quot;X-Registry-Auth\u0026quot; header for /v3.3.1/libpod/images/pull?alltags=false\u0026amp;arch=\u0026amp;authfile=\u0026amp;os=\u0026amp;password=\u0026amp;policy=always\u0026amp;quiet=false\u0026amp;reference=nginx%3Aalpine\u0026amp;username=\u0026amp;variant=: error storing credentials in temporary auth file (server: \u0026quot;https://index.docker.io/v1/\u0026quot;, user: \u0026quot;\u0026quot;): key https://index.docker.io/v1/ contains http[s]:// prefix うーむ、~/.docker/config.json が悪さをしているらしいので rename して再チャレンジ。\nちなみにこの時の config.json は次のようになっていました。 (containers-auth.json(5))\n{ \u0026#34;auths\u0026#34;: { \u0026#34;https://index.docker.io/v1/\u0026#34;: {} }, \u0026#34;credsStore\u0026#34;: \u0026#34;osxkeychain\u0026#34; } $ podman pull nginx:alpine Error: short-name resolution enforced but cannot prompt without a TTY エラーメッセージが変わった。short-name の解決したいけど TTY が使えないとダメだよと言ってるみたいなので docker.io もつけてみる。 (podman は image のホスト名部分を省略した場合にリストを順に検索してくれる設定があるらしい。未調査)\n$ podman pull docker.io/nginx:alpine Trying to pull docker.io/library/nginx:alpine... Getting image source signatures Copying blob sha256:61074acc7dd227cfbeaf719f9b5cdfb64711bc6b60b3865c7b886b7099c15d15 Copying blob sha256:a0d0a0d46f8b52473982a3c466318f479767577551a53ffc9074c9fa7035982e Copying blob sha256:4dd4efe90939ab5711aaf5fcd9fd8feb34307bab48ba93030e8b845f8312ed8e Copying blob sha256:c1368e94e1ec563b31c3fb1fea02c9fbdc4c79a95e9ad0cac6df29c228ee2df3 Copying blob sha256:3e72c40d0ff43c52c5cc37713b75053e8cb5baea8e137a784d480123814982a2 Copying blob sha256:969825a5ca61c8320c63ff9ce0e8b24b83442503d79c5940ba4e2f0bd9e34df8 Copying blob sha256:969825a5ca61c8320c63ff9ce0e8b24b83442503d79c5940ba4e2f0bd9e34df8 Copying blob sha256:3e72c40d0ff43c52c5cc37713b75053e8cb5baea8e137a784d480123814982a2 Copying blob sha256:61074acc7dd227cfbeaf719f9b5cdfb64711bc6b60b3865c7b886b7099c15d15 Copying blob sha256:4dd4efe90939ab5711aaf5fcd9fd8feb34307bab48ba93030e8b845f8312ed8e Copying blob sha256:c1368e94e1ec563b31c3fb1fea02c9fbdc4c79a95e9ad0cac6df29c228ee2df3 Copying blob sha256:a0d0a0d46f8b52473982a3c466318f479767577551a53ffc9074c9fa7035982e Copying config sha256:513f9a9d8748b25cdb0ec6f16b4523af7bba216a6bf0f43f70af75b4cf7cb780 Writing manifest to image destination Storing signatures 513f9a9d8748b25cdb0ec6f16b4523af7bba216a6bf0f43f70af75b4cf7cb780 TTY 問題ですが、仮想マシン内で podman を実行した場合は次のような選択 UI になります。\n[core@localhost ~]$ podman pull nginx:latest ? Please select an image: ▸ registry.fedoraproject.org/nginx:latest registry.access.redhat.com/nginx:latest docker.io/library/nginx:latest quay.io/nginx:latest この registry の選択肢は仮想マシン側の /etc/containers/registries.conf に定義されていました。 (containers-registries.conf(5))\nこの設定はホスト (mac) 側の ~/.config/containers/registries.conf に書くこともできます。次のように docker.io だけにしておけば docker の場合と同じように動作します。 ホスト側のファイルで制御できるというのは間違いでした。debian のイメージで試したのですが、実はこれは下で紹介する alias によって pull できただけでした。仮想マシン内の ~/.config/containers/registries.conf に書いてください。\n[core@localhost ~]$ cat ~/.config/containers/registries.conf unqualified-search-registries = [\u0026quot;docker.io\u0026quot;] @tnk4on さんから情報をいただきました。short-name の選択は仮想マシン側でファイルに cache されるそうです。~/.cache/containers/short-name-aliases.conf にありました。\n[core@localhost containers]$ cat ~/.cache/containers/short-name-aliases.conf [aliases] nginx = \u0026quot;docker.io/library/nginx\u0026quot; この状態で pull してみると Resolved \u0026quot;nginx\u0026quot; as an alias (/var/home/core/.cache/containers/short-name-aliases.conf) って出力されてました。\n$ podman pull nginx:1.19.1 Resolved \u0026quot;nginx\u0026quot; as an alias (/var/home/core/.cache/containers/short-name-aliases.conf) Trying to pull docker.io/library/nginx:1.19.1... Getting image source signatures Copying blob sha256:1f1070938ccd20f58e90d4a03e71b859274580306295ec8b68cf39c6d7f1978f (snip) Copying blob sha256:c57dd87d0b93cc883c4af602c0a8e1a6bd9083f723fb426432427812fe3c0e31 Copying config sha256:08393e824c32d456ff69aec72c64d1ab63fecdad060ab0e8d3d42640fc3d64c5 Writing manifest to image destination Storing signatures 08393e824c32d456ff69aec72c64d1ab63fecdad060ab0e8d3d42640fc3d64c5 これまた @tnk4on さんからの情報ですが、registries.conf によるレジストリの一覧だけでなく、この　cache のような形式で /etc/containers/registries.conf.d/000-shortnames.conf に alias の一覧がありました。 次のようになっていて、ここに列挙されたものは\n[core@localhost ~]$ head /etc/containers/registries.conf.d/000-shortnames.conf [aliases] # centos \u0026quot;centos\u0026quot; = \u0026quot;quay.io/centos/centos\u0026quot; # containers \u0026quot;skopeo\u0026quot; = \u0026quot;quay.io/skopeo/stable\u0026quot; \u0026quot;buildah\u0026quot; = \u0026quot;quay.io/buildah/stable\u0026quot; \u0026quot;podman\u0026quot; = \u0026quot;quay.io/podman/stable\u0026quot; # docker \u0026quot;alpine\u0026quot; = \u0026quot;docker.io/library/alpine\u0026quot; \u0026quot;docker\u0026quot; = \u0026quot;docker.io/library/docker\u0026quot; 次に podman run を試してみる。\npodman で特権ポート (1024 未満) は listen できないよというのは知ってるけど試してみる。\n$ podman run -it -p 80:80 docker.io/nginx:alpine Error: error preparing container 800d78d0aecb23d5838d0cce77ebe7fa4ef4209c01a97d13e4979bcad387634f for attach: rootlessport cannot expose privileged port 80, you can add 'net.ipv4.ip_unprivileged_port_start=80' to /etc/sysctl.conf (currently 1024), or choose a larger port number (\u0026gt;= 1024): listen tcp 0.0.0.0:80: bind: permission denied 事前の情報通りエラーになった。次は -p 8080:80 で試す。\n$ podman run -it -p 8080:80 docker.io/nginx:alpine /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh /docker-entrypoint.sh: Configuration complete; ready for start up 2021/09/23 12:56:00 [notice] 1#1: using the \u0026quot;epoll\u0026quot; event method 2021/09/23 12:56:00 [notice] 1#1: nginx/1.21.3 2021/09/23 12:56:00 [notice] 1#1: built by gcc 10.3.1 20210424 (Alpine 10.3.1_git20210424) 2021/09/23 12:56:00 [notice] 1#1: OS: Linux 5.13.16-200.fc34.x86_64 2021/09/23 12:56:00 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 524288:524288 2021/09/23 12:56:00 [notice] 1#1: start worker processes 2021/09/23 12:56:00 [notice] 1#1: start worker process 27 podman の仮想マシン内であれば curl http://localhost:8080/ でアクセスできましたが、ホストの mac からは connection refused でした。冒頭で紹介したブログを確認すると、podman 3.3.1 では既知の問題らしいです。\n [Note:] Podman v3.3.1時点でこの機能には不具合があり、上記のように\u0026ndash;network bridgeをつけるか、他にも事前にpodman network createで作成したネットワークを指定する方法やcontainers.confファイルの[containers]セクションにrootless_networking = \u0026ldquo;cni\u0026quot;を追加する、などのワークアラウンドがあります。 この不具合については私の上げた下記Issue（すでにClose済み）で対応がなされており、次のバージョン（v3.3.2？）に修正が適用される見込みです。\n でも仮想マシン内の .config/containers/containers.conf には rootless_networking = \u0026quot;cni\u0026quot; が最初から入ってたんだけどなあ。\n[core@localhost ~]$ cat .config/containers/containers.conf [containers] netns=\u0026quot;bridge\u0026quot; rootless_networking=\u0026quot;cni\u0026quot; podman run に --network bridge をつけたら mac からも localhost でアクセスできました。\n$ curl -s http://localhost:8080/ \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026quot;http://nginx.org/\u0026quot;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026quot;http://nginx.com/\u0026quot;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ホスト (mac) 側にも containers.conf がありました。そして、こっちの [containers] に rootless_networking = \u0026quot;cni\u0026quot; を書けば --network bridge 指定なしで mac から localhost でアクセスできました。\n$ cat ~/.config/containers/containers.conf [containers] log_size_max = -1 pids_limit = 2048 userns_size = 65536 [engine] image_parallel_copies = 0 num_locks = 2048 active_service = \u0026quot;podman-machine-default\u0026quot; stop_timeout = 10 chown_copied_files = false [engine.service_destinations] [engine.service_destinations.podman-machine-default] uri = \u0026quot;ssh://core@localhost:54552/run/user/1000/podman/podman.sock\u0026quot; identity = \u0026quot;/Users/teraoka/.ssh/podman-machine-default\u0026quot; [engine.service_destinations.podman-machine-default-root] uri = \u0026quot;ssh://root@localhost:54552/run/podman/podman.sock\u0026quot; identity = \u0026quot;/Users/teraoka/.ssh/podman-machine-default\u0026quot; [engine.service_destinations.vm2] uri = \u0026quot;ssh://core@localhost:55656/run/user/1000/podman/podman.sock\u0026quot; identity = \u0026quot;/Users/teraoka/.ssh/vm2\u0026quot; [engine.service_destinations.vm2-root] uri = \u0026quot;ssh://root@localhost:55656/run/podman/podman.sock\u0026quot; identity = \u0026quot;/Users/teraoka/.ssh/vm2\u0026quot; [network] [secrets] podman build # コンテナイメージをビルドしてみる。\n手元にあったこの Dockerfile で試してみます。\nFROM fluent/fluentd:v1.12.4-debian-1.0 USER root RUN fluent-gem install fluent-plugin-kinesis -v 3.4.0 RUN fluent-gem install fluent-plugin-s3 COPY fluent.conf /fluentd/etc/fluent.conf USER fluent $ podman build . STEP 1/6: FROM fluent/fluentd:v1.12.4-debian-1.0 Error: error creating build container: short-name resolution enforced but cannot prompt without a TTY ここでも FROM の image に docker.io を足す必要がありました。\nFROM docker.io/fluent/fluentd:v1.12.4-debian-1.0 USER root RUN fluent-gem install fluent-plugin-kinesis -v 3.4.0 RUN fluent-gem install fluent-plugin-s3 COPY fluent.conf /fluentd/etc/fluent.conf USER fluent これで build はできました。\npodman push # docker hub に push するためにはログインが必要です。podman login docker.io でログインできました。認証情報は ~/.config/containers/auth.json に保存されました。\n$ cat ~/.config/containers/auth.json { \u0026quot;auths\u0026quot;: { \u0026quot;docker.io\u0026quot;: { \u0026quot;auth\u0026quot;: \u0026quot;44Om44O844K244O85ZCNOuimi+OBoeOCg+ODgOODoeOCiA==\u0026quot; } } } ボリュームマウント # 仮想マシン上のディレクトりをマウントすることは可能です。\n$ podman run -it -v /usr:/work --privileged docker.io/ubuntu しかし、mac 側のディレクトリはマウントさせられません。Docker Desktop のように仮想マシンレイヤーを意識せずにマウントさせられるようになることを期待。\nちなみに、上記の ubuntu の実行ですが、root で実行するコンテナだからか --privileged が必要でした。また、port-forward のための rootless_networking = \u0026quot;cni\u0026quot; が書いてあるとエラーになりました。\n$ podman run -it -v /usr:/work --privileged docker.io/ubuntu Error: error preparing container 98a665c8090d1693a963b7da46c5dfc0337d060f5010def610a9bad5f312ec8a for attach: error configuring network namespace for container 98a665c8090d1693a963b7da46c5dfc0337d060f5010def610a9bad5f312ec8a: error adding pod agitated_wiles_agitated_wiles to CNI network \u0026quot;podman\u0026quot;: unexpected end of JSON input こちらも @tnk4on からの情報で、これは podman machine のバグで、すでにアップストリームでは修正されているそうです。(tweet)\npodman-compose # 試していませんが docker-compose が必要な場合は Python で書かれた podman-compose というものが使えるようです。\nalias # shell の alias で docker=podman とすれば、ほぼそのまま使えると思います。\nまとめ # イメージを build するだけなら問題なさそうですが、ローカルでの開発用だとまだツラそう。今後に期待\n","date":"2021年9月23日","permalink":"/2021/09/podman-on-mac/","section":"Posts","summary":"Docker Desktop 代替シリーズ第三部、Podman です。(第一部 minkikube 編、第二部 Lima + nerdctl 編) Podman については Red Hat さんのブログが大変参考になります。 今回の Docker 社によるラ","title":"mac で podman"},{"content":"","date":"2021年9月23日","permalink":"/tags/podman/","section":"Tags","summary":"","title":"Podman"},{"content":"Docker Desktop の代わりに docker cli + Minikube ってのを試しただけど、Kubernetes は docker を非推奨にしてるし、kubernetes は不要な場合は無駄が多いしなあ\u0026hellip; ってことで lima も試してみる。\n(2021/01/05 追記: Docker on Lima も見てね)\nLima は自動のファイル共有、ポートフォワード、containerd をサポートした仮想マシンを提供してくれるツール。Windows subsystem for Linux の mac 版とも言えるとドキュメントに書かれている。\n今回は Intel Mac 環境で試しています。M1 Mac の場合は qemu に patch が必要みたいです。\nLima のインストール # Homebrew でインストール\n$ brew install lima Lima Virtual Machine の起動 # limactl コマンドを使う\n$ limactl NAME: limactl - Lima: Linux virtual machines USAGE: limactl [global options] command [command options] [arguments...] VERSION: 0.6.4 COMMANDS: start Start an instance of Lima. If the instance does not exist, open an editor for creating new one, with name \u0026quot;default\u0026quot; stop Stop an instance shell Execute shell in Lima copy, cp Copy files between host and guest list, ls List instances of Lima. delete, remove, rm Delete an instance of Lima. validate Validate yaml files prune Prune garbage objects completion Show shell completion help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --debug debug mode (default: false) --help, -h show help (default: false) --version, -v print the version (default: false) limactl start だけで実行するとインタラクティブなメニュー表示になった。とりあえず \u0026ldquo;Proceed with the default configuration\u0026rdquo; で Enter\n$ limactl start ? Creating an instance \u0026quot;default\u0026quot; [Use arrows to move, type to filter] \u0026gt; Proceed with the default configuration Open an editor to override the configuration Exit nerdctl のダウンロードが始まりました。(GitHub からのダウンロードが遅い\u0026hellip;)\n$ limactl start ? Creating an instance \u0026quot;default\u0026quot; Proceed with the default configuration INFO[0096] Downloading \u0026quot;https://github.com/containerd/nerdctl/releases/download/v0.11.2/nerdctl-full-0.11.2-linux-amd64.tar.gz\u0026quot; (sha256:27dbb238f9eb248ca68f11b412670db51db84905e3583834400305b2149915f2) 3.69 MiB / 174.89 MiB [\u0026gt;____________________________________] 2.11% 149.75 KiB/s 暇なので関連ファイルを探してみたら ~/.lima ディレクトリに作成されていました。\n$ find .lima -type f .lima/default/lima.yaml .lima/_config/user .lima/_config/user.pub default/lima.yaml が default というデフォルト仮想マシンの設定ファイルっぽいです。中身は次のようになっていました。ふむふむ、なるほどって感じですね。\n# ===================================================================== ## BASIC CONFIGURATION# ===================================================================== ## Arch: \u0026#34;default\u0026#34;, \u0026#34;x86_64\u0026#34;, \u0026#34;aarch64\u0026#34;.# \u0026#34;default\u0026#34; corresponds to the host architecture.arch:\u0026#34;default\u0026#34;# An image must support systemd and cloud-init.# Ubuntu and Fedora are known to work.# Default: none (must be specified)images:# Try to use a local image first.- location:\u0026#34;~/Downloads/hirsute-server-cloudimg-amd64.img\u0026#34;arch:\u0026#34;x86_64\u0026#34;- location:\u0026#34;~/Downloads/hirsute-server-cloudimg-arm64.img\u0026#34;arch:\u0026#34;aarch64\u0026#34;# Download the file from the internet when the local file is missing.# Hint: run `limactl prune` to invalidate the \u0026#34;current\u0026#34; cache- location:\u0026#34;https://cloud-images.ubuntu.com/hirsute/current/hirsute-server-cloudimg-amd64.img\u0026#34;arch:\u0026#34;x86_64\u0026#34;- location:\u0026#34;https://cloud-images.ubuntu.com/hirsute/current/hirsute-server-cloudimg-arm64.img\u0026#34;arch:\u0026#34;aarch64\u0026#34;# CPUs: if you see performance issues, try limiting cpus to 1.# Default: 4cpus:4# Memory size# Default: \u0026#34;4GiB\u0026#34;memory:\u0026#34;4GiB\u0026#34;# Disk size# Default: \u0026#34;100GiB\u0026#34;disk:\u0026#34;100GiB\u0026#34;# Expose host directories to the guest, the mount point might be accessible from all UIDs in the guest# Default: nonemounts:- location:\u0026#34;~\u0026#34;# CAUTION: `writable` SHOULD be false for the home directory.# Setting `writable` to true is possible, but untested and dangerous.writable:false- location:\u0026#34;/tmp/lima\u0026#34;writable:truessh:# A localhost port of the host. Forwarded to port 22 of the guest.# Currently, this port number has to be specified manually.# Default: nonelocalPort:60022# Load ~/.ssh/*.pub in addition to $LIMA_HOME/_config/user.pub .# This option is useful when you want to use other SSH-based# applications such as rsync with the Lima instance.# If you have an insecure key under ~/.ssh, do not use this option.# Default: trueloadDotSSHPubKeys:true# ===================================================================== ## ADVANCED CONFIGURATION# ===================================================================== #containerd:# Enable system-wide (aka rootful) containerd and its dependencies (BuildKit, Stargz Snapshotter)# Default: falsesystem:false# Enable user-scoped (aka rootless) containerd and its dependencies# Default: trueuser:true# Provisioning scripts need to be idempotent because they might be called# multiple times, e.g. when the host VM is being restarted.# provision:# # `system` is executed with the root privilege# - mode: system# script: |# #!/bin/bash# set -eux -o pipefail# export DEBIAN_FRONTEND=noninteractive# apt-get install -y vim# # `user` is executed without the root privilege# - mode: user# script: |# #!/bin/bash# set -eux -o pipefail# cat \u0026lt; ~/.vimrc# set number# EOF# probes:# # Only `readiness` probes are supported right now.# - mode: readiness# description: vim to be installed# script: |# #!/bin/bash# set -eux -o pipefail# if ! timeout 30s bash -c \u0026#34;until command -v vim; do sleep 3; done\u0026#34;; then# echo \u0026gt;\u0026amp;2 \u0026#34;vim is not installed yet\u0026#34;# exit 1# fi# hint: |# vim was not installed in the guest. Make sure the package system is working correctly.# Also see \u0026#34;/var/log/cloud-init-output.log\u0026#34; in the guest.# ===================================================================== ## FURTHER ADVANCED CONFIGURATION# ===================================================================== #firmware:# Use legacy BIOS instead of UEFI.# Default: falselegacyBIOS:falsevideo:# QEMU display, e.g., \u0026#34;none\u0026#34;, \u0026#34;cocoa\u0026#34;, \u0026#34;sdl\u0026#34;.# As of QEMU v5.2, enabling this is known to have negative impact# on performance on macOS hosts: https://gitlab.com/qemu-project/qemu/-/issues/334# Default: \u0026#34;none\u0026#34;display:\u0026#34;none\u0026#34;network:# The instance can get routable IP addresses from the vmnet framework using# https://github.com/lima-vm/vde_vmnet. Both vde_switch and vde_vmnet# daemons must be running before the instance is started. The interface type# (host, shared, or bridged) is configured in vde_vmnet and not lima.vde:# vnl (virtual network locator) points to the vde_switch socket directory,# optionally with vde:// prefix# - vnl: \u0026#34;vde:///var/run/vde.ctl\u0026#34;# # VDE Switch port number (not TCP/UDP port number). Set to 65535 for PTP mode.# # Default: 0# switchPort: 0# # MAC address of the instance; lima will pick one based on the instance name,# # so DHCP assigned ip addresses should remain constant over instance restarts.# macAddress: \u0026#34;\u0026#34;# # Interface name, defaults to \u0026#34;vde0\u0026#34;, \u0026#34;vde1\u0026#34;, etc.# name: \u0026#34;\u0026#34;# Port forwarding rules. Forwarding between ports 22 and ssh.localPort cannot be overridden.# Rules are checked sequentially until the first one matches.# portForwards:# - guestPort: 443# hostIP: \u0026#34;0.0.0.0\u0026#34; # overrides the default value \u0026#34;127.0.0.1\u0026#34;; allows privileged port forwarding# # default: hostPort: 443 (same as guestPort)# # default: guestIP: \u0026#34;127.0.0.1\u0026#34; (also matches bind addresses \u0026#34;0.0.0.0\u0026#34;, \u0026#34;::\u0026#34;, and \u0026#34;::1\u0026#34;)# # default: proto: \u0026#34;tcp\u0026#34; (only valid value right now)# - guestPortRange: [4000, 4999]# hostIP: \u0026#34;0.0.0.0\u0026#34; # overrides the default value \u0026#34;127.0.0.1\u0026#34;# # default: hostPortRange: [4000, 4999] (must specify same number of ports as guestPortRange)# - guestPort: 80# hostPort: 8080 # overrides the default value 80# - guestIP: \u0026#34;127.0.0.2\u0026#34; # overrides the default value \u0026#34;127.0.0.1\u0026#34;# hostIP: \u0026#34;127.0.0.2\u0026#34; # overrides the default value \u0026#34;127.0.0.1\u0026#34;# # default: guestPortRange: [1024, 65535]# # default: hostPortRange: [1024, 65535]# - guestPort: 8888# ignore: true (don\u0026#39;t forward this port)# # Lima internally appends this fallback rule at the end:# - guestIP: \u0026#34;127.0.0.1\u0026#34;# guestPortRange: [1024, 65535]# hostIP: \u0026#34;127.0.0.1\u0026#34;# hostPortRange: [1024, 65535]# # Any port still not matched by a rule will not be forwarded (ignored)# Extra environment variables that will be loaded into the VM at start up.# These variables are currently only consumed by internal init scripts, not by the user shell.# This field is experimental and may change in a future release of Lima.# https://github.com/lima-vm/lima/pull/200# env:# KEY: value# Explicitly set DNS addresses for qemu user-mode networking. By default qemu picks *one*# nameserver from the host config and forwards all queries to this server. On macOS# Lima adds the nameservers configured for the \u0026#34;en0\u0026#34; interface to the list. In case this# still doesn\u0026#39;t work (e.g. VPN setups), the servers can be specified here explicitly.# If nameservers are specified here, then the \u0026#34;en0\u0026#34; configuration will be ignored.# dns:# - 1.1.1.1# - 1.0.0.1# ===================================================================== ## END OF TEMPLATE# ===================================================================== # nerdctl のダウンロードが終わったら仮想マシンイメージのダウンロードが始まりました。こっちは速い。それが終わると仮想マシンの起動が始まりました。\n$ limactl start ? Creating an instance \u0026quot;default\u0026quot; Proceed with the default configuration INFO[0096] Downloading \u0026quot;https://github.com/containerd/nerdctl/releases/download/v0.11.2/nerdctl-full-0.11.2-linux-amd64.tar.gz\u0026quot;(sha256:27dbb238f9eb248ca68f11b412670db51db84905e3583834400305b2149915f2) 174.89 MiB / 174.89 MiB [---------------------------------] 100.00% 150.72 KiB/s INFO[1286] Downloaded \u0026quot;nerdctl-full-0.11.2-linux-amd64.tar.gz\u0026quot; INFO[1287] Attempting to download the image from \u0026quot;~/Downloads/hirsute-server-cloudimg-amd64.img\u0026quot; INFO[1287] Attempting to download the image from \u0026quot;https://cloud-images.ubuntu.com/hirsute/current/hirsute-server-cloudimg-amd64.img\u0026quot; 558.00 MiB / 558.00 MiB [-----------------------------------] 100.00% 7.57 MiB/s INFO[1362] Downloaded image from \u0026quot;https://cloud-images.ubuntu.com/hirsute/current/hirsute-server-cloudimg-amd64.img\u0026quot; INFO[1366] [hostagent] Starting QEMU (hint: to watch the boot progress, see \u0026quot;/Users/teraoka/.lima/default/serial.log\u0026quot;) INFO[1366] SSH Local Port: 60022 INFO[1366] [hostagent] Waiting for the essential requirement 1 of 4: \u0026quot;ssh\u0026quot; INFO[1394] [hostagent] Waiting for the essential requirement 1 of 4: \u0026quot;ssh\u0026quot; INFO[1395] [hostagent] The essential requirement 1 of 4 is satisfied INFO[1395] [hostagent] Waiting for the essential requirement 2 of 4: \u0026quot;sshfs binary to be installed\u0026quot; INFO[1416] [hostagent] The essential requirement 2 of 4 is satisfied INFO[1416] [hostagent] Waiting for the essential requirement 3 of 4: \u0026quot;/etc/fuse.conf to contain \\\u0026quot;user_allow_other\\\u0026quot;\u0026quot; INFO[1431] [hostagent] The essential requirement 3 of 4 is satisfied INFO[1431] [hostagent] Waiting for the essential requirement 4 of 4: \u0026quot;the guest agent to be running\u0026quot; INFO[1431] [hostagent] The essential requirement 4 of 4 is satisfied INFO[1431] [hostagent] Mounting \u0026quot;/Users/teraoka\u0026quot; INFO[1432] [hostagent] Mounting \u0026quot;/tmp/lima\u0026quot; INFO[1432] [hostagent] Waiting for the optional requirement 1 of 2: \u0026quot;systemd must be available\u0026quot; INFO[1433] [hostagent] Forwarding \u0026quot;/run/user/501/lima-guestagent.sock\u0026quot; (guest) to \u0026quot;/Users/teraoka/.lima/default/ga.sock\u0026quot; (host) INFO[1433] [hostagent] Not forwarding TCP 127.0.0.53:53 INFO[1433] [hostagent] Not forwarding TCP 0.0.0.0:22 INFO[1433] [hostagent] Not forwarding TCP [::]:22 INFO[1433] [hostagent] The optional requirement 1 of 2 is satisfied INFO[1433] [hostagent] Waiting for the optional requirement 2 of 2: \u0026quot;containerd binaries to be installed\u0026quot; INFO[1433] [hostagent] The optional requirement 2 of 2 is satisfied INFO[1433] READY. Run `lima` to open the shell. この状態で lima と実行するだけで Linux VM 内の shell に入ります。\n$ lima teraoka@lima-default:/Users/teraoka$ uname -a Linux lima-default 5.11.0-34-generic #36-Ubuntu SMP Thu Aug 26 19:22:09 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux OS は Ubuntu ですね。systemd で各種サービスが起動されています。\nteraoka@lima-default:/Users/teraoka$ cat /etc/os-release NAME=\u0026quot;Ubuntu\u0026quot; VERSION=\u0026quot;21.04 (Hirsute Hippo)\u0026quot; ID=ubuntu ID_LIKE=debian PRETTY_NAME=\u0026quot;Ubuntu 21.04\u0026quot; VERSION_ID=\u0026quot;21.04\u0026quot; HOME_URL=\u0026quot;https://www.ubuntu.com/\u0026quot; SUPPORT_URL=\u0026quot;https://help.ubuntu.com/\u0026quot; BUG_REPORT_URL=\u0026quot;https://bugs.launchpad.net/ubuntu/\u0026quot; PRIVACY_POLICY_URL=\u0026quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\u0026quot; VERSION_CODENAME=hirsute UBUNTU_CODENAME=hirsute ホームディレクトリと /tmp/lima が sshfs でマウントされています。ホームディレクトリは Read-Only です。どこをどこにマウントするかや、Read-Write を許可するかどうかは lima.yaml で指定可能っぽいです。\nteraoka@lima-default:/Users/teraoka$ df -h Filesystem Size Used Avail Use% Mounted on tmpfs 393M 1.1M 392M 1% /run /dev/vda1 97G 1.9G 95G 2% / tmpfs 2.0G 0 2.0G 0% /dev/shm tmpfs 5.0M 0 5.0M 0% /run/lock tmpfs 4.0M 0 4.0M 0% /sys/fs/cgroup /dev/vda15 105M 5.2M 100M 5% /boot/efi /dev/sr0 182M 182M 0 100% /mnt/lima-cidata tmpfs 393M 8.0K 393M 1% /run/user/501 :/Users/teraoka 466G 305G 143G 69% /Users/teraoka :/tmp/lima 466G 305G 143G 69% /tmp/lima /etc/hosts を確認したら 192.168.5.2 host.lima.internal という行がありました。Host (Mac) へのアクセスに使えそうです。\nlima コマンドの引数に linux のコマンドを渡せばそのまま Linux 内で実行されます。\n$ lima uname -a Linux lima-default 5.11.0-34-generic #36-Ubuntu SMP Thu Aug 26 19:22:09 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux コンテナの実行 # nerdctl も Linux 内にあります。次のようにしてコンテナの実行が可能。\n$ lima nerdctl run -d --name nginx -p 127.0.0.1:8080:80 nginx:alpine $ lima nerdctl ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1230bf478818 docker.io/library/nginx:alpine \u0026quot;/docker-entrypoint.…\u0026quot; About a minute ago Up 127.0.0.1:8080-\u0026gt;80/tcp nginx この状態で Mac から localhost:8080 にアクセスすると nginx にアクセスできます。~/.lima/default/lima.yaml に portForwarding 設定がありましたが、これはあれとは別物のようです。nerdctl で指定したポートでアクセスできます。\n$ curl -s http://localhost:8080/ \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026quot;http://nginx.org/\u0026quot;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026quot;http://nginx.com/\u0026quot;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Lima Virtual Machine の停止 # $ limactl stop INFO[0000] Sending SIGINT to hostagent process 18940 INFO[0000] Waiting for the host agent and the qemu processes to shut down INFO[0000] [hostagent] Received SIGINT, shutting down the host agent INFO[0000] [hostagent] Shutting down the host agent INFO[0000] [hostagent] Unmounting \u0026quot;/Users/teraoka\u0026quot; INFO[0000] [hostagent] Unmounting \u0026quot;/tmp/lima\u0026quot; WARN[0000] [hostagent] connection to the guest agent was closed unexpectedly INFO[0000] [hostagent] Shutting down QEMU with ACPI INFO[0000] [hostagent] Sending QMP system_powerdown command INFO[0001] [hostagent] QEMU has exited $ limactl ls NAME STATUS SSH ARCH DIR default Stopped 127.0.0.1:60022 x86_64 /Users/teraoka/.lima/default limactl start で再度起動させることができます。コンテナイメージや停止状態のコンテナは残っています。\nLima Virtual Machine の削除 # 削除の場合は仮想マシン名が省略できず、default も明示する必要がありました。\n$ limactl delete default INFO[0000] The QEMU process seems already stopped INFO[0000] The host agent process seems already stopped INFO[0000] Removing *.pid *.sock under \u0026quot;/Users/teraoka/.lima/default\u0026quot; INFO[0000] Removing \u0026quot;/Users/teraoka/.lima/default/ga.sock\u0026quot; INFO[0000] Deleted \u0026quot;default\u0026quot; (\u0026quot;/Users/teraoka/.lima/default\u0026quot;) limactl prune では cache が消されるっぽい。ここにはダウンロードした nerdctl や仮想マシンイメージがあり、消してしまうと limactl delete 後に再度 limactl start を実行するとまたダウンロードからやり直しです。\n$ limactl prune INFO[0000] Pruning \u0026quot;/Users/teraoka/Library/Caches/lima\u0026quot; docker エイリアス # shell の alias を設定することでほぼ docker として使えるようです。\nalias docker=\u0026quot;lima nerdctl\u0026quot; ひとつ、非互換に気づいた。nerdctl では run で -d と --rm が同時に使えない。ゴミを残したくないからよく使うんだけどなあ。\n$ lima nerdctl run -d --rm -p 8080:80 nginx:alpine FATA[0000] flag -d and --rm cannot be specified together exit status 1 docker build / docker push # lima コマンド実行時の working dictory が仮想マシン内にも引き継がれるのでホームディレクトリ配下であれば docker build -t xxx . と実行しても、仮想マシン内の同じディレクトリで実行されるので　build が可能となっているようだ。\ndocker login (nerdctl login) も当然仮想マシン内でログインするのだが、/Users/teraoka は Read-Only だし、Linux でのホームディレクトリは /home/teraoka.linux になっており、ログイン情報は /home/teraoka.linux/.docker/config.json に書かれます。keychain は使えない。\nログインしてしまえば docker push も可能です。\ndocker-compose # 試していませんが nerdctl には compose サブコマンドがあります。\n$ lima nerdctl compose NAME: nerdctl compose - Compose USAGE: nerdctl compose command [command options] [arguments...] COMMANDS: up Create and start containers logs View output from containers. build Build or rebuild services down Remove containers and associated resources help, h Shows a list of commands or help for one command OPTIONS: --file value, -f value Specify an alternate compose file --project-directory value Specify an alternate working directory --project-name value, -p value Specify an alternate project name --env-file value Specify an alternate environment file --help, -h show help (default: false) 次のようにして使えるようです。本家 docker-compose も新しいバージョンは docker のサブコマンドになっていますね。\n$ lima nerdctl compose -f ./examples/compose-wordpress/docker-compose.yaml up nerdctl の GitHub repository に example があります。\nまとめ # Minikube や Docker Desktop との違いとして nerdctl コマンドは Linux 仮想マシン内にあるというところが大きいですね。ここを意識できていればまずまず使えるんじゃないでしょうか。\nDocker Desktop の volume mount 機能は便利ですね。\n","date":"2021年9月19日","permalink":"/2021/09/lima/","section":"Posts","summary":"Docker Desktop の代わりに docker cli + Minikube ってのを試しただけど、Kubernetes は docker を非推奨にしてるし、kubernetes は不要な場合は無駄が多いしなあ\u0026","title":"Lima で nerdctl"},{"content":"Docker のおかげで今の便利なコンテナがあります、ありがとうございます。でもどうなるのかやってみたかったんです。\nGoodbye Docker Desktop, Hello Minikube! を参考に試してみます。\n環境は Intel Mac の Big Sur です。Mac で docker を使うには docker daemon を稼働させるための Linux 仮想マシンが必要で、Docker Desktop はそこをうまいことやってくれています。その docker daemon の稼働する仮想マシンとして minikube 用のものを活用しようという話です。\nDocker Desktop の Uninstall # アプリとしては消したけどでっかい VM のイメージは残ってるから、もう再度インストールすることもないという状況になったら削除しよう。\n~/Library/Containers/com.docker.docker/Data/vms/0/data/Docker.raw\nDocker CLI の Install # $ brew install docker 必要なら docker-compose も。ただし、今時は docker のサブコマンドになっているので docker-compose コマンドをインストールする必要はなさそう。\n$ brew install docker-compose hyperkit の Install (しなかった) # hyperkit ってインストール済みだと思ってたけど消えてた(?) Docker Desktop と共に消えてたりするのかな？\n$ brew install hyperkit App Store から Xcode をインストールしろと言われた。ダウンロードに時間がかかる\u0026hellip; が、途中で VirtualBox でも問題なくね？ もうインストール済みだし、と思って中断した。以前に Minikube を使ってた時も VirtualBox だったし。\nMinikube の Install # Homebrew にもあるけど asdf (www.mod.go.jp/asdf/ じゃないよ) でもインストールできます。\n$ asdf plugin add minikube $ asdf install minikube latest $ asdf global minikube latest $ minikube version minikube version: v1.23.1 commit: 84d52cd81015effbdd40c632d9de13db91d48d43 Minikube VM 起動 # $ minikube start 😄 minikube v1.23.1 on Darwin 11.6 ▪ MINIKUBE_ACTIVE_DOCKERD=minikube ✨ Automatically selected the virtualbox driver 💿 Downloading VM boot image ... \u0026gt; minikube-v1.23.1.iso.sha256: 65 B / 65 B [-------------] 100.00% ? p/s 0s \u0026gt; minikube-v1.23.1.iso: 225.22 MiB / 225.22 MiB [] 100.00% 8.65 MiB p/s 26s 👍 Starting control plane node minikube in cluster minikube 💾 Downloading Kubernetes v1.22.1 preload ... \u0026gt; preloaded-images-k8s-v13-v1...: 511.84 MiB / 511.84 MiB 100.00% 8.43 MiB 🔥 Creating virtualbox VM (CPUs=2, Memory=4000MB, Disk=20000MB) ... 🐳 Preparing Kubernetes v1.22.1 on Docker 20.10.8 ... ▪ Generating certificates and keys ... ▪ Booting up control plane ... ▪ Configuring RBAC rules ... ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5 🔎 Verifying Kubernetes components... 🌟 Enabled addons: storage-provisioner, default-storageclass 🏄 Done! kubectl is now configured to use \u0026quot;minikube\u0026quot; cluster and \u0026quot;default\u0026quot; namespace by default 昔の設定が ~/.minikube に残ってたらいくつかエラーが出てたけど消してやり直したらきれいになった。\ndocker コマンドが使えるようにする # mac 上で docker コマンドが使えるようにする。minikube docker-env コマンドの出力はこんな感じ。\n$ minikube docker-env export DOCKER_TLS_VERIFY=\u0026quot;1\u0026quot; export DOCKER_HOST=\u0026quot;tcp://192.168.99.104:2376\u0026quot; export DOCKER_CERT_PATH=\u0026quot;/Users/teraoka/.minikube/certs\u0026quot; export MINIKUBE_ACTIVE_DOCKERD=\u0026quot;minikube\u0026quot; # To point your shell to minikube's docker-daemon, run: # eval $(minikube -p minikube docker-env) eval で shell に読み込ませる\n$ eval $(minikube docker-env) まあ、わかってたことだけど、ネットワーク越しに minikube VM (Virtual Machine) 上の docker daemon を使うってだけなのよね。\n$ docker info Client: Context: default Debug Mode: false Server: Containers: 15 Running: 14 Paused: 0 Stopped: 1 Images: 10 Server Version: 20.10.8 Storage Driver: overlay2 Backing Filesystem: extfs Supports d\\_type: true Native Overlay Diff: true userxattr: false Logging Driver: json-file Cgroup Driver: systemd Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: io.containerd.runtime.v1.linux runc io.containerd.runc.v2 Default Runtime: runc Init Binary: docker-init containerd version: e25210fe30a0a703442421b0f60afac609f950a3 runc version: 4144b63817ebcc5b358fc2c8ef95f7cddd709aa7 init version: de40ad0 Security Options: seccomp Profile: default Kernel Version: 4.19.202 Operating System: Buildroot 2021.02.4 OSType: linux Architecture: x86\\_64 CPUs: 2 Total Memory: 3.753GiB Name: minikube ID: CGNK:YXRA:XBUK:QGIQ:73CZ:RNAV:NFAQ:CILA:UWB2:6SQ7:XTHN:36LK Docker Root Dir: /var/lib/docker Debug Mode: false Registry: https://index.docker.io/v1/ Labels: provider=virtualbox Experimental: false Insecure Registries: 10.96.0.0/12 127.0.0.0/8 Live Restore Enabled: false Product License: Community Engine WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support 普段から minikube を使ってるって人にとってはこれでも良だそうだけど、そうでなければ不必要なものを沢山動かすことになってリソースが勿体無い\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1aeae02d7106 6e38f40d628d \u0026quot;/storage-provisioner\u0026quot; About a minute ago Up About a minute k8s_storage-provisioner_storage-provisioner_kube-system_98d3f7de-f2e3-491e-b876-a49872bcd33c_1 de5bb6423584 8d147537fb7d \u0026quot;/coredns -conf /etc…\u0026quot; About a minute ago Up About a minute k8s_coredns_coredns-78fcd69978-qg8fw_kube-system_7f9697a5-b019-41fd-abe7-e2d07f2eb6ca_0 b8cde6bf5ee5 k8s.gcr.io/pause:3.5 \u0026quot;/pause\u0026quot; About a minute ago Up About a minute k8s_POD_coredns-78fcd69978-qg8fw_kube-system_7f9697a5-b019-41fd-abe7-e2d07f2eb6ca_0 97986df4b04b 36c4ebbc9d97 \u0026quot;/usr/local/bin/kube…\u0026quot; About a minute ago Up About a minute k8s_kube-proxy_kube-proxy-6s82v_kube-system_32554fb1-b716-4e8b-b1ec-ae4ed5385b9d_0 4068a36aea92 k8s.gcr.io/pause:3.5 \u0026quot;/pause\u0026quot; About a minute ago Up About a minute k8s_POD_kube-proxy-6s82v_kube-system_32554fb1-b716-4e8b-b1ec-ae4ed5385b9d_0 312b76f0c855 k8s.gcr.io/pause:3.5 \u0026quot;/pause\u0026quot; About a minute ago Up About a minute k8s_POD_storage-provisioner_kube-system_98d3f7de-f2e3-491e-b876-a49872bcd33c_0 9eacdd6b851a aca5ededae9c \u0026quot;kube-scheduler --au…\u0026quot; 2 minutes ago Up 2 minutes k8s_kube-scheduler_kube-scheduler-minikube_kube-system_6fd078a966e479e33d7689b1955afaa5_0 bf5f741cd675 6e002eb89a88 \u0026quot;kube-controller-man…\u0026quot; 2 minutes ago Up 2 minutes k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_e8c1f1261ee630eae00126001a8e22df_0 2836454dfc0c f30469a2491a \u0026quot;kube-apiserver --ad…\u0026quot; 2 minutes ago Up 2 minutes k8s_kube-apiserver_kube-apiserver-minikube_kube-system_be6473e3f27b6fbc4123c6b2d7489c82_0 cac78141b312 004811815584 \u0026quot;etcd --advertise-cl…\u0026quot; 2 minutes ago Up 2 minutes k8s_etcd_etcd-minikube_kube-system_651082f7ca1d6843bfefc4a589200e75_0 7f0ce289318c k8s.gcr.io/pause:3.5 \u0026quot;/pause\u0026quot; 2 minutes ago Up 2 minutes k8s_POD_kube-controller-manager-minikube_kube-system_e8c1f1261ee630eae00126001a8e22df_0 71a468581e7e k8s.gcr.io/pause:3.5 \u0026quot;/pause\u0026quot; 2 minutes ago Up 2 minutes k8s_POD_kube-apiserver-minikube_kube-system_be6473e3f27b6fbc4123c6b2d7489c82_0 cac60b0f8152 k8s.gcr.io/pause:3.5 \u0026quot;/pause\u0026quot; 2 minutes ago Up 2 minutes k8s_POD_etcd-minikube_kube-system_651082f7ca1d6843bfefc4a589200e75_0 19a35095f3d2 k8s.gcr.io/pause:3.5 \u0026quot;/pause\u0026quot; 2 minutes ago Up 2 minutes k8s_POD_kube-scheduler-minikube_kube-system_6fd078a966e479e33d7689b1955afaa5_0 コンテナを実行してみる # docker pull もできる。もちろん image は minikube の VM 内に置かれる\n$ docker pull nginx:latest latest: Pulling from library/nginx a330b6cecb98: Pull complete e0ad2c0621bc: Pull complete 9e56c3e0e6b7: Pull complete 09f31c94adc6: Pull complete 32b26e9cdb83: Pull complete 20ab512bbb07: Pull complete Digest: sha256:853b221d3341add7aaadf5f81dd088ea943ab9c918766e295321294b035f3f3e Status: Downloaded newer image for nginx:latest docker.io/library/nginx:latest port の expose も出来るし、ブラウザでアクセスすることもできる。ただし、localhost ではない。\n$ docker run -d --rm -p 80:80 nginx:latest b5ebaf240d5d53d733e0b3346b4d8c822fedc1385a7338c8b433df9010289572 $ docker ps | grep b5ebaf2 b5ebaf240d5d nginx:latest \u0026quot;/docker-entrypoint.…\u0026quot; 18 seconds ago Up 18 seconds 0.0.0.0:80-\u0026gt;80/tcp frosty_allen minikube VM の IP アドレスを確認\n$ minikube ip 192.168.99.105 $ curl http://192.168.99.105/ \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026quot;http://nginx.org/\u0026quot;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026quot;http://nginx.com/\u0026quot;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 元記事では ingress-dns や metallb addon についても触れられている。\nところで、minikube 内の Pod から Host (ここでは mac) にアクセスするために host.minikube.internal という名前が使えるようになってるんですね。 VM の /etc/hosts と CoreDNS の設定ファイル Corefile に書かれています、だから docker コマンドでデプロイしたコンテナでは使えない。 (Host access)\nDocker Desktop では host.docker.internal という名前で Host にアクセスできていました。Minikube + Docker CLI では自分で --add-host を使って指定する必要があります。ここで指定するアドレスは上記の host.minikube.internal で指定されてるアドレスです。\nCredential Helpder # Docker Desktop を削除したことで docker-credential-helper も消えてしまっており、docker push は denied: requested access to the resource is denied となり、 docker login もエラーになってしまいました。\n$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: yteraoka Password: Error saving credentials: error storing credentials - err: exec: \u0026quot;docker-credential-desktop\u0026quot;: executable file not found in $PATH, out: `` これは一旦 ~/.docker/config.json を削除することで再度 docker login が可能になるのですが、これでは base64 されただけのパスワードが json ファイルに書かれてしまいます。これは docker-credential-helper を Homebrew でインストールすれば docker-credential-osxkeychain がキーチェーンにアクセスしてくれます。Docker Desktop の場合は config.json 内の credsStore は desktop でしたが、docker-credential-helper では osxkeychain になっています。ということなのでファイルごと消さなくても credsStore の値を書き換えれば良いのかも。でも keychain へのアクセス権を与えるのは慎重に。\n$ **brew install docker-credential-helper** Minikube の停止について # 元記事では Hyperkit を使った場合、minikube stop で停止するとコンテナイメージや Persistent Volume が消えるから minikube pause / minikube unpause を使えとありますが、VirtualBox の場合は stop しても消えません。minikube delete したら VM のイメージごと削除されます。~/.minikube/cache/images/ 配下に image を置いておくと VM イメージを削除しても残ってます。\nHost のディレクトリを docker コンテナでマウントしたい # Docker を使って開発したりする場合はこの機能は必須ですね。Minikube を使う場合はまず minikube VM にマウントさせる必要があります。minikube mount コマンドを使うことで指定したディレクトリに minikube VM からアクセスすることができるようになります。\n$ minikube mount $HOME:$HOME 📁 Mounting host path /Users/teraoka into VM as /Users/teraoka ... ▪ Mount type: ▪ User ID: docker ▪ Group ID: docker ▪ Version: 9p2000.L ▪ Message Size: 262144 ▪ Permissions: 755 (-rwxr-xr-x) ▪ Options: map[] ▪ Bind Address: 192.168.99.1:62416 🚀 Userspace file server: ufs starting ✅ Successfully mounted /Users/teraoka to /Users/teraoka 📌 NOTE: This process must stay alive for the mount to be accessible ... このコマンドは実行中はマウントされているという状態で、Ctrl-C で停止すると次のように表示されるわけですが、なぜかこの状態でも VM 上からアクセスできちゃいました？？？\n🔥 Unmounting /Users/teraoka ... ❌ Exiting due to MK_INTERRUPTED: Received interrupt signal minikube stop \u0026amp;\u0026amp; minikube start してもマウントした状態は維持されてました。謎\nVM 内で mount コマンドで確認してみると vboxsf というやつだった\n$ mount | grep /Users Users on /Users type vboxsf (rw,nodev,relatime) VM 上で単に umount コマンドを実行するだけではダメで\n$ sudo umount /Users umount.nfs: Users on /Users is not an NFS filesystem -t vboxfs と -i を指定する必要があった。-i は初めて使った。--internal-only らしい。(Don\u0026rsquo;t call the /sbin/umount. helper even if it exists. By default /sbin/umount. helper is called if one exists.)\n$ sudo umount -t vboxsf -i /Users が、その後小さなディレクトリをマウントしたら minikube mount を止めた時点でマウントは解除されたし、VM からの見え方も違ってた\n$ mount | grep /html 192.168.99.1 on /html type 9p (rw,relatime,sync,dirsync,dfltuid=1000,dfltgid=1000,access=any,msize=65536,trans=tcp,noextend,port=62602) で、docker でのマウントだが、minikube mount でマウントしたディレクトリ内を docker run コマンドの -v オプションで指定すれば機体の動作をしてくれた\nめでたしめでたし # しかし、Kubernetes が runtime として docker を非推奨としているので minikube で docker が使われなくなる日が来るかもしれないということは気に留めておく必要がありそう。containerd 変わったら nerdctl が使えるようになるかな？\n参考資料 #  Goodbye Docker Desktop, Hello Minikube! Docker Desktop for Macの裏で動いている仮想マシン(Linux)に直接アクセスする方法 Virtualbox の 共有フォルダ を マウント/アンマウント したい  ","date":"2021年9月18日","permalink":"/2021/09/replace-docker-desktop-with-minikube/","section":"Posts","summary":"Docker のおかげで今の便利なコンテナがあります、ありがとうございます。でもどうなるのかやってみたかったんです。 Goodbye Docker Desktop, Hello Minikube! を参考に試してみます。 環境","title":"Docker Desktop の代わりに Minikube を使ってみる"},{"content":"","date":"2021年9月18日","permalink":"/tags/minikube/","section":"Tags","summary":"","title":"minikube"},{"content":"以前、caddy について調べてて発見した smallstep でクライアント証明書発行を便利にできないかなということで調査です。(Hashicorp Vault でもできるっぽいけど用途的にわざわざクラスタ組むの面倒だなあって)\nConnect your identity provider and issue X.509 certificates for user authentication to services というドキュメントを参考に進めます。前提に Getting Started でセットアップした step-ca が起動していることとあるので、まずはこちらから。\nGetting Started # まずは、step と step-ca コマンドをインストール。Homebrew でもインストールできます。\nCA の初期化 # ファイルの保存場所をカレントディレクトリ配下にしておきます。デフォルトでは $HOME/.step となっています。\nexport STEPPATH=./step step ca init で root と intermediate 証明書を作成します。$STEPPATH/config/defaults.json と $STEPPATH/config/ca.json というファイルも生成されます。defaults.json の方は step コマンド用で、ca.json は CA 用の設定で step-ca が読み込みます。\n$ step ca init ✔ What would you like to name your new PKI? (e.g. Smallstep): myself ✔ What DNS names or IP addresses would you like to add to your new CA? (e.g. ca.smallstep.com[,1.1.1.1,etc.]): localhost ✔ What address will your new CA listen at? (e.g. :443): 127.0.0.1:8443 ✔ What would you like to name the first provisioner for your new CA? (e.g. you@smallstep.com): user@example.com ✔ What do you want your password to be? [leave empty and we'll generate one]: ✔ Password: 00;zj3z3Z/|\u0026quot;8gW_7aaAO,f$n{l90gxD Generating root certificate... all done! Generating intermediate certificate... all done! ✔ Root certificate: ./step/certs/root_ca.crt ✔ Root private key: ./step/secrets/root_ca_key ✔ Root fingerprint: 9d6e69dbb2915e5bd8f794d6b4d992b4fba5273ffcaa85c0db11514b650db1d7 ✔ Intermediate certificate: ./step/certs/intermediate_ca.crt ✔ Intermediate private key: ./step/secrets/intermediate_ca_key ✔ Database folder: step/db ✔ Default configuration: ./step/config/defaults.json ✔ Certificate Authority configuration: ./step/config/ca.json Your PKI is ready to go. To generate certificates for individual services see 'step help ca'. FEEDBACK 😍 🍻 The step utility is not instrumented for usage statistics. It does not phone home. But your feedback is extremely valuable. Any information you can provide regarding how you’re using `step` helps. Please send us a sentence or two, good or bad: feedback@smallstep.com or join https://github.com/smallstep/certificates/discussions. What DNS names or IP addresses would you like to add to your new CA? (e.g. ca.smallstep.com[,1.1.1.1,etc.]) で localhost と指定している名前は step-ca サーバーのサーバ証明書の Subject Alternative Name として使われます。これは step-ca 起動時、リロード時などに都度発行されるようで config/ca.json の dnsNames を後から書き換えても大丈夫。config/defaults.json の ca-url のホスト名がこの dnsName に含まれるようにしておく必要があります。\nWhat address will your new CA listen at? (e.g. :443) は step-ca が listen するアドレスです。\nCertificate Authority の実行 (step-ca の起動) # step ca init 時に指定した、もしくは自動生成された private key のパスワードの入力を求められます。自動起動のためにはパスワードを書いたファイルの path を --password-file で指定します。\n$ step-ca $(step path)/config/ca.json badger 2021/04/10 00:03:17 INFO: All 0 tables opened in 0s Please enter the password to decrypt ./step/secrets/intermediate_ca_key: 2021/04/10 00:03:27 Serving HTTPS on 127.0.0.1:8443 ... 試しに localhost のサーバ証明書を発行してみる # step ca certificate コマンドで証明書の発行ができます。--san で任意の数の Subject Alternative Name を設定できます(複数回指定可能)。--not-after=1h とすると有効期間を1時間にできます。\n$ step ca certificate localhost srv.crt srv.key ✔ Provisioner: user@example.com (JWK) [kid: JAffF2XYO3U1qpRkLzSDDqZR8SSZPwGf23i6xO9dyAg] ✔ Please enter the password to decrypt the provisioner key: (init 時の password を入力) ✔ CA: https://localhost:8443 ✔ Certificate: srv.crt ✔ Private Key: srv.key パスワード入力をコマンドラインで指定するためには --provisioner と --provisioner-password を指定します。user@example.com というのは step ca init 時に first provisioner として指定した値です。\nstep ca certificate example.com example.crt example.key \\ --provisioner user@example.com --provisioner-password-file password.txt 生成された証明書ファイルです。\n$ cat srv.crt -----BEGIN CERTIFICATE----- MIICJDCCAcqgAwIBAgIRALOhvX+yzcdRtESGXIt0nSEwCgYIKoZIzj0EAwIwMjEP MA0GA1UEChMGbXlzZWxmMR8wHQYDVQQDExZteXNlbGYgSW50ZXJtZWRpYXRlIENB MB4XDTIxMDQwOTE1MjA0OFoXDTIxMDQxMDE1MjE0OFowFDESMBAGA1UEAxMJbG9j YWxob3N0MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAER5Ss0fIkHglM0Gy7pA8o Th/uQbbDIKZ+w8Y6bOhxixe0Wf11ctpz2hNhX2RXMM/Fjtop83MG2tNGPWa9uYMd dqOB3jCB2zAOBgNVHQ8BAf8EBAMCB4AwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsG AQUFBwMCMB0GA1UdDgQWBBQ/Xp+RQ1rNKwymALq/NBNM5LDNHzAfBgNVHSMEGDAW gBRkV6X/rjHYGF3SCaj8MdmDlFBw1jAUBgNVHREEDTALgglsb2NhbGhvc3QwVAYM KwYBBAGCpGTGKEABBEQwQgIBAQQQdXNlckBleGFtcGxlLmNvbQQrSkFmZkYyWFlP M1UxcXBSa0x6U0REcVpSOFNTWlB3R2YyM2k2eE85ZHlBZzAKBggqhkjOPQQDAgNI ADBFAiEA5JbgGVX7M9oroBu/DHMxWhpRKy0T8WkeekItnCaJYFQCIBXExx5GaCiQ ZS2tubNai9HHyx2OAmkVFj95yP5KoWmt -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- MIIBwTCCAWegAwIBAgIRAMsNU1wV7Pe2Bu8BzBmqjW0wCgYIKoZIzj0EAwIwKjEP MA0GA1UEChMGbXlzZWxmMRcwFQYDVQQDEw5teXNlbGYgUm9vdCBDQTAeFw0yMTA0 MDkxNDU4MjVaFw0zMTA0MDcxNDU4MjVaMDIxDzANBgNVBAoTBm15c2VsZjEfMB0G A1UEAxMWbXlzZWxmIEludGVybWVkaWF0ZSBDQTBZMBMGByqGSM49AgEGCCqGSM49 AwEHA0IABFB9zHmlOnlSffs6FkVmXhP1TTl3WlsLoaoDcXbQqH3OkoB2uNXDljIu ufKlTaeMG3pArn13wcvjs5FBAGqlzCWjZjBkMA4GA1UdDwEB/wQEAwIBBjASBgNV HRMBAf8ECDAGAQH/AgEAMB0GA1UdDgQWBBRkV6X/rjHYGF3SCaj8MdmDlFBw1jAf BgNVHSMEGDAWgBRNrE8AQGvyj8adZxT9j799lSGP2TAKBggqhkjOPQQDAgNIADBF AiAYZz8AUdLe6Qa4rpraG048UVkIt8pPtPaB/pzYaaXWIgIhALLAAjOUE8rM7cp2 +o14WqC5T6vcb/V/+Fh1ME85c7GZ -----END CERTIFICATE----- \u0026ldquo;myself Intermediate CA\u0026rdquo; によって署名されています。myself というのは step ca init 時に What would you like to name your new PKI? (e.g. Smallstep) で指定した値です。\n生成した証明書の中身は次のようになっています。\nCertificate: Data: Version: 3 (0x2) Serial Number: b3:a1:bd:7f:b2💿c7:51:b4:44:86:5c:8b:74:9d:21 Signature Algorithm: ecdsa-with-SHA256 Issuer: O=myself, CN=myself Intermediate CA Validity Not Before: Apr 9 15:20:48 2021 GMT Not After : Apr 10 15:21:48 2021 GMT Subject: CN=localhost Subject Public Key Info: Public Key Algorithm: id-ecPublicKey Public-Key: (256 bit) pub: 04:47:94:ac:d1:f2:24:1e:09:4c:d0:6c:bb:a4:0f: 28:4e:1f:ee:41:b6:c3:20:a6:7e:c3:c6:3a:6c:e8: 71:8b:17:b4:59:fd:75:72:da:73:da:13:61:5f:64: 57:30:cf:c5:8e:da:29:f3:73:06:da:d3:46:3d:66: bd:b9:83:1d:76 ASN1 OID: prime256v1 NIST CURVE: P-256 X509v3 extensions: X509v3 Key Usage: critical Digital Signature X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Subject Key Identifier: 3F:5E:9F:91:43:5A:CD:2B:0C:A6:00:BA:BF:34:13:4C:E4:B0:CD:1F X509v3 Authority Key Identifier: keyid:64:57:A5:FF:AE:31:D8:18:5D:D2:09:A8:FC:31:D9:83:94:50:70:D6 X509v3 Subject Alternative Name: DNS:localhost 1.3.6.1.4.1.37476.9000.64.1: 0B.....user@example.com.+JAffF2XYO3U1qpRkLzSDDqZR8SSZPwGf23i6xO9dyAg Signature Algorithm: ecdsa-with-SHA256 30:45:02:21:00:e4:96:e0:19:55:fb:33:da:2b:a0:1b:bf:0c: 73:31:5a:1a:51:2b:2d:13:f1:69:1e:7a:42:2d:9c:26:89:60: 54:02:20:15:c4:c7:1e:46:68:28:90:65:2d:ad:b9:b3:5a:8b: d1:c7:cb:1d:8e:02:69:15:16:3f:79:c8:fe:4a:a1:69:ad この後、テスト用の HTTPS サーバーを書いて実行し、curl でアクセスしてみるという手順になっています。サーバーは Intermediate 証明書も返してくるので root 証明書があれば検証可能です。これは [step ca root](https://smallstep.com/docs/step-cli/reference/ca/root) コマンドで取得可能です。step ca root root.crt などとファイル名も指定すればファイルに書き出されます。\n$ step ca root root.crt The root certificate has been saved in root.crt. Personal certificates via OAuth OpenID Connect # それでは元に戻って、OIDC 連携を進めます。\nOIDC Provisioner の追加 # step ca provisioner add Google --type oidc --ca-config $(step path)/config/ca.json \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ --configuration-endpoint https://accounts.google.com/.well-known/openid-configuration \\ --domain example.com \\ --listen-address :10000 --listen-address はドキュメントの例にありませんでしたが、この provisioner の場合、[step oauth](https://smallstep.com/docs/step-cli/reference/oauth) コマンドが手元で実行されて、それが Google での認証後の redirect を待ち受けます。port 指定がないと毎回違う port で listen するため Google 側の OAuth 設定の Authorized redirect URIs で指定することができません。\n追加した後は step-ca プロセスに SIGHUP を送って (pkill -HUP -x step-ca) リロードさせる必要があります。step-ca の起動時にパスワードをファイルで渡していない場合、リロードに失敗してしまいます。その場合は再起動させましょう。\nパスワードが指定されていなくてリロードに失敗した場合のログ\nPlease enter the password to decrypt /Users/teraoka/work/20210409-smallstep/step/secrets/intermediate_ca_key: 2021/04/10 09:43:00 Reload failed because the CA with new configuration could not be initialized. 2021/04/10 09:43:00 Continuing to run with the original configuration. 2021/04/10 09:43:00 You can force a restart by sending a SIGTERM signal and then restarting the step-ca. 2021/04/10 09:43:00 error reloading server: x509: decryption password incorrect error decrypting /Users/teraoka/work/20210409-smallstep/step/secrets/intermediate_ca_key 設定は $(step path)/config/ca.json の authority.provisioners に追加されています。設定内容の詳細は \u0026ldquo;OAUTH/OIDC SINGLE SIGN-ON\u0026rdquo; にあります。\n追加した provisioner を削除するには次のようにします。\nstep ca provisioner remove Google --type oidc 証明書の発行 # 次のコマンドを実行すると provisioner の選択メニューが現れます。\nstep ca certificate teraoka@example.com personal.crt personal.key ここで Google を指定するとブラウザで OAuth の認証・認可に進みます。URL も表示されるため、開かれたブラウザが期待のものでなかった場合はその URL をコピペして開くと良いでしょう。\n$ step ca certificate teraoka@example.com teraoka.crt teraoka.key Use the arrow keys to navigate: ↓ ↑ → ← What provisioner key do you want to use? ▸ user@example.com (JWK) [kid: JAffF2XYO3U1qpRkLzSDDqZR8SSZPwGf23i6xO9dyAg] Google (OIDC) [client: 123456789012-aiquu3ehoh6aix7diezoh2Aekeepe4ah.apps.googleusercontent.com] 指定したメールアドレスと Google アカウントのメールアドレスが異なる場合は拒否されます。\ntoken email 'authenticated@example.com' and argument 'teraoka@example.com' do not match oidc provisioner には admin メールアドレスを指定する設定があり、admin に指定されたメールアドレスでログインすれば別のメールアドレスでの証明書も発行できます。\n作成された証明書の確認。\n$ openssl x509 -text -noout -in personal.crt Certificate: Data: Version: 3 (0x2) Serial Number: 74:d9:d9:ec:ed:2a:fb:fd:ce:fb:21:4c:e6:03:33:c4 Signature Algorithm: ecdsa-with-SHA256 Issuer: O=myself, CN=myself Intermediate CA Validity Not Before: Apr 10 01:37:03 2021 GMT Not After : Apr 11 01:38:03 2021 GMT Subject: CN=XXXXXXXXXXXXXXXXXXXXX Subject Public Key Info: Public Key Algorithm: id-ecPublicKey Public-Key: (256 bit) pub: 04:32:71:c4:87:a4:51:78:6f:67:fe:3d:8b:61:49: 54:67:43:2c:0d:13:20:95:5c:51:a4:0b:23:40:29: 44:b2:03:3d:a1:90:cf:f2:a9:8a:2b:c4:c9:8f:b1: 11:e6:9b:30:87:23:14:e8:e6:da:8b:65:eb:70:d8: 63:07:56:cf:9b ASN1 OID: prime256v1 NIST CURVE: P-256 X509v3 extensions: X509v3 Key Usage: critical Digital Signature X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Subject Key Identifier: B0:AC:D7:9A:83:5F:0D:92:EC:B7:80:66:F1:90:F0:7C:0A:ED:37:7F X509v3 Authority Key Identifier: keyid:64:57:A5:FF:AE:31:D8:18:5D:D2:09:A8:FC:31:D9:83:94:50:70:D6 X509v3 Subject Alternative Name: email:teraoka@example.com, URI:https://accounts.google.com#XXXXXXXXXXXXXXXXXXXXX 1.3.6.1.4.1.37476.9000.64.1: 0U.....Google.H123456789012-aiquu3ehoh6aix7diezoh2Aekeepe4ah.apps.googleusercontent.com Signature Algorithm: ecdsa-with-SHA256 30:45:02:21:00:d5:5d:bd:31:9e:8c:ba:63:76:74:52:1f:5d: 47:a1:f3:65:e3:64:f4:d5:74:ba:9a:cb:0a:a0:a8:85:d2:cb: 76:02:20:3d:65:d6🇩🇪8f:bf:ce:93:33:c6:49:19:66:6a:4c: 80:d9:51:11:43:cb:3f:ff:a4:8a🇩🇪32:d2🇩🇪2a:77:bb openssl コマンドを使わなくても step certificate inspect personal.crt [--short] で確認できました。\n証明書の Revoke # 紛失したり、不要になった証明書は失効させる必要があります。ただし、現状 smallstep は CRL や OCSP をサポートしておらず、revoke された証明書は更新できなくなるだけです。\nstep ca revoke \u0026lt;serial-number\u0026gt; step ca revoke --cert certfile --key keyfile Serial Number は openssl コマンドで取得できる16進のやつではダメなので step certificate inspect で取得します。step certificate inspect personal.crt --format json | jq -r .serial_number\n証明書と、秘密鍵のファイルがあるならそれを指定することでも失効可能。\n[step ca revoke](https://smallstep.com/docs/step-cli/reference/ca/revoke) コマンド実行時にも provisioner の選択メニューが表示されるのですが、ここでは Google じゃなくて JWK の方を選択して、init 時のパスワードを入力する必要がありました。Google の方を使うと token subject と serial number が一致しないというエラーになって revoke できませんでした。後でわかりましたが、Google provisioner の場合は serial number ではなくて、代わりに CN の値を指定すれば Revoke できました。また、--cert--key を指定した場合は認証が不要でした。鍵で認証できるからでしょうね。\ntoken subject '101631527727627020288' and serial number '155321595983482556839109003173565772740' do not match リモートサーバー # 証明書発行の流れはわかりましたが、複数人で使うためにはサーバーを用意して使うことになります。全部手元にある環境で試していたので分離してより理解を深めてみます。\nGCE の debian 10 で nginx + Let\u0026rsquo;s Encrypt の背後に step-ca を配置する構成にしてみます。\nstep (CLI) --(https)--\u0026gt; nginx (Let's Encrypt) --(https)--\u0026gt; step-ca nginx と Let\u0026rsquo;s Encrypt で TLS な Reverse Proxy をセットアップ # $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y python3-certbot-nginx $ sudo certbot --nginx -d step.teraoka.me --post-hook \u0026quot;/usr/sbin/nginx -s reload\u0026quot; --agree-tos -m メールアドレス Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator nginx, Installer nginx - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Would you be willing to share your email address with the Electronic Frontier Foundation, a founding partner of the Let's Encrypt project and the non-profit organization that develops Certbot? We'd like to send you email about our work encrypting the web, EFF news, campaigns, and ways to support digital freedom. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - (Y)es/(N)o: N Obtaining a new certificate Performing the following challenges: http-01 challenge for step.teraoka.me Waiting for verification... Cleaning up challenges Running post-hook command: /usr/sbin/nginx -s reload Deploying Certificate to VirtualHost /etc/nginx/sites-enabled/default Please choose whether or not to redirect HTTP traffic to HTTPS, removing HTTP access. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: No redirect - Make no further changes to the webserver configuration. 2: Redirect - Make all requests redirect to secure HTTPS access. Choose this for new sites, or if you're confident your site works on HTTPS. You can undo this change by editing your web server's configuration. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate number [1-2] then [enter] (press 'c' to cancel): 2 Redirecting all traffic on port 80 to ssl in /etc/nginx/sites-enabled/default - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Congratulations! You have successfully enabled https://step.teraoka.me You should test your configuration at: !https://www.ssllabs.com/ssltest/analyze.html?d=step.teraoka.me - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/step.teraoka.me/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/step.teraoka.me/privkey.pem Your cert will expire on 2021-07-09. To obtain a new or tweaked version of this certificate in the future, simply run certbot again with the \u0026quot;certonly\u0026quot; option. To non-interactively renew *all* of your certificates, run \u0026quot;certbot renew\u0026quot; - Your account credentials have been saved in your Certbot configuration directory at /etc/letsencrypt. You should make a secure backup of this folder now. This configuration directory will also contain certificates and private keys obtained by Certbot so making regular backups of this folder is ideal. - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le これで /etc/nginx/sites-available/default に https 設定が挿入されます。/etc/cron.d/certbot に自動更新のための設定もあります。\nstep-ca の default port は 4343 だったので /etc/nginx/sites-available/default をさらに編集して / を 127.0.0.1:4343 に proxy するように設定する。\nlocation / { proxy_pass https://127.0.0.1:4343; } 変更を反映させる。\nsudo nginx -s reload step, step-ca コマンドをインストール # curl -LO https://github.com/smallstep/cli/releases/download/v0.15.14/step-cli_0.15.14_amd64.deb sudo dpkg -i step-cli_0.15.14_amd64.deb curl -LO https://github.com/smallstep/certificates/releases/download/v0.15.11/step-ca_0.15.11_amd64.deb sudo dpkg -i step-ca_0.15.11_amd64.deb step-ca 用のユーザーを作成 # sudo useradd -m -s /bin/bash step step ca init # sudo -iu step dd if=/dev/urandom count=1 bs=48 | base64 \u0026gt; password.txt step ca init --name smallstep \\ --address 127.0.0.1:4343 \\ --dns step.teraoka.me,127.0.0.1 \\ --provisioner admin \\ --password-file password.txt Generating root certificate... all done! Generating intermediate certificate... all done! ✔ Root certificate: /home/step/.step/certs/root_ca.crt ✔ Root private key: /home/step/.step/secrets/root_ca_key ✔ Root fingerprint: 926633fc1ef97f58ca359e665ca509093080e4ed1d3a3d289493935eaf528694 ✔ Intermediate certificate: /home/step/.step/certs/intermediate_ca.crt ✔ Intermediate private key: /home/step/.step/secrets/intermediate_ca_key ✔ Database folder: /home/step/.step/db ✔ Default configuration: /home/step/.step/config/defaults.json ✔ Certificate Authority configuration: /home/step/.step/config/ca.json Your PKI is ready to go. To generate certificates for individual services see 'step help ca'. FEEDBACK 😍 🍻 The step utility is not instrumented for usage statistics. It does not phone home. But your feedback is extremely valuable. Any information you can provide regarding how you’re using `step` helps. Please send us a sentence or two, good or bad: feedback@smallstep.com or join https://github.com/smallstep/certificates/discussions. Google OIDC Provisioner の追加 # step ca provisioner add Google --type oidc --ca-config $(step path)/config/ca.json \\ --client-id ${CLIENT_ID} \\ --client-secret ${CLIENT_SECRET} \\ --configuration-endpoint https://accounts.google.com/.well-known/openid-configuration \\ --domain teraoka.me \\ --listen-address :10000 systemd で step-ca が起動するようにする # sudo cat \u0026gt; /etc/systemd/system/step-ca.service \u0026lt;\u0026lt;EOF [Unit] Description=Smallstep CA Server Documentation=https://smallstep.com/docs After=network.target [Service] Type=simple Environment=SETPPATH=/home/step/.step User=step ExecStart=/usr/bin/step-ca /home/step/.step/config/ca.json --password-file /home/step/password.txt Restart=always [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl start step-ca sudo systemctl status step-ca sudo systemctl enable step-ca ネットワーク越しに step コマンドでアクセスする # $(step path)/config/defaults.json の存在しない状態で動作確認します。サーバーの URL を --ca-url で指定します。\n$ step ca health --ca-url https://step.teraoka.me 'step ca health' requires the '--root' flag --root で CA の証明書ファイル指定する必要があるみたいです。ローカルで試していた時は defaults.json で $(step path)/certs/root_ca.crt が指定してありました。これはサーバー側から持ってくる必要があります。サーバー側で step ca root すれば取得できるのでこれを ca.crt として保存して試します。\n$ step ca health --ca-url https://step.teraoka.me --root ca.crt The certificate authority encountered an Internal Server Error. Please see the certificate authority logs for more info. Re-run with STEPDEBUG=1 for more info. Internal Server Error ???\nSTEPDEBUG=1 を設定して実行すると次のメッセージが表示されました。証明書が問題のようです。\nGet \u0026quot;https://step.teraoka.me/health\u0026quot;: x509: certificate signed by unknown authority client.Health; client GET https://step.teraoka.me/health failed (以下省略) が、curl でアクセスした場合は問題ありません。どうやら step コマンドが証明書の検証に使うのが --root で指定したファイルにある証明書だけのようです。よって Let\u0026rsquo;s Encrypt の DST Root CA X3 の証明書を ca.crt に追加しました。\n$ step ca health --ca-url https://step.teraoka.me --root ca.crt ok 成功です。nginx が邪魔だったみたい。\nでもこれでわかりました。step-ca サーバーを別建てにした場合、step コマンドを実行したい端末に root ca の証明書を配る必要がありました。それだけ。Google からの redirect を受ける step oauth コマンドもクライアント側で実行されるため Authorized redirect URIs も localhost のままで大丈夫です。\nroot ca の証明書は fingerprint が分かれば step ca root コマンドで取得可能っぽい。\nデフォルトの証明書の期限は 24h だったので、必要なら config/ca.json 書き換える必要がある。\n残す謎は oidc provisioner の admin 機能がうまく機能しなかったこと。step コマンドには admin 判定処理はないのに、step コマンド側で email と argument が違うと言って弾いちゃってるんだよなあ。\nまあ、admin 相当の人は JWK Provisioner を使えば任意の証明書を発行できるので良いか。担当者の数だけ作れるし。\nstep ca provisioner add admin-1@example.com --create --password-file password.txt この処理は ca.json への追加で、reload も必要なのでリモートからはできない。\nCSR を作って署名する # Google Workspace のアカウントがない人向けの証明書作成においても Private Key のやりとりは避けたいので CSR を送ってもらって署名するという運用にしたいものです。\n[step certificate create](https://smallstep.com/docs/step-cli/reference/certificate/create) に --csr をつけると証明書をいきなり作成するのではなく CSR と Private Key が作成されます。--insecure と --no-password の両方を追加すると Private Key へのパスワードが不要になります。(まあ後からでもなんとでもなりますけど)\nstep certificate create teraoka@example.com teraoka.csr teraoka.key --csr [--insecure --no-password] step certificate inspect で CSR も確認できます。\nCSR に対して署名するには [step ca sign](https://smallstep.com/docs/step-cli/reference/ca/sign) を使います。\nstep ca sign teraoka.csr teraoka.crt ","date":"2021年4月10日","permalink":"/2021/04/step-ca-with-google-oidc/","section":"Posts","summary":"以前、caddy について調べてて発見した smallstep でクライアント証明書発行を便利にできないかなということで調査です。(Hashicorp Vault でもできる","title":"Google 認証でクライアント証明書発行のセルフサービス化"},{"content":"","date":"2021年4月10日","permalink":"/tags/smallstep/","section":"Tags","summary":"","title":"smallstep"},{"content":"","date":"2021年4月10日","permalink":"/tags/tls/","section":"Tags","summary":"","title":"TLS"},{"content":"healthchecks.io が大変便利なので Self-hosted なサーバーを用意して、設定を terraform で管理したいなあということがありまして、terraform-provider-healthchecksio の接続先サーバーを指定可能にしようと思いました。provider のコードを編集して build するところまではすぐに出来たのですが、このバイナリを terraform からどうやって使うのかな？でハマったのでメモです。\nTerraform 0.14 から ~/.terraformrc の provider_installation 内に dev_overrides という設定を書くことができるようになっていました。(Development Overrides for Provider Developers)\n~/.terraformrc に次の設定を入れておけば、指定したディレクトリ直下にあるバイナリを使ってくれます。場所はどこでも良いですが、ここで go build すればバイナリが生成されるのでそのまま使えるようにここを指定してみました。\nprovider_installation { dev_overrides { \u0026#34;kristofferahl/healthchecksio\u0026#34; = \u0026#34;/Users/teraoka/ghq/github.com/yteraoka/terraform-provider-healthchecksio\u0026#34; } direct {} } terraform init では次のような Warning が表示されます。\n% terraform init Initializing the backend... Initializing provider plugins... - Finding latest version of kristofferahl/healthchecksio... - Using kristofferahl/healthchecksio v1.7.0 from the shared cache directory Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \u0026quot;terraform init\u0026quot; in the future. Warning: Provider development overrides are in effect The following provider development overrides are set in the CLI configuration: - kristofferahl/healthchecksio in /Users/teraoka/ghq/github.com/yteraoka/terraform-provider-healthchecksio Skip terraform init when using provider development overrides. It is not necessary and may error unexpectedly. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026quot;terraform plan\u0026quot; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. ところで、この機能は 0.14 から使えるようになったものなので、0.13 など古いバージョンではエラーになります。\nThere are some problems with the CLI configuration: Error: Invalid provider_installation method block Unknown provider installation method \u0026quot;dev_overrides\u0026quot; at X:Y. As a result of the above problems, Terraform may not behave as intended. 以前、elasticsearch provider を使う場合は README にあるように ~/.terraformrc の providers に書いていました。\nproviders { elasticsearch = \u0026#34;$HOME/.terraform.d/plugins/terraform-provider-elasticsearch.v1.3.0\u0026#34; } この providers 設定は 0.13 から非推奨になっているようです。\n0.13 の場合はどう設定すれば手元のバイナリを指定して実行できるのだろう？\n","date":"2021年4月4日","permalink":"/2021/04/terraform-dev_overrides/","section":"Posts","summary":"healthchecks.io が大変便利なので Self-hosted なサーバーを用意して、設定を terraform で管理したいなあということがありまして、terraform-provider-health","title":"Terraform でカスタム provider を使うための dev_overrides 設定"},{"content":"我が家では古いラズパイを VPN Server としてや、定期的に GoogleHome を喋らせたり、Pi-hole サーバーとして使ってたりするのですが、たまにお亡くなりになっていることがあります。あれ？何か DNS がおかしいな、とか、お喋りしないなということで気づいて電源を入れ直していたわけですが、やっぱり異変には早めに気づいて対処したいなと思い healthchecks.io を使って監視することにしました。\n家で常時起動しているサーバー的なものはこのラズパイだけなので他の機器から監視することはできなかったため、cron で healthchecks.io に対して curl でハートビート的なアクセスをして、一定期間音沙汰がないと通知がくる仕組みです。\nhealthchecks.io の料金プランです。なんと、Hobbyist プランなら無料です。ステキ！\n healthchecks.io は定期実行する処理がきちんと実行されているかどうかを確認して通知してくれるサービスです。通知先も Slack, Mattermost, E-mail はもちろん LINE, PagerDuty, Discord, Webhook などなど沢山あります。\nhealthchecks.io の API に対して cron job などの成功、失敗を連携することで失敗時の通知や、一定期間アクセスがない場合の通知が受けられるというのがメインの機能ですが、数はプランによるものの過去の履歴も確認できますし、job の開始時と終了時に連携すればかかった時間も確認できるし、job の出力などを送ることも可能です。\ncurl でアクセスするだけでも使えますが、runitor を使えばこれらの機能を簡単に使えます。\nhealthchecks のコードは GitHub で公開されているため、セルフホストすることも可能ですし、Terraform provider まであるみたいです。\nこの記事を書いていて、このブログのバックアップスクリプトが shell script で結果を slack に垂れ流しているのを runitor で置き換えようと思いました。\nそれから、ラズパイが死んでいることに気づいても、電源を入れ直すのが面倒でした。設置場所まで行き、棚の扉を開けて引っ張り出して電源の USB ケーブルを抜き差しするというのが。で、TP-Link の WiFi スマートプラグを購入しました。\nスマートプラグって原始的な照明機器くらいにしか使えないじゃん、誰が買うの？とか思ってたけど買いました。ただ、今のところまだどちらも活躍してない。良いことなんですけど。\n","date":"2021年3月2日","permalink":"/2021/03/monitoring-raspberry-pi-with-healthchecks-io/","section":"Posts","summary":"我が家では古いラズパイを VPN Server としてや、定期的に GoogleHome を喋らせたり、Pi-hole サーバーとして使ってたりするのですが、たまにお亡くなりになってい","title":"healthchecks.io でラズパイの生死監視"},{"content":"","date":"2021年3月2日","permalink":"/tags/monitoring/","section":"Tags","summary":"","title":"monitoring"},{"content":"ほとんどサボってしまった Advent Calendar 2020 全部オレシリーズ ですが、最後の日は 2020 年の OSS 活動について振り返ってみる。\n2月頃は Vitess を触ってみてましたね。で、helm chart におかしなところがあったから issue を上げた。あの頃は Kubernetes Operator もまだ若かったし、諸々完成度はまだまだって感じだった。planet scale のやつの方が良さそうだなと思ったけど、その後触ってないからもう良く知らない。\nhttps://github.com/vitessio/vitess/issues/5843\n4月頃は Amazon ES にログを入れようかなって思って、Elasticsearch の template やら ingest pipeline やら watcher Alerting 設定をどうしようかなと思ってて、mihirsoni/odfe-monitor-cli が使えるんじゃないかと調べてました。\n#7 は書き出すファイルを上書きするんだけど、truncate しないから、上書きする方が元のサイズよりも小さいと後ろにゴミが残るというバグ修正。ホントは一時ファイルに書き出して rename する方が安全だけど。\nhttps://github.com/mihirsoni/odfe-monitor-cli/pull/7\n#8 はなぜかデフォルトで Basic 認証のユーザー名、パスワードが指定されてて、それに気付かずハマったのでデフォルトでは無効にしない？って言ってみたけど受け入れてもらえなかったやつ。\nhttps://github.com/mihirsoni/odfe-monitor-cli/pull/8\n#9 は Alerting の Action Throttle 機能への対応。だいぶ経ってから Merge されたみたい。\nhttps://github.com/mihirsoni/odfe-monitor-cli/pull/9\nでも odfe-monitor-cli だと terraform の中で shell script を実行するのも気持ち悪いしなあと思って、terraform-provider-elasticsearch でもいいんじゃね？ json 管理辛いけど、、、となりかけたところで、やっぱり Elasticsearch はやめて CloudWatch Logs にすることにしたのでした。\n6月、terraform-aws-lambda module で output の変数名と実態がずれていたので直してもらった\nhttps://github.com/terraform-module/terraform-aws-lambda/issues/10\nこちらも6月、Kubernetes のログをどうやってもれなく集めようか、ということで docker のログ出力先を journald にして、それを Fluent Bit で読み出そうとしてた (そうすることで、docker のログのローテーション間隔やタイミングの問題での取りこぼしを回避できるかと考えた) ら SIGTERM での停止時に読み込んだけどまだ buffer に書かれていないデータなのかな？が失われてしまう問題が見つかったので報告。\nhttps://github.com/fluent/fluent-bit/issues/2277\n7月、このツイート を見て kubectl-isolate っていう shell script を書いた。実際にこれを役立てたことはまだないけど。\nhttps://github.com/yteraoka/kubectl-isolate\n7月にもう一つ、と言ってもしばらく放置したので merge されたのは9月。Prometheus のメトリクスは Grafana で見ることがほとんどで、その場合は TYPE コメントの型が counter だろうが gauge だろうが問題ないのですが、たまたま sysdig monitor を使う機会があり、当時の sysdig ではその型が意味を持っていたので counter であるべきものはそうなるように修正。ただ、その後 sysdig も native な Prometheus 対応をしたのでもしかしたらもうあまり重要ではないかもしれない\u0026hellip;\nhttps://github.com/fluent/fluent-plugin-prometheus/pull/163\n7月の終わりにもう一つ。Fluentd の file buffer でレースコンディション問題があるっぽいので報告。\nhttps://github.com/fluent/fluentd/issues/3089\n9月、Kuberhealthy の helm chart で tolerations を指定する方法が無かったので追加。\nhttps://github.com/Comcast/kuberhealthy/pull/641\n11月は Google の Container Registry で古い image の削除を行いたいけど AWS の ECR みたいに機能が用意されていないため、ツールを作ろうかと思っていたところ、gcr-lifecycle-policy ってのを見つけたのでこれ使えるんじゃね？ということで試したらまあ、いろいろ問題があったのでした\u0026hellip; そのうち Google さんが機能を追加してくれるだろうと信じています\nまずは、Container Registry にしか対応してなかったのを Artifact Registry に対応させる\nhttps://github.com/marekaf/gcr-lifecycle-policy/issues/6\n次に sort 順が逆だよという指摘\nhttps://github.com/marekaf/gcr-lifecycle-policy/pull/7\nお次はヨーロッパスタイルの日付表示は辛いよ\nhttps://github.com/marekaf/gcr-lifecycle-policy/issues/9\nこのツールは Kubernetes クラスタ内で使われているイメージは削除しないという機能があるのだけれど、それが機能しないパターンがいくつかあったので修正\nhttps://github.com/marekaf/gcr-lifecycle-policy/pull/10\nImage の tag が特定のパターンにマッチするものは消さないという指定ができるように機能追加\nhttps://github.com/marekaf/gcr-lifecycle-policy/pull/13\nと、こんなところでした。お疲れ様でした。みなさま良いお年を。\n","date":"2020年12月24日","permalink":"/2020/12/look-back-on-2020/","section":"Posts","summary":"ほとんどサボってしまった Advent Calendar 2020 全部オレシリーズ ですが、最後の日は 2020 年の OSS 活動について振り返ってみる。 2月頃は Vitess を触ってみてましたね。で、he","title":"2020年のOSS活動ふり返り"},{"content":"","date":"2020年12月24日","permalink":"/tags/advent-calendar-2020/","section":"Tags","summary":"","title":"advent calendar 2020"},{"content":"","date":"2020年12月24日","permalink":"/tags/uncategorized/","section":"Tags","summary":"","title":"Uncategorized"},{"content":"Advent Calendar 2020 全部オレシリーズ 14日目です。誕生日記念号です。おめでとうございます。ありがとうございます。\n先日も gRPC と NLB での idle timeout の問題について触れましたが、今回は Istio の Ingress Gateway で NLB の idle timeout に対応する方法です。\nGo言語ではクライアント側もサーバー側もデフォルトで15秒間隔の TCP keepalive が有効になるということも書きましたが、Istio を導入した環境では、サーバーの接続相手は localhost の Envoy (istio-proxy) となるため、TCP keepalive もその間で閉じてしまってクライアントには届きません。クライアントと TCP 接続するのは Istio Ingress Gateway なのです。\nまた、クライアント視点ではクライアントから keepalive を送っておけば問題なさそうに見えますが、サーバー側からも送っておかないと、黙っていなくなられるともう相手がいないコネクションをずっと維持することになってしまい、ファイルディスクリプタが溢れたり、無駄なメモリを使ってしまいます。\nクライアントと直接接続している Istio Ingress Gateway が TCP keepalive を送ってくれれば良いわけですが、Istio のドキュメントをみても TCP keepalive の設定項目があるのは DestinationRule だけです。\nそれで、ググっていたら見つかりました。\nhttps://github.com/envoyproxy/envoy/issues/3634\nEnovy の issue #3634 で「downstream の TCP keepalive を設定したいんだけど」というのがあり、それは socket_options でできるよということでした。さらに Gardener という Kubernetes-as-a-service を構築するプロジェクトでそれを EnvoyProxy で実装したという PullRequest へのリンクもありました。ありがたや、ありがたや。\nhttps://github.com/gardener/gardener/pull/3104\nということで次のような EnvoyFilter を適用すれば Istio Ingress Gateway で downstream (クライアント向け) の TCP keepalive を設定することができます。\nmatch のところを見るとわかりますが、この書き方の場合、listener の port ごとに configPatches に並べる必要があります。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:labels:istio.io/rev:defaultname:istio-ingressgatewaynamespace:istio-systemspec:configPatches:- applyTo:LISTENERmatch:context:GATEWAYlistener:name:0.0.0.0_8443portNumber:8443patch:operation:MERGEvalue:socket_options:# /usr/include/asm-generic/socket.h# SOL_SOCKET = 1# SO_KEEPALIVE = 9- level:1name:9int_value:1# keepalive を有効にするstate:STATE_LISTENING# /usr/include/linux/tcp.h# IPPROTO_TCP = 6# TCP_KEEPIDLE = 4- level:6name:4int_value:15# 15秒間の無通信が発生したら keepalive を送り始めるstate:STATE_LISTENING# IPPROTO_TCP = 6# TCP_KEEPINTVL = 5- level:6name:5int_value:15# 15秒間隔で keepalive を送るstate:STATE_LISTENING# IPPROTO_TCP = 6# TCP_KEEPCNT = 6- level:6name:6int_value:3# 3回応答がなかったら close する (FIN を送る)state:STATE_LISTENINGこの設定を使うと、最後の通信から15秒後から15秒間隔で keepalive を送信します。3回連続で応答がなかったら RST を送って強制切断します。TCP_KEEPINTVL の間レスポンスを待つという感じで TCP_KEEPIDLE の15秒待った後、15秒を3回で60秒後に RST が送られます。3回目を送った直後ではありません。\n","date":"2020年12月14日","permalink":"/2020/12/istio-downstream-tcpkeepalive/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 14日目です。誕生日記念号です。おめでとうございます。ありがとうございます。 先日も gRPC と NLB での idle timeout の問題について触れまし","title":"Istio で Downstream への TCP keepalive を送る方法"},{"content":"","date":"2020年12月10日","permalink":"/tags/go/","section":"Tags","summary":"","title":"go"},{"content":"Advent Calendar 2020 全部オレシリーズ 10日目です。もうめんどくせえなあ。\ngRPC と NLB での Idle Timeout というあるある問題の調査で Go 言語で書いたクライアントとサーバーを使った際に知ったことのメモです。\nまずは TCP keepalive を送らないクライアントとサーバーでの動作検証だ、と思って tcpdump を実行しながら試してみたらなぜか 15 秒おきに TCP keepalive を送り合うんです\u0026hellip; は！？\nそんなコード書いてないのに？？\nで、確認してみるとクライアント(Dial)側は Go 1.12 からデフォルトで TCP keepalive が有効になっていたみたいです。1.12 ってもうだいぶ前ですね。keepalive の送信は最後の通信から15秒後から15秒おきに送信します。切断までの連続失敗回数は未指定なので kernel の設定が使われます。Linux のデフォルトは9回が多いようです。\nhttps://github.com/golang/go/commit/5bd7e9c54f946eec95d32762e7e9e1222504bfc1\nGo 1.13 ではサーバー側もデフォルトで有効になりました。最初は3分間隔でしたが\nhttps://github.com/golang/go/commit/1abf3aa55bb8b346bb1575ac8db5022f215df65a\nその後、クライアント側と同じ15秒に変更されました\nhttps://github.com/golang/go/commit/b98cecff882e42c7f0842c7adae1deeca1b99002\nKernel の設定確認はこんな感じで\n$ sudo sysctl -a 2\u0026gt; /dev/null | grep tcp_keepalive net.ipv4.tcp_keepalive_intvl = 75 net.ipv4.tcp_keepalive_probes = 9 net.ipv4.tcp_keepalive_time = 7200 TCP keepalive は　net.ipv4.tcp_keepalive_time で最後の通信から何秒無通信が続いたら送り始めるか、net.ipv4.tcp_keepalive_intvl で何秒おきに送信するか、net.ipv4.tcp_keepalive_probes で何回連続して応答が返ってこなかったら切断するかを指定します。\n上記の Go のコードでは net.ipv4.tcp_keepalive_time (syscall.TCP_KEEPIDLE) と net.ipv4.tcp_keepalive_intvl (syscall.TCP_KEEPINTVL) を同じ値で設定するようになっています。\n回数 (net.ipv4.tcp_keepalive_probes) も指定したい場合は、socket に対して自分で syscall.SetsockoptInt() で設定する必要があります。Kubernetes の場合、Pod の kernel パラメーターをいじるのはだいぶ面倒です。\nところで、gRPC を使うコードで私は TCP keepalive を無効にしたかったのです、1.11 は流石に古過ぎるし https://pkg.go.dev/google.golang.org/grpc の Dial でどうやって無効にするのかな、ちゃんとやるの面倒だなと思ってた時に気付きました。src/net/dial.go を書き換えれば良いんだ！！って気付きました。例えば vi /usr/local/go/src/net/dial.go です。15秒のところをずっと大きな数字にしちゃえば ok です。この状態で go build すれば keepalive が送られないようになります。\nところで ALB も End-to-End の HTTP2 で gRPC をサポートしましたが、これには罠はないんですかね？\nnginx の場合は http2_idle_timeout (デフォルト180秒) ってのがあって、この間リクエストが無いと nginx 側から切断してくれます。\nnginx ついでに言うと listen 設定でサーバー側の TCP keepalive を設定することが可能です。\nso_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt] listen 443 http2 so_keepalive=on; listen 443 http2 so_keepalive=7200:75:9; でわでわ〜\nあー、そういえば netstat の -o / --timers や ss の -o / --options, -e / --extended で keepalive の timer を確認できるんですね。\n","date":"2020年12月10日","permalink":"/2020/12/tcp-keepalive-in-golang/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 10日目です。もうめんどくせえなあ。 gRPC と NLB での Idle Timeout というあるある問題の調査で Go 言語で書いたクライアントとサーバーを使っ","title":"Go 言語での TCP keepalive"},{"content":"","date":"2020年12月9日","permalink":"/tags/brim/","section":"Tags","summary":"","title":"Brim"},{"content":"","date":"2020年12月9日","permalink":"/tags/network/","section":"Tags","summary":"","title":"Network"},{"content":"Advent Calendar 2020 全部オレシリーズ 9日目です。もう完走は諦めましたがなんとか続いています。\n今日も tcpdump と戯れてました。クラウドメインになってからも tcpdump は必須です。\nですが、問題の再現手順がはっきりしない場合は30分とか1時間、あるいは数時間のキャプチャファイルからお目当てのものを探す必要があったりします、wireshark だけでは苦行すぎます。そこで使えるのが今日紹介する Brim です。\n紹介と言ってもサイトに動画へのリンクがあって、それを見てねで終わってしまうのですが\u0026hellip;\n  まず起動して pcap ファイルを開くと、巨大なキャプチャファイルでも解析が用意な znq ファイルへの変換処理が走って次のような表示になります。裏で zqd サーバーが起動してそれとの通信を行っているようです。春先に使ってた時とちょっと様子が違う。昔は pcap ファイルと同じディレクトリに .brim suffix のついたディレクトリができてそこに zng ファイルとメタデータファイルが保存されていたのに今はちょっと違うようだ。今日、しばらくぶりに起動したらアプリの更新が走ってその後データの migration が行われたようだ。今は ~/Library/Application Support/Brim/data/spaces 配下に作られるみたい。\n 過去に開いて変換したファイルはずっと残っていて左上のペインにリスト表示されている。不要になったものは右クリックから削除をしておくのが良さそうです。\nさて、開いたら検索を行うわけですが、各値のところで右クリックをすると次のようなメニューが表示されます。\n ここで Filter = value か Filter != value を選択するとその条件が上の検索バーに追加されて、その条件に合うものだけの表示にフィルターされます。\n_path = \u0026quot;conn\u0026quot; を検索条件にしたのが次の画面です。どんどん右クリックで追加していけば AND 条件で絞っていけます。この検索クエリは ZQL (zq log query language) というものです。詳細はドキュメントを参照ください。\n ↑これは一行一行が TCP Session です。\nダブルクリックすると次のようなウインドウが開きます。\n この FIELDS に表示されている項目は全てクエリに使えます。\nconn_state は次のような意味を持っています。ドキュメントはこちら。これを検索の条件として使うことでおかしな TCP Session を素早く見つけることができます。\n 例えば、conn_state が SF のものは Normal establishment and termination. ですから conn_state!=\u0026quot;SF\u0026quot; で除外すると S1 と S3 というのが見つかりました。S1 は問題ない session ですが、S3 はまずそうですね、そう、今日はこれを調べていたのです。\n (上部の View というボタンで Left Pane, Right Pane の表示・非表示を切り替えられます。Left を消して Right を表示してみました。)\nで、ここからが Brim のさらに便利なところなのですが、Session を選択して上の Wireshark ボタンをクリックするとその Session だけを抜き出した pcap ファイルを生成して Wireshark で開いてくれるのです。さらには、2つ、3つと別ウインドウで開くことができるので並べて表示できて超便利なのです！！！\n 他にも history は次のような意味を持っているので、SYN を送ったけど SYN,ACK が返ってきていないものとか、SYN の再送があるものとか、RST を含むものとかを探すことができます。\n それでは良いパケキャプライフを。\n","date":"2020年12月9日","permalink":"/2020/12/brim-introduction/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 9日目です。もう完走は諦めましたがなんとか続いています。 今日も tcpdump と戯れてました。クラウドメインになってからも tcpdump は必須で","title":"tcpdump のお供に Brim を使いましょう"},{"content":"","date":"2020年12月8日","permalink":"/tags/nginx/","section":"Tags","summary":"","title":"nginx"},{"content":"Advent Calendar 2020 全部オレシリーズ 8日目です。もう完走は諦めました。(再掲)\nNginx Ingress Controller と oauth2-proxy を組み合わせて簡単に SSO を導入するためのメモです。複数のサービスがあって、Nginx Ingress Controller を使ってて、どれも同じ SSO 設定で良いという場合に便利です。nginx の auth_request 設定を Nginx Ingress Controller がいい感じにやってくれます。\nNginx Ingress Controller のインストール # ドキュメントを見てお好みの方法でインストールしてください。\n私は helm でインストールしました。\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install my-release ingress-nginx/ingress-nginx oauth2-proxy のインストール # Helm Chart はあるにはあるけど、もうアーカイブ状態なんですよね。でも使えます。ここでも使っています。\nKeycloak で SSO する場合の例です、次の YAML を customize.yaml として保存して helm に -f で渡します。Keycloak のセットアップや Client ID の発行は別途行ってください。Keycloak 以外にも沢山の Provier に対応しています。便利です。\nconfig:clientID:example-clientclientSecret:71d0785e-3aeb-4e9c-8790-7dc34699afc1cookieSecret:bmljaHVjb0xvaHJpZTVnYWhyYWU0ZXY0cmFoMGRvbzZvb0RhaGNoZWVzYXJlNGFTextraArgs:provider:keycloaklogin-url:https://keycloak.example.com/auth/realms/example/protocol/openid-connect/authredeem-url:https://keycloak.example.com/auth/realms/example/protocol/openid-connect/tokenvalidate-url:https://keycloak.example.com/auth/realms/example/protocol/openid-connect/userinfoscope:profileingress:enabled:truepath:/oauth2hosts:- alertmanager.example.com- kiali.example.com- prometheus.example.comannotations:kubernetes.io/ingress.class:nginxhelm repo add stable https://charts.helm.sh/stable helm repo update helm install stable/oauth2-proxy -f customize.yaml これで次のような Ingress が作成されています。この後出てきますが、各サービスでもそれぞれの Ingress を定義しますが、/oauth2 についてはそれとは別に定義する必要があるというのがここでの重要なポイントです。私はこれになかなか気付けずに時間がかかりました。\napiVersion:extensions/v1beta1kind:Ingressmetadata:annotations:kubernetes.io/ingress.class:nginxmeta.helm.sh/release-name:oauth2-proxymeta.helm.sh/release-namespace:ingress-nginxlabels:app:oauth2-proxyapp.kubernetes.io/managed-by:Helmchart:oauth2-proxy-3.2.3heritage:Helmrelease:oauth2-proxyname:oauth2-proxynamespace:ingress-nginxspec:rules:- host:alertmanager.example.comhttp:paths:- backend:serviceName:oauth2-proxyservicePort:80path:/oauth2pathType:ImplementationSpecific- host:kiali.example.comhttp:paths:- backend:serviceName:oauth2-proxyservicePort:80path:/oauth2pathType:ImplementationSpecific- host:prometheus.example.comhttp:paths:- backend:serviceName:oauth2-proxyservicePort:80path:/oauth2pathType:ImplementationSpecific各サービスの Ingress 設定 # oauth2-proxy の設定に alertmanager.example.com, kiali.example.com, prometheus.example.com というのを入れてみました。\nどれでも同じですが、alertmanager.example.com について設定するとしましょう。おそらく kube-prometheus-stack という helm chart で入れるでしょうから直接 Ingress の manifest を書くことはないでしょうが。\napiVersion:extensions/v1beta1kind:Ingressmetadata:annotations:kubernetes.io/ingress.class:nginxmeta.helm.sh/release-name:kube-prometheus-stackmeta.helm.sh/release-namespace:prometheusnginx.ingress.kubernetes.io/auth-signin:https://$host/oauth2/start?rd=$escaped\\_request\\_uringinx.ingress.kubernetes.io/auth-url:http://oauth2-proxy.ingress-nginx.svc.cluster.local/oauth2/authname:kube-prometheus-stack-alertmanagernamespace:prometheusspec:rules:- host:alertmanager.example.comhttp:paths:- backend:serviceName:kube-prometheus-stack-alertmanagerservicePort:9093path:/pathType:ImplementationSpecificここで大事なのは nginx.ingress.kubernetes.io/auth-signin と nginx.ingress.kubernetes.io/auth-url という annotation です。\nauth_request は nginx が受けたリクエストのヘッダーを一部いじるものの、ほぼそのまま oauth2-proxy に proxy します。そして proxy 先で cookie の値やらドメイン名、URIなどからログイン済みかどうか、アクセスが許可されているのかどうかを判断して、レスポンスを返します。それを受け取った nginx が本来の upstream に proxy したり認証ページへリダイレクトしたりします。\nで、nginx.ingress.kubernetes.io/auth-url に指定するのが認証のための proxy 先です。nginx からアクセスできれば良いため Kubernetes クラスタ内通信で良いので .svc.cluster.local のドメインで指定しています。\n一方、nginx.ingress.kubernetes.io/auth-signin はログインされていない場合のリダイレクト先として使われるため $host を使って受け付けた Host ヘッダー (VirtualHost) のドメインが入るようにしてあります。\n/oauth2/ 配下へのアクセスまで auth_request の対象としてしまうとループしてしまうため、そうならないようにするのが先に少し触れた 同じドメインだけど Ingress が2ヵ所で設定されている理由です。ついつい次のように書いてしまいそうになりますが、これではループしてしまうのです。\nspec:rules:- host:alertmanager.example.comhttp:paths:- backend:serviceName:kube-prometheus-stack-alertmanagerservicePort:9093path:/- backend:serviceName:oauth2-proxyservicePort:80path:/oauth2同じドメインを複数 Ingress に分けて定義しても Nginx Ingress Controller は同一ドメインは同じ VirtualHost にまとめてくれます。\nNginx Ingress Controller が生成する nginx.conf を見ると何をやっているのかがわかります。\nちなみに、oauth2-proxy はその名の通り、それ単体でも認証付きの proxy として動作します。\n","date":"2020年12月8日","permalink":"/2020/12/sso-using-nginx-ingress-controller-and-oauth2-proxy/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 8日目です。もう完走は諦めました。(再掲) Nginx Ingress Controller と oauth2-proxy を組み合わせて簡単に SSO を導入するためのメモです。複数のサービスがあ","title":"Nginx Ingress Controller と oauth2-proxy で SSO"},{"content":"Advent Calendar 2020 全部オレシリーズ 7日目です。もう完走は諦めました。\n8月にリリースされた Istio 1.7 で追加され、「メインコンテナの起動前に istio-proxy の起動を完了させる」で紹介した機能ですが、1.8 で設定方法に変更が入っていました。\nこれまで通りに Istio のインストールを行おうとしたら早くも deprecated だと警告が出ました。\n! values.global.proxy.holdApplicationUntilProxyStarts is deprecated; use ProxyConfig holdApplicationUntilProxyStarts instead Istio 1.7 では次のように values.global.proxy.holdApplicationUntilProxyStarts に設定していたのですがもう推奨されない。\napiVersion:install.istio.io/v1alpha2kind:IstioOperatorspec:values:global:proxy:holdApplicationUntilProxyStarts:trueIstio 1.8 では Pod 単位でこれを有効・無効化できるようになったみたいです。Change Notes にも次のように書かれています。\n Added holdApplicationUntilProxyStarts field to ProxyConfig, allowing it to be configured at the pod level. Should not be used in conjunction with the deprecated values.global.proxy.holdApplicationUntilProxyStarts value. (Issue #27696)\n ProxyConfig ってなんだよ！っていろいろ探したのですがこれは source code を追う必要がありそう。これだろうってのは分かりましたけど、証拠は掴んでいない。\nIstio 1.8 ではどう設定するかですが、Pod 単位で上書きできるので meshConfig.defaultConfig.holdApplicationUntilProxyStarts でデフォルトを指定します。\napiVersion:install.istio.io/v1alpha2kind:IstioOperatorspec:meshConfig:defaultConfig:holdApplicationUntilProxyStarts:truePod で指定する場合は次のように annotation に設定します。\nannotations:proxy.istio.io/config:\u0026#39;{ \u0026#34;holdApplicationUntilProxyStarts\u0026#34;: false }\u0026#39;これらの設定は istio-proxy container の env に次のように入っています。\nenv:- name:PROXY_CONFIGvalue:|{\u0026#34;proxyMetadata\u0026#34;:{\u0026#34;DNS_AGENT\u0026#34;:\u0026#34;\u0026#34;},\u0026#34;holdApplicationUntilProxyStarts\u0026#34;:false}おそらくこれが ProxyConfig の元になるやつですね。\nistio-system の istio-sidecar-injector という ConfigMap の template という値に次のような感じで使われているので 1.7 の設定でもまだ使えます。\n{{- $holdProxy := or .ProxyConfig.HoldApplicationUntilProxyStarts.GetValue .Values.global.proxy.holdApplicationUntilProxyStarts }} ","date":"2020年12月7日","permalink":"/2020/12/istio-1-8-holdapplicationuntilproxystarts-changes/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 7日目です。もう完走は諦めました。 8月にリリースされた Istio 1.7 で追加され、「メインコンテナの起動前に istio-proxy の起動を完了させる」","title":"Istio 1.8 で holdApplicationUntilProxyStarts 設定に変更がありました"},{"content":"Advent Calendar 2020 全部オレシリーズ 6日目です。\n前2回の内容で Object Storage に日次のバックアップファイルを保存するようにしましたが、ファイル名に日付を入れるようにしたため、古いものを削除しないと無駄な費用が発生してしまいます。そこで Lifecycle Policy を使って古いものを削除する設定を行います。\nサービス権限 # Object Storage サービスが私の Object を削除することになるため、サービスに対してそれを許可するという「サービス権限」設定する必要があります。\nサービス権限は「アイデンティティ」→「ポリシー」で設定します。グループに対して許可する場合は Allow group ... でしたが、サービスであるため Allow service ... とします。で対象となるサービスは objectstorage なので objectstorage-\u0026lt;region_identifier\u0026gt; を指定するようです。東京リージョンの場合は objectstorage-ap-tokyo-1 となります。(リージョンおよび可用性ドメイン)\nto manage object-family では in で指定された範囲の Object Storage の管理権限が与えられるようです。(ポリシー・リファレンス)\nAllow service objectstorage-\u0026lt;region_identifier\u0026gt; to manage object-family in compartment \u0026lt;compartment_name\u0026gt; to manage object-family の代わりに個別の権限も設定出来るということなので次のようにしてみました。\nAllow service objectstorage-ap-tokyo-1 to {BUCKET_INSPECT, BUCKET_READ, OBJECT_INSPECT, OBJECT_CREATE, OBJECT_DELETE} in tenancy where target.bucket.name='blog-1q77-com' Bucket の Lifecycle Policy 設定 # オブジェクト・ストレージで対象のバケットを選択しライフサイクル・ポリシー・ルールからルールの作成ボタンをクリックして作成します。Amazon S3 のやつと似た感じですね。\n ","date":"2020年12月6日","permalink":"/2020/12/oracle-cloud-object-storage-lifecycle-policy/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 6日目です。 前2回の内容で Object Storage に日次のバックアップファイルを保存するようにしましたが、ファイル名に日付を入れるようにし","title":"Oracle Cloud の Object Storage での Lifecycle Policy 設定"},{"content":"","date":"2020年12月6日","permalink":"/tags/oraclecloud/","section":"Tags","summary":"","title":"OracleCloud"},{"content":"Advent Calendar 2020 全部オレシリーズ 5日目です。 やっと 20% です\u0026hellip; まだ先は長い\n前回の「Oracle Cloud の oci コマンドセットアップ」で oci コマンドが使えるようになったので、これを使ってファイルを Oracle Cloud の Object Storage にアップロードします。\nファイルのアップロード # Put するには次のようなコマンドを実行します。もっと多くのオプションがありますが、単純にファイルを保存するだけならこれだけ。すでに存在するものを上書きする場合は --force が必要。\noci os object put \\  --namespace namespace \\  --bucket-name mybucket \\  --name myfile.txt \\  --file /Users/me/myfile.txt --namespace は -ns、--bucket-name は -bn という省略形もあります。\nOjbect Storage のネームスペースはテナントごとに割り当てられるっぽく、「管理」→「テナンシ詳細」 で確認できます。しかし、まあこれが覚えられるようなものじゃなくてツライ\u0026hellip;\nさて、当初の目的はこの wordpress のデータのバックアップです。cron で実行することにしますが、その処理で使うアカウントの権限は最小にしておきたいものです。専用ユーザーを作って必要な権限だけを割り当てることにします。\nユーザーの作成 # 「アイデンティティ」→「ユーザー」で IAM ユーザーを作成します。説明にマルチバイト文字列が入れられるのは便利ですね。\nCreate User  グループの作成 # 次に「アイデンティティ」→「グループ」でグループを作成し、先ほど作成したユーザーをそのメンバーとして登録します。\nCreate Group  権限設定 # 次に「アイデンティティ」→「ポリシー」でポリシーを作成します。\nCreate Policy (1)  ポリシービルダー欄の「ポリシー・ユース・ケース」で「ストレージ管理」を選択し、「共通ポリシー・テンプレート」で「ユーザーがオブジェクト・ストレージ・バケットにオブジェクトを書き込むことができるようにします」を選択すると、下にポリシー・ステートメントが表示され、「グループ」で先ほど作成したグループを選択して場所でテナント全体(ルート)かコンパートメントを選択するとポリシー・ステートメントが更新されます。\nCreate Policy (2)  これで出来上がったポリシーが次の2つ\nAllow group wp-backup to read buckets in tenancy Allow group wp-backup to manage objects in tenancy where any {request.permission='OBJECT_CREATE', request.permission='OBJECT_INSPECT'} 1つ目のポリシーでは bucket の一覧が参照可能ですが、特定の bucket 内へ書き込むだけなら不要なので削除します。2つ目のポリシーではテナント内の任意の bucket に対して書き込みができてしまうため、bucket も条件に追加したいです。そうすると次のようになります。\nAllow group wp-backup to manage objects in tenancy where all { target.bucket.name='blog-1q77-com', any { request.permission='OBJECT_CREATE', request.permission='OBJECT_INSPECT' } } where に trget.bucket.name 条件を追加しました、そして request.permission の条件と AND になるように all { } で囲みました。\nAWS の IAM Policy だと「どのリソースに対して何ができるのか」を定義し、それを「誰か（ユーザー、グループ、ロール）」に紐づけるわけですが、Orale Cloud の場合は、このポリシー文の中に「誰」を含めるようになっています。今回の例では「Allow group wp-backup」とあり、wp-backup グループに対してオブジェクトの書き込み権限を設定するということになっています。これを踏まえたポリシーの名前付けをする必要があります。「ところ変われば・・・」ですね。\nさて、これで完了かと思いきや、バックアップファイルの保存先を日付単位にしており、テストで同じ日に複数回実行しようとすると上書きができませんでした。OBJECT_OVERWRITE 権限が必要でした、一方、OBJECT_INSPECT は無くても問題ありませんでした。でこの2つを入れ替えて、最終的には次のポリシーとなりました。\nAllow group wp-backup to manage objects in tenancy where all { target.bucket.name='blog-1q77-com', any { request.permission='OBJECT_CREATE', request.permission='OBJECT_OVERWRITE' } } Object Storage の権原一覧は「Details for Object Storage, Archive Storage, and Data Transfer」にありました。\n How Policies Work Securing Object Storage Details for Object Storage, Archive Storage, and Data Transfer  ","date":"2020年12月4日","permalink":"/2020/12/file-upload-using-oci-os-command/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 5日目です。 やっと 20% です\u0026hellip; まだ先は長い 前回の「Oracle Cloud の oci コマンドセットアップ」で oci コマンドが使え","title":"oci コマンドでのファイルアップロードと権限の最小化"},{"content":"Advent Calendar 2020 全部オレシリーズ 4日目です。\nOracle Cloud の無料枠でこのブログを運用することにしたわけですが、docker-compose で起動させるようにしたものの、バックアップ設定を後回しにしていました。でもデータが飛んでしまうと悲しいのでそろそろやることにしました。\nOracle Cloud の Object Storage は S3 互換の API が存在するとのことで、s3cmd を使おうかと思ったのですが、まあよーわからん。\nひとまずは oci コマンドが使えるようにしてみます。\nMac での oci コマンドのインストールは Homebrew でいけました。(oci コマンドのインストール)\nbrew install oci-cli oci setup config コマンドで初期設定を行います。\n$ oci setup config This command provides a walkthrough of creating a valid CLI config file. The following links explain where to find the information required by this script: User API Signing Key, OCID and Tenancy OCID: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#Other Region: https://docs.cloud.oracle.com/Content/General/Concepts/regions.htm General config documentation: https://docs.cloud.oracle.com/Content/API/Concepts/sdkconfig.htm Enter a location for your config [/Users/teraoka/.oci/config]: Enter a user OCID: ocid1.user.oc1..aaaaaaaanaeng2up6fae5ievee7waech3soo1oephahvieshee5ain5ceaga Enter a tenancy OCID: ocid1.tenancy.oc1..aaaaaaaazoo7urooxee3to2shuha5roo2roh6pha0vooxohr9eiwee5nohy9 Enter a region (e.g. ap-chiyoda-1, ap-chuncheon-1, ap-hyderabad-1, ap-melbourne-1, ap-mumbai-1, ap-osaka-1, ap-seoul-1, ap-sydney-1, ap-tokyo-1, ca-montreal-1, ca-toronto-1, eu-amsterdam-1, eu-frankfurt-1, eu-zurich-1, me-dubai-1, me-jeddah-1, sa-saopaulo-1, uk-cardiff-1, uk-gov-cardiff-1, uk-gov-london-1, uk-london-1, us-ashburn-1, us-gov-ashburn-1, us-gov-chicago-1, us-gov-phoenix-1, us-langley-1, us-luke-1, us-phoenix-1, us-sanjose-1): ap-tokyo-1 Do you want to generate a new API Signing RSA key pair? (If you decline you will be asked to supply the path to an existing key.) [Y/n]: Enter a directory for your keys to be created [/Users/teraoka/.oci]: Enter a name for your key [oci_api_key]: Public key written to: /Users/teraoka/.oci/oci_api_key_public.pem Enter a passphrase for your private key (empty for no passphrase): Private key written to: /Users/teraoka/.oci/oci_api_key.pem Fingerprint: 2c:48🇩🇪0e:a1:4f:af:1d:b0:02:c3:99:64:9b:d8:cd Config written to /Users/teraoka/.oci/config If you haven\u0026#39;t already uploaded your API Signing public key through the console, follow the instructions on the page linked below in the section \u0026#39;How to upload the public key\u0026#39;: [https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2](https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2) user OCID は Web Console の 「アイデンティティ」→「ユーザー」 で、 tenancy OCID は 「管理」→「テナンシ詳細」 で確認できます。なんか無駄に長い感じがしますね\u0026hellip;\n最後に秘密鍵 (oci_api_key.pem) と公開鍵 (oci_api_key_public.pem) のペアが ~/.oci ディレクトリ内に作成されました。この公開鍵を Oracle Cloud の「アイデンティティ」→「ユーザー」で、「APIキー」に登録します。\n設定は ~/.oci/config に書かれています。設定項目は SDK and CLI Configuration File に説明があります。AWS のやつみたいに複数 profile の設定を書くことができるようです。\n$ cat ~/.oci/config [DEFAULT] user=ocid1.user.oc1..aaaaaaaanaeng2up6fae5ievee7waech3soo1oephahvieshee5ain5ceaga fingerprint=96:6e:8d:9c:fb:a2:e8:e9:16:a3:68:f3:d6:d4:fd:06 key_file=/Users/teraoka/.oci/oci_api_key.pem tenancy=ocid1.tenancy.oc1..aaaaaaaazoo7urooxee3to2shuha5roo2roh6pha0vooxohr9eiwee5nohy9 region=ap-tokyo-1 Object Storage の bucket 一覧を取得してみる。\n$ oci os bucket list Usage: oci os bucket list [OPTIONS] Error: Missing option(s) --compartment-id. --compartment-id ってのが必須らしい。comparment-id を取得するには oci iam compartment list で良さそう。用語集によるとコンパートメントとは「組織の管理者から権限を付与された特定のグループのみがアクセスできる関連リソースのコレクション」だそうです。デフォルトで root って呼ばれるコンパートメントが存在するんですけどそれを root という名前で使うことはできないんですね。\n$ oci iam compartment list { \u0026quot;data\u0026quot;: [ { \u0026quot;compartment-id\u0026quot;: \u0026quot;ocid1.tenancy.oc1..aaaaaaaaiew1xaph0vei0thainoh9ikiod6fuxooquoo7ohxe9ahsh8booju\u0026quot;, \u0026quot;defined-tags\u0026quot;: {}, \u0026quot;description\u0026quot;: \u0026quot;xxxx\u0026quot;, \u0026quot;freeform-tags\u0026quot;: {}, \u0026quot;id\u0026quot;: \u0026quot;ocid1.compartment.oc1..aaaaaaaaque9quoowais0riey1ahcahniemahahquac3tu2jahogh4ees0la\u0026quot;, \u0026quot;inactive-status\u0026quot;: null, \u0026quot;is-accessible\u0026quot;: null, \u0026quot;lifecycle-state\u0026quot;: \u0026quot;ACTIVE\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ManagedCompartmentForPaaS\u0026quot;, \u0026quot;time-created\u0026quot;: \u0026quot;2019-09-21T03:24:43.546000+00:00\u0026quot; } ] } 長っ！\n$ oci os bucket list --compartment-id ocid1.tenancy.oc1..aaaaaaaaiew1xaph0vei0thainoh9ikiod6fuxooquoo7ohxe9ahsh8booju { \u0026quot;data\u0026quot;: [ { \u0026quot;compartment-id\u0026quot;: \u0026quot;ocid1.tenancy.oc1..aaaaaaaaiew1xaph0vei0thainoh9ikiod6fuxooquoo7ohxe9ahsh8booju\u0026quot;, \u0026quot;created-by\u0026quot;: \u0026quot;ocid1.saml2idp.oc1..aaaaaaaaaiepeezeuwaijoo6xaivootahthiikiewauvahnga8eh4iequaej/xxxxxxxx@gmail.com\u0026quot;, \u0026quot;defined-tags\u0026quot;: null, \u0026quot;etag\u0026quot;: \u0026quot;c8a9ff5e-bc4e-4141-8344-bb6aba10d7cd\u0026quot;, \u0026quot;freeform-tags\u0026quot;: null, \u0026quot;name\u0026quot;: \u0026quot;blog-1q77-com\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;aeph4tie2ahm\u0026quot;, \u0026quot;time-created\u0026quot;: \u0026quot;2020-11-29T10:42:11.442000+00:00\u0026quot; } ] } めんどくさ\u0026hellip; --compartment-id を環境変数か設定ファイルに書ければ良いのですが、そんな設定は見当たらないですね\u0026hellip;\nただ、bucket 内の object 一覧の取得時には指定の必要がないんですね。\noci os object list --bucket-name _BUCKET-NAME_ os サブコマンドは Object Storage の省略形。\n次回は backup 用ユーザーの作成と、バックアップ用の最小権限設定を行って、object の put を行うところまでを書こうと思います。\nその後、bucket に対する lifecycle 設定で古いファイルを自動で削除する設定を書こうかなと。\n(記事中の id などはランダムに生成したものに置き換えています)\n","date":"2020年12月3日","permalink":"/2020/12/setup-oci-command/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 4日目です。 Oracle Cloud の無料枠でこのブログを運用することにしたわけですが、docker-compose で起動させるようにした","title":"Oracle Cloud の oci コマンドセットアップ"},{"content":"Advent Calendar 2020 全部オレシリーズ 3日目です。\nAWS の新サービス、新機能が続々と発表されて困っちゃいますね。\n今回は最近 Terraform を書いていて、null という便利な値があるということを今更知ったのでそのメモです。\nTerraform のドキュメントはこちら (Types and Values)\nnull については次のように書かれています。\n Finally, there is one special value that has no type: null: a value that represents absence or omission. If you set an argument of a resource or module to null, Terraform behaves as though you had completely omitted it — it will use the argument\u0026rsquo;s default value if it has one, or raise an error if the argument is mandatory. null is most useful in conditional expressions, so you can dynamically omit an argument if a condition isn\u0026rsquo;t met.\n Ansible での default(omit) のようなやつですね。\nTerraform の variable 定義で default を null にしておけば、実行時に入力は求められないけど、値としては未定義なので resource で指定していても null のままであれば指定していないことになります。 #221\n例えば aws instance で次のようなリソース定義があったとして\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { ami = var.ami_id instance_type = \u0026#34;t3.micro\u0026#34; iam_instance_profile = var.profile_name tags = { Name = \u0026#34;HelloWorld\u0026#34; } } iam_instance_profile は optional なので variable 定義で次のように default を null としておけば、terraform.tfvars などで指定された場合にだけ設定することができます。default を指定しない場合は実行時に入力を求められてしまいます。\nvariable \u0026#34;profile_name\u0026#34; { default = null } これだけでも便利ですね。\n直近で私が使ったパターンは map に対して lookup で key に対する値を取り出すが、その key が存在しない場合は null とするというものでした。具体的には launch_template で block_device_mapping を指定してもしなくても良い module を書くのに使用しました。\nvariable \u0026#34;block_device_mappings\u0026#34; { description = \u0026#34;Block Device Mappings for launch template\u0026#34; type =map(map(string)) default = {} } resource \u0026#34;aws_launch_template\u0026#34; \u0026#34;some_template\u0026#34; { name = var.template_name image_id = var.image_id ... dynamic \u0026#34;block_device_mappings\u0026#34; { for_each = var.block_device_mappings content { device_name = block_device_mappings.key ebs { encrypted = tobool(lookup(block_device_mappings.value, \u0026#34;encrypted\u0026#34;, \u0026#34;true\u0026#34;)) volume_size = tonumber(block_device_mappings.value[\u0026#34;volume_size\u0026#34;]) volume_type =lookup(block_device_mappings.value, \u0026#34;volume_type\u0026#34;, null) iops = tonumber(lookup(block_device_mappings.value, \u0026#34;iops\u0026#34;, null)) snapshot_id =lookup(block_device_mappings.value, \u0026#34;snapshot_id\u0026#34;, null) delete_on_termination = tobool(lookup(block_device_mappings.value, \u0026#34;delete_on_termination\u0026#34;, null)) } } } ... } dynamic block の for_each のおかげで block_device_mappings が空の map であれば何も生成されません。指定する場合は、次のように変数を定義するのですが\nblock_device_mappings = { /dev/xvda = { volume_size = 10 volume_type = \u0026#34;gp2\u0026#34; encrypted = true snapshot_id = \u0026#34;snap-0123456789abcdef0\u0026#34; delete_on_termination = true } /dev/xvdb = { volume_size = 10 volume_type = \u0026#34;gp2\u0026#34; encrypted = true snapshot_id = \u0026#34;snap-0123456789abcdef1\u0026#34; delete_on_termination = true } /dev/xvdc = { volume_size = 10 volume_type = \u0026#34;gp2\u0026#34; encrypted = true snapshot_id = \u0026#34;snap-0123456789abcdef2\u0026#34; delete_on_termination = true } } AMI に含まれている volume については volume_size だけが必須で、他は省略可能です。省略しておけば AMI 作成時の値が使われます。AMI をアカウント跨ぎで共有するために AMI 作成時の volume 暗号化をやめて、EC2 Instance 作成時に暗号化するという選択肢を可能にするためにこういう作りにしました。\n便利な機能があってよかったよかった。\n","date":"2020年12月2日","permalink":"/2020/12/terraform-null-value/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 3日目です。 AWS の新サービス、新機能が続々と発表されて困っちゃいますね。 今回は最近 Terraform を書いていて、null という便利な値","title":"Terraform の便利な null value"},{"content":"","date":"2020年12月1日","permalink":"/tags/java/","section":"Tags","summary":"","title":"java"},{"content":"Advent Calendar 2020 全部オレシリーズ 2日目です。書きかけで放置されていたやつを掘り起こしました💦\nコンテナ内での Java のメモリの扱いについては 「JVMのヒープサイズとコンテナ時代のチューニング - Folioscope」 に詳しく説明されています。ありがたい。\nそれでは Kubernetes で実行するコンテナ内での CPU のコア数はどのように見えるでしょうか？\nresources.limits.cpu の値によります。指定がなければホストの CPU のコア数となり、指定した場合は 1 までなら 1 だし、1 より大きく 2 以下であれば 2 になるといった具合。\nところで、Java はこの CPU のコア数を元に計算した値がデフォルト値になったりするものがあります。物理サーバーや仮想サーバーからコンテナにして CPU Limit を小さく絞ったら期待の動作じゃなくなったりするかもしれません。CPU 沢山使うわけじゃないけどコア数から計算される値をとりあえず上げたい、とかデフォルトの GC が変わってしまったとかいう場合、本来はそれぞれを適切に指定するべきですが、見せかけの CPU コア数を増やす方法はないでしょうか？\n-XX:ActiveProcessorCount=N で指定可能なようです。\n試してみます。CPU コアを 4 個割り当てた minikube 環境です。\nCPU Limit を 100m にした OpenJDK Pod を作成します。(100m は小さすぎてスロットリングがひどい\u0026hellip;)\nkubectl run java --image openjdk:16-slim -it --rm --limits=cpu=100m --command -- bash lscpu では CPU は 4 つに見えますね。\nroot@java:/# **lscpu** Architecture: x86\\_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian Address sizes: 39 bits physical, 48 bits virtual CPU(s): 4 On-line CPU(s) list: 0-3 Thread(s) per core: 1 Core(s) per socket: 1 Socket(s): 4 NUMA node(s): 1 Vendor ID: GenuineIntel CPU family: 6 Model: 158 Model name: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz Stepping: 10 CPU MHz: 2600.000 BogoMIPS: 5184.00 L1d cache: 32K L1i cache: 32K L2 cache: 256K L3 cache: 12288K NUMA node0 CPU(s): 0-3 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht pbe syscall nx pdpe1gb lm constant\\_tsc rep\\_good nopl xtopology nonstop\\_tsc cpuid tsc\\_known\\_freq pni pclmulqdq dtes64 ds\\_cpl ssse3 sdbg fma cx16 xtpr pcid sse4\\_1 sse4\\_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf\\_lm abm 3dnowprefetch pti fsgsbase bmi1 avx2 bmi2 erms xsaveopt arat Jshell で Java プロセスから CPU の数がいくつに見えるか確認してみます。\nroot@java:/# **jshell --execution local** Dec 01, 2020 3:41:53 PM java.util.prefs.FileSystemPreferences$1 run INFO: Created user preferences directory. | Welcome to JShell -- Version 16-ea | For an introduction type: /help intro jshell\u0026gt; **Runtime.getRuntime().availableProcessors()** $1 ==\u0026gt; 1 1 ですね。\n次に -J-XX:ActiveProcessorCount=2 をつけて試します。\nroot@java:/# **jshell --execution local -J-XX:ActiveProcessorCount=2** | Welcome to JShell -- Version 16-ea | For an introduction type: /help intro jshell\u0026gt; **Runtime.getRuntime().availableProcessors()** $1 ==\u0026gt; 2 2 になりました。\nホストが持っている以上の値を指定するとどうなるでしょう？\nroot@java:/# **jshell --execution local -J-XX:ActiveProcessorCount=8** | Welcome to JShell -- Version 16-ea | For an introduction type: /help intro jshell\u0026gt; **Runtime.getRuntime().availableProcessors()** $1 ==\u0026gt; 8 4 コアのホストですが 8 個になってますね。\nPod の CPU Limit を 3 にした環境でも試してみます。\nkubectl run java --image openjdk:16-slim -it --rm --limits=cpu=3 --command -- bash root@java:/# jshell \u0026ndash;execution local Dec 01, 2020 3:58:52 PM java.util.prefs.FileSystemPreferences$1 run INFO: Created user preferences directory. | Welcome to JShell \u0026ndash; Version 16-ea | For an introduction type: /help intro\njshell\u0026gt; Runtime.getRuntime().availableProcessors() $1 ==\u0026gt; 3\n `-J-XX:ActiveProcessorCount` 未指定で 3 ですね。 デフォルトの GC も変わります。 CPU が 1 個の時は SerialGC root@java:/# java -XX:+PrintCommandLineFlags -version -XX:InitialHeapSize=97560512 -XX:MaxHeapSize=1560968192 -XX:MinHeapSize=6815736 -XX:+PrintCommandLineFlags -XX:ReservedCodeCacheSize=251658240 -XX:+SegmentedCodeCache -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseSerialGC openjdk version \u0026ldquo;16-ea\u0026rdquo; 2021-03-16 OpenJDK Runtime Environment (build 16-ea+25-1633) OpenJDK 64-Bit Server VM (build 16-ea+25-1633, mixed mode, sharing)\n 2 個以上で G1GC になります。GC の Thread も増えるので、見せかけの CPU の数を増やすのは得策ではなさそうです、どうしても大きくしたい場合は GC 関連 Thread の数を直接指定するのかな。 root@java:/# java -XX:+PrintCommandLineFlags -XX:ActiveProcessorCount=2 -version -XX:ActiveProcessorCount=2 -XX:ConcGCThreads=1 -XX:G1ConcRefinementThreads=2 -XX:GCDrainStackTargetSize=64 -XX:InitialHeapSize=97560512 -XX:MarkStackSize=4194304 -XX:MaxHeapSize=1560968192 -XX:MinHeapSize=6815736 -XX:+PrintCommandLineFlags -XX:ReservedCodeCacheSize=251658240 -XX:+SegmentedCodeCache -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC openjdk version \u0026ldquo;16-ea\u0026rdquo; 2021-03-16 OpenJDK Runtime Environment (build 16-ea+25-1633) OpenJDK 64-Bit Server VM (build 16-ea+25-1633, mixed mode, sharing)\nroot@java:/# **java -XX:+PrintCommandLineFlags -XX:ActiveProcessorCount=8 -version** -XX:ActiveProcessorCount=8 -XX:ConcGCThreads=2 -XX:G1ConcRefinementThreads=8 -XX:GCDrainStackTargetSize=64 -XX:InitialHeapSize=97560512 -XX:MarkStackSize=4194304 -XX:MaxHeapSize=1560968192 -XX:MinHeapSize=6815736 -XX:+PrintCommandLineFlags -XX:ReservedCodeCacheSize=251658240 -XX:+SegmentedCodeCache -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC openjdk version \u0026quot;16-ea\u0026quot; 2021-03-16 OpenJDK Runtime Environment (build 16-ea+25-1633) OpenJDK 64-Bit Server VM (build 16-ea+25-1633, mixed mode, sharing) ``` 調べはしたけど、使ってないので書きかけで放置されていた記事でした。","date":"2020年12月1日","permalink":"/2020/12/cpu-cores-in-jvm-in-container/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 2日目です。書きかけで放置されていたやつを掘り起こしました💦 コンテナ内での Java のメモリの扱いについては 「JVMのヒープサ","title":"コンテナ内での Java の CPU Cores"},{"content":"Advent Calendar 2020 全部オレシリーズ 1日目です。完走できるか心配です。\n先日、Homebrew で grpc_cli (grpc) をインストールしたんですけれども、実行すると次のように共有ライブラリが見つからないというエラーになりました。\n$ **grpc\\_cli** dyld: Library not loaded: @rpath/libgrpc++.1.dylib Referenced from: /Users/teraoka/.homebrew/bin/grpc\\_cli Reason: image not found zsh: abort grpc\\_cli libgrpc++.1.dylib は /Users/teraoka/.homebrew/Cellar/grpc/1.33.2_1/lib/libgrpc++.1.dylib にあって、/Users/teraoka/.homebrew/lib/libgrpc++.1.dylib への symbolic link もありました。\nMac を使い始めて日が浅いし、Homebrew まかせで自分でコンパイルすることもないので良く分かりませんが、Linux での rpath (LD_RUN_PATH) には覚えがありますね。\nMac の場合は DYLD_LIBRARY_PATH が使えるようです。次のようにすれば実行できました。\nDYLD\\_LIBRARY\\_PATH=/Users/teraoka/.homebrew/lib grpc\\_cli しかし、この環境変数を .zshrc に書くことの影響も読めないので Linux での compile (link) 時の -rpath 指定のようにバイナリに埋め込む path を指定したいものです。\nMac (LLVM) には otool というものがあり、これでバイナリ内の rpath を確認できるっぽいです。\n$ **otool -l /Users/teraoka/.homebrew/Cellar/grpc/1.33.2\\_1/bin/grpc\\_cli | grep -B 1 -A 2 LC\\_RPATH** Load command 30 cmd LC\\_RPATH cmdsize 64 path /tmp/grpc-20201123-11869-1aso5q3/cmake/build (offset 12) rpath はビルド時のテンポラリディレクトリになってるっぽい？\nちなみに、otool -L で Linux の ldd コマンドみたいな結果が得られました。\n$ **otool -L /Users/teraoka/.homebrew/Cellar/grpc/1.33.2\\_1/bin/grpc\\_cli** /Users/teraoka/.homebrew/Cellar/grpc/1.33.2\\_1/bin/grpc\\_cli: /Users/teraoka/.homebrew/opt/gflags/lib/libgflags.2.2.dylib (compatibility version 2.2.0, current version 2.2.2) /Users/teraoka/.homebrew/opt/protobuf/lib/libprotobuf.25.dylib (compatibility version 26.0.0, current version 26.0.0) /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1281.100.1) @rpath/libgrpc++.1.dylib (compatibility version 1.0.0, current version 1.33.2) @rpath/libgrpc++\\_test\\_config.1.dylib (compatibility version 1.0.0, current version 1.33.2) @rpath/libgrpc.13.dylib (compatibility version 13.0.0, current version 13.0.0) @rpath/libgpr.13.dylib (compatibility version 13.0.0, current version 13.0.0) @rpath/libaddress\\_sorting.13.dylib (compatibility version 13.0.0, current version 13.0.0) @rpath/libupb.13.dylib (compatibility version 13.0.0, current version 13.0.0) /Users/teraoka/.homebrew/opt/openssl@1.1/lib/libssl.1.1.dylib (compatibility version 1.1.0, current version 1.1.0) /Users/teraoka/.homebrew/opt/openssl@1.1/lib/libcrypto.1.1.dylib (compatibility version 1.1.0, current version 1.1.0) /usr/lib/libz.1.dylib (compatibility version 1.0.0, current version 1.2.11) /Users/teraoka/.homebrew/opt/c-ares/lib/libcares.2.dylib (compatibility version 2.0.0, current version 2.4.2) /usr/lib/libresolv.9.dylib (compatibility version 1.0.0, current version 1.0.0) /Users/teraoka/.homebrew/opt/re2/lib/libre2.dylib (compatibility version 0.0.0, current version 0.0.0) /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation (compatibility version 150.0.0, current version 1677.104.0) /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 902.1.0) Homebrew で rpath を指定する方法はまだ分からないけど、install_name_tool というコマンドでバイナリ内の rpath (LC_RPATH) を変更できることが分かりました。-add_rpath で追加するか -rpath で変更が可能だと分かりました。\ninstall_name_tool はバイナリを書き換えるため、write 権限をつけます。\n$ chmod u+w /Users/teraoka/.homebrew/Cellar/grpc/1.33.2\\_1/bin/grpc\\_cli -rpath で書き換えます。\n$ install\\_name\\_tool \\\\ -rpath /tmp/grpc-20201123-11869-1aso5q3/cmake/build \\\\ /Users/teraoka/.homebrew/lib \\\\ /Users/teraoka/.homebrew/Cellar/grpc/1.33.2\\_1/bin/grpc\\_cli 書き換えられました。\n$ **otool -l /Users/teraoka/.homebrew/Cellar/grpc/1.33.2\\_1/bin/grpc\\_cli | grep -B 1 -A 2 LC\\_RPATH** Load command 30 cmd LC\\_RPATH cmdsize 48 path /Users/teraoka/.homebrew/lib (offset 12) -add_rpath で追加した場合は次のようになります。\n$ **otool -l /Users/teraoka/.homebrew/Cellar/grpc/1.33.2\\_1/bin/grpc\\_cli | grep -B 1 -A 2 LC\\_RPATH** Load command 30 cmd LC\\_RPATH cmdsize 64 path /tmp/grpc-20201123-11869-1aso5q3/cmake/build (offset 12) -- -- Load command 33 cmd LC\\_RPATH cmdsize 48 path /Users/teraoka/.homebrew/lib (offset 12) どちらの場合も dyld: Library not loaded エラーを回避できます。インストールの度にやるのは面倒ですね、Homebrew の設定とかでなんとかならないものか。\nと、調べていたら気付きました\n\\==\u0026gt; Downloading https://homebrew.bintray.com/bottles/grpc-1.33.2\\_1.catalina.bottle.tar.gz あれ？ bottles ってお前、バイナリ配布だったのか\u0026hellip;\nということで、またその後知った brew edit というコマンドでで grpc_cli を build する args のところに -DCMAKE_BUILD_RPATH=/Users/teraoka/.homebrew/lib を追加して\n$ brew edit grpc $ brew reinstall --build-from-source grpc とすることで LC_RPATH が追加されました。\n$ **otool -l /Users/teraoka/.homebrew/Cellar/grpc/1.33.2\\_1/bin/grpc\\_cli | grep -B 1 -A 2 LC\\_RPATH** Load command 30 cmd LC\\_RPATH cmdsize 48 path /Users/teraoka/.homebrew/lib (offset 12) -- -- Load command 31 cmd LC\\_RPATH cmdsize 64 path /tmp/grpc-20201130-46642-1oq94ry/cmake/build (offset 12) めでたし、めでたし。\nが、コンパイルに時間かかるし、結局 edit しなきゃいけないしうーむ\u0026hellip;\n付録 # otool \u0026ndash;version``` $ otool \u0026ndash;version llvm-otool(1): Apple Inc. version cctools-949.0.1 Apple LLVM version 11.0.0 (clang-1100.0.33.17) Optimized build. Default target: x86_64-apple-darwin19.6.0 Host CPU: skylake\nRegistered Targets: aarch64 - AArch64 (little endian) aarch64_be - AArch64 (big endian) arm - ARM arm64 - ARM64 (little endian) armeb - ARM (big endian) thumb - Thumb thumbeb - Thumb (big endian) x86 - 32-bit X86: Pentium-Pro and above x86-64 - 64-bit X86: EM64T and AMD64\n otool usage $ otool Usage: /Library/Developer/CommandLineTools/usr/bin/otool [-arch arch_type] [-fahlLDtdorSTMRIHGvVcXmqQjCP] [-mcpu=arg] [\u0026ndash;version] \u0026hellip; -f print the fat headers -a print the archive header -h print the mach header -l print the load commands -L print shared libraries used -D print shared library id name -t print the text section (disassemble with -v) -x print all text sections (disassemble with -v) -p start dissassemble from routine name -s print contents of section -d print the data section -o print the Objective-C segment -r print the relocation entries -S print the table of contents of a library (obsolete) -T print the table of contents of a dynamic shared library (obsolete) -M print the module table of a dynamic shared library (obsolete) -R print the reference table of a dynamic shared library (obsolete) -I print the indirect symbol table -H print the two-level hints table (obsolete) -G print the data in code table -v print verbosely (symbolically) when possible -V print disassembled operands symbolically -c print argument strings of a core file -X print no leading addresses or headers -m don\u0026rsquo;t use archive(member) syntax -B force Thumb disassembly (ARM objects only) -q use llvm\u0026rsquo;s disassembler (the default) -Q use otool(1)\u0026rsquo;s disassembler -mcpu=arg use `arg' as the cpu for disassembly -j print opcode bytes -P print the info plist section as strings -C print linker optimization hints \u0026ndash;version print the version of /Library/Developer/CommandLineTools/usr/bin/otool  install\\_name\\_tool(1) INSTALL_NAME_TOOL(1) INSTALL_NAME_TOOL(1)\nNAME install_name_tool - change dynamic shared library install names\nSYNOPSIS install_name_tool [-change old new ] \u0026hellip; [-rpath old new ] \u0026hellip; [-add_rpath new ] \u0026hellip; [-delete_rpath new ] \u0026hellip; [-id name] file\nDESCRIPTION Install_name_tool changes the dynamic shared library install names and or adds, changes or deletes the rpaths recorded in a Mach-O binary. For this tool to work when the install names or rpaths are larger the binary should be built with the ld(1) -headerpad_max_install_names option.\n -change old new Changes the dependent shared library install name old to new in the specified Mach-O binary. More than one of these options can be specified. If the Mach-O binary does not contain the old install name in a specified -change option the option is ignored. -id name Changes the shared library identification name of a dynamic shared library to name. If the Mach-O binary is not a dynamic shared library and the -id option is specified it is ignored. -rpath old new Changes the rpath path name old to new in the specified Mach-O binary. More than one of these options can be specified. If the Mach-O binary does not contain the old rpath path name in a specified -rpath it is an error. -add\\_rpath new Adds the rpath path name new in the specified Mach-O binary. More than one of these options can be specified. If the Mach-O binary already contains the new rpath path name specified in -add\\_rpath it is an error. -delete\\_rpath old deletes the rpath path name old in the specified Mach-O binary. More than one of these options can be specified. If the Mach-O binary does not contains the old rpath path name specified in -delete\\_rpath it is an error.  SEE ALSO ld(1)\nApple, Inc. March 4, 2009 INSTALL_NAME_TOOL(1)\n","date":"2020年11月30日","permalink":"/2020/11/dyld-library-not-loaded/","section":"Posts","summary":"Advent Calendar 2020 全部オレシリーズ 1日目です。完走できるか心配です。 先日、Homebrew で grpc_cli (grpc) をインストールしたんですけれども、実行すると次のように共","title":"dyld: Library not loaded への対応方法"},{"content":"","date":"2020年11月30日","permalink":"/tags/macos/","section":"Tags","summary":"","title":"macOS"},{"content":"","date":"2020年11月24日","permalink":"/tags/chaos-engineering/","section":"Tags","summary":"","title":"Chaos Engineering"},{"content":"","date":"2020年11月24日","permalink":"/tags/litmus/","section":"Tags","summary":"","title":"Litmus"},{"content":"Chaos Mesh を少しかじっていましたが、最近話題の Litmus に入門してみます。Litmus には Chaos Mesh にはなかった EC2 Instance の停止や Docker Daemon の停止や kubelet の停止などができるのが魅力ですね。\n商用 Chaos Engineering ツールを提供している Gremlin が Chaos Engineering tools comparison というドキュメントを公開してくれていて、触ったことがあるものは納得感のある説明でした。Litmus は確かに面倒、Chaos Mesh がセキュリティ的に良くないというのもわかるが、そもそも Production で使おうなどとは思っていなかった。\n以下、minikube v1.15.1 での Kubernetes 1.18.8 と litmus 1.10.0 で試しました。\nインストール # Helm もありますが、今回は manifest をそのまま適用しました。\nkubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v1.10.0.yaml litmus Namespace に chaos-operator-cd Deployment とそれ用の litmus ServiceAccount と CRD が3つ (chaosengines.litmuschaos.io, chaosexperiments.litmuschaos.io, chaosresults.litmuschaos.io) 作成されます。\nnginx の deploy # Pod Delete Experiment を実行するために、delete 対象となる Deployment を deploy します。\nhelm create コマンドで nginx の deployment 用の chart が作成されるのでこれを使います。\nkubectl create namespace nginx helm create nginx cd nginx helm install nginx . -n nginx --set replicaCount=3 これで replicas 3 の nginx Deployment がデプロイされます。\n❯ kubectl get pod -n nginx NAME READY STATUS RESTARTS AGE nginx-577ccbcdd5-hsdpj 1/1 Running 0 165m nginx-577ccbcdd5-lvrrk 1/1 Running 0 165m nginx-577ccbcdd5-z97jr 1/1 Running 0 165m Chaos Experiments のインストール # Litmus では experiment を実行する各 namespace に chaosexperiment をインストールする必要があるようです。\nkubectl apply -f \u0026quot;https://hub.litmuschaos.io/api/chaos/1.10.0?file=charts/generic/experiments.yaml\u0026quot; -n nginx ❯ kubectl get chaosexperiments -n nginx NAME AGE container-kill 22h disk-fill 22h disk-loss 22h docker-service-kill 22h k8-pod-delete 22h k8-service-kill 22h kubelet-service-kill 22h node-cpu-hog 22h node-drain 22h node-io-stress 22h node-memory-hog 22h node-taint 22h pod-autoscaler 22h pod-cpu-hog 22h pod-delete 22h pod-io-stress 22h pod-memory-hog 22h pod-network-corruption 22h pod-network-duplication 22h pod-network-latency 22h pod-network-loss 22h 今回使うのは pod-delete だけですけどね。\nPod Delete Experiment 実行用の ServiceAccount を作成 # pod-delete 用の ServiceAccount を作成します。pod-delete に限定しなくてもこの namespace で使う experiment 用の権限を一つにまとめてしまっても良いと思いますけど、まあ今回は pod-delete しかしないので。\nkubectl apply -f - \u0026lt;\u0026lt;EOF --- apiVersion: v1 kind: ServiceAccount metadata: name: pod-delete-sa namespace: nginx labels: name: pod-delete-sa --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: pod-delete-sa namespace: nginx labels: name: pod-delete-sa rules: - apiGroups: [\u0026quot;\u0026quot;,\u0026quot;litmuschaos.io\u0026quot;,\u0026quot;batch\u0026quot;,\u0026quot;apps\u0026quot;] resources: [\u0026quot;pods\u0026quot;,\u0026quot;deployments\u0026quot;,\u0026quot;pods/log\u0026quot;,\u0026quot;events\u0026quot;,\u0026quot;jobs\u0026quot;,\u0026quot;chaosengines\u0026quot;,\u0026quot;chaosexperiments\u0026quot;,\u0026quot;chaosresults\u0026quot;] verbs: [\u0026quot;create\u0026quot;,\u0026quot;list\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;patch\u0026quot;,\u0026quot;update\u0026quot;,\u0026quot;delete\u0026quot;,\u0026quot;deletecollection\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: pod-delete-sa namespace: nginx labels: name: pod-delete-sa roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: pod-delete-sa subjects: - kind: ServiceAccount name: pod-delete-sa namespace: nginx EOF Experiment 対象となるように Deployment に annotation を設定する # 勝手に experiment 対象とされないようにアプリ (Deployment) のオーナーが litmuschaos.io/chaos=\u0026quot;true\u0026quot; という annotation をつけないと対象とならないようになっています。安全ですね、と思いかけたけど ChaosEngine リソースの annotationCheck を false にしたらそんなの無視するみたいです\u0026hellip;\nkubectl annotate deploy/nginx litmuschaos.io/chaos=\u0026quot;true\u0026quot; -n nginx これが設定されていないと Unable to filter app by specified info, Chaos stopped due to failed app identification とすぐに終了してしまいます (kubectl get events より)。\n0s Normal ChaosEngineInitialized chaosengine/nginx-chaos nginx-chaos-runner created successfully 0s Warning ChaosResourcesOperationFailed chaosengine/nginx-chaos Unable to filter app by specified info 0s Warning ChaosEngineStopped chaosengine/nginx-chaos Chaos stopped due to failed app identification ChaosEngine リソースの作成 # ようやく準備ができたので ChaosEngine リソースを作成することでやっと Pod の delete を行うことができます。applabel の値は今回の helm でデプロイした nginx にはついていない label であるため変更しました。\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: litmuschaos.io/v1alpha1 kind: ChaosEngine metadata: name: nginx-chaos namespace: nginx spec: appinfo: appns: 'nginx' applabel: 'app.kubernetes.io/name=nginx' # 'app=nginx' から変更 appkind: 'deployment' # It can be true/false annotationCheck: 'true' # It can be active/stop engineState: 'active' #ex. values: ns1:name=percona,ns2:run=nginx auxiliaryAppInfo: '' chaosServiceAccount: pod-delete-sa monitoring: false # It can be delete/retain jobCleanUpPolicy: 'delete' experiments: - name: pod-delete spec: components: env: # set chaos duration (in sec) as desired - name: TOTAL_CHAOS_DURATION value: '30' # set chaos interval (in sec) as desired - name: CHAOS_INTERVAL value: '10' # pod failures without '--force' \u0026amp; default terminationGracePeriodSeconds - name: FORCE value: 'false' EOF これを apply すると次のような状況になります。まず、nginx-chaos-runner という Pod が起動され、そこから pod-delete-xxxxxx という Job が作成され pod-delete-xxxxxx-zzzzz という Pod が起動されて label にマッチする Pod を delete します。\n❯ kubectl get pod -n nginx NAME READY STATUS RESTARTS AGE nginx-577ccbcdd5-8spjq 1/1 Running 0 4m14s nginx-577ccbcdd5-g5xsb 0/1 ContainerCreating 0 2s nginx-577ccbcdd5-l9mqr 1/1 Running 0 4m23s nginx-577ccbcdd5-qwk6g 0/1 Terminating 0 7m53s nginx-chaos-runner 1/1 Running 0 13s pod-delete-o04orr-k4xm6 1/1 Running 0 10s 対象選択の appinfo の appns, appkind は見たままですが、 applabel ですこしハマりました。appkind で指定された Deployment の label にもマッチする必要があるし、そこから作成され、実際に削除される Pod もこの label にマッチする必要がありました。\nDeployment の label にマッチしない場合は nginx-chaos-runner がすぐさま終了します。Deployment にはマッチしたが Pod にはマッチしなかった場合は pod-delete-xxxxxx-zzzzz という Pod が3分ほど待機してマッチする Pod が現れるのを待ちます。それでも現れない場合は Application status check failed, err: Unable to find the pods with matching labels, err:  というエラーで終了します。\nChaosEngine の env で指定されている値は Supported Experiment Tunables に説明があります。ここで使われているものはコメントが入っていますが TOTAL_CHAOS_DURATION はこの期間(秒) Pod の削除が繰り返されるという意味で、削除と削除の間隔が CHAOS_INTERVAL (秒) です。\nTOTAL_CHAOS_DURATION (秒) 経過すると pod-delete-xxxxxx-zzzzz という Pod も pod-delete-xxxxxx という Job も nginx-chaos-runner という Pod も終了します。jobCleanUpPolicy が delete であれば消えます。大した情報は入っていませんが、ChaosResult というリソースが作成されます。\n❯ kubectl get chaosresult -n nginx NAME AGE nginx-chaos-pod-delete 6h25m 終了すると engineState が stop になっています。これを再度 active に書き換えると再度実行されます。\n以上。他の Experiment や Litmus Portal (User Guide) や Litmus Probes も気になりますね。ただ、Chaos Mesh と比べるとだいぶ面倒。\nYouTube に動画がありました。\n","date":"2020年11月24日","permalink":"/2020/11/litmus-getstarted/","section":"Posts","summary":"Chaos Mesh を少しかじっていましたが、最近話題の Litmus に入門してみます。Litmus には Chaos Mesh にはなかった EC2 Instance の停止や Docker Daemon の停止や kubelet の停止などができるのが","title":"Litmus 入門"},{"content":"以前、「SSM Session Manager 経由での SSH」で、Public IP address を持たない EC2 Instance に対して SSH 接続する方法を確認したが、SSM の Session Manager だけでは事前に EC2 Instance 側に Public Key が登録されている必要があった。\nしかし、今回 Public Key の登録されていない Instance に SSH したくなった。確か、一時的な Public Key を登録する機能があったよな、ということでメモっておく。\n一時的な Public Key を送った後に Session Manger を使って接続すれば今回やりたいことができる。\nPublic Key の登録は aws ec2-instance-connect send-ssh-public-key コマンドで行う。\naws ec2-instance-connect send-ssh-public-key \\\\ --instance-id i-xxxxxxxxxxxxxxxxx \\\\ --instance-os-user ec2-user \\\\ --availability-zone ap-northeast-1c \\\\ --ssh-public-key file://$HOME/.ssh/id\\_rsa.pub わざわざ --availability-zone を指定しなくてはならないというのが面倒だが Instance Id から取ってくる wrapper を書く。\naws ec2 describe-instances \\\\ --instance-ids i-xxxxxxxxxxxxxxxxx \\\\ --query 'Reservations\\[0\\].Instances\\[0\\].Placement.AvailabilityZone' \\\\ --output text send-ssh-public-key で登録した Public Key は 60 秒間だけ有効なのでその間に SSM の Session Manager で接続します。これは「SSM Session Manager 経由での SSH」で書いた通りで ~/.ssh/config に次の様に書いておけば ssh ec2-user@i-xxxxxxxxxxxxxxxxx で接続できます。\nhost i-\\* mi-\\* ProxyCommand sh -c \u0026quot;aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters 'portNumber=%p'\u0026quot; とりあえずこんな wrapper で\n#!/bin/bash instance\\_id=$1 user=ec2-user if \\[ -z \u0026quot;$instance\\_id\u0026quot; \\] ; then echo \u0026quot;Usage: ssmssh \\[username@\\]instance\\_id\u0026quot; 1\u0026gt;\u0026amp;2 exit 1 fi echo $instance\\_id | grep -q @ if \\[ $? -eq 0 \\] ; then user=$(echo $instance\\_id | cut -d @ -f 1) instance\\_id=$(echo $instance\\_id | cut -d @ -f 2) fi echo $instance\\_id | grep -q ^i- if \\[ $? -ne 0 \\] ; then echo \u0026quot;invalid instance id: $instance\\_id\u0026quot; 1\u0026gt;\u0026amp;2 exit 2 fi echo \u0026quot;Getting availability zone of instance\u0026quot; 1\u0026gt;\u0026amp;2 az=$(aws ec2 describe-instances \\\\ --instance-ids $instance\\_id \\\\ --query 'Reservations\\[0\\].Instances\\[0\\].Placement.AvailabilityZone' \\\\ --output text ) echo \u0026quot;Sending ssh public key\u0026quot; 1\u0026gt;\u0026amp;2 aws ec2-instance-connect send-ssh-public-key \\\\ --instance-id $instance\\_id \\\\ --instance-os-user $user \\\\ --availability-zone $az \\\\ --ssh-public-key file://$HOME/.ssh/id\\_rsa.pub echo \u0026quot;exec ssh $user@$instance\\_id\u0026quot; 1\u0026gt;\u0026amp;2 exec ssh $user@$instance\\_id ","date":"2020年11月23日","permalink":"/2020/11/send-ssh-public-key-and-ssm-start-session/","section":"Posts","summary":"以前、「SSM Session Manager 経由での SSH」で、Public IP address を持たない EC2 Instance に対して SSH 接続する方法を確認したが、SSM の Session Manager だけでは事前に EC2 Instance 側に Public","title":"send-ssh-public-key と ssm start-session の合わせ技"},{"content":"Linux で何か調査をしていて、lsof が使えない場合に /proc/{PID}/fd 配下でそのプロセスが開いているファイルやソケットを確認したりしますが、ソケットの場合、通信相手が分かりませんでした。私は。でも知ってしまったのですその方法を。（数ヶ月前に）\nということで次回以降のためにメモです。\n\\# ls -l /proc/5322/fd total 0 lr-x------. 1 root root 64 Oct 17 15:23 0 -\u0026gt; /dev/null lrwx------. 1 root root 64 Oct 17 15:23 1 -\u0026gt; /dev/null lrwx------. 1 root root 64 Oct 17 15:23 2 -\u0026gt; /dev/null lrwx------. 1 root root 64 Oct 17 15:23 3 -\u0026gt; socket:\\[**40495282**\\] lrwx------. 1 root root 64 Oct 17 15:23 4 -\u0026gt; socket:\\[40496301\\] lrwx------. 1 root root 64 Oct 17 15:23 5 -\u0026gt; /dev/ptmx l-wx------. 1 root root 64 Oct 17 15:23 6 -\u0026gt; /run/systemd/sessions/7567.ref lrwx------. 1 root root 64 Oct 17 15:23 7 -\u0026gt; socket:\\[40495333\\] fd 配下の symbolic link 先の socket の後ろにある番号で /proc/{PID}/net/tcp を grep すると次のようなテキストが見つかります\n\\# head -n 1 /proc/5322/net/tcp; grep 40495282 /proc/5322/net/tcp sl local\\_address rem\\_address st tx\\_queue rx\\_queue tr tm-\u0026gt;when retrnsmt uid timeout inode 9: 0200000A:0016 7100CBEA:C6CE 01 00000000:00000000 02:00098690 00000000 0 0 **40495282** 2 ffff9fd048647800 21 4 1 10 35 この local_address, rem_address がそれぞれ ipaddress:port を表しています。IP アドレスは 2 オクテットづつに分割して 16 進数を 10 進数にして順序を逆にしてドットでつなげれば見慣れた形式になります。\n$ printf \u0026quot;%d.%d.%d.%d\\\\n\u0026quot; 0x0a 0x00 0x00 0x02 10.0.0.2 $ printf \u0026quot;%d.%d.%d.%d\\\\n\u0026quot; 0xEA 0xCB 0x00 0x71 234.203.0.113 ポート番号\n$ printf \u0026quot;%d\\\\n\u0026quot; 0x0016 22 $ printf \u0026quot;%d\\\\n\u0026quot; 0xC6CE 50894 gist.github.com/jkstill/5095725 にファイルの中身の説明があります。st の tcp state は tcp_states.h にありまして 01 は ESTABLISHED です。\nsocket は tcp に限らないため grep は /proc/{PID}/net/* を対象にすると良いかもしれません。tcp6 かもしれないし unix かもしれない。unix domain socket ってどうやって相手を探すんだろうな？\nこれを知るきっかけは「Why Fluentd stopped to send logs to ElasticSearch on Kubernetes (Related To SSL)」でした。ありがとうございます。 m(_ _)m\n","date":"2020年10月17日","permalink":"/2020/10/socket-in-proc-pid-fd/","section":"Posts","summary":"Linux で何か調査をしていて、lsof が使えない場合に /proc/{PID}/fd 配下でそのプロセスが開いているファイルやソケットを確認したりしますが、ソケットの場合、通信","title":"/proc/PID/fd の socket の接続先を調べる方法"},{"content":"","date":"2020年10月17日","permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux"},{"content":"Lightsail の wordpress (bitnami) イメージを使ってこのブログを運用していましたが、PHP の更新が必要だけど bitnami でのやり方がよくわからんし、調べるのも面倒ということで 1 vCPU, 1GB メモリの VM を2まで無料で使える Oracle Cloud に移設 \u0026amp; コンテナ化してしまうことにしました。 (が、Oracle Cloud の使い方を調べるのも超面倒\u0026hellip; しかも学ぶモチベーションが\u0026hellip; やっちまった)\nコンテナとして実行するわけですが単一ホストでの実行とする、あるいはメモリ的に厳しければ DB を別サーバーに分けることにします。まずは単一サーバーで docker-compose での実行を試みます。シングルノードの swarm で rolling update というのもありますがとりあえずまだ考えない。\nDocker Engine, docker-compose のインストール # Oracle Cloud なので Oracle Linux が最も最適化されているのだろうという勝手な思い込みで Oracle Linux 7.x を使うことにします。\nsudo yum -y install docker-engine sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.27.3/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose.yml の作成 # Wordpress のコンテナイメージは wordpress:5.5.1-php7.4-apache で、MySQL のコンテナイメージは mysql/mysql-server:8.0.21 を使うことにします。こっちの MySQL は Oracle の MySQL Team がメンテナンスしているようです。Dockerfile などは github.com/mysql/mysql-docker にあります。\nwordpress コンテナが起動時にどんな処理を行っているのかは docker-entrypoint.sh を参照。\nで、次の docker-compose.yml となりました。wp-content/plugins と wp-content/themes には旧環境のファイルをコピーしておきます。\nversion:\u0026#34;3.8\u0026#34;services:db:image:mysql/mysql-server:8.0.21volumes:- ./mysql/data:/var/lib/mysql- ./mysql/backup:/backupenvironment:MYSQL_ROOT_PASSWORD:$MYSQL_ROOT_PASSWORDMYSQL_USER:wordpressMYSQL_PASSWORD:$DB_PASSWORDMYSQL_DATABASE:wordpresswordpress:image:wordpress:5.5.1-php7.4-apachevolumes:- ./wordpress/wp-content/plugins:/var/www/html/wp-content/plugins- ./wordpress/wp-content/themes:/var/www/html/wp-content/themes- ./wordpress/wp-content/uploads:/var/www/html/wp-content/uploads- ./wordpress/php.ini:/usr/local/etc/php/php.ini:ro- ./wordpress/backup:/backupdepends_on:- dbenvironment:# https://api.wordpress.org/secret-key/1.1/salt/WORDPRESS_AUTH_KEY:$WORDPRESS_AUTH_KEYWORDPRESS_SECURE_AUTH_KEY:$WORDPRESS_SECURE_AUTH_KEYWORDPRESS_LOGGED_IN_KEY:$WORDPRESS_LOGGED_IN_KEYWORDPRESS_NONCE_KEY:$WORDPRESS_NONCE_KEYWORDPRESS_AUTH_SALT:$WORDPRESS_AUTH_SALTWORDPRESS_SECURE_AUTH_SALT:$WORDPRESS_SECURE_AUTH_SALTWORDPRESS_LOGGED_IN_SALT:$WORDPRESS_LOGGED_IN_SALTWORDPRESS_NONCE_SALT:$WORDPRESS_NONCE_SALTWORDPRESS_DB_HOST:dbWORDPRESS_DB_USER:wordpressWORDPRESS_DB_PASSWORD:$DB_PASSWORDWORDPRESS_DB_NAME:wordpressWORDPRESS_DB_CHARSET:utf8mb4WORDPRESS_DB_COLLATE:utf8mb4_binWORDPRESS_CONFIG_EXTRA:|define(\u0026#39;WPMS_ON\u0026#39;, true); define(\u0026#39;WPMS_MAIL_FROM\u0026#39;, \u0026#39;$SMTP_USER\u0026#39;); define(\u0026#39;WPMS_MAIL_FROM_FORCE\u0026#39;, true); define(\u0026#39;WPMS_MAILER\u0026#39;, \u0026#39;smtp\u0026#39;); define(\u0026#39;WPMS_SMTP_HOST\u0026#39;, \u0026#39;smtp.gmail.com\u0026#39;); define(\u0026#39;WPMS_SMTP_PORT\u0026#39;, 465); define(\u0026#39;WPMS_SSL\u0026#39;, \u0026#39;ssl\u0026#39;); define(\u0026#39;WPMS_SMTP_AUTH\u0026#39;, true); define(\u0026#39;WPMS_SMTP_USER\u0026#39;, \u0026#39;$SMTP_USER\u0026#39;); define(\u0026#39;WPMS_SMTP_PASS\u0026#39;, \u0026#39;$SMTP_PASS\u0026#39;); define(\u0026#39;WPMS_SMTP_AUTOTLS\u0026#39;, true); if (strpos($$_SERVER[\u0026#39;HTTP_CLOUDFRONT_FORWARDED_PROTO\u0026#39;], \u0026#39;https\u0026#39;) !== false) { $$_SERVER[\u0026#39;HTTPS\u0026#39;]=\u0026#39;on\u0026#39;; } if (strtolower($$_SERVER[\u0026#39;HTTPS\u0026#39;]) == \u0026#39;on\u0026#39;) { define(\u0026#39;WP_SITEURL\u0026#39;, \u0026#39;https://\u0026#39; . $$_SERVER[\u0026#39;HTTP_HOST\u0026#39;] . \u0026#39;/\u0026#39;); define(\u0026#39;WP_HOME\u0026#39;, \u0026#39;https://\u0026#39; . $$_SERVER[\u0026#39;HTTP_HOST\u0026#39;] . \u0026#39;/\u0026#39;); } else { define(\u0026#39;WP_SITEURL\u0026#39;, \u0026#39;http://\u0026#39; . $$_SERVER[\u0026#39;HTTP_HOST\u0026#39;] . \u0026#39;/\u0026#39;); define(\u0026#39;WP_HOME\u0026#39;, \u0026#39;http://\u0026#39; . $$_SERVER[\u0026#39;HTTP_HOST\u0026#39;] . \u0026#39;/\u0026#39;); }ports:- 80:80tmpfs:- /run- /tmpWORDPRESS_CONFIG_EXTRA は wp-config.php に追記するコードです。WPMS_ で始まるものは WP Mail SMTP 用の設定です。ここでは gmail の SMTP サーバーを使うことにしています。Oracle Cloud にも SMTP サービスあるんですね。CloudFront は X-Forwarded-Proto ではなく CloudFront-Forwarded-Proto ヘッダーを挿入してくるのでそのための設定も入れています。$XXX は docker-compose 実行時の環境変数に置き換えられますが、WORDPRESS_CONFIG_EXTRA の中での PHP の変数としての $_SERVER などは $$ として $ をエスケープする必要があります。\nphp.ini のカスタマイズは /usr/local/etc/php/conf.d ディレクトリ内にファイルをマウントすれば良いのですが、php.ini-production を使うべく、これをコンテナ内から取り出して編集して php.ini としてマウントすることにしました。\ncapabilities 設定もやるべきかな\nOracle Cloud の Security List (SecurityGroup みたいなやつ) # 初期状態では外部から port 80 へはアクセスできませんでした。Instance の Network Security Groups ってのを設定すれば良さそうな感じではあるものの、その方法がわかりませんでした。Subnet の Default Security List というのがデフォルトでは 22/tcp と ICMP の type 3,4 だけを許可するようになっていたのでとりあえずここに 80/tcp を追加しました。Subnet のデフォルトルールで許可してしまうのは本来は良くないとは思うものの現状、他のインスタンスは使わないし、本来の設定方法もすぐには見つからないし、どれが無料でどれが有料かもわからないのでとりあえずこれで。仕事で使うわけでもないので Oracle Cloud を真面目に調査するの嫌だ\u0026hellip;\nBackup # docker-compose exec で db コンテナ内で mysqldump を実行しホストのディレクトリをマウントしている /backup に書き出します。パスワードはコンテナ起動時に環境変数で渡してあるのでそれを使いますが、-p で指定するとセキュアじゃないよと Warning メッセージが出力されてうざいので MYSQL_PWD 変数に設定しています。conf ファイルに書くべきなのかもしれない。\ndocker-compose exec -T db \\  bash -c \u0026#34;MYSQL_PWD=\\$MYSQL_ROOT_PASSWORD \\ mysqldump --add-drop-table -u root wordpress \\ | gzip -9 \u0026gt; /backup/wordpress.dump.\\$(date +%a).sql.gz\u0026#34; upload したファイルはディレクトリをマウントしているだけなのでホスト側で tar にでもすれば ok。\nさて、これらを Oracle Cloud の Object Storage に保存するには\u0026hellip;\noci コマンドを使うと良いらしい Getting started with the OCI Command Line Interface (CLI)\nbash -c \u0026#34;$(curl –L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34; でインストールできると書いてあるけど、Python 3 が入っていれば Home directory 配下に venv でインストールするだけっぽいので root で実行しなくても大丈夫。~/lib/oracle-cli 配下にインストールされて、~/bin/oci にシンボリックリンクが張られました。\nが、権限設定周りが全然わからん\u0026hellip;\nデータ移行 # 旧環境からのデータ移行は Wordpress の export / import 機能を使ったのだけれど、widget 設定とかが飛んんでしまうのはなんとかならないものか？ ファイルのコピーと DB の export / import にした方が良かったのかな。\n","date":"2020年9月21日","permalink":"/2020/09/setup-wordpress-using-docker-compose/","section":"Posts","summary":"Lightsail の wordpress (bitnami) イメージを使ってこのブログを運用していましたが、PHP の更新が必要だけど bitnami でのやり方がよくわからんし、調べるのも面倒ということで 1 vCPU,","title":"docker-compose で wordpress サーバーを構築"},{"content":"","date":"2020年9月21日","permalink":"/tags/wordpress/","section":"Tags","summary":"","title":"WordPress"},{"content":"2020年8月21日に Istio 1.7 がリリースされました。その RELEASE NOTE の Production operability improvements 項に次の節を見つけました。\n You can delay the application start until after the sidecar is started. This increases the reliability for deployments where the application needs to access resources via its proxy immediately upon its boot.\n 「サイドカーの起動が完了するまでアプリケーションの開始を遅らせることが出来るよ」とありますね、みんなが待ち望んでいたやつです。istoi-proxy (envoy) の起動が完了する前にアプリが起動しちゃって通信でコケるということにならないように envoy の status port にアクセスして起動を確認してからアプリのプロセスを起動するとか書かなくても良くなります。ステキ！\n仕組みはリンク先で説明されています、kubelet は manifest の containers に書かれた順にシーケンシャルに起動させ、postStart があればそれを待つから、サイドカーをリストの先に定義して postStart でそのサイドカープロセスの起動が完了するのを待てば良いということのようです。\nこれを実現するための変更が istio/istio #24737 で、Istio 1.7.0 に含まれています。\nIstioOperator 用の manifest で次の設定を入れるか、istioctl manifest generate --set values.global.proxy.holdApplicationUntilProxyStarts=true などとすれば istio-proxy サイドカーが containers の先頭に挿入され、postStart hook が挿入されます。\n values: global: proxy: holdApplicationUntilProxyStarts: true istio-sidecar-injector という ConfigMap に次のような箇所があります。\n {{- if .Values.global.proxy.lifecycle }} lifecycle: {{ toYaml .Values.global.proxy.lifecycle | indent 4 }} {{- else if .Values.global.proxy.holdApplicationUntilProxyStarts}} lifecycle: postStart: exec: command: - pilot-agent - wait {{- end }} あれ？このコードだと lifecyle をすでに指定してたら postStart も自前で書く必要がありますね。Pod 停止時に先に istio-proxy が停止してしまうと通信できなくなってしまうため、istio-proxy の preStop にはすでになんらかの処理を入れてますよね？ ということはそこで postStart も設定する必要があります。\nこういうことですね。\n values: global: proxy: holdApplicationUntilProxyStarts: true lifecycle: preStop: exec: command: - \u0026quot;/bin/sh\u0026quot; - \u0026quot;-c\u0026quot; - \u0026quot;while \\[ $(netstat -plnt | grep tcp | egrep -v 'envoy|pilot-agent' | wc -l) -ne 0 \\]; do sleep 1; done\u0026quot; postStart: exec: command: - pilot-agent - wait 検証してね！\n","date":"2020年8月25日","permalink":"/2020/08/delaying-application-start-until-sidecar-is-ready/","section":"Posts","summary":"2020年8月21日に Istio 1.7 がリリースされました。その RELEASE NOTE の Production operability improvements 項に次の節を見つけました。 You can delay the application start until after the sidecar is started. This increases the reliability for deployments where the application needs to access resources","title":"メインコンテナの起動前に istio-proxy の起動を完了させる"},{"content":"","date":"2020年8月19日","permalink":"/tags/caddy/","section":"Tags","summary":"","title":"Caddy"},{"content":"ローカルで使うための https な Reverse Proxy が欲しい # Kubernetes で実行している Web サービスにて対して kubectl port-forward でアクセスすることが良くありますが、そのサービスが Cookie を使っており、secure フラグが必須となっている場合があります。大変面倒です。便利な Reverse Proxy サーバーがないものかと探しました。nodejs で書かれた local-ssl-proxy は見つかりましたが、私は nodejs が好きじゃないのでこれをローカルには入れたくありません。Docker で動かすにしても mac なので docker から host の localhost にアクセスするにはどうすれば良いのでしょう？調べるのも面倒です\u0026hellip;\n追記\nDocker on Mac, Docker on Windows の場合、 host.docker.internal という名前で Host にアクセスすることができます。Host 側で loopback device (127.0.0.1, ::1) しか bind していなくてもアクセス可能でした。でも、先の local-ssl-proxy は proxy 先のホストを自由に指定できないので使えそうにない。\n思い出した # Go で書かれたシングルバイナリのものとかないかな、なかったら自分用に書こうかななんて思っていた時、思い出しました。Caddy です。このブログでも 2017 年に紹介しました。 当時のライセンスの影響か、広く普及はしませんでしたがまだ死んでいません。\nMac の homebrew でインストール可能です。\nワンライナーで Reverse Proxy # サイト にもある通り、Production Ready な Reverse Proxy がたったこの1行で起動するんですって！！ ステキですね、望んでいたやつですね。\n$ caddy reverse-proxy --from example.com --to localhost:9000 で、実際にこれを実行すると example.com の証明書を Let\u0026rsquo;s Encrypt の TLS-ALPN-01 や HTTP-01 で取得しようとします。公開サーバーで使うなら必要ですが、ローカルではそれは望んでいないんですよね。\nそこで、from 指定をやめてみると localhost 用の証明書を作ってくれました。\n$ caddy reverse-proxy --to localhost:9000 certutil が無いよと言われたら brew install nss でインストールしましょう。\nWarning: \u0026quot;certutil\u0026quot; is not available, install \u0026quot;certutil\u0026quot; with \u0026quot;brew install nss\u0026quot; and try again Mac では証明書などは ~/Library/Application Support/Caddy 配下にファイルとして保存されます。また root 証明書として Keychain や Java の keystore、 Firefox に保存しようとします。これを削除したい場合は caddy untrust コマンドを実行すれば良いみたいです。\n$ find ~/Library/Application\\ Support/Caddy -type f /Users/teraoka/Library/Application Support/Caddy/certificates/local/localhost/localhost.crt /Users/teraoka/Library/Application Support/Caddy/certificates/local/localhost/localhost.json /Users/teraoka/Library/Application Support/Caddy/certificates/local/localhost/localhost.key /Users/teraoka/Library/Application Support/Caddy/autosave.json /Users/teraoka/Library/Application Support/Caddy/pki/authorities/local/root.crt /Users/teraoka/Library/Application Support/Caddy/pki/authorities/local/intermediate.key /Users/teraoka/Library/Application Support/Caddy/pki/authorities/local/root.key /Users/teraoka/Library/Application Support/Caddy/pki/authorities/local/intermediate.crt autosave.json は Caddy 用の設定ファイルで Reverse Proxy 設定が保存されています。\n証明書の情報は次のようになっていました。\n# root.crt Issuer: CN=Caddy Local Authority - 2020 ECC Root Validity Not Before: Aug 19 05:41:08 2020 GMT Not After : Jun 28 05:41:08 2030 GMT Subject: CN=Caddy Local Authority - 2020 ECC Root # intermediate.crt Issuer: CN=Caddy Local Authority - 2020 ECC Root Validity Not Before: Aug 19 05:41:08 2020 GMT Not After : Aug 26 05:41:08 2020 GMT Subject: CN=Caddy Local Authority - ECC Intermediate # localhost.crt Issuer: CN=Caddy Local Authority - ECC Intermediate Validity Not Before: Aug 19 14:37:39 2020 GMT Not After : Aug 20 02:38:39 2020 GMT Subject: ... X509v3 Subject Alternative Name: DNS:localhost SNI が必須なので openssl コマンドでは -servername オプションが必要です。 (ちゃんと設定ファイルを書くなら default_sni という設定もある)\n$ openssl s_client -connect localhost:443 -servername localhost -showcerts curl でアクセスするには -k / --insecure をつけるか --cacert で root.crt を指定します。\n$ curl --cacert \u0026quot;/Users/teraoka/Library/Application Support/Caddy/pki/authorities/local/root.crt\u0026quot; https://localhost/ 肝心のブラウザからのアクセスですが、Keychain に root 証明書として登録されているけど Chrome はアクセスを認めてくれませんし、Firefox でも警告が出ます 😢\n数ヶ月後、再度試してみたところ警告は出なくなっていました。警告が出る場合はキーチェーンアクセスを開いて Caddy Local Authority が信頼されているかどうかを確認します。Caddy Local Authority をダブルクリックで開いて信頼セクションを開いて SSL が「常に信頼」となっていることを確認します。なっていなければ変更します。\nその他、証明書に関する情報は Automatic HTTPS ページに書かれています。\nlocalhost 以外で証明書を自動発行させる方法 # これまでの情報で localhost や 127.0.0.1 に対しては intermediate.crt を使って証明書を発行してくれることがわかりましたが、実際にサービスで使っているドメインを使用したい場合もあります。\nどうするか\ncaddy reverse-proxy コマンドを諦めて、設定ファイルを書きます。\nでも非常に簡単です。例えば、caddy.1q77.com というドメインを使い、localhost:8080 に Proxy したい場合は次のように任意のファイルに書くだけです。ここではファイル名は Caddyfile とします。\ncaddy.1q77.com tls internal reverse_proxy localhost:8080 これを使って起動させるには caddy run コマンドを使います。background で実行したい場合は caddy start とします。\n$ caddy run --config Caddyfile 別で発行した証明書を使いたい場合 # せっかく任意のドメインでも証明書が発行できるようになったのですが、やはり Chrome と Safari は中間証明書が信用ならんと言ってアクセスさせてくれません\u0026hellip; この件はまた後日調査する\nこれでは困るので別途取得済みの証明書を使う方法です。lego を使い、 Let\u0026rsquo;s Encrypt の DNS-01 で取得したものを使ってみます。これも簡単で設定ファイルに次のように書くだけです。\ncaddy.1q77.com tls /Users/teraoka/.lego/certificates/caddy.1q77.com.crt /Users/teraoka/.lego/certificates/caddy.1q77.com.key reverse_proxy localhost:8080 起動方法は先ほどと同じ。ドキュメントはこちら(tls)。マルチドメインで Proxy したい場合は {} を使った構文にする必要があります。ドキュメントを参照してください。\nCaddy 内での証明書発行には https://smallstep.com/certificates/ が使われているみたいです。\nではでは、良いローカル開発ライフを！\n","date":"2020年8月19日","permalink":"/2020/08/one-liner-https-reverse-proxy-caddy/","section":"Posts","summary":"ローカルで使うための https な Reverse Proxy が欲しい # Kubernetes で実行している Web サービスにて対して kubectl port-forward でアクセスすることが良くありますが、そのサービスが Cookie を使ってお","title":"ワンライナーで https の Reverse Proxy を実行する"},{"content":"https://httpbin.org/ は HTTP クライアントや Reverse Proxy のテストなどで非常に便利なサイトです。Docker Image も公開されているのでローカルでも使えます。大変お世話になっております。\nでもなぜか /headers などにアクセスしても X-Forwarded-For や X-Forwarded-Proto などが表示されません。\nそれを確認するために HTTP サーバーを書いたりもしていたのですが、show_env というパラメータを渡すことで確認できることを知ったのでメモ。\nhttps://httpbin.org/get?show_env=1 や https://httpbin.org/headers?show_env=1\nhttps://httpbin.org/legacy には例が載っていました。\n❯ curl -s http://httpbin.org/headers\\?show_env=1 { \u0026quot;headers\u0026quot;: { \u0026quot;Accept\u0026quot;: \u0026quot;*/*\u0026quot;, \u0026quot;Host\u0026quot;: \u0026quot;httpbin.org\u0026quot;, \u0026quot;User-Agent\u0026quot;: \u0026quot;curl/7.64.1\u0026quot;, \u0026quot;X-Amzn-Trace-Id\u0026quot;: \u0026quot;Root=1-5f2ac9c1-43a9f7d5cf1b714ad7798979\u0026quot;, \u0026quot;X-Forwarded-For\u0026quot;: \u0026quot;203.0.113.123\u0026quot;, \u0026quot;X-Forwarded-Port\u0026quot;: \u0026quot;80\u0026quot;, \u0026quot;X-Forwarded-Proto\u0026quot;: \u0026quot;http\u0026quot; } } ❯ curl -s http://httpbin.org/headers { \u0026quot;headers\u0026quot;: { \u0026quot;Accept\u0026quot;: \u0026quot;*/*\u0026quot;, \u0026quot;Host\u0026quot;: \u0026quot;httpbin.org\u0026quot;, \u0026quot;User-Agent\u0026quot;: \u0026quot;curl/7.64.1\u0026quot;, \u0026quot;X-Amzn-Trace-Id\u0026quot;: \u0026quot;Root=1-5f2ac9c5-d01ecd74ee650c947ce36d6c\u0026quot; } } ちなみに 203.0.113.0/24 は例示用 IP アドレスらしいです。[RFC6890]\n参考サイト: 例示専用のIPアドレスとドメインを使いこなす | ギークを目指して\n","date":"2020年8月5日","permalink":"/2020/08/httpbin-org-show_env/","section":"Posts","summary":"https://httpbin.org/ は HTTP クライアントや Reverse Proxy のテストなどで非常に便利なサイトです。Docker Image も公開されているのでローカルでも使えます。大変お世話になっており","title":"httpbin.org で X-Forwarded-For ヘッダーを確認する方法"},{"content":"Terraform の小ネタです。どうせまた自分でググることになるのでメモ。\nformatlist です。 Security Group の設定を行う場合には IP アドレスではなく CIDR 表記で指定する必要があります。1つの IPv4 アドレスであれば /32 をつける必要があります。 でも、何かのリソースで作成された IP アドレスは IP アドレス単体でしか取得できなかったりします。例えば NAT Gateway の IP アドレス。 これに /32 をつけるのに便利なのが formatlist です。\n次の例では concat と組み合わせていますが、これもメモです。ここで注目すべきは formatlist の部分。sprintf のように \u0026quot;%s/32\u0026quot; でフォーマットしていていて、これが後ろの module.vpc.nat_public_ips というリストの各要素に適用されて、その結果がリストで返されます。\nresource \u0026quot;aws\\_security\\_group\\_rule\u0026quot; \u0026quot;some\\_ingress\u0026quot; { type = \u0026quot;ingress\u0026quot; from\\_port = var.some\\_port\\_number to\\_port = var.some\\_port\\_number protocol = \u0026quot;TCP\u0026quot; security\\_group\\_id = aws\\_security\\_group.some\\_sg.id cidr\\_blocks = concat(concat(var.some\\_cidrs, var.additional\\_cidrs), formatlist(\u0026quot;%s/32\u0026quot;, module.vpc.nat\\_public\\_ips)) description = \u0026quot;example\u0026quot; } もう見つけたと思いますが format は sprintf のように使えます。\nネットワーク関連では cidrhost、cidrnetmask、cidrsubnet という便利 Function もあります。\n","date":"2020年7月27日","permalink":"/2020/07/terraform-tips-formatlist/","section":"Posts","summary":"Terraform の小ネタです。どうせまた自分でググることになるのでメモ。 formatlist です。 Security Group の設定を行う場合には IP アドレスではなく CIDR 表記で指定する必要があります。","title":"Terraform 小ネタ - formatlist"},{"content":"","date":"2020年5月31日","permalink":"/tags/home/","section":"Tags","summary":"","title":"Home"},{"content":"過去の投稿の通り、我が家の無線LANには TP-Link の Deco M5 を使用しており、TP-Link HomeCare も有効にしてあります。\nMac book 内の Vagrant で起動した CentOS から yum update ができずにハマったのですが、調べたところ HomeCare の Parental Control が原因でした。誰かの役に立てば。\n$ sudo yum check-update Loaded plugins: fastestmirror Determining fastest mirrors One of the configured repositories failed (Unknown), and yum doesn't have enough cached data to continue. At this point the only safe thing yum can do is fail. There are a few ways to work \u0026quot;fix\u0026quot; this: 1. Contact the upstream for the repository and get them to fix the problem. 2. Reconfigure the baseurl/etc. for the repository, to point to a working upstream. This is most often useful if you are using a newer distribution release than is supported by the repository (and the packages for the previous distribution release still work). 3. Run the command with the repository temporarily disabled yum --disablerepo=\u0026lt;repoid\u0026gt; ... 4. Disable the repository permanently, so yum won't use it by default. Yum will then just ignore the repository until you permanently enable it again or use --enablerepo for temporary usage: yum-config-manager --disable \u0026lt;repoid\u0026gt; or subscription-manager repos --disable=\u0026lt;repoid\u0026gt; 5. Configure the failing repository to be skipped, if it is unavailable. Note that yum will try to contact the repo. when it runs most commands, so will have to try and fail each time (and thus. yum will be be much slower). If it is a very temporary problem though, this is often a nice compromise: yum-config-manager --save --setopt=.skip_if_unavailable=true Cannot find a valid baseurl for repo: base/7/x86_64 base/7/x86_64 の baseurl が見つからない？\n/etc/yum.repos.d/CentOS-Base.repo の mirrorlist をやめて baseurl を指定するようにしてみたら\u0026hellip;\n$ sudo yum check-update Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile base | 176 B 00:00:00 !http://ftp.jaist.ac.jp/pub/Linux/CentOS/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno -1] Error importing repomd.xml for base: Damaged repomd.xml file Trying other mirror. One of the configured repositories failed (CentOS-7 - Base), and yum doesn't have enough cached data to continue. At this point the only safe thing yum can do is fail. There are a few ways to work \u0026quot;fix\u0026quot; this: 1. Contact the upstream for the repository and get them to fix the problem. 2. Reconfigure the baseurl/etc. for the repository, to point to a working upstream. This is most often useful if you are using a newer distribution release than is supported by the repository (and the packages for the previous distribution release still work). 3. Run the command with the repository temporarily disabled yum --disablerepo=base ... 4. Disable the repository permanently, so yum won't use it by default. Yum will then just ignore the repository until you permanently enable it again or use --enablerepo for temporary usage: yum-config-manager --disable base or subscription-manager repos --disable=base 5. Configure the failing repository to be skipped, if it is unavailable. Note that yum will try to contact the repo. when it runs most commands, so will have to try and fail each time (and thus. yum will be be much slower). If it is a very temporary problem though, this is often a nice compromise: yum-config-manager --save --setopt=base.skip_if_unavailable=true failure: repodata/repomd.xml from base: [Errno 256] No more mirrors to try. !http://ftp.jaist.ac.jp/pub/Linux/CentOS/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno -1] Error importing repomd.xml for base: Damaged repomd.xml file 今度は [Errno -1] Error importing repomd.xml for updates: Damaged repomd.xml file で repomd.xml が壊れてるとか言われる\u0026hellip;\nしかし、curl でアクセスしてみても、ブラウザからアクセスしても問題なくアクセスできる\nわからん\u0026hellip;\n次に tcpdump で何が起きてるのかのぞいてみた。yum では https ではなく http が使われていたので tcpdump -i eth0 -s 0 -A port 80 で http でのやりとりは容易に見ることができる。(MACアドレス部分は一部マスクした)\nGET /pub/Linux/CentOS/7.8.2003/os/x86_64/repodata/repomd.xml HTTP/1.1 User-Agent: urlgrabber/3.10 yum/3.4.3 Host: ftp.jaist.ac.jp Accept: */* HTTP/1.1 200 OK Server: Jetty/4.2.x (Windows XP/5.1 x86 java/1.6.0_17) Content-Type: text/html Content-Length: 176 Accept-Ranges: bytes \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta HTTP-EQUIV=\u0026quot;REFRESH\u0026quot; content=\u0026quot;0; url=http://192.168.200.1:80/shn_blocking.html?app_cid=15\u0026amp;app_id=12\u0026amp;mac=F8FFC2******\u0026quot;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; なんかルーターがブロックしてるっぽい URL へのリンクが見える。ブラウザでアクセスしてみると、\n  Parental Control Restrictions\nYour access to this website is blocked.\nClassificatioin: Yum, Pay to Sur\n は？いや、確かに Pay to Surf (P2S) をブロックするようには設定しましたよ。しかし、なんで Yum をブロックするわけ！？\nでも curl やブラウザではアクセスできたじゃん？\nどうやら User-Agent を見てブロックしているようです。urlgrabber/ と yum/ の両方にマッチするとブロックされました。\nしかたがないので Parental Control 設定を変更して回避しました。付いてきたから使ってるけどあまり意味がある感じもしないんですよねぇ。9.9.9.9 とか使いたいけど DAZN が見れなくなったりするからなあ。話題の 8GB メモリのラズパイ買って Pi-hole でも動かそうかな。\n","date":"2020年5月31日","permalink":"/2020/05/tp-link-parental-control-blocks-yum/","section":"Posts","summary":"過去の投稿の通り、我が家の無線LANには TP-Link の Deco M5 を使用しており、TP-Link HomeCare も有効にしてあります。 Mac book 内の Vagrant で起動した CentOS から yum update ができず","title":"TP-Link Deco の Parental Control で yum が block されてハマる"},{"content":"前回『「OK Google, おはよう」で TimeTree の予定を教えてもらう』で Google Home のルーティンをカスタマイズする方法を紹介しました。\nその中で\n 「おやすみ」とかで明日の予定を確認するのも良いかもしれません。\n って書いたのですが、明日の予定を確認したいのはおやすみの時じゃないよなと思い、カスタムルーティンの追加を試しました。すると、大変便利になりました。\n  「＋ ルーティンを追加」でカスタムルーティンを追加することができます。このキャプチャは「今日の予定」と「明日の予定」というカスタムルーティンを追加した状態です。これを登録しない状態で「OK Google, 今日の予定」って話しかけると Google Calendar の予定を探して答えてくれるのですが、カスタムルーティンのタイミングを**「今日の予定」と言ったとき**として登録すると動作を上書きできました。\n  このキャプチャは「明日の予定」の方ですが、「アシスタントが行う操作」に「TimeTree につないで明日の予定を教えて」として登録しています。これで「OK Google, 明日の予定」って話しかけると TimeTree に登録されている予定を教えてくれます。「おはよう」ルーティンには天気予報も入れてあるのでスケジュールが確認したいだけの時は「OK Google, 今日の予定」というだけで TimeTree の予定を教えてもらえるようになりました。便利便利！！\n","date":"2020年5月30日","permalink":"/2020/05/google-home-custom-routine/","section":"Posts","summary":"前回『「OK Google, おはよう」で TimeTree の予定を教えてもらう』で Google Home のルーティンをカスタマイズする方法を紹介しました。 その中で 「おやすみ」とかで明日の予","title":"Google Home のカスタムルーティンでショートカット"},{"content":"タイトルの通りです。家の予定は TimeTree で共有しておりまして、我が家には Google Home mini も Amazon Echo Dot もあるわけですが、Google はもちろん Google Calendar ですし、Alexa も Google, Microsoft, Apple のカレンダーとのリンクしかサポートしていません。TimeTree と連携させてもわざわざ「TimeTree につないで今日の予定を教えて」なんて言わないといけないわけで、使っていませんでした。Google Home や Alexa のアプリってこんな感じで呼び出す必要があって呼び出し方覚えられねーよって感じ。\nそれでも毎朝、今日の予定くらいは読み上げて欲しいなと思ってました。TimeTree の API はしばらく前に公開されてて、今は予定の取得も可能になっていました。（公開当初は予定の取得ができなくてなんじゃコレって思ってました）。これを使って定期的に予定を取ってきて Google cast で喋らせればいいんじゃないかと思ってコードを書きかけていましたが、ふと Google Home のルーチン機能を思い出して調べてみたら任意の操作を登録できるし、TimeTree アプリも存在したのでこの設定を行うことにしました。\nTimeTree を Google Home から呼び出せるようにするための設定は「Google Home連携」で説明されています。Google Home のルーティン設定は「ルーティンの設定と管理」にあります。\n  Google Home アプリ (Android, iOS)を起動して「ルーティン」をタップします。\n  「おはよう」を選択します。「おやすみ」とかで明日の予定を確認するのも良いかもしれません。\n  デフォルトでは他にも選択されていた気がしますが私は天気予報が聞ければ良いので、それだけ選択して、後は TimeTree を追加するために「＋ 操作の追加」をタップします。\n  「TimeTree につないで今日の予定を教えて」と入力して右上の「追加」をタップ。\n  コレで「保存」すれば完成です。「Ok Google, おはよう」と話しかけてみましょう。\n タイマーと天気予報以外に使い道ができてめでたしめでたし・・・\nところで、Google Home なの？ Google Nest なの？\n（まあ、TimeTree じゃなくて Google Calendar 使えば良いんですけどねぇ、共有だってできるし）\n","date":"2020年5月30日","permalink":"/2020/05/google-home-routine-timetree/","section":"Posts","summary":"タイトルの通りです。家の予定は TimeTree で共有しておりまして、我が家には Google Home mini も Amazon Echo Dot もあるわけですが、Google はもちろん Google Calendar ですし、Alexa","title":"「OK Google, おはよう」で TimeTree の予定を教えてもらう"},{"content":"コンテナへの権限は必要最低限に絞るべしということで、プロセスの実行ユーザーは root ではないし、特権モードで動かすなんことにはなっていないと思います。それでも調査などのために root で入って調査用コマンド追加したり tcpdump したりしたくなることがあります。Pod の設定変更して deploy し直すとかすれば良かったりもしますが面倒ですし、場合によってはプロセスの実行ユーザーを変更するのは意外と厄介かもしれません。\nそこで deploy 済みのコンテナに root で横入りする方法ないかな？って調べました。\nググれば解決する問題は楽ちんだ \u0026ldquo;bash - Exec commands on kubernetes pods with root access - Stack Overflow\u0026rdquo; にありました。\nRuntime は Docker 前提です。他は知らない。\nまずは node 上の docker の ID を取得します。今回は Istio sidecar に入ろうと思うので container name を \u0026ldquo;istio-proxy\u0026rdquo; と指定しています。\njsonpath で取り出す方法\n$ kubectl get pod -n secure-server httpbin-8475c5b859-pclvs \\ -o jsonpath='{.status.containerStatuses[?(@.name == \u0026quot;istio-proxy\u0026quot;)].containerID}' docker://ccf2251464b35df7954587b7f1ba7a1c6215df35f05ae52f75b8796cf475e719 jq の方が慣れてるよって場合\n$ kubectl get pod -n secure-server httpbin-8475c5b859-pclvs -o json \\ | jq -r '.status.containerStatuses[] | select(.name == \u0026quot;istio-proxy\u0026quot;) | .containerID' docker://ccf2251464b35df7954587b7f1ba7a1c6215df35f05ae52f75b8796cf475e719 次に、この Pod が稼働している node に ssh などでログインします。minikube であれば minikube ssh コマンドでいける。\ndocker exec に -u root を付ければ root ユーザーでアクセスできますが、これだけでは tcpdump とか iptables いじったりできないので --privileged もつけます。\n$ docker exec -it -u root --privileged ccf2251464b35df7954587b7f1ba7a1c6215df35f05ae52f75b8796cf475e719 /bin/bash root@httpbin-8475c5b859-pclvs:/# id uid=0(root) gid=0(root) groups=0(root),1337(istio-proxy) 普通に kubectl exec すると istio-proxy ユーザーになります。（ここではどうでも良い話ですが Istio は UID が重要です）\nistio-proxy@httpbin-8475c5b859-pclvs:/$ id uid=1337(istio-proxy) gid=1337(istio-proxy) groups=1337(istio-proxy) root では入れたけど、ファイルシステムが Read-Only でマウントされていたら？\nroot@httpbin-8475c5b859-pclvs:/# mount | awk '{if ($3 == \u0026quot;/\u0026quot;) {print $0}}' overlay on / type overlay (ro,relatime,lowerdir=/var/lib/docker/overlay2/l/...(略) root@httpbin-8475c5b859-pclvs:/# touch /tmp/hoge touch: cannot touch '/tmp/hoge': Read-only file system こんな時には remount です。\nroot@httpbin-8475c5b859-pclvs:/# mount -o rw,remount / root@httpbin-8475c5b859-pclvs:/# mount | awk '{if ($3 == \u0026quot;/\u0026quot;) {print $0}}' overlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/...(略) root@httpbin-8475c5b859-pclvs:/# touch /tmp/hoge root@httpbin-8475c5b859-pclvs:/# ls -l /tmp/hoge -rw-r--r-- 1 root root 0 May 10 14:33 /tmp/hoge 書き込めるようになりました。これで後は好きにできる。\nで、この一連の作業を一発でやってくれる便利コマンドが kubectl-plugins にある kubectl-ssh です。ssh っていう名前なんですが ssh は使ってません。docker.sock をマウントする Pod を対象となる Pod が稼働している Node に deploy してその中で docker exec を実行します。終わったら Pod の削除もやってくれます。はぁ、なるほどねぇって感じです。\nUsage: kubectl ssh [OPTIONS] [-- ] Run a command in a running container Options: -h Show usage -d Enable debug mode. Print a trace of each commands -n If present, the namespace scope for this CLI request -u Username or UID (format: [:]) -c Container name. If omitted, the first container in the pod will be chosen それでは良い kubernetes debug ライフを\u0026hellip;\n","date":"2020年5月10日","permalink":"/2020/05/kubectl-ssh/","section":"Posts","summary":"コンテナへの権限は必要最低限に絞るべしということで、プロセスの実行ユーザーは root ではないし、特権モードで動かすなんことにはなっていないと思いま","title":"kubernetes に deploy 済みの Container に root で入っていじりたい"},{"content":"もうずっと家にいるのが辛い今日この頃ですが、家で仕事をしていて会議中に家族から「ごはんだよ」とか声を掛けられても返事が出来ないよってことがありますよね。そんな時に Google Home （今は Google Nest って呼ぶのかな？） を使って伝えることを思いついたのでやってみます。\n「Google Home に任意のメッセージを喋らせる」という記事を過去に書きました。やることは同じです。go-chromecast 日本語使えるようにする patch とかを merge してもらったので --language-code ja-JP でいけますよ。\n使い方は以前の投稿時と同じです。返事をしたい時にさっと実行できるように cast という名前の wrapper スクリプト (bash) を用意しました。\n$ cast 今会議中 $ cast 5分で行く なんて出来ます。が、落とし穴が\u0026hellip;\nVPN です。VPN 使わなきゃアクセス出来ないものがあって使ってるんですが、そうすると mDNS でデバイスを見つけられないんです 😭 （VPN の設定などにも依存するかもしれません、go-chromecast はインターフェースを指定する機能もあってローカルの NIC を指定してもダメでした）\nしかし、諦めるのはまだ早い。go-chromecast は IP address と port を指定して直接 Device にアクセスできます。この情報は前もって go-chromecast ls コマンドで確認しておくことができます。でもきっと DHCP なんで変更があると面倒だから DHCP サーバー （ルーターや WiFi AP） で IP アドレスを固定してしまっても良いでしょう。\nそれでは良い在宅ワークを。\n今 (2020-05-02) は Google Nest Mini が2,000円 OFF で買えるみたいですね。赤いやつしか残ってないっぽい。天気予報とキッチンタイマー以外の使い道があるのかどうかわかりませんけど\u0026hellip;\n","date":"2020年5月1日","permalink":"/2020/05/wfh-chromecast/","section":"Posts","summary":"もうずっと家にいるのが辛い今日この頃ですが、家で仕事をしていて会議中に家族から「ごはんだよ」とか声を掛けられても返事が出来ないよってことがあ","title":"在宅ワークでの会議中に家族へメッセージを伝える"},{"content":"","date":"2020年5月1日","permalink":"/tags/%E6%9C%AA%E5%88%86%E9%A1%9E/","section":"Tags","summary":"","title":"未分類"},{"content":"kubectl などで複数の Kubernetes クラスタを切り替えるのに kubectx を使っていますが、これでは別ターミナルにしても同時に複数のクラスタにアクセスすることができません。ファイルを分けて環境変数 KUBECONFIG を切り替えるようにすれば良いのですが、この shell は今どの環境だっけ？とか考えなければならなくもなります。\nそんな面倒を解消してくれるのが kubie です。一時ファイルと環境変数を駆使して shell 毎に切り替えられるようになります。prompt への追加もやってくれます。prompt は設定で向こうにもできます。\n作者による紹介ブログ記事 \u0026ldquo;Introducing Kubie\u0026rdquo; です。\nインストールはバイナリをダウンロードして実行権限をつけるだけ。\n❯ curl -Lo ~/bin/kubie https://github.com/sbstp/kubie/releases/download/v0.8.4/kubie-darwin-amd64 ❯ chmod +x ~/bin/kubie ❯ kubie kubie 0.8.4 USAGE: kubie FLAGS: -h, --help Prints help information -V, --version Prints version information SUBCOMMANDS: ctx Spawn a shell in the given context. The shell is isolated from other shells. Kubie shells can be spawned recursively without any issue edit Edit the given context edit-config Edit kubie's config file exec Execute a command inside of the given context and namespace help Prints this message or the help of the given subcommand(s) info View info about the current kubie shell, such as the context name and the current namespace lint Check the Kubernetes config files for issues ns Change the namespace in which the current shell operates. The namespace change does not affect other shells update Check for a Kubie update and replace Kubie's binary if needed. This function can ask for sudo- mode minikube を指定してみます。私の元のプロンプト表示のせいでちょっと分かりづらいですが、context と namespace がプロンプトに追加されました。\n~ ❯ kubie ctx minikube [minikube|default] ~ ❯ zsh の場合は ~/.kube/kubie.yaml で右側に表示させることもできます。（RPS1 っていう変数があるんですね）\nprompt: zsh_use_rps1: true namespace も指定してみましょう。\n[minikube|default] ~ ❯ kubie ns kube-system [minikube|kube-system] ~ ❯ k get pod NAME READY STATUS RESTARTS AGE coredns-5d4dd4b4db-47pl4 1/1 Running 1 49d coredns-5d4dd4b4db-jdgjt 1/1 Running 1 49d etcd-m01 1/1 Running 1 49d kube-apiserver-m01 1/1 Running 1 49d kube-controller-manager-m01 1/1 Running 36 49d kube-proxy-t8wmf 1/1 Running 1 49d kube-scheduler-m01 1/1 Running 35 49d storage-provisioner 1/1 Running 3 49d tiller-deploy-54f7455d59-jmsdw 1/1 Running 9 48d [minikube|kube-system] ~ ❯ KUBECONFIG を確認。 minikube だけのファイルが作成されています。\n[minikube|kube-system] ~ ❯ echo $KUBECONFIG /var/folders/nd/8mk6834s31g8dymd1_9pnqq00000gn/T/kubie-configskP9Dc.yaml [minikube|kube-system] ~ ❯ cat /var/folders/nd/8mk6834s31g8dymd1_9pnqq00000gn/T/kubie-configskP9Dc.yaml --- clusters: - name: minikube cluster: certificate-authority: /Users/teraoka/.minikube/ca.crt server: \u0026quot;https://192.168.64.33:8443\u0026quot; users: - name: minikube user: client-certificate: /Users/teraoka/.minikube/client.crt client-key: /Users/teraoka/.minikube/client.key contexts: - name: minikube context: cluster: minikube namespace: kube-system user: minikube current-context: minikube kind: Config apiVersion: v1 ~/.kube/config にはもっと沢山の設定が入っています。\n[minikube|kube-system] ~ ❯ yq r -j ~/.kube/config | jq -r '.contexts[].name' | wc -l 10 良い感じですね。\nしかし、このコマンド、ファイル作って環境変数設定してくれるだけじゃなくてこのプロセスから fork した SHELL の中で作業することになるんですよね。exit とか Ctrl-D で抜けると元の shell に戻ります。作ったファイル消す必要があるからかなとは思うけど。で、この fork の影響なのか zsh では Ctrl-P とか Ctrl-A とか普段の shell 作業で使うショートカット(?)が使えないんです\u0026hellip;\nが、「zsh でいつの間にか Ctrl+R とか Ctrl+A とかきかなくなっていた」という記事を見つけて ~/.zshrc に bindkey -e を追加することで解決しました。。zsh は mac のデフォルトが zsh だったから使ってるだけのにわかユーザーなので知りませんでした。しかし、なぜ .zshrc で指定してないのに通常は使えてるんだろうか？？\nprompt じゃなくて status bar に表示できるらしいということで iTerm2 を使おうかと思ってたのに fork しちゃってるからか status bar の情報が更新されないんですよね。。。 iTerm2 使わなくていっか。\n最後に twitter で見かけたこれを貼っておきましょう。kubectl.info （私はキューブシーティーエル）\n","date":"2020年4月26日","permalink":"/2020/04/kubie/","section":"Posts","summary":"kubectl などで複数の Kubernetes クラスタを切り替えるのに kubectx を使っていますが、これでは別ターミナルにしても同時に複数のクラスタにアクセスすることができません。","title":"kubie 3分 Cooking!"},{"content":"","date":"2020年4月11日","permalink":"/tags/github/","section":"Tags","summary":"","title":"GitHub"},{"content":"go でちょっとした調査用ツールを書いたのでついでに goreleaser を使ってみたのと、コンテナでも使いたかったので Docker Image を作って Registory への Push も GitHub Actions でやってみたメモです。\ngoreleaser # goreleaser は .goreleaser.yml ファイル (-f, \u0026ndash;config= で指定も可能) で設定を行います。goreleaser init で土台となるファイルを生成してくれます。1からファイルを作ってしまうならローカルに goreleaser コマンドをインストールする必要はありません。goreleaser check でファイルの validation を行ってはくれますが。\ninit で作られるファイルは次の通り。(goreleaser のバージョンは 0.131.1)\n\\# This is an example goreleaser.yaml file with some sane defaults. # Make sure to check the documentation at http://goreleaser.com before: hooks: # You may remove this if you don't use go modules. - go mod download # you may remove this if you don't need go generate - go generate ./... builds: - env: - CGO\\_ENABLED=0 archives: - replacements: darwin: Darwin linux: Linux windows: Windows 386: i386 amd64: x86\\_64 checksum: name\\_template: 'checksums.txt' snapshot: name\\_template: \u0026quot;{{ .Tag }}-next\u0026quot; changelog: sort: asc filters: exclude: - '^docs:' - '^test:' 「Customization · GoReleaser」に各項目の説明があります。archives.replacements は必須ではないけれども uname コマンドの出力に合わせる感じですかね。GitHub Actions で使う方法も ドキュメント にあります。Action は Marcketplace にあります。(source)\n結果、次のような .goreleaser.yml になりました。\nbefore: hooks: - go mod download builds: - goos: # default では linux と darwin だけだけど windows 用のバイナリも作るようにしてみる - linux - darwin - windows goarch: # default では 386 と amd64 だけど今更 32bit は不要かなと - amd64 ldflags: # code 側で version の更新が不要で便利 - -s -w - -X main.version={{.Version}} - -X main.commit={{.ShortCommit}} - -X main.date={{.Date}} env: - CGO\\_ENABLED=0 archives: - format: binary # 複数ファイルの zip とかじゃなくて単一のバイナリファイル配布にする (展開が面倒) replacements: darwin: Darwin linux: Linux windows: Windows 386: i386 amd64: x86\\_64 format\\_overrides: # Windows だけは zip にする (exe をダウンロードさせるのは都合が悪い) - goos: windows format: zip checksum: name\\_template: checksums.txt snapshot: name\\_template: \u0026quot;{{ .Tag }}-next\u0026quot; changelog: skip: true GitHub Actions の workflow の方は次のようになりました。ほぼ、ドキュメントのままです。違いは go-version くらいかな。これを .github/workflows ディレクトリ内の任意の .yaml (.yml) ファイルとして保存します。\nname: release on: push: tags: - \u0026quot;\\*\u0026quot; jobs: goreleaser: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Unshallow run: git fetch --prune --unshallow - name: Setup Go uses: actions/setup-go@v1 with: go-version: 1.14.2 - name: Run GoReleaser uses: goreleaser/goreleaser-action@v1 with: version: latest args: release --rm-dist env: GITHUB\\_TOKEN: ${{ secrets.GITHUB\\_TOKEN }} 今回は使いませんでしたが homebrew-tap にも対応してるんですね。\nDocker Image の Push # 以前にも GitHub Actions で Docker Hub へ Push するという設定を行ったことはありましたが、その時は workflow ファイルに docker コマンドを並べていました。今回再度調査していて docker/build-push-action@v1 (Markcetplace) っていう Action の存在を知ったのでこちらを使いました。\nGitHub Actions のドキュメント「Publishing Docker images - GitHub Help」にはサンプルとして次のような設定が掲載されています。\nname: Publish Docker image on: release: types: \\[published\\] jobs: push\\_to\\_registry: name: Push Docker image to Docker Hub runs-on: ubuntu-latest steps: - name: Check out the repo uses: actions/checkout@v2 - name: Push to Docker Hub uses: docker/build-push-action@v1 with: username: ${{ secrets.DOCKER\\_USERNAME }} password: ${{ secrets.DOCKER\\_PASSWORD }} repository: my-docker-hub-namespace/my-docker-hub-repository tag\\_with\\_ref: true on の trigger で release が published になったら、となっています。上の設定で goreleaser が publish してくれているという理解でしたが、なぜかこちらの workflow が実行されませんでした。詳しく調べたわけではないですが、どうやら goreleaser の action ではこれが使えないようです。create-a-release action (source) だったら使えるのかな。\nということで GitHub への code の push を trigger に実行するようにしました。master branch の場合は image の tag が latest になるようです。\non: push: branches: - '\\*' tags: - '\\*' 後は repository を自分のものにして、docker のログイン情報を secrets として登録すれば終わりです。Docker Hub の場合は自分のログインパスワードとは別に token を発行してパスワードとして設定します。\n以上\n","date":"2020年4月11日","permalink":"/2020/04/github-actions-goreleaser-docker-image-push/","section":"Posts","summary":"go でちょっとした調査用ツールを書いたのでついでに goreleaser を使ってみたのと、コンテナでも使いたかったので Docker Image を作って Registory への Push も GitHub Actions でやってみたメモで","title":"GitHub Actions での goreleaser と Docker Image の Push"},{"content":"","date":"2020年4月5日","permalink":"/tags/ssh/","section":"Tags","summary":"","title":"ssh"},{"content":"\u0026ldquo;Step 7: (Optional) Enable SSH Connections Through Session Manager\u0026rdquo; にある通りだが、SSH クライアント側に session-manager-plugin をインストール して、~/.ssh/config に次のように設定すれば\n\\# SSH over Session Manager host i-\\* mi-\\* ProxyCommand sh -c \u0026quot;aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters 'portNumber=%p'\u0026quot; ssh ec2-user@i-0897e5bf469826a3c などとするだけで SSH 接続することができる。サーバー側はクライアント側から直接アクセス可能な IP アドレスを持っている必要がなく、もちろん SecurityGroup で 22/tcp を開けておく必要もない。\nただし、サーバー側に公開鍵 (~/.ssh/authorized_keys) の設置が必要。\nSSH クライアント側では session-manager-plugin が起動して AWS と https (443/tcp) で通信します。ssh コマンドはこの session-manager-plugin と pipe で通信します。\nサーバー側では amazon-ssm-agent から起動される ssm-session-worker が localhost の sshd (22/tcp) に接続しています。\nUSER PID COMMAND root 3456 /usr/sbin/sshd -D root 20830 \\\\\\_ sshd: ec2-user \\[priv\\] ec2-user 20862 \\\\\\_ sshd: ec2-user@pts/1 ec2-user 20863 \\\\\\_ -bash ec2-user 20902 \\\\\\_ ps auxwwf ... root 4291 /usr/bin/amazon-ssm-agent root 20821 \\\\\\_ /usr/bin/ssm-session-worker yteraoka-0b10612850cc08e6e i-04bf9e371e9f6b863 関連するプロセスの流れは次のような感じですが、\nssh -\u0026gt; session-manager-plugin -\u0026gt; (AWS) -\u0026gt; amazon-ssm-agent -\u0026gt; ssm-session-worker -\u0026gt; sshd 接続の方向としては次のようになっていました。\nssh -\u0026gt; session-manager-plugin -\u0026gt; (AWS) \u0026lt;- amazon-ssm-agent ^ | ssm-session-worker -\u0026gt; sshd ここで、amazon-ssm-agent や ssm-session-worker の接続先となっている (AWS) というのが Global IP Address であるため、Private な Subnet にいるインスタンスの場合は NAT Gateway などでインターネットに出られるようになっているか VPC Endpoint や PrivateLink の設定 が必要です。\nブラウザから Session Manager でインスタンスに接続する場合は ssm-session-worker が直接 shell を起動させます。\nUSER PID COMMAND root 4291 /usr/bin/amazon-ssm-agent root 20569 \\\\\\_ /usr/bin/ssm-session-worker yteraoka-05b675cf4885dd674 i-04bf9e371e9f6b863 ssm-user 20582 \\\\\\_ sh ssm-user 20583 \\\\\\_ ps auxwwf Session Manager 経由の SSH を禁止したい場合は IAM Policy で次のように Deny します。\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: \\[ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor1\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;ssm:StartSession\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:ssm:\\*:\\*:document/AWS-StartSSHSession\u0026quot; } \\] } その他のメモ\n Session Manager を使うための最小の IAM Policy 設定 Auditing and Logging Session Activity (CloudWatch Logs や S3 にログを送ることができる) SSMのセッションマネージャをTerraformで設定する (aws_ssm_document で設定するみたい)  ","date":"2020年4月5日","permalink":"/2020/04/ssh-connection-through-session-manager/","section":"Posts","summary":"\u0026ldquo;Step 7: (Optional) Enable SSH Connections Through Session Manager\u0026rdquo; にある通りだが、SSH クライアント側に session-manager-plugin をインストール して、~/.ssh/config に次のように設定すれば \\# SSH over Session Manager host i-\\*","title":"SSM Session Manager 経由での SSH"},{"content":"Istio シリーズ 第12回です。\nIstio は各 Pod に sidecar として Envoy コンテナを差し込み、通信の受信も送信も Envoy を経由します。アプリの更新時などに旧バージョンの Pod の停止する時、先に Envoy コンテナが停止してしまうとアプリのコンテナが通信できなくなり処理が完了できなくなったりします。開始時にもコンテナの起動順序は不定であるため起動スクリプトの調整や LivnessProbe, ReadinessProbe は重要です。\npreStop フック # そこで、sidecar である Envoy が先に終了してしまわないようにするために Pod の preStop hook を活用することができます。\nEnvoy sidecar は istio によって deploy 時に inject されますが、どんな設定を inject するかは istio-system ネームスペースにある istio-sidecar-injector という ConfigMap に定義されています。\n$ kubectl get configmap -n istio-system NAME DATA AGE istio 3 21d istio-ca-root-cert 1 21d istio-leader 0 21d istio-security 1 21d istio-sidecar-injector 2 21d pilot-envoy-config 1 21d prometheus 1 21d config にテンプレートなどと、そのテンプレートに渡す values という変数が入っています。\nistio-sidecar-injector ConfigMap の config # $ kubectl get configmap -n istio-system istio-sidecar-injector -o jsonpath='{.data.config}' policy: enabled alwaysInjectSelector: [] neverInjectSelector: [] injectedAnnotations: # Configmap optimized for Istiod. Please DO NOT MERGE all changes from istio - in particular those dependent on # Values.yaml, which should not be used by istiod. # Istiod only uses SDS based config ( files will mapped/handled by SDS). template: | rewriteAppHTTPProbe: {{ valueOrDefault .Values.sidecarInjectorWebhook.rewriteAppHTTPProbe false }} initContainers: {{ if ne (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) `NONE` }} {{ if .Values.istio_cni.enabled -}} - name: istio-validation {{ else -}} - name: istio-init {{ end -}} {{- if contains \u0026quot;/\u0026quot; .Values.global.proxy_init.image }} image: \u0026quot;{{ .Values.global.proxy_init.image }}\u0026quot; {{- else }} image: \u0026quot;{{ .Values.global.hub }}/{{ .Values.global.proxy_init.image }}:{{ .Values.global.tag }}\u0026quot; {{- end }} command: - istio-iptables - \u0026quot;-p\u0026quot; - 15001 - \u0026quot;-z\u0026quot; - \u0026quot;15006\u0026quot; - \u0026quot;-u\u0026quot; - 1337 - \u0026quot;-m\u0026quot; - \u0026quot;{{ annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode }}\u0026quot; - \u0026quot;-i\u0026quot; - \u0026quot;{{ annotation .ObjectMeta `traffic.sidecar.istio.io/includeOutboundIPRanges` .Values.global.proxy.includeIPRanges }}\u0026quot; - \u0026quot;-x\u0026quot; - \u0026quot;{{ annotation .ObjectMeta `traffic.sidecar.istio.io/excludeOutboundIPRanges` .Values.global.proxy.excludeIPRanges }}\u0026quot; - \u0026quot;-b\u0026quot; - \u0026quot;{{ annotation .ObjectMeta `traffic.sidecar.istio.io/includeInboundPorts` `*` }}\u0026quot; - \u0026quot;-d\u0026quot; - \u0026quot;15090,{{ excludeInboundPort (annotation .ObjectMeta `status.sidecar.istio.io/port` .Values.global.proxy.statusPort) (annotation .ObjectMeta `traffic.sidecar.istio.io/excludeInboundPorts` .Values.global.proxy.excludeInboundPorts) }}\u0026quot; {{ if or (isset .ObjectMeta.Annotations `traffic.sidecar.istio.io/excludeOutboundPorts`) (ne (valueOrDefault .Values.global.proxy.excludeOutboundPorts \u0026quot;\u0026quot;) \u0026quot;\u0026quot;) -}} - \u0026quot;-o\u0026quot; - \u0026quot;{{ annotation .ObjectMeta `traffic.sidecar.istio.io/excludeOutboundPorts` .Values.global.proxy.excludeOutboundPorts }}\u0026quot; {{ end -}} {{ if (isset .ObjectMeta.Annotations `traffic.sidecar.istio.io/kubevirtInterfaces`) -}} - \u0026quot;-k\u0026quot; - \u0026quot;{{ index .ObjectMeta.Annotations `traffic.sidecar.istio.io/kubevirtInterfaces` }}\u0026quot; {{ end -}} {{ if .Values.istio_cni.enabled -}} - \u0026quot;--run-validation\u0026quot; - \u0026quot;--skip-rule-apply\u0026quot; {{ end -}} imagePullPolicy: \u0026quot;{{ valueOrDefault .Values.global.imagePullPolicy `Always` }}\u0026quot; {{- if .Values.global.proxy_init.resources }} resources: {{ toYaml .Values.global.proxy_init.resources | indent 4 }} {{- else }} resources: {} {{- end }} securityContext: allowPrivilegeEscalation: {{ .Values.global.proxy.privileged }} privileged: {{ .Values.global.proxy.privileged }} capabilities: {{- if not .Values.istio_cni.enabled }} add: - NET_ADMIN - NET_RAW {{- end }} drop: - ALL readOnlyRootFilesystem: false {{- if not .Values.istio_cni.enabled }} runAsGroup: 0 runAsNonRoot: false runAsUser: 0 {{- else }} runAsGroup: 1337 runAsUser: 1337 runAsNonRoot: true {{- end }} restartPolicy: Always {{ end -}} {{- if eq .Values.global.proxy.enableCoreDump true }} - name: enable-core-dump args: - -c - sysctl -w kernel.core_pattern=/var/lib/istio/core.proxy \u0026amp;\u0026amp; ulimit -c unlimited command: - /bin/sh {{- if contains \u0026quot;/\u0026quot; .Values.global.proxy_init.image }} image: \u0026quot;{{ .Values.global.proxy_init.image }}\u0026quot; {{- else }} image: \u0026quot;{{ .Values.global.hub }}/{{ .Values.global.proxy_init.image }}:{{ .Values.global.tag }}\u0026quot; {{- end }} imagePullPolicy: \u0026quot;{{ valueOrDefault .Values.global.imagePullPolicy `Always` }}\u0026quot; resources: {} securityContext: allowPrivilegeEscalation: true capabilities: add: - SYS_ADMIN drop: - ALL privileged: true readOnlyRootFilesystem: false runAsGroup: 0 runAsNonRoot: false runAsUser: 0 {{ end }} containers: - name: istio-proxy {{- if contains \u0026quot;/\u0026quot; (annotation .ObjectMeta `sidecar.istio.io/proxyImage` .Values.global.proxy.image) }} image: \u0026quot;{{ annotation .ObjectMeta `sidecar.istio.io/proxyImage` .Values.global.proxy.image }}\u0026quot; {{- else }} image: \u0026quot;{{ .Values.global.hub }}/{{ .Values.global.proxy.image }}:{{ .Values.global.tag }}\u0026quot; {{- end }} ports: - containerPort: 15090 protocol: TCP name: http-envoy-prom args: - proxy - sidecar - --domain - $(POD_NAMESPACE).svc.{{ .Values.global.proxy.clusterDomain }} - --configPath - \u0026quot;/etc/istio/proxy\u0026quot; - --binaryPath - \u0026quot;/usr/local/bin/envoy\u0026quot; - --serviceCluster {{ if ne \u0026quot;\u0026quot; (index .ObjectMeta.Labels \u0026quot;app\u0026quot;) -}} - \u0026quot;{{ index .ObjectMeta.Labels `app` }}.$(POD_NAMESPACE)\u0026quot; {{ else -}} - \u0026quot;{{ valueOrDefault .DeploymentMeta.Name `istio-proxy` }}.{{ valueOrDefault .DeploymentMeta.Namespace `default` }}\u0026quot; {{ end -}} - --drainDuration - \u0026quot;{{ formatDuration .ProxyConfig.DrainDuration }}\u0026quot; - --parentShutdownDuration - \u0026quot;{{ formatDuration .ProxyConfig.ParentShutdownDuration }}\u0026quot; - --discoveryAddress - \u0026quot;{{ annotation .ObjectMeta `sidecar.istio.io/discoveryAddress` .ProxyConfig.DiscoveryAddress }}\u0026quot; {{- if eq .Values.global.proxy.tracer \u0026quot;lightstep\u0026quot; }} - --lightstepAddress - \u0026quot;{{ .ProxyConfig.GetTracing.GetLightstep.GetAddress }}\u0026quot; - --lightstepAccessToken - \u0026quot;{{ .ProxyConfig.GetTracing.GetLightstep.GetAccessToken }}\u0026quot; - --lightstepSecure={{ .ProxyConfig.GetTracing.GetLightstep.GetSecure }} - --lightstepCacertPath - \u0026quot;{{ .ProxyConfig.GetTracing.GetLightstep.GetCacertPath }}\u0026quot; {{- else if eq .Values.global.proxy.tracer \u0026quot;zipkin\u0026quot; }} - --zipkinAddress - \u0026quot;{{ .ProxyConfig.GetTracing.GetZipkin.GetAddress }}\u0026quot; {{- else if eq .Values.global.proxy.tracer \u0026quot;datadog\u0026quot; }} - --datadogAgentAddress - \u0026quot;{{ .ProxyConfig.GetTracing.GetDatadog.GetAddress }}\u0026quot; {{- end }} - --proxyLogLevel={{ annotation .ObjectMeta `sidecar.istio.io/logLevel` .Values.global.proxy.logLevel}} - --proxyComponentLogLevel={{ annotation .ObjectMeta `sidecar.istio.io/componentLogLevel` .Values.global.proxy.componentLogLevel}} - --connectTimeout - \u0026quot;{{ formatDuration .ProxyConfig.ConnectTimeout }}\u0026quot; {{- if .Values.global.proxy.envoyStatsd.enabled }} - --statsdUdpAddress - \u0026quot;{{ .ProxyConfig.StatsdUdpAddress }}\u0026quot; {{- end }} {{- if .Values.global.proxy.envoyMetricsService.enabled }} - --envoyMetricsService - '{{ protoToJSON .ProxyConfig.EnvoyMetricsService }}' {{- end }} {{- if .Values.global.proxy.envoyAccessLogService.enabled }} - --envoyAccessLogService - '{{ protoToJSON .ProxyConfig.EnvoyAccessLogService }}' {{- end }} - --proxyAdminPort - \u0026quot;{{ .ProxyConfig.ProxyAdminPort }}\u0026quot; {{ if gt .ProxyConfig.Concurrency 0 -}} - --concurrency - \u0026quot;{{ .ProxyConfig.Concurrency }}\u0026quot; {{ end -}} {{- if .Values.global.istiod.enabled }} - --controlPlaneAuthPolicy - NONE {{- else if .Values.global.controlPlaneSecurityEnabled }} - --controlPlaneAuthPolicy - MUTUAL_TLS {{- else }} - --controlPlaneAuthPolicy - NONE {{- end }} - --dnsRefreshRate - {{ valueOrDefault .Values.global.proxy.dnsRefreshRate \u0026quot;300s\u0026quot; }} {{- if (ne (annotation .ObjectMeta \u0026quot;status.sidecar.istio.io/port\u0026quot; .Values.global.proxy.statusPort) \u0026quot;0\u0026quot;) }} - --statusPort - \u0026quot;{{ annotation .ObjectMeta `status.sidecar.istio.io/port` .Values.global.proxy.statusPort }}\u0026quot; {{- end }} {{- if .Values.global.sts.servicePort }} - --stsPort={{ .Values.global.sts.servicePort }} {{- end }} {{- if .Values.global.trustDomain }} - --trust-domain={{ .Values.global.trustDomain }} {{- end }} {{- if .Values.global.logAsJson }} - --log_as_json {{- end }} - --controlPlaneBootstrap=false {{- if .Values.global.proxy.lifecycle }} lifecycle: {{ toYaml .Values.global.proxy.lifecycle | indent 4 }} {{- end }} env: - name: JWT_POLICY value: {{ .Values.global.jwtPolicy }} - name: PILOT_CERT_PROVIDER value: {{ .Values.global.pilotCertProvider }} # Temp, pending PR to make it default or based on the istiodAddr env - name: CA_ADDR {{- if .Values.global.configNamespace }} value: istio-pilot.{{ .Values.global.configNamespace }}.svc:15012 {{- else }} value: istio-pilot.istio-system.svc:15012 {{- end }} - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_ACCOUNT valueFrom: fieldRef: fieldPath: spec.serviceAccountName - name: HOST_IP valueFrom: fieldRef: fieldPath: status.hostIP {{- if eq .Values.global.proxy.tracer \u0026quot;datadog\u0026quot; }} {{- if isset .ObjectMeta.Annotations `apm.datadoghq.com/env` }} {{- range $key, $value := fromJSON (index .ObjectMeta.Annotations `apm.datadoghq.com/env`) }} - name: {{ $key }} value: \u0026quot;{{ $value }}\u0026quot; {{- end }} {{- end }} {{- end }} - name: ISTIO_META_POD_PORTS value: |- [ {{- $first := true }} {{- range $index1, $c := .Spec.Containers }} {{- range $index2, $p := $c.Ports }} {{- if (structToJSON $p) }} {{if not $first}},{{end}}{{ structToJSON $p }} {{- $first = false }} {{- end }} {{- end}} {{- end}} ] - name: ISTIO_META_CLUSTER_ID value: \u0026quot;{{ valueOrDefault .Values.global.multiCluster.clusterName `Kubernetes` }}\u0026quot; - name: ISTIO_META_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: ISTIO_META_CONFIG_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: ISTIO_META_INTERCEPTION_MODE value: \u0026quot;{{ or (index .ObjectMeta.Annotations `sidecar.istio.io/interceptionMode`) .ProxyConfig.InterceptionMode.String }}\u0026quot; {{- if .Values.global.network }} - name: ISTIO_META_NETWORK value: \u0026quot;{{ .Values.global.network }}\u0026quot; {{- end }} {{ if .ObjectMeta.Annotations }} - name: ISTIO_METAJSON_ANNOTATIONS value: | {{ toJSON .ObjectMeta.Annotations }} {{ end }} {{- if .DeploymentMeta.Name }} - name: ISTIO_META_WORKLOAD_NAME value: {{ .DeploymentMeta.Name }} {{ end }} {{- if and .TypeMeta.APIVersion .DeploymentMeta.Name }} - name: ISTIO_META_OWNER value: kubernetes://apis/{{ .TypeMeta.APIVersion }}/namespaces/{{ valueOrDefault .DeploymentMeta.Namespace `default` }}/{{ toLower .TypeMeta.Kind}}s/{{ .DeploymentMeta.Name }} {{- end}} {{- if (isset .ObjectMeta.Annotations `sidecar.istio.io/bootstrapOverride`) }} - name: ISTIO_BOOTSTRAP_OVERRIDE value: \u0026quot;/etc/istio/custom-bootstrap/custom_bootstrap.json\u0026quot; {{- end }} {{- if .Values.global.meshID }} - name: ISTIO_META_MESH_ID value: \u0026quot;{{ .Values.global.meshID }}\u0026quot; {{- else if .Values.global.trustDomain }} - name: ISTIO_META_MESH_ID value: \u0026quot;{{ .Values.global.trustDomain }}\u0026quot; {{- end }} {{- if eq .Values.global.proxy.tracer \u0026quot;stackdriver\u0026quot; }} - name: STACKDRIVER_TRACING_ENABLED value: \u0026quot;true\u0026quot; - name: STACKDRIVER_TRACING_DEBUG value: \u0026quot;{{ .ProxyConfig.GetTracing.GetStackdriver.GetDebug }}\u0026quot; - name: STACKDRIVER_TRACING_MAX_NUMBER_OF_ANNOTATIONS value: \u0026quot;{{ .ProxyConfig.GetTracing.GetStackdriver.GetMaxNumberOfAnnotations.Value }}\u0026quot; - name: STACKDRIVER_TRACING_MAX_NUMBER_OF_ATTRIBUTES value: \u0026quot;{{ .ProxyConfig.GetTracing.GetStackdriver.GetMaxNumberOfAttributes.Value }}\u0026quot; - name: STACKDRIVER_TRACING_MAX_NUMBER_OF_MESSAGE_EVENTS value: \u0026quot;{{ .ProxyConfig.GetTracing.GetStackdriver.GetMaxNumberOfMessageEvents.Value }}\u0026quot; {{- end }} {{- if and (eq .Values.global.proxy.tracer \u0026quot;datadog\u0026quot;) (isset .ObjectMeta.Annotations `apm.datadoghq.com/env`) }} {{- range $key, $value := fromJSON (index .ObjectMeta.Annotations `apm.datadoghq.com/env`) }} - name: {{ $key }} value: \u0026quot;{{ $value }}\u0026quot; {{- end }} {{- end }} {{- range $key, $value := .ProxyConfig.ProxyMetadata }} - name: {{ $key }} value: \u0026quot;{{ $value }}\u0026quot; {{- end }} imagePullPolicy: \u0026quot;{{ valueOrDefault .Values.global.imagePullPolicy `Always` }}\u0026quot; {{ if ne (annotation .ObjectMeta `status.sidecar.istio.io/port` .Values.global.proxy.statusPort) `0` }} readinessProbe: httpGet: path: /healthz/ready port: {{ annotation .ObjectMeta `status.sidecar.istio.io/port` .Values.global.proxy.statusPort }} initialDelaySeconds: {{ annotation .ObjectMeta `readiness.status.sidecar.istio.io/initialDelaySeconds` .Values.global.proxy.readinessInitialDelaySeconds }} periodSeconds: {{ annotation .ObjectMeta `readiness.status.sidecar.istio.io/periodSeconds` .Values.global.proxy.readinessPeriodSeconds }} failureThreshold: {{ annotation .ObjectMeta `readiness.status.sidecar.istio.io/failureThreshold` .Values.global.proxy.readinessFailureThreshold }} {{ end -}} securityContext: allowPrivilegeEscalation: {{ .Values.global.proxy.privileged }} capabilities: {{ if or (eq (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) `TPROXY`) (eq (annotation .ObjectMeta `sidecar.istio.io/capNetBindService` .Values.global.proxy.capNetBindService) `true`) -}} add: {{ if eq (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) `TPROXY` -}} - NET_ADMIN {{- end }} {{ if eq (annotation .ObjectMeta `sidecar.istio.io/capNetBindService` .Values.global.proxy.capNetBindService) `true` -}} - NET_BIND_SERVICE {{- end }} {{- end }} drop: - ALL privileged: {{ .Values.global.proxy.privileged }} readOnlyRootFilesystem: {{ not .Values.global.proxy.enableCoreDump }} runAsGroup: 1337 fsGroup: 1337 {{ if or (eq (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) `TPROXY`) (eq (annotation .ObjectMeta `sidecar.istio.io/capNetBindService` .Values.global.proxy.capNetBindService) `true`) -}} runAsNonRoot: false runAsUser: 0 {{- else -}} runAsNonRoot: true runAsUser: 1337 {{- end }} resources: {{ if or (isset .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU`) (isset .ObjectMeta.Annotations `sidecar.istio.io/proxyMemory`) -}} requests: {{ if (isset .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU`) -}} cpu: \u0026quot;{{ index .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU` }}\u0026quot; {{ end}} {{ if (isset .ObjectMeta.Annotations `sidecar.istio.io/proxyMemory`) -}} memory: \u0026quot;{{ index .ObjectMeta.Annotations `sidecar.istio.io/proxyMemory` }}\u0026quot; {{ end }} {{ else -}} {{- if .Values.global.proxy.resources }} {{ toYaml .Values.global.proxy.resources | indent 4 }} {{- end }} {{ end -}} volumeMounts: {{- if eq .Values.global.pilotCertProvider \u0026quot;istiod\u0026quot; }} - mountPath: /var/run/secrets/istio name: istiod-ca-cert {{- end }} {{ if (isset .ObjectMeta.Annotations `sidecar.istio.io/bootstrapOverride`) }} - mountPath: /etc/istio/custom-bootstrap name: custom-bootstrap-volume {{- end }} # SDS channel between istioagent and Envoy - mountPath: /etc/istio/proxy name: istio-envoy {{- if eq .Values.global.jwtPolicy \u0026quot;third-party-jwt\u0026quot; }} - mountPath: /var/run/secrets/tokens name: istio-token {{- end }} {{- if .Values.global.mountMtlsCerts }} # Use the key and cert mounted to /etc/certs/ for the in-cluster mTLS communications. - mountPath: /etc/certs/ name: istio-certs readOnly: true {{- end }} - name: podinfo mountPath: /etc/istio/pod {{- if and (eq .Values.global.proxy.tracer \u0026quot;lightstep\u0026quot;) .Values.global.tracer.lightstep.cacertPath }} - mountPath: {{ directory .ProxyConfig.GetTracing.GetLightstep.GetCacertPath }} name: lightstep-certs readOnly: true {{- end }} {{- if isset .ObjectMeta.Annotations `sidecar.istio.io/userVolumeMount` }} {{ range $index, $value := fromJSON (index .ObjectMeta.Annotations `sidecar.istio.io/userVolumeMount`) }} - name: \u0026quot;{{ $index }}\u0026quot; {{ toYaml $value | indent 4 }} {{ end }} {{- end }} volumes: {{- if (isset .ObjectMeta.Annotations `sidecar.istio.io/bootstrapOverride`) }} - name: custom-bootstrap-volume configMap: name: {{ annotation .ObjectMeta `sidecar.istio.io/bootstrapOverride` \u0026quot;\u0026quot; }} {{- end }} # SDS channel between istioagent and Envoy - emptyDir: medium: Memory name: istio-envoy - name: podinfo downwardAPI: items: - path: \u0026quot;labels\u0026quot; fieldRef: fieldPath: metadata.labels - path: \u0026quot;annotations\u0026quot; fieldRef: fieldPath: metadata.annotations {{- if eq .Values.global.jwtPolicy \u0026quot;third-party-jwt\u0026quot; }} - name: istio-token projected: sources: - serviceAccountToken: path: istio-token expirationSeconds: 43200 audience: {{ .Values.global.sds.token.aud }} {{- end }} {{- if eq .Values.global.pilotCertProvider \u0026quot;istiod\u0026quot; }} - name: istiod-ca-cert configMap: name: istio-ca-root-cert {{- end }} {{- if .Values.global.mountMtlsCerts }} # Use the key and cert mounted to /etc/certs/ for the in-cluster mTLS communications. - name: istio-certs secret: optional: true {{ if eq .Spec.ServiceAccountName \u0026quot;\u0026quot; }} secretName: istio.default {{ else -}} secretName: {{ printf \u0026quot;istio.%s\u0026quot; .Spec.ServiceAccountName }} {{ end -}} {{- end }} {{- if isset .ObjectMeta.Annotations `sidecar.istio.io/userVolume` }} {{range $index, $value := fromJSON (index .ObjectMeta.Annotations `sidecar.istio.io/userVolume`) }} - name: \u0026quot;{{ $index }}\u0026quot; {{ toYaml $value | indent 2 }} {{ end }} {{ end }} {{- if and (eq .Values.global.proxy.tracer \u0026quot;lightstep\u0026quot;) .Values.global.tracer.lightstep.cacertPath }} - name: lightstep-certs secret: optional: true secretName: lightstep.cacert {{- end }} {{- if .Values.global.podDNSSearchNamespaces }} dnsConfig: searches: {{- range .Values.global.podDNSSearchNamespaces }} - {{ render . }} {{- end }} {{- end }} podRedirectAnnot: sidecar.istio.io/interceptionMode: \u0026quot;{{ annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode }}\u0026quot; traffic.sidecar.istio.io/includeOutboundIPRanges: \u0026quot;{{ annotation .ObjectMeta `traffic.sidecar.istio.io/includeOutboundIPRanges` .Values.global.proxy.includeIPRanges }}\u0026quot; traffic.sidecar.istio.io/excludeOutboundIPRanges: \u0026quot;{{ annotation .ObjectMeta `traffic.sidecar.istio.io/excludeOutboundIPRanges` .Values.global.proxy.excludeIPRanges }}\u0026quot; traffic.sidecar.istio.io/includeInboundPorts: \u0026quot;{{ annotation .ObjectMeta `traffic.sidecar.istio.io/includeInboundPorts` (includeInboundPorts .Spec.Containers) }}\u0026quot; traffic.sidecar.istio.io/excludeInboundPorts: \u0026quot;{{ excludeInboundPort (annotation .ObjectMeta `status.sidecar.istio.io/port` .Values.global.proxy.statusPort) (annotation .ObjectMeta `traffic.sidecar.istio.io/excludeInboundPorts` .Values.global.proxy.excludeInboundPorts) }}\u0026quot; {{ if or (isset .ObjectMeta.Annotations `traffic.sidecar.istio.io/excludeOutboundPorts`) (ne .Values.global.proxy.excludeOutboundPorts \u0026quot;\u0026quot;) }} traffic.sidecar.istio.io/excludeOutboundPorts: \u0026quot;{{ annotation .ObjectMeta `traffic.sidecar.istio.io/excludeOutboundPorts` .Values.global.proxy.excludeOutboundPorts }}\u0026quot; {{- end }} traffic.sidecar.istio.io/kubevirtInterfaces: \u0026quot;{{ index .ObjectMeta.Annotations `traffic.sidecar.istio.io/kubevirtInterfaces` }}\u0026quot; istio-sidecar-injector ConfigMap の values # $ kubectl get configmap -n istio-system istio-sidecar-injector -o jsonpath='{.data.values}' { \u0026#34;global\u0026#34;: { \u0026#34;arch\u0026#34;: { \u0026#34;amd64\u0026#34;: 2, \u0026#34;ppc64le\u0026#34;: 2, \u0026#34;s390x\u0026#34;: 2 }, \u0026#34;certificates\u0026#34;: [], \u0026#34;configNamespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;configValidation\u0026#34;: true, \u0026#34;controlPlaneSecurityEnabled\u0026#34;: true, \u0026#34;defaultNodeSelector\u0026#34;: {}, \u0026#34;defaultPodDisruptionBudget\u0026#34;: { \u0026#34;enabled\u0026#34;: true }, \u0026#34;defaultResources\u0026#34;: { \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;10m\u0026#34; } }, \u0026#34;disablePolicyChecks\u0026#34;: true, \u0026#34;enableHelmTest\u0026#34;: false, \u0026#34;enableTracing\u0026#34;: true, \u0026#34;enabled\u0026#34;: true, \u0026#34;hub\u0026#34;: \u0026#34;docker.io/istio\u0026#34;, \u0026#34;imagePullPolicy\u0026#34;: \u0026#34;IfNotPresent\u0026#34;, \u0026#34;imagePullSecrets\u0026#34;: [], \u0026#34;istioNamespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;istiod\u0026#34;: { \u0026#34;enabled\u0026#34;: true }, \u0026#34;jwtPolicy\u0026#34;: \u0026#34;first-party-jwt\u0026#34;, \u0026#34;k8sIngress\u0026#34;: { \u0026#34;enableHttps\u0026#34;: false, \u0026#34;enabled\u0026#34;: false, \u0026#34;gatewayName\u0026#34;: \u0026#34;ingressgateway\u0026#34; }, \u0026#34;localityLbSetting\u0026#34;: { \u0026#34;enabled\u0026#34;: true }, \u0026#34;logAsJson\u0026#34;: false, \u0026#34;logging\u0026#34;: { \u0026#34;level\u0026#34;: \u0026#34;default:info\u0026#34; }, \u0026#34;meshExpansion\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;useILB\u0026#34;: false }, \u0026#34;meshNetworks\u0026#34;: {}, \u0026#34;mountMtlsCerts\u0026#34;: false, \u0026#34;mtls\u0026#34;: { \u0026#34;auto\u0026#34;: true, \u0026#34;enabled\u0026#34;: false }, \u0026#34;multiCluster\u0026#34;: { \u0026#34;clusterName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;enabled\u0026#34;: false }, \u0026#34;namespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;omitSidecarInjectorConfigMap\u0026#34;: false, \u0026#34;oneNamespace\u0026#34;: false, \u0026#34;operatorManageWebhooks\u0026#34;: false, \u0026#34;outboundTrafficPolicy\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;ALLOW_ANY\u0026#34; }, \u0026#34;pilotCertProvider\u0026#34;: \u0026#34;istiod\u0026#34;, \u0026#34;policyCheckFailOpen\u0026#34;: false, \u0026#34;policyNamespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;priorityClassName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prometheusNamespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;proxy\u0026#34;: { \u0026#34;accessLogEncoding\u0026#34;: \u0026#34;JSON\u0026#34;, \u0026#34;accessLogFile\u0026#34;: \u0026#34;/dev/stdout\u0026#34;, \u0026#34;accessLogFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;autoInject\u0026#34;: \u0026#34;enabled\u0026#34;, \u0026#34;clusterDomain\u0026#34;: \u0026#34;cluster.local\u0026#34;, \u0026#34;componentLogLevel\u0026#34;: \u0026#34;misc:error\u0026#34;, \u0026#34;concurrency\u0026#34;: 2, \u0026#34;dnsRefreshRate\u0026#34;: \u0026#34;300s\u0026#34;, \u0026#34;enableCoreDump\u0026#34;: false, \u0026#34;envoyAccessLogService\u0026#34;: { \u0026#34;enabled\u0026#34;: false }, \u0026#34;envoyMetricsService\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;tcpKeepalive\u0026#34;: { \u0026#34;interval\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;probes\u0026#34;: 3, \u0026#34;time\u0026#34;: \u0026#34;10s\u0026#34; }, \u0026#34;tlsSettings\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;DISABLE\u0026#34;, \u0026#34;subjectAltNames\u0026#34;: [] } }, \u0026#34;envoyStatsd\u0026#34;: { \u0026#34;enabled\u0026#34;: false }, \u0026#34;excludeIPRanges\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;excludeInboundPorts\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;excludeOutboundPorts\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;proxyv2\u0026#34;, \u0026#34;includeIPRanges\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;includeInboundPorts\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;kubevirtInterfaces\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;logLevel\u0026#34;: \u0026#34;warning\u0026#34;, \u0026#34;privileged\u0026#34;: false, \u0026#34;protocolDetectionTimeout\u0026#34;: \u0026#34;100ms\u0026#34;, \u0026#34;readinessFailureThreshold\u0026#34;: 30, \u0026#34;readinessInitialDelaySeconds\u0026#34;: 1, \u0026#34;readinessPeriodSeconds\u0026#34;: 2, \u0026#34;resources\u0026#34;: { \u0026#34;limits\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;2000m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;1024Mi\u0026#34; }, \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;100m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;128Mi\u0026#34; } }, \u0026#34;statusPort\u0026#34;: 15020, \u0026#34;tracer\u0026#34;: \u0026#34;zipkin\u0026#34; }, \u0026#34;proxy_init\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;proxyv2\u0026#34;, \u0026#34;resources\u0026#34;: { \u0026#34;limits\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;100m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;50Mi\u0026#34; }, \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;10m\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;10Mi\u0026#34; } } }, \u0026#34;sds\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;token\u0026#34;: { \u0026#34;aud\u0026#34;: \u0026#34;istio-ca\u0026#34; }, \u0026#34;udsPath\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;securityNamespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;sts\u0026#34;: { \u0026#34;servicePort\u0026#34;: 0 }, \u0026#34;tag\u0026#34;: \u0026#34;1.5.0\u0026#34;, \u0026#34;telemetryNamespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;tracer\u0026#34;: { \u0026#34;datadog\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;$(HOST_IP):8126\u0026#34; }, \u0026#34;lightstep\u0026#34;: { \u0026#34;accessToken\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cacertPath\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;secure\u0026#34;: true }, \u0026#34;stackdriver\u0026#34;: { \u0026#34;debug\u0026#34;: false, \u0026#34;maxNumberOfAnnotations\u0026#34;: 200, \u0026#34;maxNumberOfAttributes\u0026#34;: 200, \u0026#34;maxNumberOfMessageEvents\u0026#34;: 200 }, \u0026#34;zipkin\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;trustDomain\u0026#34;: \u0026#34;cluster.local\u0026#34;, \u0026#34;useMCP\u0026#34;: false }, \u0026#34;istio_cni\u0026#34;: { \u0026#34;enabled\u0026#34;: false }, \u0026#34;sidecarInjectorWebhook\u0026#34;: { \u0026#34;alwaysInjectSelector\u0026#34;: [], \u0026#34;enableNamespacesByDefault\u0026#34;: false, \u0026#34;enabled\u0026#34;: false, \u0026#34;image\u0026#34;: \u0026#34;sidecar_injector\u0026#34;, \u0026#34;injectLabel\u0026#34;: \u0026#34;istio-injection\u0026#34;, \u0026#34;injectedAnnotations\u0026#34;: {}, \u0026#34;namespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;neverInjectSelector\u0026#34;: [], \u0026#34;objectSelector\u0026#34;: { \u0026#34;autoInject\u0026#34;: true, \u0026#34;enabled\u0026#34;: false }, \u0026#34;rewriteAppHTTPProbe\u0026#34;: false, \u0026#34;selfSigned\u0026#34;: false } } preStop hook のカスタマイズ # template の中に次のような箇所があります。よって、values でこの global.proxy.lifecycle を定義してやれば preStop hook を入れることが可能になります。\n {{- if .Values.global.proxy.lifecycle }} lifecycle: {{ toYaml .Values.global.proxy.lifecycle | indent 4 }} {{- end }} preStop hook にどんなコマンドを入れるかですが「本番環境のマルチテナント Kubernetes クラスタへの Istio 導入」で Istio の issue #7136 に投稿されているものが紹介されています。\nlifecycle:preStop:exec:command:- \u0026#34;/bin/sh\u0026#34;- \u0026#34;-c\u0026#34;- \u0026#34;while [ $(netstat -plunt | grep tcp | grep -v envoy | wc -l | xargs) -ne 0 ]; do sleep 1; done\u0026#34;netstat で envoy 以外に port を listen しているプロセスが存在するかどうかを確認して、存在しなくなるまで sleep 1 して再チェックを繰り返し、存在しなくなったら終了します。その後コンテナのプロセスに対して SIGTERM が送られます。\n試しに Istio 1.5 環境で netstat -plunt を実行してみると次のようになりました。pilot-agent も istio-proxy コンテナ内で実行されているため、このコマンドのままでは terminationGracePeriodSeconds （デフォルト30秒） を待っても終了せず、その2秒後に SIGTERM が送られることになります。\n$ netstat -plunt Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:15090 0.0.0.0:* LISTEN 18/envoy tcp 0 0 127.0.0.1:15000 0.0.0.0:* LISTEN 18/envoy tcp 0 0 0.0.0.0:15001 0.0.0.0:* LISTEN 18/envoy tcp 0 0 0.0.0.0:15006 0.0.0.0:* LISTEN 18/envoy tcp6 0 0 :::15020 :::* LISTEN 1/pilot-agent ところで、netstat に -u がついてるけど grep tcp してるから意味ないな。あと、xargs って何の意味があるんだろうか？？\nということで global.proxy.lifecycle に設定するのは次のようになる。kubectl edit cm istio-sidecar-injector -n istio-system で編集します。\n$ kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath='{.data.values}' | jq .global.proxy.lifecycle { \u0026quot;preStop\u0026quot;: { \u0026quot;exec\u0026quot;: { \u0026quot;command\u0026quot;: [ \u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;while [ $(netstat -plnt | grep tcp | egrep -v 'envoy|pilot-agent' | wc -l) -ne 0 ]; do sleep 1; done\u0026quot; ] } } } Deployment の Pod を削除すれば新しい Pod が作成される時に新しい設定で istio-proxy が Inject されるため、kubectl get pod で確認できる。\n$ kubectl get pod httpbin-deployment-v2-ccd49cc9c-5z9mt -o jsonpath='{.spec.containers[*].lifecycle.preStop}' map[exec:map[command:[/bin/sh -c while [ $(netstat -plnt | grep tcp | egrep -v 'envoy|pilot-agent' | wc -l) -ne 0 ]; do sleep 1; done]]]  Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編 Istio 導入への道 (12) – sidecar の調整編  ","date":"2020年3月29日","permalink":"/2020/03/istio-part12/","section":"Posts","summary":"Istio シリーズ 第12回です。 Istio は各 Pod に sidecar として Envoy コンテナを差し込み、通信の受信も送信も Envoy を経由します。アプリの更新時などに旧バージョンの Pod の停止","title":"Istio 導入への道 – sidecar の調整編"},{"content":"","date":"2020年3月28日","permalink":"/tags/cert-manager/","section":"Tags","summary":"","title":"cert-manager"},{"content":"前回の「ArgoCD と Istio Ingress Gateway」と、前々回の「 Istio 導入への道 – Ingress Gateway で TLS Termination 編 」で TLS の証明書を手動で取得して Secret として登録したが、登録もさることながら更新が大変です。これを cert-manager にやってもらうことにします。\ncert-manager のインストール # ドキュメントにある通りです。\n$ kubectl apply -f [https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.yaml](https://github.com/jetstack/cert-manager/releases/download/v0.14.1/cert-manager.yaml) 沢山のリソースが作成されます。\ncustomresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created namespace/cert-manager created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager created serviceaccount/cert-manager-webhook created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrole.rbac.authorization.k8s.io/cert-manager-view created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrole.rbac.authorization.k8s.io/cert-manager-edit created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created role.rbac.authorization.k8s.io/cert-manager:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created cert-manager ネームスペースに cert-manager, cert-manager-cainjector (CA Injector), cert-manager-webhook Deployment がデプロイされています。\n$ kubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-665f89d4d6-vfpdx 1/1 Running 0 83m cert-manager-cainjector-78c8947f5c-r8rsd 1/1 Running 0 83m cert-manager-webhook-84f59fdf49-l59ld 1/1 Running 0 83m リソースの登録 # ここでは Let\u0026rsquo;s Encrypt の証明書を dns01 で取得します。DNS には Route53 を使います。Kubernetes クラスタは相変わらず Minikube なので IAM Role ではなく IAM User を作成して Access Key ID と Secret Access Key を使います。\nSecret Access Key を K8s Secret に登録 # route53-credentials-secret という名前の Secret を cert-manager ネームスペースに登録します。キーは secret-access-key とします。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: route53-credentials-secret namespace: cert-manager type: Opeque stringData: secret-access-key: ai/Vdmhv2qhekGe1nE1u39HC48LIAwtBap+5TP81 EOF Issuer の登録 # Issuer と ClusterIssuer があり、Issuer は namespace に閉じたリソースです。ネームスペースを跨いで利用する場合は ClusterIssuer を使います。Issuer には Let\u0026rsquo;s Encrypt などを使用するのに必要な情報をセットします。今回は Let\u0026rsquo;s Encrypt の dns01 と Route53 ですから、\nLet\u0026rsquo;s Encrypt 用に\n メールアドレス ACME サーバーの URL Private Key の保存先としての Secret 名  Route53 用に\n Region 名 Access Key ID Secret Access Key を登録した Secret 名  を設定します。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt namespace: cert-manager spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: username@gmail.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt solvers: - dns01: route53: region: ap-northeast-1 accessKeyID: AKIA5Z6TEOZE8VOPCH6K secretAccessKeySecretRef: name: route53-credentials-secret key: secret-access-key EOF solvers はリストで複数のプロバイダを登録できます。ドメインによって DNS プロバイダが違う場合や Route53 用の AWS Account が違う場合、または dns01 ではなく web01 を使う場合などの混在が可能です。\nCertificate の登録 # issuerRef でどの Issuer を使うのかを指定します。ClusterIssuer を使ったため default ネームスペースからでも発行可能です。secretName で指定した Secret に秘密鍵と証明書が保存されます。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: wildcard-local-1q77-com namespace: default spec: secretName: wildcard-local-1q77-com dnsNames: - '\\*.local.1q77.com' - local.1q77.com issuerRef: name: letsencrypt kind: ClusterIssuer EOF これで次のような CommonName, SANs の証明書を取得することができます。\nCertificate: Data: Version: 3 (0x2) Serial Number: 04:94:64:04:e7:54:ba:6a:b5:cf:30:3a:fd:e3:d3:7f:95:af Signature Algorithm: sha256WithRSAEncryption Issuer: C=US, O=Let's Encrypt, CN=Let's Encrypt Authority X3 Validity Not Before: Mar 28 08:21:40 2020 GMT Not After : Jun 26 08:21:40 2020 GMT Subject: CN=\\*.local.1q77.com Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:c5:5f:18:bd:46:7e:91:6c:d3:0e:04:0e:fc:6c: ef:9d:25:98:10:e9:c1:a1:68:3c:23:5e:13:98💿 2d:60:4e:5d:06:87:f3:10:c1:86:cb:4e:7d:cf:b4: fe:9f:20:3a:88:5a:4d:0a:ff:01:34:23:74:a4:22: a8:c3:32:74:b1:23:25:f2:f1:11:f1:7c:77:ee:7e: 41:d9:7b:4f:ac:ca:ea:c0:6d:f1:46:bf:d1:c3:06: fa:45:66:dc:ee:03:e9:25:23:46:c9:54:57:88:eb: 35:53:f5:ea:db:5c:09:d3:fa:a5:98:34:2d:c6:50: aa:80:ef:25:72:48:04:9b:48:4d:bb:dc:f8:9a:56: dc:f5:e4:f6:b4:34:d2:d0:a8:54:ce:77:4a:d5:83: 60:e3:16:20:6e:12:6b:d5:0c:86:d2:3c:5a:ba:64: 5f:cf:05:cb:db:0a:64:35:3d:e9:8d:18:65:2b:fd: 11:fe:32:c5:5e:29:44:f6:85:61:4c:ae:9d:33:f6: e1:d8:9a:4c:2d:9e:fa:58:ff:0b:45:89:61:1c:3d: cf:8c:58:b5:c6:76:ca:95:e3:6d:14:ef:4e:81:8d: dd:a0:85:52:4b:b3:61:e4:c0:f5:be:12:c7:7e:35: ab:36:b7:ea:54:55:2d:8d:a5:3f:cc:3e:84:a5:84: 4d:a1 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Key Identifier: A8:0D:95:E2:F2:CA:5A:11:4A:DD:83:70:A6:07:77:83:12:36:27:7A X509v3 Authority Key Identifier: keyid:A8:4A:6A:63:04:7D:DD:BA:E6:D1:39:B7:A6:45:65:EF:F3:A8:EC:A1 Authority Information Access: OCSP - URI:http://ocsp.int-x3.letsencrypt.org CA Issuers - URI:http://cert.int-x3.letsencrypt.org/ X509v3 Subject Alternative Name: DNS:\\*.local.1q77.com, DNS:local.1q77.com X509v3 Certificate Policies: Policy: 2.23.140.1.2.1 Policy: 1.3.6.1.4.1.44947.1.1.1 CPS: http://cps.letsencrypt.org Certificate リソースは次のような状態。\n$ kubectl get certificate NAME READY SECRET AGE wildcard-local-1q77-com True wildcard-local-1q77-com 4h36m $ kubectl describe certificate Name: wildcard-local-1q77-com Namespace: default Labels:  Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026ldquo;apiVersion\u0026rdquo;:\u0026ldquo;cert-manager.io/v1alpha2\u0026rdquo;,\u0026ldquo;kind\u0026rdquo;:\u0026ldquo;Certificate\u0026rdquo;,\u0026ldquo;metadata\u0026rdquo;:{\u0026ldquo;annotations\u0026rdquo;:{},\u0026ldquo;name\u0026rdquo;:\u0026ldquo;wildcard-local-1q77-com\u0026rdquo;,\u0026ldquo;namespace\u0026rdquo;:\u0026ldquo;d\u0026hellip; API Version: cert-manager.io/v1alpha3 Kind: Certificate Metadata: Creation Timestamp: 2020-03-28T08:59:27Z Generation: 2 Resource Version: 1027821 Self Link: /apis/cert-manager.io/v1alpha3/namespaces/default/certificates/wildcard-local-1q77-com UID: f5e5d067-2fac-4fdd-8a4b-59ba26916137 Spec: Dns Names: *.local.1q77.com local.1q77.com Issuer Ref: Kind: ClusterIssuer Name: letsencrypt Secret Name: wildcard-local-1q77-com Status: Conditions: Last Transition Time: 2020-03-28T09:21:41Z Message: Certificate is up to date and has not expired Reason: Ready Status: True Type: Ready Not After: 2020-06-26T08:21:40Z Events:\n Certificate を作成すると CertificateRequest リソースが作成されます。何か問題がある場合はこの中身をみると原因が分かったりします。 $ kubectl get certificaterequest NAME READY AGE wildcard-local-1q77-com-4173496889 True 4h17m\n$ kubectl describe certificaterequest Name: wildcard-local-1q77-com-4173496889 Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: cert-manager.io/certificate-name: wildcard-local-1q77-com cert-manager.io/private-key-secret-name: wildcard-local-1q77-com kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;cert-manager.io/v1alpha2\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Certificate\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;wildcard-local-1q77-com\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;d... API Version: cert-manager.io/v1alpha3 Kind: CertificateRequest Metadata: Creation Timestamp: 2020-03-28T09:17:50Z Generation: 1 Owner References: API Version: cert-manager.io/v1alpha2 Block Owner Deletion: true Controller: true Kind: Certificate Name: wildcard-local-1q77-com UID: f5e5d067-2fac-4fdd-8a4b-59ba26916137 Resource Version: 1027815 Self Link: /apis/cert-manager.io/v1alpha3/namespaces/default/certificaterequests/wildcard-local-1q77-com-4173496889 UID: 8c73b0a5-172e-4fee-bed1-5208406d3e50 Spec: Csr: LS0tLS1C (中略... PEM がさらに Base64 されて表示されている) U1QtLS0tLQo= Issuer Ref: Kind: ClusterIssuer Name: letsencrypt Status: Certificate: LS0tLS1CRU (中略... PEM がさらに Base64 されて表示されている) LS0tLS0K Conditions: Last Transition Time: 2020-03-28T09:21:40Z Message: Certificate fetched from issuer successfully Reason: Issued Status: True Type: Ready Events: \u0026lt;none\u0026gt; ``` **Certificate** リソースで指定した Secret に証明書と秘密鍵が入っています。 ``` $ kubectl get secret wildcard-local-1q77-com NAME TYPE DATA AGE wildcard-local-1q77-com kubernetes.io/tls 3 4h37m $ kubectl describe secret wildcard-local-1q77-com Name: wildcard-local-1q77-com Namespace: default Labels:  Annotations: cert-manager.io/alt-names: *.local.1q77.com,local.1q77.com cert-manager.io/certificate-name: wildcard-local-1q77-com cert-manager.io/common-name: *.local.1q77.com cert-manager.io/ip-sans: cert-manager.io/issuer-kind: ClusterIssuer cert-manager.io/issuer-name: letsencrypt cert-manager.io/uri-sans:\nType: kubernetes.io/tls\nData # ca.crt: 0 bytes tls.crt: 3582 bytes tls.key: 1675 bytes\n **Certificate** 設定をミスってた時に **CertificateRequest** で確認されたメッセージです。 status: conditions:\n lastTransitionTime: \u0026ldquo;2020-03-28T08:59:27Z\u0026rdquo; message: \u0026lsquo;The CSR PEM requests a commonName that is not present in the list of dnsNames. If a commonName is set, ACME requires that the value is also present in the list of dnsNames: \u0026ldquo;local.1q77.com\u0026rdquo; does not exist in [*.local.1q77.com]\u0026rsquo; reason: Failed status: \u0026ldquo;False\u0026rdquo; type: Ready failureTime: \u0026ldquo;2020-03-28T08:59:27Z\u0026rdquo;   自動更新 ---- 自動更新までは動作確認できていませんが、[ドキュメント](https://cert-manager.io/docs/tutorials/acme/dns-validation/)には次のように書いてあります。 \u0026gt; Once our certificate has been obtained, cert-manager will periodically check its validity and attempt to renew it if it gets close to expiry. cert-manager considers certificates to be close to expiry when the ‘Not After’ field on the certificate is less than the current time plus 30 days. 証明書取得後、cert-manager は定期的に有効性を確認し、期限が近づくと更新を試みます。cert-manager は Not After フィールドの日付が現在時刻プラス30日よりも近い場合に期限切れ間近と判断する。","date":"2020年3月28日","permalink":"/2020/03/cert-manager/","section":"Posts","summary":"前回の「ArgoCD と Istio Ingress Gateway」と、前々回の「 Istio 導入への道 – Ingress Gateway で TLS Termination 編 」で TLS の証明書を手動で取得して Secret として登録したが、登録もさ","title":"cert-manager で証明書管理"},{"content":"","date":"2020年3月21日","permalink":"/tags/argocd/","section":"Tags","summary":"","title":"ArgoCD"},{"content":"ArgoCD という Kubernetes 用の CD ツールがあります。\n Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.\n これを Istio Ingress Gateway と共に使う方法をまとめます。それだけでそこそこの量になったので。\nArgoCD の deploy # argocd という namespace を作って、そこに Manifest を apply するだけです。\n$ kubectl create namespace argocd $ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v1.4.2/manifests/install.yaml\n これだけで起動します。次のように port-forward すれば https://localhost:8443/ でアクセスできます。 $ kubectl -n argocd port-forward svc/argocd-server 8443:443\n HA 構成の場合は [manifests/ha/install.yaml](https://raw.githubusercontent.com/argoproj/argo-cd/v1.4.2/manifests/ha/install.yaml) を使うようです。 $ diff -u \\ \u0026lt;(curl -s https://raw.githubusercontent.com/argoproj/argo-cd/v1.4.2/manifests/install.yaml) \\ \u0026lt;(curl -s https://raw.githubusercontent.com/argoproj/argo-cd/v1.4.2/manifests/ha/install.yaml)\n HA の方は Redis sentinel で Redis が冗長構成になるようです。 ArgoCD 用 Istio Ingress の設定 -------------------------- ArgoCD は cli 用の gRPC とブラウザ向けの HTTP を同じサーバー、同じポートで処理しており、そこが Ingress 設定におけるポイントです。 ### TLS Passthrough argocd-server はデフォルトで TLS 対応しているため、これをそのまま活かす方法です。 Gateway で port 443 を `tls.mode: PASSTHROUGH` とします。 apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: argocd-gw namespace: argocd spec: selector: istio: ingressgateway servers:\n hosts:  argocd.example.com port: name: http number: 80 protocol: HTTP   hosts:  argocd.example.com port: number: 443 name: https protocol: HTTPS tls: mode: PASSTHROUGH     VirtualService は argocd-gw (Gateway) と紐付け、argocd.example.com 宛て (https では SNI が必須) を argocd-server (Service) に転送します。 apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: argocd-vsvc namespace: argocd spec: gateways:\n argocd-gw hosts: argocd.example.com http: name: argocd-http route:  destination: host: argocd-server tls:   name: argocd-https match:  port: 443 sniHosts:  argocd.example.com route:   destination: host: argocd-server     argocd の `argocd-secret` Secret に入っている証明書 (tls.crt と tls.key) を更新する必要があります。 argocd cli からもアクセス可能です。 $ argocd \u0026ndash;server argocd.example.com:443 app list\n ### TLS Termination (方法1) Ingress で TLS を Termination した場合の設定方法です。argocd-server (Pod) へのアクセスは TLS を使わないため、argocd-server Deployment の設定を変更して argocd-server の起動オプションに `--insecure` を追加する必要があります。 $ kubectl -n argocd edit deployment argocd-server\n **command** に `--insecure` を追加します。これを追加しないと argocd-server が https でのアクセスを求めて redirect loop となります。 $ kubectl -n argocd patch deployment argocd-server -p ' { \u0026ldquo;spec\u0026rdquo;: { \u0026ldquo;template\u0026rdquo;: { \u0026ldquo;spec\u0026rdquo;: { \u0026ldquo;containers\u0026rdquo;: [ { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;argocd-server\u0026rdquo;, \u0026ldquo;command\u0026rdquo;: [\u0026ldquo;argocd-server\u0026rdquo;,\u0026quot;\u0026ndash;staticassets\u0026quot;,\u0026quot;/shared/app\u0026quot;,\u0026quot;\u0026ndash;insecure\u0026quot;] } ] } } } }'\n template: spec: containers: - command: - argocd-server - --staticassets - /shared/app **\\- --insecure** ``` Gateway で argocd.example.com を port 80 と port 443 で受け入れます。443 は tls.mode を SIMPLE とします。TLS の証明書と秘密鍵が必要となりますが、 istio-system namespace に argocd-certificate という名前の Secret が事前に作成されている前提です（ここの詳しい話は[以前の投稿](/2020/03/istio-part11/)を参照）。argocd-server が **\\--insecure** の影響で https への redirect を行わなくなっているため、Gateway で port 80 のところに `tls.httpsRedirect: true` を入れてあります。これで 301 Redirect を返してくれます。 ``` apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: argocd-gw namespace: argocd spec: selector: istio: ingressgateway servers: - hosts: - argocd.example.com port: name: http number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - argocd.example.com port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE credentialName: argocd-certificate ``` VirtualService で argocd.example.com 宛てを argocd-server (Service) に送ります。 ``` apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: argocd-vsvc namespace: argocd spec: gateways: - argocd-gw hosts: - argocd.example.com http: - name: http route: - destination: host: argocd-server ``` この方法では Ingress の Envoy と ArgoCD Pod の間を gRPC として処理しないため、argocd cli からアクセスする場合に `--grpc-web` オプションの指定が必要になります。 ``` $ argocd --server argocd.example.com:443 --grpc-web app list ``` `--grpc-web` オプションをつけないと次のようなエラーとなります。 ``` FATA\\[0000\\] rpc error: code = Internal desc = transport: received the unexpected content-type \u0026quot;text/plain; charset=utf-8\u0026quot; ``` ### TLS Termination (方法2) 先の方法（方法1）では gRPC が使えなくなってしまいましたが、User-Agent で判断して argocd コマンドからの場合は gRPC でアクセスできるようにします。 方法1と同じく argocd-server に `--insecure` の追加が必要です。 ``` $ kubectl -n argocd patch deployment argocd-server -p ' { \u0026quot;spec\u0026quot;: { \u0026quot;template\u0026quot;: { \u0026quot;spec\u0026quot;: { \u0026quot;containers\u0026quot;: \\[ { \u0026quot;name\u0026quot;: \u0026quot;argocd-server\u0026quot;, \u0026quot;command\u0026quot;: \\[\u0026quot;argocd-server\u0026quot;,\u0026quot;--staticassets\u0026quot;,\u0026quot;/shared/app\u0026quot;,\u0026quot;--insecure\u0026quot;\\] } \\] } } } }' ``` さらに、argocd の manifest で作成されている argocd-server Service も編集します。port 443 の **name** を **grpc** に変更します。名前が重要。 ``` $ kubectl -n argocd edit svc argocd-server spec: ports:\n name: http port: 80 protocol: TCP targetPort: 8080 name: grpc port: 443 protocol: TCP targetPort: 8080   その上で、Gateway と VirtualServer を作成する Gateway は方法1と同じ apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: argocd-gw namespace: argocd spec: selector: istio: ingressgateway servers:\n hosts:  argocd.example.com port: name: http number: 80 protocol: HTTP tls: httpsRedirect: true   hosts:  argocd.example.com port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE credentialName: argocd-certificate     VirtualService では User-Agent が argocd-client で始まる場合は grpc に名前を変更した port 443 に、それ意外は port 80 へ。 apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: argocd-vsvc namespace: argocd spec: gateways:\n argocd-gw hosts: argocd.example.com http: name: grpc match:  headers: user-agent: prefix: argocd-client route: destination: host: argocd-server port: number: 443   name: http route:  destination: host: argocd-server port: number: 80     これでブラウザでも argocd cli からでも `--grpc-web` オプションなしでアクセス可能です。","date":"2020年3月21日","permalink":"/2020/03/argocd-istio-ingress/","section":"Posts","summary":"ArgoCD という Kubernetes 用の CD ツールがあります。 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. これを Istio Ingress Gateway と共に使う方法をまとめます。それだけでそこそこの量になったので。 ArgoCD の deploy #","title":"ArgoCD と Istio Ingress Gateway"},{"content":"Istio シリーズ 第11回です。\nTLS Termination # 外部からのアクセスを Istio Ingrress Gateway に TLS の Temination をさせたいことがありますね。今回はこれを試します。\nTLS Termination の設定は Gateway で行います。\nGateway のドキュメントには次のような設定をしろとあります。\napiVersion:networking.istio.io/v1beta1kind:Gatewaymetadata:name:my-tls-ingressspec:selector:app:my-tls-ingress-gatewayservers:- port:number:443name:httpsprotocol:HTTPShosts:- \u0026#34;*\u0026#34;tls:mode:SIMPLEserverCertificate:/etc/certs/server.pemprivateKey:/etc/certs/privatekey.pemが、、、証明書や鍵のファイルパスが指定されています 🤔\nドメイン追加の度に新たな Secrets をマウントするの？まさか\nということでさらに調べてみると「Secure Gateways (SDS)」というものが見つかりました。SDS とは Secret Discovery Service の略でした。\n自己署名の証明書作成 # $ openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 \\ -keyout server.key -out server.crt \\ -subj \u0026quot;/CN=httpbin.example.com/\u0026quot; などとすれば作れますが、最近はブラウザがうるさいので *.local.1q77.com の証明書を Let\u0026rsquo;s Encrypt で取得しました。これは後で cert-manager 管理にしよう。（後日、cert-manager で証明書管理という記事を書きました。）\nSDS を有効にする （のは不要っぽい） # Istio インストール時に有効にしていない場合は SDS を有効にする必要があると書いてありますが、1.5.0 の istioctl に入ってる helm に gateways.istio-ingressgateway.sds.enabled は見当たらないので不要みたいです。global.sds.enabled っていうのはあるけどこれはまた別用途っぽい。\n秘密鍵と証明書を Secrets として登録する # Secrets の名前を istio や prometheus で始めてはダメらしい。また、中に token というフィールドを入れてもダメらしい。今回は httpbin サービスで使うのでドキュメントの例と同じく httpbin-credential という名前にしました。istio-system namespace 内の istio-ingressgateway Pod で使われるため istio-system namespace に作る必要があるみたい。\n$ kubectl create -n istio-system secret generic httpbin-credential \\ --from-file=key=_.local.1q77.com.key \\ --from-file=cert=_.local.1q77.com.crt Gateway を設定する # Istio Ingress Gateway に対して Gateway を設定する。これは Ingress で受け入れるトラフィックを指定する。port 80 の HTTP, port 443 の HTTPS で httpbin.local.1q77.com 宛て（Header や SNI）のトラフィックを受け入れます。TLS Termination も Gateway で設定します。tls.mode の SIMPLE が通常の TLS モードです。証明書は Secret の名前で指定しています。\napiVersion:networking.istio.io/v1beta1kind:Gatewaymetadata:name:httpbin-gatewayspec:selector:istio:ingressgatewayservers:- port:name:httpnumber:80protocol:HTTPhosts:- httpbin.local.1q77.com- port:number:443name:httpsprotocol:HTTPShosts:- httpbin.local.1q77.comtls:mode:SIMPLEcredentialName:httpbin-credentialistio-ingressgateway の Envoy の設定には次のようなものが入っていました。Unix Domain Socket で gRPC 通信して証明書を取得してるんですね。/var/run/ingress_gateway は EmptyDir をマウントしてるようですが、initContainer も sidecar も無いのに何とどうやって通信してるのだろうか？と思ったら istio-ingressgateway では pilot-agent と envoy の2つのプロセスが起動してました。\n\u0026#34;transport_socket\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;envoy.transport_sockets.tls\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.api.v2.auth.DownstreamTlsContext\u0026#34;, \u0026#34;common_tls_context\u0026#34;: { \u0026#34;alpn_protocols\u0026#34;: [ \u0026#34;h2\u0026#34;, \u0026#34;http/1.1\u0026#34; ], \u0026#34;tls_certificate_sds_secret_configs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;httpbin-credential\u0026#34;, \u0026#34;sds_config\u0026#34;: { \u0026#34;api_config_source\u0026#34;: { \u0026#34;api_type\u0026#34;: \u0026#34;GRPC\u0026#34;, \u0026#34;grpc_services\u0026#34;: [ { \u0026#34;google_grpc\u0026#34;: { \u0026#34;target_uri\u0026#34;: \u0026#34;unix:/var/run/ingress_gateway/sds\u0026#34;, \u0026#34;stat_prefix\u0026#34;: \u0026#34;sdsstat\u0026#34; } } ] } } } ] }, \u0026#34;require_client_certificate\u0026#34;: false } } SDS 経由で取得した証明書や鍵も config に入っている。秘密鍵は Envoy の config_dump endpoint では隠されているようです。\n{ \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.SecretsConfigDump\u0026#34;, \u0026#34;dynamic_active_secrets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;httpbin-credential\u0026#34;, \u0026#34;version_info\u0026#34;: \u0026#34;2020-03-20 13:41:23.834387716 +0000 UTC m=+500189.387794816\u0026#34;, \u0026#34;last_updated\u0026#34;: \u0026#34;2020-03-20T13:41:23.839Z\u0026#34;, \u0026#34;secret\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;httpbin-credential\u0026#34;, \u0026#34;tls_certificate\u0026#34;: { \u0026#34;certificate_chain\u0026#34;: { \u0026#34;inline_bytes\u0026#34;: \u0026#34;PEM がさらに base64 でエンコードされた値\u0026#34; }, \u0026#34;private_key\u0026#34;: { \u0026#34;inline_bytes\u0026#34;: \u0026#34;W3JlZGFjdGVkXQ==\u0026#34; } } } }, VirtualService を設定する # Gateway と Service を紐づけるのが VirtualService で Fault Injection や Path や Header による Routing を設定するのも VirtualService です。\nhttpbin-virtual-service という名前でこれまでも設定してありましたが、hosts に httpbin.local.1q77.com を追加しました。\napiVersion:networking.istio.io/v1beta1kind:VirtualServicemetadata:name:httpbin-virtual-servicespec:gateways:- httpbin-gateway- meshhosts:- httpbin-service- httpbin.local.1q77.comhttp:- route:- destination:host:httpbin-servicesubset:v1weight:0- destination:host:httpbin-servicesubset:v2weight:100gateways に httpbin-gateway が入っているので、上の Gateway 設定を紐づいています。これにより Gateway で受け入れた httpbin.local.1q77.com 宛てのリクエストは httpbin-service に送られます。destination が2つ設定されていることは今回の件では特に意味はありません。\nログを確認する # Istio Ingress Gateway の Service が Listen してるところに curl でアクセスすれば httpbin サービスが結果を返すはずです。hosts なり DNS なりで設定すると良いでしょう。（ところで mac は hosts で同じ IP アドレスに沢山設定しすぎると5秒待たされたりする？？ちゃんと調べてないけどそんな感じだったのでもう Route53 にワイルドカードでプライベートアドレスを入れることにした）\nクラスタ外から curl でアクセス # minikube tunnel を使ってクラスタ外から curl https://httpbin.local.1q77.com/ip としてアクセスしています。\nistio-ingressgateway Pod の Envoy のログ # port 443 で受けて TLS 終端の後に httpbin Pod の　port 80 に送っています。\n{ \u0026#34;authority\u0026#34;: \u0026#34;httpbin.local.1q77.com\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;31\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;172.17.0.6:443\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;192.168.64.1:51276\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/ip\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP/2\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;26d8034e-df98-4d79-be70-9f3d20287623\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;httpbin.local.1q77.com\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-20T13:12:11.904Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;outbound|80|v2|httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;172.17.0.13:80\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;172.17.0.6:34718\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;curl/7.64.1\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;192.168.64.1\u0026#34; } httpbin Pod の Envoy のログ # Sidecar Envoy が port 80 で受けて　127.0.0.1:80 に流しています。\n{ \u0026#34;authority\u0026#34;: \u0026#34;httpbin.local.1q77.com\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;31\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;172.17.0.13:80\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;192.168.64.1:0\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/ip\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;26d8034e-df98-4d79-be70-9f3d20287623\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;outbound_.80_.v2_.httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-20T13:12:11.904Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;inbound|80|http|httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;127.0.0.1:80\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;127.0.0.1:50922\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;curl/7.64.1\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;192.168.64.1\u0026#34; } クラスタ内から curl でアクセス # クラスタ内で名前解決すると Istio Ingress Gateway Service の Cluster IP が返ってきました。\nroot@ubuntu-deployment-54bbd6f4ff-q9sdj:/# host httpbin.local.1q77.com httpbin.local.1q77.com has address 10.108.149.40 $ kubectl get svc -n istio-system -l app=istio-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.108.149.40 10.108.149.40 15020:30271/TCP,80:30723/TCP,443:32691/TCP,15029:30831/TCP,15030:30169/TCP,15031:32095/TCP,15032:30604/TCP,15443:30854/TCP 13d よって、クライアントとしての ubuntu Pod から istio-ingressgateway Pod で TLS が終端され、httpbin Pod にリクエストが届いています。\nクライアントとしての ubuntu Pod の Envoy のログ # https なのでリクエストの中身は見えていません。\n{ \u0026#34;authority\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;853\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;3801\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;10.108.149.40:443\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;172.17.0.8:45198\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;34\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-20T13:16:49.990Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;outbound|443||istio-ingressgateway.istio-system.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;172.17.0.6:443\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;172.17.0.8:39294\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;-\u0026#34; } istio-ingressgateway Pod の Envoy のログ # ここでは TLS が終端されているため、HTTP リクエストの中身がログに出ています。downstream_local_address と upstream_host から port 443 で受けて port 80 に流していることがわかります。\n{ \u0026#34;authority\u0026#34;: \u0026#34;httpbin.local.1q77.com\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;29\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;172.17.0.6:443\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;172.17.0.8:39294\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/ip\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP/2\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;9f6c5280-554b-449a-b82f-f2df6a43d785\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;httpbin.local.1q77.com\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-20T13:16:50.002Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;outbound|80|v2|httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;172.17.0.13:80\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;172.17.0.6:34718\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;curl/7.58.0\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;172.17.0.8\u0026#34; } httpbin Pod の Envoy のログ # { \u0026#34;authority\u0026#34;: \u0026#34;httpbin.local.1q77.com\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;29\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;172.17.0.13:80\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;172.17.0.8:0\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/ip\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;9f6c5280-554b-449a-b82f-f2df6a43d785\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;outbound_.80_.v2_.httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-20T13:16:50.003Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;inbound|80|http|httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;127.0.0.1:80\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;127.0.0.1:59168\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;curl/7.58.0\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;172.17.0.8\u0026#34; }  Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月20日","permalink":"/2020/03/istio-part11/","section":"Posts","summary":"Istio シリーズ 第11回です。 TLS Termination # 外部からのアクセスを Istio Ingrress Gateway に TLS の Temination をさせたいことがありますね。今回はこれを試します。 TLS Termination の設定は Gateway で行います。","title":"Istio 導入への道 – Ingress Gateway で TLS Termination 編"},{"content":"Istio シリーズ 第10回です。\nそろそろ図解してみようと思ったのだが\u0026hellip;\n正確に描くのは非常に難しい、そのうち Argo CD + Argo rollouts についても書くので Argo CD が描画する図を見る方が良いかもしれない。\n一応貼っておく。Service 間通信では Gateway は関係なかったり、ServiceEntry が入ってなかったり、Pod と Envoy の関係も見えないけど\u0026hellip;\n  Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月19日","permalink":"/2020/03/istio-part10/","section":"Posts","summary":"Istio シリーズ 第10回です。 そろそろ図解してみようと思ったのだが\u0026hellip; 正確に描くのは非常に難しい、そのうち Argo CD + Argo rollouts についても書くので","title":"Istio 導入への道 – 図解"},{"content":"Istio シリーズ 第9回です。\nIstio by Example に gRPC というのがあった。 gRPC に delay が挿入されている。できるんだねそれも。\nということで試してみます。gRPC については全然分かってないのだけれど。。。\nテスト用の gRPC サーバーとクライアントは github.com/yteraoka/grpc-helloworld に用意しました。Docker Image も yteraoka/grpc-helloworld:latest にあります。\n非 TLS の場合 # Deployment と Service を作る Manifest です。サーバーに -tls オプションをつけずに実行して、Service の port name は grpc にしてあります。\napiVersion: apps/v1 kind: Deployment metadata: name: grpc-helloworld-deployment labels: app: grpc-helloworld spec: replicas: 1 selector: matchLabels: app: grpc-helloworld template: metadata: labels: app: grpc-helloworld spec: containers: - name: helloworld image: yteraoka/grpc-helloworld:latest imagePullPolicy: IfNotPresent command: - \u0026quot;/server\u0026quot; ports: - containerPort: 10000 readinessProbe: exec: command: - \u0026quot;/client\u0026quot; initialDelaySeconds: 10 periodSeconds: 5 livenessProbe: exec: command: - \u0026quot;/client\u0026quot; initialDelaySeconds: 15 periodSeconds: 5 --- apiVersion: v1 kind: Service metadata: name: grpc-helloworld-service spec: selector: app: grpc-helloworld type: ClusterIP ports: - name: **grpc** protocol: TCP port: 10000 クライアントに使う Pod 用の Deployment はこちら。ここから作られる Pod 内から /client コマンドを実行します。\napiVersion: apps/v1 kind: Deployment metadata: name: grpc-client-deployment labels: app: grpc-client spec: replicas: 1 selector: matchLabels: app: grpc-client template: metadata: labels: app: grpc-client spec: containers: - name: grpc-client image: yteraoka/grpc-helloworld:latest imagePullPolicy: IfNotPresent command: - sleep - infinity クライアントからの接続テストは次のようにします。grpc-go って接続に成功するまでずっとリトライするんですね。間隔が開いていくんで Retry with Exponential Backoff and jitter ってやつかな。\n$ /client -server\\_addr grpc-helloworld-service.default.svc.cluster.local:10000 -timeout 10 VirtualService 無しの状態でのアクセス # Envoy は protocol を理解していますね。\n{ \u0026quot;authority\u0026quot;: \u0026quot;grpc-helloworld-service.default.svc.cluster.local:10000\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;12\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;18\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;10.98.145.150:10000\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.16:38298\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, **\u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;**, **\u0026quot;path\u0026quot;: \u0026quot;/helloworld.Greeter/SayHello\u0026quot;**, **\u0026quot;protocol\u0026quot;: \u0026quot;HTTP/2\u0026quot;**, \u0026quot;request\\_id\u0026quot;: \u0026quot;132f13ce-9503-4b4e-aaa9-ea9caf34d9a0\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;response\\_code\u0026quot;: \u0026quot;200\u0026quot;, \u0026quot;response\\_flags\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;route\\_name\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-14T15:36:25.619Z\u0026quot;, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;outbound|10000||grpc-helloworld-service.default.svc.cluster.local\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;172.17.0.17:10000\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;172.17.0.16:56496\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;grpc-go/1.28.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } VirtualService で Delay を挿入する # Fault Injection のために VirtualService を作成します。\nVirtualService の Manifest。常に 3 秒の delay を入れます。\napiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: grpc-helloworld-virtual-service spec: hosts: - grpc-helloworld-service http: - **fault:** **delay:** **percentage:** **value: 100.0** **fixedDelay: 3s** route: - destination: host: grpc-helloworld-service delay を挿入した場合のログです。\n{ \u0026quot;authority\u0026quot;: \u0026quot;grpc-helloworld-service.default.svc.cluster.local:10000\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;12\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;18\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;10.98.145.150:10000\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.16:51828\u0026quot;, **\u0026quot;duration\u0026quot;: \u0026quot;3004\u0026quot;**, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/helloworld.Greeter/SayHello\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/2\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;5340f5d7-f6a8-45e5-897f-ceeb3526dfd5\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;response\\_code\u0026quot;: \u0026quot;200\u0026quot;, **\u0026quot;response\\_flags\u0026quot;: \u0026quot;DI\u0026quot;**, \u0026quot;route\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-14T12:31:59.995Z\u0026quot;, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;outbound|10000||grpc-helloworld-service.default.svc.cluster.local\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;172.17.0.17:10000\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;172.17.0.16:41198\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;grpc-go/1.28.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } TLS 有効な場合 # 次に TLS を有効にした gRPC サーバーとの通信で試します。サーバーとヘルスチェック用のコマンドに -tls オプションをつけます。Service は port name を tls にします。TLS を有効にしているのに port name を grpc にしてしまうと通信できません。\napiVersion: apps/v1 kind: Deployment metadata: name: grpc-helloworld-deployment labels: app: grpc-helloworld spec: replicas: 1 selector: matchLabels: app: grpc-helloworld template: metadata: labels: app: grpc-helloworld spec: containers: - name: helloworld image: yteraoka/grpc-helloworld:latest imagePullPolicy: IfNotPresent command: - \u0026quot;/server\u0026quot; - \u0026quot;**\\-tls**\u0026quot; ports: - containerPort: 10000 readinessProbe: exec: command: - \u0026quot;/client\u0026quot; - \u0026quot;**\\-tls**\u0026quot; initialDelaySeconds: 10 periodSeconds: 5 livenessProbe: exec: command: - \u0026quot;/client\u0026quot; - \u0026quot;**\\-tls**\u0026quot; initialDelaySeconds: 15 periodSeconds: 5 --- apiVersion: v1 kind: Service metadata: name: grpc-helloworld-service spec: selector: app: grpc-helloworld type: ClusterIP ports: - name: **tls** protocol: TCP port: 10000 アクセスした場合のログ。TLS の中は見れません。よって delay を入れることもできません。\n{ \u0026quot;authority\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;688\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;1359\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;10.98.145.150:10000\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.16:46948\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;11\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;response\\_code\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;response\\_flags\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;route\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-14T15:43:52.547Z\u0026quot;, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;outbound|10000||grpc-helloworld-service.default.svc.cluster.local\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;172.17.0.7:10000\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;172.17.0.16:55454\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } まとめ # 考えてみれば「そりゃそうだよな」なんだけど TLS が有効では Fault Injection できませんでした。mesh 内の通信は非TLSでも良さそうですが、gRPC (HTTP2) で非TLSっていうのがどの程度一般的なのかが良くわからない。Envoy で gRPC の TLS 終端と負荷分散っていうのをちょいちょい見かけるので結構使われているのかな？\nIstio 使うなら mTLS もあるし、通信の暗号化という意味では問題なさそう。TLS 無しであればリクエスト単位でアクセスが分散されるのでこちらの方が良いですね。\nChaos Engineering 系ツールで delay 入れられたりするんだけどあれは API サーバーのレスポンスが遅い的な再現には使いづらいんですよね。\n Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月14日","permalink":"/2020/03/istio-part9/","section":"Posts","summary":"Istio シリーズ 第9回です。 Istio by Example に gRPC というのがあった。 gRPC に delay が挿入されている。できるんだねそれも。 ということで試してみます。gRPC については全","title":"Istio 導入への道 - gRPC でも Fault Injection 編"},{"content":"Docker for Mac で、moby linux にアクセスする | Developers.IO を参考にさせていただきました。\nmac で Docker を動かしてる hyperkit vm の中を覗いてみたいなと思い、ググったら上記の記事が見つかりましたが、私の今の環境では Path が変わってましたので、そのメモです。\nDocker Desktop のバージョンは「docker desktop community の 2.2.0.3 (42716)」でした。\n $ screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty 切断は Ctrl-a k\n","date":"2020年3月14日","permalink":"/2020/03/dive-into-docker-for-mac-vm/","section":"Posts","summary":"Docker for Mac で、moby linux にアクセスする | Developers.IO を参考にさせていただきました。 mac で Docker を動かしてる hyperkit vm の中を覗いてみたいなと思い、ググったら上記の記事が","title":"Docker Desktop for Mac で docker 用 VM の中に入る"},{"content":"","date":"2020年3月13日","permalink":"/tags/fluentd/","section":"Tags","summary":"","title":"fluentd"},{"content":"","date":"2020年3月13日","permalink":"/tags/gcp/","section":"Tags","summary":"","title":"GCP"},{"content":"GKE は何もしなくてもログを Cloud Logging (旧 Stackdriver Logging) に送ってくれて便利なんだけどどうやって送ってるのかな？と思って調べたメモ。なかなか興味深かった。\nメトリクスの方も調べてみたがそれはまた別途、と思ったけどここでも登場した。\nGKE クラスタの作成 # クラスタの作成。画面ポチポチして取得した gcloud コマンドは省略可能なものも沢山ついてて長い。gcloud の config に compute/region, compute/zone が設定されている前提。でも設定されてたら指定の必要もないのか？\n$ PROJECT\\_ID=$(gcloud config get-value project) $ REGION=$(gcloud config get-value compute/region) $ ZONE=$(gcloud config get-value compute/zone) $ CLUSTER\\_NAME=cluster-1 $ gcloud beta container --project \u0026quot;${PROJECT\\_ID}\u0026quot; clusters create \u0026quot;${CLUSTER\\_NAME}\u0026quot; \\\\ --zone \u0026quot;${ZONE}\u0026quot; \\\\ --no-enable-basic-auth \\\\ --cluster-version \u0026quot;1.14.10-gke.17\u0026quot; \\\\ --machine-type \u0026quot;n1-standard-1\u0026quot; \\\\ --image-type \u0026quot;COS\u0026quot; \\\\ --disk-type \u0026quot;pd-standard\u0026quot; \\\\ --disk-size \u0026quot;100\u0026quot; \\\\ --metadata disable-legacy-endpoints=true \\\\ --scopes \\\\ \u0026quot;https://www.googleapis.com/auth/devstorage.read\\_only\u0026quot;,\\\\ \u0026quot;https://www.googleapis.com/auth/logging.write\u0026quot;,\\\\ \u0026quot;https://www.googleapis.com/auth/monitoring\u0026quot;,\\\\ \u0026quot;https://www.googleapis.com/auth/servicecontrol\u0026quot;,\\\\ \u0026quot;https://www.googleapis.com/auth/service.management.readonly\u0026quot;,\\\\ \u0026quot;https://www.googleapis.com/auth/trace.append\u0026quot; \\\\ --preemptible \\\\ --num-nodes \u0026quot;1\u0026quot; \\\\ --enable-stackdriver-kubernetes \\\\ --enable-ip-alias \\\\ --network \u0026quot;projects/${PROJECT\\_ID}/global/networks/default\u0026quot; \\\\ --subnetwork \u0026quot;projects/${PROJECT\\_ID}/regions/${REGION}/subnetworks/default\u0026quot; \\\\ --default-max-pods-per-node \u0026quot;110\u0026quot; \\\\ --enable-autoscaling --min-nodes \u0026quot;0\u0026quot; --max-nodes \u0026quot;1\u0026quot; \\\\ --no-enable-master-authorized-networks \\\\ --addons HorizontalPodAutoscaling,HttpLoadBalancing \\\\ --enable-autoupgrade \\\\ --enable-autorepair \\\\ --max-surge-upgrade 1 \\\\ --max-unavailable-upgrade 0 kube-system 内の Pod などを確認 # kube-system 内に作られているものを確認。この中の daemonset.apps/fluentd-gcp-v3.1.1 がログを送っている。\n$ kubectl get all -n kube-system NAME READY STATUS RESTARTS AGE pod/event-exporter-v0.2.5-7df89f4b8f-kzzz5 2/2 Running 0 4m5s pod/fluentd-gcp-scaler-54ccb89d5-6c6cf 1/1 Running 0 4m1s pod/fluentd-gcp-v3.1.1-w8rhw 2/2 Running 0 3m38s pod/heapster-gke-54fdfc9bd4-gs9cf 3/3 Running 0 3m6s pod/kube-dns-5877696fb4-5b2dp 4/4 Running 0 4m6s pod/kube-dns-autoscaler-8687c64fc-gj9rg 1/1 Running 0 4m1s pod/kube-proxy-gke-cluster-1-default-pool-fe6f5f88-br21 1/1 Running 0 3m57s pod/l7-default-backend-8f479dd9-fpcch 1/1 Running 0 4m6s pod/metrics-server-v0.3.1-5c6fbf777-rs45z 2/2 Running 0 3m43s pod/prometheus-to-sd-84v7h 2/2 Running 0 3m57s pod/stackdriver-metadata-agent-cluster-level-69454f8dd5-p7w7b 1/1 Running 0 4m5s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/default-http-backend NodePort 10.0.5.107 \u0026lt;none\u0026gt; 80:30406/TCP 4m6s service/heapster ClusterIP 10.0.0.114 \u0026lt;none\u0026gt; 80/TCP 4m5s service/kube-dns ClusterIP 10.0.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 4m7s service/metrics-server ClusterIP 10.0.5.80 \u0026lt;none\u0026gt; 443/TCP 4m3s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/fluentd-gcp-v3.1.1 1 1 1 1 1 beta.kubernetes.io/fluentd-ds-ready=true,beta.kubernetes.io/os=linux 4m6s daemonset.apps/metadata-proxy-v0.1 0 0 0 0 0 beta.kubernetes.io/metadata-proxy-ready=true,beta.kubernetes.io/os=linux 4m5s daemonset.apps/nvidia-gpu-device-plugin 0 0 0 0 0 \u0026lt;none\u0026gt; 4m1s daemonset.apps/prometheus-to-sd 1 1 1 1 1 beta.kubernetes.io/os=linux 4m5s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/event-exporter-v0.2.5 1/1 1 1 4m5s deployment.apps/fluentd-gcp-scaler 1/1 1 1 4m1s deployment.apps/heapster-gke 1/1 1 1 4m6s deployment.apps/kube-dns 1/1 1 1 4m7s deployment.apps/kube-dns-autoscaler 1/1 1 1 4m6s deployment.apps/l7-default-backend 1/1 1 1 4m6s deployment.apps/metrics-server-v0.3.1 1/1 1 1 4m4s deployment.apps/stackdriver-metadata-agent-cluster-level 1/1 1 1 4m5s NAME DESIRED CURRENT READY AGE replicaset.apps/event-exporter-v0.2.5-7df89f4b8f 1 1 1 4m5s replicaset.apps/fluentd-gcp-scaler-54ccb89d5 1 1 1 4m1s replicaset.apps/heapster-gke-54fdfc9bd4 1 1 1 3m6s replicaset.apps/heapster-gke-7fbb79848 0 0 0 4m6s replicaset.apps/kube-dns-5877696fb4 1 1 1 4m7s replicaset.apps/kube-dns-autoscaler-8687c64fc 1 1 1 4m6s replicaset.apps/l7-default-backend-8f479dd9 1 1 1 4m6s replicaset.apps/metrics-server-v0.3.1-5c6fbf777 1 1 1 3m43s replicaset.apps/metrics-server-v0.3.1-8559697b9c 0 0 0 4m4s replicaset.apps/stackdriver-metadata-agent-cluster-level-69454f8dd5 1 1 1 4m5s fluentd-gcp-v3.1.1 の定義を確認 # Pod 内には fluentd-gcp と prometheus-to-sd-exporter という2つのコンテナが含まれています。後者は fluentd の prometheus plugin が提供する endpoint を polling して定期的にメトリクスを Cloud Monitoring (旧 Stackdriver Monitoring) に送信しています。似たようなやつが DamonSet にもいて、各 node で kubelet, kube-proxy の Prometheus 用メトリクスを Cloud Monitoring に送っています。\n$ kubectl get -n kube-system daemonset.apps/fluentd-gcp-v3.1.1 -o yaml apiVersion: apps/v1 kind: DaemonSet metadata: annotations: (省略) generation: 2 labels: addonmanager.kubernetes.io/mode: Reconcile k8s-app: fluentd-gcp kubernetes.io/cluster-service: \u0026quot;true\u0026quot; version: v3.1.1 name: fluentd-gcp-v3.1.1 namespace: kube-system resourceVersion: \u0026quot;786\u0026quot; selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/fluentd-gcp-v3.1.1 uid: 3eeada2a-6530-11ea-a16f-42010a920085 spec: revisionHistoryLimit: 10 selector: matchLabels: k8s-app: fluentd-gcp kubernetes.io/cluster-service: \u0026quot;true\u0026quot; version: v3.1.1 template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026quot;\u0026quot; creationTimestamp: null labels: k8s-app: fluentd-gcp kubernetes.io/cluster-service: \u0026quot;true\u0026quot; version: v3.1.1 spec: containers: - env: - name: NODE\\_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: K8S\\_NODE\\_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName - name: STACKDRIVER\\_METADATA\\_AGENT\\_URL value: http://$(NODE\\_NAME):8799 image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.17-16060 imagePullPolicy: IfNotPresent livenessProbe: exec: command: - /bin/sh - -c - |2 LIVENESS\\_THRESHOLD\\_SECONDS=${LIVENESS\\_THRESHOLD\\_SECONDS:-300}; STUCK\\_THRESHOLD\\_SECONDS=${STUCK\\_THRESHOLD\\_SECONDS:-900}; if \\[ ! -e /var/run/google-fluentd/buffers \\]; then exit 1; fi; touch -d \u0026quot;${STUCK\\_THRESHOLD\\_SECONDS} seconds ago\u0026quot; /tmp/marker-stuck; if \\[ -z \u0026quot;$(find /var/run/google-fluentd/buffers -type d -newer /tmp/marker-stuck -print -quit)\u0026quot; \\]; then rm -rf /var/run/google-fluentd/buffers; exit 1; fi; touch -d \u0026quot;${LIVENESS\\_THRESHOLD\\_SECONDS} seconds ago\u0026quot; /tmp/marker-liveness; if \\[ -z \u0026quot;$(find /var/run/google-fluentd/buffers -type d -newer /tmp/marker-liveness -print -quit)\u0026quot; \\]; then exit 1; fi; failureThreshold: 3 initialDelaySeconds: 600 periodSeconds: 60 successThreshold: 1 timeoutSeconds: 1 name: fluentd-gcp resources: limits: cpu: \u0026quot;1\u0026quot; memory: 500Mi requests: cpu: 100m memory: 200Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/google-fluentd name: varrun - mountPath: /var/log name: varlog - mountPath: /var/lib/docker/containers name: varlibdockercontainers readOnly: true - mountPath: /etc/google-fluentd/config.d name: config-volume - command: - /monitor - --stackdriver-prefix=container.googleapis.com/internal/addons - --api-override=https://monitoring.googleapis.com/ - --source=fluentd:http://localhost:24231?whitelisted=stackdriver\\_successful\\_requests\\_count,stackdriver\\_failed\\_requests\\_count,stackdriver\\_ingested\\_entries\\_count,stackdriver\\_dropped\\_entries\\_count - --pod-id=$(POD\\_NAME) - --namespace-id=$(POD\\_NAMESPACE) env: - name: POD\\_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD\\_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace image: k8s.gcr.io/prometheus-to-sd:v0.5.0 imagePullPolicy: IfNotPresent name: prometheus-to-sd-exporter resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: Default hostNetwork: true nodeSelector: beta.kubernetes.io/fluentd-ds-ready: \u0026quot;true\u0026quot; beta.kubernetes.io/os: linux priorityClassName: system-node-critical restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: fluentd-gcp serviceAccountName: fluentd-gcp terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute operator: Exists - effect: NoSchedule operator: Exists volumes: - hostPath: path: /var/run/google-fluentd type: \u0026quot;\u0026quot; name: varrun - hostPath: path: /var/log type: \u0026quot;\u0026quot; name: varlog - hostPath: path: /var/lib/docker/containers type: \u0026quot;\u0026quot; name: varlibdockercontainers - configMap: defaultMode: 420 name: fluentd-gcp-config-v1.2.6 name: config-volume updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate status: (省略) fluentd の設定ファイルを読み解く # fluentd の config file を確認します。config.d サブディレクトリ配下のファイルは fluentd-gcp-config-v1.2.6 という ConfigMap に入っています。\n$ kubectl -n kube-system exec -it -c fluentd-gcp \\\\ $(kubectl get -n kube-system pods -l k8s-app=fluentd-gcp -o jsonpath='{.items\\[0\\].metadata.name}') \\\\ -- ls -l /etc/google-fluentd/ total 12 drwxrwxrwx 3 root root 4096 Mar 13 13:41 config.d -rw-r--r-- 1 root root 2195 Sep 24 21:11 google-fluentd.conf drwxr-xr-x 2 root root 4096 Sep 24 21:11 plugin $ kubectl -n kube-system exec -it -c fluentd-gcp \\\\ $(kubectl get -n kube-system pods -l k8s-app=fluentd-gcp -o jsonpath='{.items\\[0\\].metadata.name}') \\\\ -- ls -l /etc/google-fluentd/config.d/ total 0 lrwxrwxrwx 1 root root 28 Mar 13 13:41 containers.input.conf -\u0026gt; ..data/containers.input.conf lrwxrwxrwx 1 root root 22 Mar 13 13:41 monitoring.conf -\u0026gt; ..data/monitoring.conf lrwxrwxrwx 1 root root 18 Mar 13 13:41 output.conf -\u0026gt; ..data/output.conf lrwxrwxrwx 1 root root 24 Mar 13 13:41 system.input.conf -\u0026gt; ..data/system.input.conf /etc/google-fluentd/google-fluentd.conf # まずはメインのファイルです。この中に @include config.d/*.conf とあり、config.d 配下のファイルが読み込まれています。\n@type prometheus, @type prometheus_monitor というのがありますが。「fluent-plugin-prometheusをリリースしました - Qiita」ですね。先に紹介した prometheus-to-sd-exporter コンテナはこの plugin が提供する endpoint にアクセスしています。\n@type add_insert_ids は送った先で重複してログを保存（もしくは集計）しないように各行にユニークなIDを振っているようです。そんな工夫がされていたんですね。retry の影響でログが重複してるよーって事にならないのは嬉しい。(filter_add_insert_ids.rb)\n次の @type google_cloud も Google の plugin で BufferedOutput で Cloud Logging に送る部分を担当しています。(out_google_cloud.rb)ファイルに buffering して複数スレッドで gRPC で送るみたいですね。\n\\# Master configuration file for google-fluentd # Include any configuration files in the config.d directory. # # An example \u0026quot;catch-all\u0026quot; configuration can be found at # https://github.com/GoogleCloudPlatform/fluentd-catch-all-config @include config.d/\\*.conf # Prometheus monitoring. \u0026lt;source\u0026gt; @type prometheus port 24231 \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type prometheus\\_monitor \u0026lt;/source\u0026gt; # Do not collect fluentd's own logs to avoid infinite loops. \u0026lt;match fluent.\\*\\*\u0026gt; @type null \u0026lt;/match\u0026gt; # Add a unique insertId to each log entry that doesn't already have it. # This helps guarantee the order and prevent log duplication. \u0026lt;filter \\*\\*\u0026gt; @type add\\_insert\\_ids \u0026lt;/filter\u0026gt; # Configure all sources to output to Google Cloud Logging \u0026lt;match \\*\\*\u0026gt; @type google\\_cloud buffer\\_type file buffer\\_path /var/log/google-fluentd/buffers # Set the chunk limit conservatively to avoid exceeding the recommended # chunk size of 5MB per write request. buffer\\_chunk\\_limit 512KB # Flush logs every 5 seconds, even if the buffer is not full. flush\\_interval 5s # Enforce some limit on the number of retries. disable\\_retry\\_limit false # After 3 retries, a given chunk will be discarded. retry\\_limit 3 # Wait 10 seconds before the first retry. The wait interval will be doubled on # each following retry (20s, 40s...) until it hits the retry limit. retry\\_wait 10 # Never wait longer than 5 minutes between retries. If the wait interval # reaches this limit, the exponentiation stops. # Given the default config, this limit should never be reached, but if # retry\\_limit and retry\\_wait are customized, this limit might take effect. max\\_retry\\_wait 300 # Use multiple threads for processing. num\\_threads 8 detect\\_json true # Enable metadata agent lookups. enable\\_metadata\\_agent true metadata\\_agent\\_url \u0026quot;http://local-metadata-agent.stackdriver.com:8000\u0026quot; # Use the gRPC transport. use\\_grpc true # If a request is a mix of valid log entries and invalid ones, ingest the # valid ones and drop the invalid ones instead of dropping everything. partial\\_success true # Enable monitoring via Prometheus integration. enable\\_monitoring true monitoring\\_type prometheus \u0026lt;/match\u0026gt; /etc/google-fluentd/config.d/containers.input.conf # 次は containers.input.conf です。名前の通り各コンテナのログを /var/log/containers/*.log をtail で読み出してます。そう、docker の fluentd log driver を使うわけではないんですね。(docker と containerd の関係とか良くわからん)\nその後は kubernetes の metadata を付加して行きます。plugin 書いてたりするのに record_modifier で ruby で処理したりもするんですね。\n最後の @type detect_exceptions は複数行に渡る stack trace を発見すると1つのメッセージにまとめてくれる便利 plugin のようです。(GoogleCloudPlatform/fluent-plugin-detect-exceptions)\n\\# This configuration file for Fluentd is used # to watch changes to Docker log files that live in the # directory /var/lib/docker/containers/ and are symbolically # linked to from the /var/log/containers directory using names that capture the # pod name and container name. These logs are then submitted to # Google Cloud Logging which assumes the installation of the cloud-logging plug-in. # # Example # ======= # A line in the Docker log file might look like this JSON: # # {\u0026quot;log\u0026quot;:\u0026quot;2014/09/25 21:15:03 Got request with path wombat\\\\\\\\n\u0026quot;, # \u0026quot;stream\u0026quot;:\u0026quot;stderr\u0026quot;, # \u0026quot;time\u0026quot;:\u0026quot;2014-09-25T21:15:03.499185026Z\u0026quot;} # # The original tag is derived from the log file's location. # For example a Docker container's logs might be in the directory: # /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b # and in the file: # 997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log # where 997599971ee6... is the Docker ID of the running container. # The Kubernetes kubelet makes a symbolic link to this file on the host # machine in the /var/log/containers directory which includes the pod name, # the namespace name and the Kubernetes container name: # synthetic-logger-0.25lps-pod\\_default\\_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log # -\u0026gt; # /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log # The /var/log directory on the host is mapped to the /var/log directory in the container # running this instance of Fluentd and we end up collecting the file: # /var/log/containers/synthetic-logger-0.25lps-pod\\_default\\_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log # This results in the tag: # var.log.containers.synthetic-logger-0.25lps-pod\\_default\\_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log # where 'synthetic-logger-0.25lps-pod' is the pod name, 'default' is the # namespace name, 'synth-lgr' is the container name and '997599971ee6..' is # the container ID. # The record reformer is used to extract pod\\_name, namespace\\_name and # container\\_name from the tag and set them in a local\\_resource\\_id in the # format of: # 'k8s\\_container.\u0026lt;NAMESPACE\\_NAME\u0026gt;.\u0026lt;POD\\_NAME\u0026gt;.\u0026lt;CONTAINER\\_NAME\u0026gt;'. # The reformer also changes the tags to 'stderr' or 'stdout' based on the # value of 'stream'. # local\\_resource\\_id is later used by google\\_cloud plugin to determine the # monitored resource to ingest logs against. # Json Log Example: # {\u0026quot;log\u0026quot;:\u0026quot;\\[info:2016-02-16T16:04:05.930-08:00\\] Some log text here\\\\n\u0026quot;,\u0026quot;stream\u0026quot;:\u0026quot;stdout\u0026quot;,\u0026quot;time\u0026quot;:\u0026quot;2016-02-17T00:04:05.931087621Z\u0026quot;} # CRI Log Example: # 2016-02-17T00:04:05.931087621Z stdout F \\[info:2016-02-16T16:04:05.930-08:00\\] Some log text here \u0026lt;source\u0026gt; @type tail path /var/log/containers/\\*.log pos\\_file /var/run/google-fluentd/pos-files/gcp-containers.pos # Tags at this point are in the format of: # reform.var.log.containers.\u0026lt;POD\\_NAME\u0026gt;\\_\u0026lt;NAMESPACE\\_NAME\u0026gt;\\_\u0026lt;CONTAINER\\_NAME\u0026gt;-\u0026lt;CONTAINER\\_ID\u0026gt;.log tag reform.\\* read\\_from\\_head true \u0026lt;parse\u0026gt; @type multi\\_format \u0026lt;pattern\u0026gt; format json time\\_key time time\\_format %Y-%m-%dT%H:%M:%S.%NZ \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) \\[^ \\]\\* (?\u0026lt;log\u0026gt;.\\*)$/ time\\_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;filter reform.\\*\\*\u0026gt; @type parser format /^(?\u0026lt;severity\u0026gt;\\\\w)(?\u0026lt;time\u0026gt;\\\\d{4} \\[^\\\\s\\]\\*)\\\\s+(?\u0026lt;pid\u0026gt;\\\\d+)\\\\s+(?\u0026lt;source\u0026gt;\\[^ \\\\\\]\\]+)\\\\\\] (?\u0026lt;log\u0026gt;.\\*)/ reserve\\_data true suppress\\_parse\\_error\\_log true emit\\_invalid\\_record\\_to\\_error false key\\_name log \u0026lt;/filter\u0026gt; \u0026lt;filter reform.\\*\\*\u0026gt; # This plugin uses environment variables KUBERNETES\\_SERVICE\\_HOST and # KUBERNETES\\_SERVICE\\_PORT to talk to the API server. These environment # variables are added by kubelet automatically. @type kubernetes\\_metadata # Interval in seconds to dump cache stats locally in the Fluentd log. stats\\_interval 300 # TTL in seconds of each cached element. cache\\_ttl 30 # Skip fetching unused metadata. skip\\_container\\_metadata true skip\\_master\\_url true skip\\_namespace\\_metadata true \u0026lt;/filter\u0026gt; \u0026lt;filter reform.\\*\\*\u0026gt; # We have to use record\\_modifier because only this plugin supports complex # logic to modify record the way we need. @type record\\_modifier enable\\_ruby true \u0026lt;record\u0026gt; # Extract \u0026quot;kubernetes\u0026quot;-\u0026gt;\u0026quot;labels\u0026quot; and set them as # \u0026quot;logging.googleapis.com/labels\u0026quot;. Prefix these labels with # \u0026quot;k8s-pod\u0026quot; to distinguish with other labels and avoid # label name collision with other types of labels. \\_dummy\\_ ${if record.is\\_a?(Hash) \u0026amp;\u0026amp; record.has\\_key?('kubernetes') \u0026amp;\u0026amp; record\\['kubernetes'\\].has\\_key?('labels') \u0026amp;\u0026amp; record\\['kubernetes'\\]\\['labels'\\].is\\_a?(Hash); then; record\\[\u0026quot;logging.googleapis.com/labels\u0026quot;\\] = record\\['kubernetes'\\]\\['labels'\\].map{ |k, v| \\[\u0026quot;k8s-pod/#{k}\u0026quot;, v\\]}.to\\_h; end; nil} \u0026lt;/record\u0026gt; # Delete this dummy field and the rest of \u0026quot;kubernetes\u0026quot; and \u0026quot;docker\u0026quot;. remove\\_keys \\_dummy\\_,kubernetes,docker \u0026lt;/filter\u0026gt; \u0026lt;match reform.\\*\\*\u0026gt; @type record\\_reformer enable\\_ruby true \u0026lt;record\u0026gt; # Extract local\\_resource\\_id from tag for 'k8s\\_container' monitored # resource. The format is: # 'k8s\\_container.\u0026lt;namespace\\_name\u0026gt;.\u0026lt;pod\\_name\u0026gt;.\u0026lt;container\\_name\u0026gt;'. \u0026quot;logging.googleapis.com/local\\_resource\\_id\u0026quot; ${\u0026quot;k8s\\_container.#{tag\\_suffix\\[4\\].rpartition('.')\\[0\\].split('\\_')\\[1\\]}.#{tag\\_suffix\\[4\\].rpartition('.')\\[0\\].split('\\_')\\[0\\]}.#{tag\\_suffix\\[4\\].rpartition('.')\\[0\\].split('\\_')\\[2\\].rpartition('-')\\[0\\]}\u0026quot;} # Rename the field 'log' to a more generic field 'message'. This way the # fluent-plugin-google-cloud knows to flatten the field as textPayload # instead of jsonPayload after extracting 'time', 'severity' and # 'stream' from the record. message ${record\\['log'\\]} # If 'severity' is not set, assume stderr is ERROR and stdout is INFO. severity ${record\\['severity'\\] || if record\\['stream'\\] == 'stderr' then 'ERROR' else 'INFO' end} \u0026lt;/record\u0026gt; tag ${if record\\['stream'\\] == 'stderr' then 'raw.stderr' else 'raw.stdout' end} remove\\_keys stream,log \u0026lt;/match\u0026gt; # Detect exceptions in the log output and forward them as one log entry. \u0026lt;match {raw.stderr,raw.stdout}\u0026gt; @type detect\\_exceptions remove\\_tag\\_prefix raw message message stream \u0026quot;logging.googleapis.com/local\\_resource\\_id\u0026quot; multiline\\_flush\\_interval 5 max\\_bytes 500000 max\\_lines 1000 \u0026lt;/match\u0026gt; /etc/google-fluentd/config.d/monitoring.conf # 次は monitoring.onf です。これは短い。\n@type exec で date +%s コマンドを実行し、unix timestamp を process_start_timestamp という key に入れます。\nその後、型を Integer に変換して終わり。続きは次のファイルです。\n\\# This source is used to acquire approximate process start timestamp, # which purpose is explained before the corresponding output plugin. \u0026lt;source\u0026gt; @type exec command /bin/sh -c 'date +%s' tag process\\_start time\\_format %Y-%m-%d %H:%M:%S keys process\\_start\\_timestamp \u0026lt;/source\u0026gt; # This filter is used to convert process start timestamp to integer # value for correct ingestion in the prometheus output plugin. \u0026lt;filter process\\_start\u0026gt; @type record\\_transformer enable\\_ruby true auto\\_typecast true \u0026lt;record\u0026gt; process\\_start\\_timestamp ${record\\[\u0026quot;process\\_start\\_timestamp\u0026quot;\\].to\\_i} \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; /etc/google-fluentd/config.d/output.conf # 次は output.conf です。最初のファイルで全部 Cloud Logging に送るようになってたじゃんって思うわけですが、ここでも出てきます。\nまずは @type prometheus です。なるほどそういうことかって感じですね。さっきの process_start_timestamp は prometheus で良くあるプロセスの起動時刻用だったのです。\n次も prometheus 用で読み込んだログの行数カウンターです。\nその次は stdout, stderr タグのメッセージについて、Cloud Logging は1メッセージあたり100KBという制限があるために、100000 Bytes を超える場合はそれ以降を切り捨てて [Trimmed] っていう prefix を入れます。しかし、.length だとマルチバイトの時に制限超えちゃうんじゃないの？？？\n\u0026ldquo;This section is exclusive for k8s_container logs.\u0026rdquo; ってのは k8s_container っていう新しい Kubernetes Engine Monitoring 専用って事ですね。\n次は fluent.* は fluentd 自身のログなので捨てる。\nその後また @type add_insert_ids が出てくるけど、すでに unique id が存在したら何もしないらしいので重複して適用しても大丈夫みたい。\n後はまた metadata いじって Cloud Logging に送る。\n\\# This match is placed before the all-matching output to provide metric # exporter with a process start timestamp for correct exporting of # cumulative metrics to Stackdriver. \u0026lt;match process\\_start\u0026gt; @type prometheus \u0026lt;metric\u0026gt; type gauge name process\\_start\\_time\\_seconds desc Timestamp of the process start in seconds key process\\_start\\_timestamp \u0026lt;/metric\u0026gt; \u0026lt;/match\u0026gt; # This filter allows to count the number of log entries read by fluentd # before they are processed by the output plugin. This in turn allows to # monitor the number of log entries that were read but never sent, e.g. # because of liveness probe removing buffer. \u0026lt;filter \\*\\*\u0026gt; @type prometheus \u0026lt;metric\u0026gt; type counter name logging\\_entry\\_count desc Total number of log entries generated by either application containers or system components \u0026lt;/metric\u0026gt; \u0026lt;/filter\u0026gt; # This section is exclusive for k8s\\_container logs. Those come with # 'stderr'/'stdout' tags. # TODO(instrumentation): Reconsider this workaround later. # Trim the entries which exceed slightly less than 100KB, to avoid # dropping them. It is a necessity, because Stackdriver only supports # entries that are up to 100KB in size. \u0026lt;filter {stderr,stdout}\u0026gt; @type record\\_transformer enable\\_ruby true \u0026lt;record\u0026gt; message ${record\\['message'\\].length \u0026gt; 100000 ? \u0026quot;\\[Trimmed\\]#{record\\['message'\\]\\[0..100000\\]}...\u0026quot; : record\\['message'\\]} \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; # Do not collect fluentd's own logs to avoid infinite loops. \u0026lt;match fluent.\\*\\*\u0026gt; @type null \u0026lt;/match\u0026gt; # Add a unique insertId to each log entry that doesn't already have it. # This helps guarantee the order and prevent log duplication. \u0026lt;filter \\*\\*\u0026gt; @type add\\_insert\\_ids \u0026lt;/filter\u0026gt; # This filter parses the 'source' field created for glog lines into a single # top-level field, for proper processing by the output plugin. # For example, if a record includes: # {\u0026quot;source\u0026quot;:\u0026quot;handlers.go:131\u0026quot;}, # then the following entry will be added to the record: # {\u0026quot;logging.googleapis.com/sourceLocation\u0026quot;: # {\u0026quot;file\u0026quot;:\u0026quot;handlers.go\u0026quot;, \u0026quot;line\u0026quot;:\u0026quot;131\u0026quot;} # } \u0026lt;filter \\*\\*\u0026gt; @type record\\_transformer enable\\_ruby true \u0026lt;record\u0026gt; \u0026quot;logging.googleapis.com/sourceLocation\u0026quot; ${if record.is\\_a?(Hash) \u0026amp;\u0026amp; record.has\\_key?('source'); source\\_parts = record\\['source'\\].split(':', 2); {'file' =\u0026gt; source\\_parts\\[0\\], 'line' =\u0026gt; source\\_parts\\[1\\]} if source\\_parts.length == 2; else; nil; end} \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; # This section is exclusive for k8s\\_container logs. These logs come with # 'stderr'/'stdout' tags. # We use a separate output stanza for 'k8s\\_node' logs with a smaller buffer # because node logs are less important than user's container logs. \u0026lt;match {stderr,stdout}\u0026gt; @type google\\_cloud # Try to detect JSON formatted log entries. detect\\_json true # Collect metrics in Prometheus registry about plugin activity. enable\\_monitoring true monitoring\\_type prometheus # Allow log entries from multiple containers to be sent in the same request. split\\_logs\\_by\\_tag false # Set the buffer type to file to improve the reliability and reduce the memory consumption buffer\\_type file buffer\\_path /var/run/google-fluentd/buffers/kubernetes.containers.buffer # Set queue\\_full action to block because we want to pause gracefully # in case of the off-the-limits load instead of throwing an exception buffer\\_queue\\_full\\_action block # Set the chunk limit conservatively to avoid exceeding the recommended # chunk size of 5MB per write request. buffer\\_chunk\\_limit 512k # Cap the combined memory usage of this buffer and the one below to # 512KiB/chunk \\* (6 + 2) chunks = 4 MiB buffer\\_queue\\_limit 6 # Never wait more than 5 seconds before flushing logs in the non-error case. flush\\_interval 5s # Never wait longer than 30 seconds between retries. max\\_retry\\_wait 30 # Disable the limit on the number of retries (retry forever). disable\\_retry\\_limit # Use multiple threads for processing. num\\_threads 2 use\\_grpc true # Skip timestamp adjustment as this is in a controlled environment with # known timestamp format. This helps with CPU usage. adjust\\_invalid\\_timestamps false \u0026lt;/match\u0026gt; # Attach local\\_resource\\_id for 'k8s\\_node' monitored resource. \u0026lt;filter \\*\\*\u0026gt; @type record\\_transformer enable\\_ruby true \u0026lt;record\u0026gt; \u0026quot;logging.googleapis.com/local\\_resource\\_id\u0026quot; ${\u0026quot;k8s\\_node.#{ENV\\['NODE\\_NAME'\\]}\u0026quot;} \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; # This section is exclusive for 'k8s\\_node' logs. These logs come with tags # that are neither 'stderr' or 'stdout'. # We use a separate output stanza for 'k8s\\_container' logs with a larger # buffer because user's container logs are more important than node logs. \u0026lt;match \\*\\*\u0026gt; @type google\\_cloud detect\\_json true enable\\_monitoring true monitoring\\_type prometheus # Allow entries from multiple system logs to be sent in the same request. split\\_logs\\_by\\_tag false detect\\_subservice false buffer\\_type file buffer\\_path /var/run/google-fluentd/buffers/kubernetes.system.buffer buffer\\_queue\\_full\\_action block buffer\\_chunk\\_limit 512k buffer\\_queue\\_limit 2 flush\\_interval 5s max\\_retry\\_wait 30 disable\\_retry\\_limit num\\_threads 2 use\\_grpc true # Skip timestamp adjustment as this is in a controlled environment with # known timestamp format. This helps with CPU usage. adjust\\_invalid\\_timestamps false \u0026lt;/match\u0026gt; /etc/google-fluentd/config.d/system.input.conf # これで最後、system.input.conf です。\n/var/log 配下のログや systemd の journal log から読み出す設定です。Control Plane でも使われているのか api server とか etcd 用の設定も入ってますね。\n\\# Example: # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script \u0026lt;source\u0026gt; @type tail format syslog path /var/log/startupscript.log pos\\_file /var/run/google-fluentd/pos-files/gcp-startupscript.pos tag startupscript \u0026lt;/source\u0026gt; # Examples: # time=\u0026quot;2016-02-04T06:51:03.053580605Z\u0026quot; level=info msg=\u0026quot;GET /containers/json\u0026quot; # time=\u0026quot;2016-02-04T07:53:57.505612354Z\u0026quot; level=error msg=\u0026quot;HTTP Error\u0026quot; err=\u0026quot;No such image: -f\u0026quot; statusCode=404 # TODO(random-liu): Remove this after cri container runtime rolls out. \u0026lt;source\u0026gt; @type tail format /^time=\u0026quot;(?\u0026lt;time\u0026gt;\\[^)\\]\\*)\u0026quot; level=(?\u0026lt;severity\u0026gt;\\[^ \\]\\*) msg=\u0026quot;(?\u0026lt;message\u0026gt;\\[^\u0026quot;\\]\\*)\u0026quot;( err=\u0026quot;(?\u0026lt;error\u0026gt;\\[^\u0026quot;\\]\\*)\u0026quot;)?( statusCode=($\u0026lt;status\\_code\u0026gt;\\\\d+))?/ path /var/log/docker.log pos\\_file /var/run/google-fluentd/pos-files/gcp-docker.pos tag docker \u0026lt;/source\u0026gt; # Example: # 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal \u0026lt;source\u0026gt; @type tail # Not parsing this, because it doesn't have anything particularly useful to # parse out of it (like severities). format none path /var/log/etcd.log pos\\_file /var/run/google-fluentd/pos-files/gcp-etcd.pos tag etcd \u0026lt;/source\u0026gt; # Multi-line parsing is required for all the kube logs because very large log # statements, such as those that include entire object bodies, get split into # multiple lines by glog. # Example: # I0204 07:32:30.020537 3368 server.go:1048\\] POST /stats/container/: (13.972191ms) 200 \\[\\[Go-http-client/1.1\\] 10.244.1.3:40537\\] \u0026lt;source\u0026gt; @type tail format multiline multiline\\_flush\\_interval 5s format\\_firstline /^\\\\w\\\\d{4}/ format1 /^(?\u0026lt;severity\u0026gt;\\\\w)(?\u0026lt;time\u0026gt;\\\\d{4} \\[^\\\\s\\]\\*)\\\\s+(?\u0026lt;pid\u0026gt;\\\\d+)\\\\s+(?\u0026lt;source\u0026gt;\\[^ \\\\\\]\\]+)\\\\\\] (?\u0026lt;message\u0026gt;.\\*)/ time\\_format %m%d %H:%M:%S.%N path /var/log/kubelet.log pos\\_file /var/run/google-fluentd/pos-files/gcp-kubelet.pos tag kubelet \u0026lt;/source\u0026gt; # Example: # I1118 21:26:53.975789 6 proxier.go:1096\\] Port \u0026quot;nodePort for kube-system/default-http-backend:http\u0026quot; (:31429/tcp) was open before and is still needed \u0026lt;source\u0026gt; @type tail format multiline multiline\\_flush\\_interval 5s format\\_firstline /^\\\\w\\\\d{4}/ format1 /^(?\u0026lt;severity\u0026gt;\\\\w)(?\u0026lt;time\u0026gt;\\\\d{4} \\[^\\\\s\\]\\*)\\\\s+(?\u0026lt;pid\u0026gt;\\\\d+)\\\\s+(?\u0026lt;source\u0026gt;\\[^ \\\\\\]\\]+)\\\\\\] (?\u0026lt;message\u0026gt;.\\*)/ time\\_format %m%d %H:%M:%S.%N path /var/log/kube-proxy.log pos\\_file /var/run/google-fluentd/pos-files/gcp-kube-proxy.pos tag kube-proxy \u0026lt;/source\u0026gt; # Example: # I0204 07:00:19.604280 5 handlers.go:131\\] GET /api/v1/nodes: (1.624207ms) 200 \\[\\[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50\\] 127.0.0.1:38266\\] \u0026lt;source\u0026gt; @type tail format multiline multiline\\_flush\\_interval 5s format\\_firstline /^\\\\w\\\\d{4}/ format1 /^(?\u0026lt;severity\u0026gt;\\\\w)(?\u0026lt;time\u0026gt;\\\\d{4} \\[^\\\\s\\]\\*)\\\\s+(?\u0026lt;pid\u0026gt;\\\\d+)\\\\s+(?\u0026lt;source\u0026gt;\\[^ \\\\\\]\\]+)\\\\\\] (?\u0026lt;message\u0026gt;.\\*)/ time\\_format %m%d %H:%M:%S.%N path /var/log/kube-apiserver.log pos\\_file /var/run/google-fluentd/pos-files/gcp-kube-apiserver.pos tag kube-apiserver \u0026lt;/source\u0026gt; # Example: # I0204 06:55:31.872680 5 servicecontroller.go:277\\] LB already exists and doesn't need update for service kube-system/kube-ui \u0026lt;source\u0026gt; @type tail format multiline multiline\\_flush\\_interval 5s format\\_firstline /^\\\\w\\\\d{4}/ format1 /^(?\u0026lt;severity\u0026gt;\\\\w)(?\u0026lt;time\u0026gt;\\\\d{4} \\[^\\\\s\\]\\*)\\\\s+(?\u0026lt;pid\u0026gt;\\\\d+)\\\\s+(?\u0026lt;source\u0026gt;\\[^ \\\\\\]\\]+)\\\\\\] (?\u0026lt;message\u0026gt;.\\*)/ time\\_format %m%d %H:%M:%S.%N path /var/log/kube-controller-manager.log pos\\_file /var/run/google-fluentd/pos-files/gcp-kube-controller-manager.pos tag kube-controller-manager \u0026lt;/source\u0026gt; # Example: # W0204 06:49:18.239674 7 reflector.go:245\\] pkg/scheduler/factory/factory.go:193: watch of \\*api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared \\[2578313/2577886\\]) \\[2579312\\] \u0026lt;source\u0026gt; @type tail format multiline multiline\\_flush\\_interval 5s format\\_firstline /^\\\\w\\\\d{4}/ format1 /^(?\u0026lt;severity\u0026gt;\\\\w)(?\u0026lt;time\u0026gt;\\\\d{4} \\[^\\\\s\\]\\*)\\\\s+(?\u0026lt;pid\u0026gt;\\\\d+)\\\\s+(?\u0026lt;source\u0026gt;\\[^ \\\\\\]\\]+)\\\\\\] (?\u0026lt;message\u0026gt;.\\*)/ time\\_format %m%d %H:%M:%S.%N path /var/log/kube-scheduler.log pos\\_file /var/run/google-fluentd/pos-files/gcp-kube-scheduler.pos tag kube-scheduler \u0026lt;/source\u0026gt; # Example: # I1104 10:36:20.242766 5 rescheduler.go:73\\] Running Rescheduler \u0026lt;source\u0026gt; @type tail format multiline multiline\\_flush\\_interval 5s format\\_firstline /^\\\\w\\\\d{4}/ format1 /^(?\u0026lt;severity\u0026gt;\\\\w)(?\u0026lt;time\u0026gt;\\\\d{4} \\[^\\\\s\\]\\*)\\\\s+(?\u0026lt;pid\u0026gt;\\\\d+)\\\\s+(?\u0026lt;source\u0026gt;\\[^ \\\\\\]\\]+)\\\\\\] (?\u0026lt;message\u0026gt;.\\*)/ time\\_format %m%d %H:%M:%S.%N path /var/log/rescheduler.log pos\\_file /var/run/google-fluentd/pos-files/gcp-rescheduler.pos tag rescheduler \u0026lt;/source\u0026gt; # Example: # I0603 15:31:05.793605 6 cluster\\_manager.go:230\\] Reading config from path /etc/gce.conf \u0026lt;source\u0026gt; @type tail format multiline multiline\\_flush\\_interval 5s format\\_firstline /^\\\\w\\\\d{4}/ format1 /^(?\u0026lt;severity\u0026gt;\\\\w)(?\u0026lt;time\u0026gt;\\\\d{4} \\[^\\\\s\\]\\*)\\\\s+(?\u0026lt;pid\u0026gt;\\\\d+)\\\\s+(?\u0026lt;source\u0026gt;\\[^ \\\\\\]\\]+)\\\\\\] (?\u0026lt;message\u0026gt;.\\*)/ time\\_format %m%d %H:%M:%S.%N path /var/log/glbc.log pos\\_file /var/run/google-fluentd/pos-files/gcp-glbc.pos tag glbc \u0026lt;/source\u0026gt; # Example: # I0603 15:31:05.793605 6 cluster\\_manager.go:230\\] Reading config from path /etc/gce.conf \u0026lt;source\u0026gt; @type tail format multiline multiline\\_flush\\_interval 5s format\\_firstline /^\\\\w\\\\d{4}/ format1 /^(?\u0026lt;severity\u0026gt;\\\\w)(?\u0026lt;time\u0026gt;\\\\d{4} \\[^\\\\s\\]\\*)\\\\s+(?\u0026lt;pid\u0026gt;\\\\d+)\\\\s+(?\u0026lt;source\u0026gt;\\[^ \\\\\\]\\]+)\\\\\\] (?\u0026lt;message\u0026gt;.\\*)/ time\\_format %m%d %H:%M:%S.%N path /var/log/cluster-autoscaler.log pos\\_file /var/run/google-fluentd/pos-files/gcp-cluster-autoscaler.pos tag cluster-autoscaler \u0026lt;/source\u0026gt; # Logs from systemd-journal for interesting services. # TODO(random-liu): Keep this for compatibility, remove this after # cri container runtime rolls out. \u0026lt;source\u0026gt; @type systemd filters \\[{ \u0026quot;\\_SYSTEMD\\_UNIT\u0026quot;: \u0026quot;docker.service\u0026quot; }\\] \u0026lt;storage\u0026gt; @type local path /var/run/google-fluentd/pos-files/gcp-journald-docker.pos \u0026lt;/storage\u0026gt; read\\_from\\_head true tag docker \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type systemd filters \\[{ \u0026quot;\\_SYSTEMD\\_UNIT\u0026quot;: \u0026quot;docker.service\u0026quot; }\\] \u0026lt;storage\u0026gt; @type local path /var/run/google-fluentd/pos-files/gcp-journald-container-runtime.pos \u0026lt;/storage\u0026gt; read\\_from\\_head true tag container-runtime \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type systemd filters \\[{ \u0026quot;\\_SYSTEMD\\_UNIT\u0026quot;: \u0026quot;kubelet.service\u0026quot; }\\] \u0026lt;storage\u0026gt; @type local path /var/run/google-fluentd/pos-files/gcp-journald-kubelet.pos \u0026lt;/storage\u0026gt; read\\_from\\_head true tag kubelet \u0026lt;/source\u0026gt; # kube-node-installation, kube-node-configuration, and kube-logrotate are # oneshots, but it's extremely valuable to have their logs on Stackdriver # as they can diagnose critical issues with node startup. # See http://cs/cloud-gke-kubernetes/cluster/gce/gci/node.yaml. \u0026lt;source\u0026gt; @type systemd filters \\[{ \u0026quot;\\_SYSTEMD\\_UNIT\u0026quot;: \u0026quot;kube-node-installation.service\u0026quot; }\\] \u0026lt;storage\u0026gt; @type local path /var/run/google-fluentd/pos-files/gcp-journald-kube-node-installation.pos \u0026lt;/storage\u0026gt; read\\_from\\_head true tag kube-node-installation \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type systemd filters \\[{ \u0026quot;\\_SYSTEMD\\_UNIT\u0026quot;: \u0026quot;kube-node-configuration.service\u0026quot; }\\] \u0026lt;storage\u0026gt; @type local path /var/run/google-fluentd/pos-files/gcp-journald-kube-node-configuration.pos \u0026lt;/storage\u0026gt; read\\_from\\_head true tag kube-node-configuration \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type systemd filters \\[{ \u0026quot;\\_SYSTEMD\\_UNIT\u0026quot;: \u0026quot;kube-logrotate.service\u0026quot; }\\] \u0026lt;storage\u0026gt; @type local path /var/run/google-fluentd/pos-files/gcp-journald-kube-logrotate.pos \u0026lt;/storage\u0026gt; read\\_from\\_head true tag kube-logrotate \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type systemd filters \\[{ \u0026quot;\\_SYSTEMD\\_UNIT\u0026quot;: \u0026quot;node-problem-detector.service\u0026quot; }\\] pos\\_file /var/log/gcp-journald-node-problem-detector.pos read\\_from\\_head true tag node-problem-detector \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type systemd filters \\[{ \u0026quot;\\_SYSTEMD\\_UNIT\u0026quot;: \u0026quot;kube-container-runtime-monitor.service\u0026quot; }\\] pos\\_file /var/log/gcp-journald-kube-container-runtime-monitor.pos read\\_from\\_head true tag kube-container-runtime-monitor \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type systemd filters \\[{ \u0026quot;\\_SYSTEMD\\_UNIT\u0026quot;: \u0026quot;kubelet-monitor.service\u0026quot; }\\] pos\\_file /var/log/gcp-journald-kubelet-monitor.pos read\\_from\\_head true tag kubelet-monitor \u0026lt;/source\u0026gt; まとめ # へぇ、GCP でもそんな風にやってるのかあって感じられて興味深かったです。\n","date":"2020年3月13日","permalink":"/2020/03/how-gke-node-send-logs/","section":"Posts","summary":"GKE は何もしなくてもログを Cloud Logging (旧 Stackdriver Logging) に送ってくれて便利なんだけどどうやって送ってるのかな？と思って調べたメモ。なかなか興味深かった。 メトリク","title":"GKE の node はどのようにログを転送しているのか"},{"content":"Istio シリーズです。\n前回の予告通り今回は「外部サービスでも Fault Injection したいぞ」編です。\n「Fault Injection 編」でその便利さを取り上げましたが、外部の API を使用している時にもそこに Inject したいですよね？依存している外部サービスでエラーが発生したらどうなるのかとか、レスポンスが遅かった場合どうなるかとか、それを Istio の設定で調整することが可能になります。\n前回設定した ServiceEntry の確認 # 前回は最終的に次の設定を行いました。これで httpbin.org と www.google.com 宛ては通信が許可されました。(outboundTrafficPolicy は REGISTRY_ONLY でした)\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: ServiceEntry metadata: name: external-svc spec: hosts: - httpbin.org - www.google.com location: MESH\\_EXTERNAL ports: - number: 80 name: http protocol: HTTP - number: 443 name: tls protocol: TLS resolution: DNS EOF しかし、これでは直接各サイトに出ていくだけです。Fault Injection は VirtualService の機能でした。「VirtualService 編」を読み返しましょう。VirtualService にはそのサービスの転送先として DestinationRule しました。\nVirtualService の作成 # httpbin.org # httpbin.org 用の VirtualService を作成します。httpbin.org は HTTPS には対応していないため port 80 だけです。全リクエストに3秒の delay を入れ、30% のリクエストは 500 Internal Server Error を返すようにしてみます。DestinationRule は登録せずとも ServiceEntry があるため接続できます。ServiceEntry がない場合は \u0026quot;response_flags\u0026quot;:\u0026quot;NR,DI\u0026quot; で Injection の後でエラーになります。これは outboundTrafficPolicy が ALLOW_ANY でも同じです。ServiceEntry が必要です。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin-org spec: hosts: - httpbin.org http: - match: - port: 80 **fault:** **delay:** **fixedDelay: 3s** **percentage:** **value: 100** **abort:** **httpStatus: 500** **percentage:** **value: 30** route: - destination: host: httpbin.org EOF Delay だけが適用された時のログです。\n{ \u0026quot;authority\u0026quot;: \u0026quot;httpbin.org\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;34\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;35.170.216.115:80\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.8:43426\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;3366\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/ip\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;fa22dc61-1cc5-45c4-bc97-4f02d8a7c468\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, **\u0026quot;response\\_code\u0026quot;: \u0026quot;200\u0026quot;**, **\u0026quot;response\\_flags\u0026quot;: \u0026quot;DI\u0026quot;**, \u0026quot;route\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-11T15:24:31.272Z\u0026quot;, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;outbound|80||httpbin.org\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;34.230.193.231:80\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;172.17.0.8:54090\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;361\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } こちらは Delay と Fault が両方適用された時のログです。\n{ \u0026quot;authority\u0026quot;: \u0026quot;httpbin.org\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;18\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;3.232.168.170:80\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.8:50018\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;3001\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/ip\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;98a3a4c5-4a0d-4325-a800-da7c3b6db3e3\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, **\u0026quot;response\\_code\u0026quot;: \u0026quot;500\u0026quot;**, **\u0026quot;response\\_flags\u0026quot;: \u0026quot;DI,FI\u0026quot;**, \u0026quot;route\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-11T15:24:47.524Z\u0026quot;, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } www.google.com # それでは次に www.google.com の VirtualService を作成します。こちらは https にも対応しています。で、ここがキモですが、そのままでは Envoy は https の中身には関与しませんでした。でも Fault Injection をするためにはそれではいけませんから、http で受けたリクエストを Envoy が proxy する際に https にして接続するようにします。そうすることで元のリクエストは http なので中身をいじることができます。さっきとの違いがわかるように今回は delay を1秒に、abort を 400 Bad Request にしてみます。\n今回は destination に tls-origination という subset を指定してあります。これは下に続く DestinationRule で定義してあります。tls.mode を SIMPLE にして sni で www.google.com を指定しています。接続先が SNI 必須の場合はこの sni 指定が必要です。SIMPLE って何？って思いますが、他の選択肢は MUTUAL がクライアント証明書を必要とするやつです。あとは PASSTHROUGH とか。\nhttpbin.org の時と違って DestinationRule はありますが、こちらも ServiceEntry を消すとアクセスできなくなります。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: www-google-com spec: hosts: - www.google.com http: - match: - port: 80 **fault:** **delay:** **fixedDelay: 1s** **percentage:** **value: 100** **abort:** **httpStatus: 400** **percentage:** **value: 30** route: - destination: host: www.google.com **subset: tls-origination** port: number: 443 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: www-google-com spec: host: www.google.com subsets: **\\- name: tls-origination** **trafficPolicy:** **portLevelSettings:** **\\- port:** **number: 443** **tls:** **mode: SIMPLE** **sni: www.google.com** EOF これで http://www.google.com/ にアクセスすれば Envoy が Injection したうえで https で www.google.com に proxy してくれます。\nログです。\u0026quot;downstream_local_address\u0026quot;: \u0026quot;216.58.197.196:80\u0026quot; で curl は www.google.com の port 80 にアクセスしようとしたことがわかります。\u0026quot;upstream_cluster\u0026quot;: \u0026quot;outbound|443|tls-origination|www.google.com\u0026quot; で Envoy 内の経路がわかります。DestinationRule の tls-origination が使われています。\u0026quot;upstream_host\u0026quot;: \u0026quot;216.58.197.196:443\u0026quot; で proxy 先が port 443 (https) であることがわかります。\n{ \u0026quot;authority\u0026quot;: \u0026quot;www.google.com\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;13062\u0026quot;, **\u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;216.58.197.196:80\u0026quot;**, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.8:41130\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;1189\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;069baa9b-7d0a-4e84-9b4b-a11e34226fce\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;response\\_code\u0026quot;: \u0026quot;200\u0026quot;, **\u0026quot;response\\_flags\u0026quot;: \u0026quot;DI\u0026quot;**, \u0026quot;route\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-11T15:38:11.379Z\u0026quot;, **\u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;outbound|443|tls-origination|www.google.com\u0026quot;**, **\u0026quot;upstream\\_host\u0026quot;: \u0026quot;216.58.197.196:443\u0026quot;**, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;172.17.0.8:43430\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;186\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } 次は Fault が Inject された時のログです。これは proxy せずに 400 を返しているため proxy 先の情報はありません。\n{ \u0026quot;authority\u0026quot;: \u0026quot;www.google.com\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;18\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;216.58.197.196:80\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.8:41304\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;1001\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;78dabd1b-fe57-41fb-8eaf-44ccc6517e3b\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, **\u0026quot;response\\_code\u0026quot;: \u0026quot;400\u0026quot;**, **\u0026quot;response\\_flags\u0026quot;: \u0026quot;DI,FI\u0026quot;**, \u0026quot;route\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-11T15:38:21.648Z\u0026quot;, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } www.google.com に https でアクセスしようとするとどうなるか # 本来、https でアクセスするべきところに http でアクセスするようにしなければならないわけですが、https のままアクセスしようとするとどうなるでしょうか。\n冒頭で前回までの設定を確認しましたが、そこで ServiceEntry に https の設定が残っているため、https の場合はその設定が有効でそのままアクセスできます。もちろん outboundTrafficPolicy が ALLOW_ANY であればその ServiceEntry も不要ですね。\nコンテナ化されたアプリケーションでは外部リソースの定義は ConfigMap などを使って容易に変更可能なはずですよね。Fault Injection 使っていきましょう。\nまとめ # クラスタ外へのアクセスでも VirtualService を使うことができ、Fault Injection することができることを確認しました。https の termination だけじゃなくて orinination も Envoy に任せられることも確認できました。\nAccessing External Services とか Egress TLS Origination に書いてあります。\nは〜〜、まだまだいろいろあるなあ、先は長い。続く\n Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月11日","permalink":"/2020/03/istio-part8/","section":"Posts","summary":"Istio シリーズです。 前回の予告通り今回は「外部サービスでも Fault Injection したいぞ」編です。 「Fault Injection 編」でその便利さを取り上げましたが、外部の API を使用","title":"Istio 導入への道 - 外部へのアクセスでも Fault Injection 編"},{"content":"Istio シリーズです。\n今回はクラスタ内から外部のサービスへのアクセスについてです。ServiceEntry ってやつが登場です。（これを書く中でだいぶ自分の理解の誤りが訂正されました、良かった良かった）\nクラスタ内から外部へのアクセスモードについて # Istio のデフォルト設定では istio-system namespace の istio という ConfigMap で次のように outboundTrafficPolicy の mode が ALLOW_ANY となっており、中から外は自由に通信できます。\n$ kubectl get cm -n istio-system istio -o yaml | grep \u0026quot;^ outboundTrafficPolicy:\u0026quot; -A 1 outboundTrafficPolicy: mode: ALLOW\\_ANY これを REGISTRY_ONLY に変更すると登録された宛先にしかアクセスできなくなります。\n次の様にして変更することができます。インストール時に指定しておくには --set values.global.outboundTrafficPolicy.mode=REGISTRY_ONLY とします。\n$ kubectl get configmap istio -n istio-system -o yaml \\\\ | sed 's/mode: ALLOW\\_ANY/mode: REGISTRY\\_ONLY/g' \\\\ | kubectl replace -n istio-system -f - 変更されました。\n$ kubectl get cm -n istio-system istio -o yaml | grep \u0026quot;^ outboundTrafficPolicy:\u0026quot; -A 1 outboundTrafficPolicy: mode: **REGISTRY\\_ONLY** 戻す場合はこれで。\n$ kubectl get configmap istio -n istio-system -o yaml \\\\ | sed 's/mode: REGISTRY\\_ONLY/mode: ALLOW\\_ANY/g' \\\\ | kubectl replace -n istio-system -f - HTTP の外部サービスを登録する # REGISTRY_ONLY になったら未登録の外部サービス（アドレス）にはアクセス出来ません。\n502 Bad Gateway となりました。\nroot@ubuntu-deployment-54bbd6f4ff-q9sdj:/# curl -sv http://httpbin.org/ip \\* Trying 52.202.2.199... \\* TCP\\_NODELAY set \\* Connected to httpbin.org (52.202.2.199) port 80 (#0) \u0026gt; GET /ip HTTP/1.1 \u0026gt; Host: httpbin.org \u0026gt; User-Agent: curl/7.58.0 \u0026gt; Accept: \\*/\\* \u0026gt; \u0026lt; **HTTP/1.1 502 Bad Gateway** \u0026lt; date: Mon, 09 Mar 2020 16:11:01 GMT \u0026lt; server: envoy \u0026lt; content-length: 0 \u0026lt; \\* Connection #0 to host httpbin.org left intact アプリを Kubernetes でコンテナとして動かしていても、データベースなどはクラウドのマネージドサービスを使うことが多いと思います。クラスタ外にアクセス出来ないということはそういったサービスへもアクセス出来ないことを意味します。それでは困るので ServiceEntry というもので宛先を登録します。\n登録は簡単です。次の様にします。hosts に許可したい宛先 FQDN を指定します。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: ServiceEntry metadata: name: external-svc spec: hosts: - httpbin.org location: MESH\\_EXTERNAL ports: - number: 80 name: http protocol: HTTP resolution: DNS EOF アクセスできるようになりました。\nroot@ubuntu-deployment-54bbd6f4ff-q9sdj:/# curl -sv http://httpbin.org/headers \\* Trying 3.232.168.170... \\* TCP\\_NODELAY set \\* Connected to httpbin.org (3.232.168.170) port 80 (#0) \u0026gt; GET /headers HTTP/1.1 \u0026gt; Host: httpbin.org \u0026gt; User-Agent: curl/7.58.0 \u0026gt; Accept: \\*/\\* \u0026gt; \u0026lt; **HTTP/1.1 200 OK** \u0026lt; date: Mon, 09 Mar 2020 16:18:21 GMT \u0026lt; content-type: application/json \u0026lt; content-length: 1186 \u0026lt; server: envoy \u0026lt; access-control-allow-origin: \\* \u0026lt; access-control-allow-credentials: true \u0026lt; x-envoy-upstream-service-time: 347 \u0026lt; { \u0026quot;headers\u0026quot;: { \u0026quot;Accept\u0026quot;: \u0026quot;\\*/\\*\u0026quot;, \u0026quot;Content-Length\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Host\u0026quot;: \u0026quot;httpbin.org\u0026quot;, \u0026quot;User-Agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;X-Amzn-Trace-Id\u0026quot;: \u0026quot;Root=1-5e666c4d-f5fff240a5be87147f2cc6a4\u0026quot;, \u0026quot;X-B3-Sampled\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;X-B3-Spanid\u0026quot;: \u0026quot;db1df3e34fb690a9\u0026quot;, \u0026quot;X-B3-Traceid\u0026quot;: \u0026quot;e12ef759daff1ddcdb1df3e34fb690a9\u0026quot;, \u0026quot;X-Envoy-Decorator-Operation\u0026quot;: \u0026quot;httpbin.org:80/\\*\u0026quot;, \u0026quot;X-Envoy-Peer-Metadata\u0026quot;: \u0026quot;ChwKDElOU1RBTkNFX0lQUxIMGgoxNzIuMTcuMC44CsYBCgZMQUJFTFMSuwEquAEKDwoDYXBwEggaBnVidW50dQohChFwb2QtdGVtcGxhdGUtaGFzaBIMGgo1NGJiZDZmNGZmCiQKGXNlY3VyaXR5LmlzdGlvLmlvL3Rsc01vZGUSBxoFaXN0aW8KKwofc2VydmljZS5pc3Rpby5pby9jYW5vbmljYWwtbmFtZRIIGgZ1YnVudHUKLwojc2VydmljZS5pc3Rpby5pby9jYW5vbmljYWwtcmV2aXNpb24SCBoGbGF0ZXN0ChoKB01FU0hfSUQSDxoNY2x1c3Rlci5sb2NhbAosCgROQU1FEiQaInVidW50dS1kZXBsb3ltZW50LTU0YmJkNmY0ZmYtcTlzZGoKFgoJTkFNRVNQQUNFEgkaB2RlZmF1bHQKVQoFT1dORVISTBpKa3ViZXJuZXRlczovL2FwaXMvYXBwcy92MS9uYW1lc3BhY2VzL2RlZmF1bHQvZGVwbG95bWVudHMvdWJ1bnR1LWRlcGxveW1lbnQKHAoPU0VSVklDRV9BQ0NPVU5UEgkaB2RlZmF1bHQKJAoNV09SS0xPQURfTkFNRRITGhF1YnVudHUtZGVwbG95bWVudA==\u0026quot;, \u0026quot;X-Envoy-Peer-Metadata-Id\u0026quot;: \u0026quot;sidecar~172.17.0.8~ubuntu-deployment-54bbd6f4ff-q9sdj.default~default.svc.cluster.local\u0026quot; } } \\* Connection #0 to host httpbin.org left intact しかし、なんだか余計なデータを header に詰めて送ってますね。X-Envoy-Peer-Metadata には送信元 Pod の metadata が Base64 encode されて入っています。何に使うのだろう？\nistioctl コマンドで ENDPOINT に登録されていることがわかります。\n$ istioctl proxy-config endpoint ubuntu-deployment-54bbd6f4ff-q9sdj | egrep 'ENDPOINT|httpbin.org' ENDPOINT STATUS OUTLIER CHECK CLUSTER 3.232.168.170:80 HEALTHY OK outbound|80||httpbin.org 52.202.2.199:80 HEALTHY OK outbound|80||httpbin.org resolution: DNS と指定しているため、この宛先のIPアドレスは DNS から取得した値となっており、エラーが増えたり、接続できなかったら状態が変わるのでしょう。Envoy は自分で名前解決して接続するようです。curl の \u0026ndash;resolve で全然関係の無い IP アドレスを指定していても Envoy は Host ヘッダーのサーバーへ自分で名前解決を行って接続するようです。\nHTTP や HTTPS の場合は Host ヘッダーや SNI に接続先ホスト情報がありますが、他の protocol ではそうはいきませんから、接続先 IP アドレスが ServiceEntry の addresses にマッチしているかどうかがチェックされます。 resolution: STATIC の場合は endpoints に指定した IP アドレスに接続を試みます。\nresolution: NONE とした場合は元の接続先 IP アドレスがそのまま使われます。つまり、curl で \u0026ndash;resolve で指定した場合、Envoy もそこに接続します。\nHTTPS の外部サービスを登録する # 次に HTTPS でも接続できるようにします。www.google.com でテストしてみます。port 443 を追加するだけですね。resolution が DNS や NONE であれば先の HTTP のやつとまとめてしまうことが可能です。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: ServiceEntry metadata: name: external-svc spec: hosts: - httpbin.org - **www.google.com** location: MESH\\_EXTERNAL ports: - number: 80 name: http protocol: HTTP **\\- number: 443** **name: tls** **protocol: TLS** resolution: DNS EOF name: tls, protocol: TLS は name: https, protol: HTTPS でも機能します。が、明確な違いがわかりません。GitHub に issue (ServiceEntry protocol HTTPS vs TLS documentation + Virtual Services requirements #19188) を見つけました。全く同感ですね。 name の方はルールがあるっぽいけど https と tls の違いはわからない。http2 もあるのか\u0026hellip;\nresolution: DNS よりも resolution: NONE の方が良さそうですね。\n次回は「外部サービスでも Fault Injection したいぞ」です。\n Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月10日","permalink":"/2020/03/istio-part7/","section":"Posts","summary":"Istio シリーズです。 今回はクラスタ内から外部のサービスへのアクセスについてです。ServiceEntry ってやつが登場です。（これを書く中でだい","title":"Istio 導入への道 - 外部へのアクセス / ServiceEntry 編"},{"content":"Istio シリーズです。\nいよいよ Ingress Gateway を試します。Istio でクラスタ外からのリクエストをサービスに流すためにはこれが必要です。\nIngress Gateway の確認 # Istio のインストール時に istio-system namespace に istio-ingressgateway という Deployment がデプロイされています。\n$ kubectl get deployment -n istio-system NAME READY UP-TO-DATE AVAILABLE AGE istio-ingressgateway 1/1 1 1 29h istiod 1/1 1 1 29h prometheus 1/1 1 1 29h istio=ingressgateway という label がついていて、Gateway で通常これが指定されます。\n$ kubectl get deployment -n istio-system istio-ingressgateway -o json | jq .metadata.labels { \u0026quot;app\u0026quot;: \u0026quot;istio-ingressgateway\u0026quot;, \u0026quot;istio\u0026quot;: \u0026quot;ingressgateway\u0026quot;, \u0026quot;operator.istio.io/component\u0026quot;: \u0026quot;IngressGateways\u0026quot;, \u0026quot;operator.istio.io/managed\u0026quot;: \u0026quot;Reconcile\u0026quot;, \u0026quot;operator.istio.io/version\u0026quot;: \u0026quot;1.5.0\u0026quot;, \u0026quot;release\u0026quot;: \u0026quot;istio\u0026quot; } また、istio-ingressgateway という Service も存在します。これが外部からのリクエストの受け口です。デフォルトで沢山の port が登録されてます（なぜなのかはまだ良く知らないけど Grafana, Prometheus, Kiali などにアクセスするためかな？）。また、デフォルトで type: LoadBalancer となっているため Minikube でも EXTERNAL-IP でアクセスできるように1回目のインストール編で minikube tunnel を実行していたのでした。やっと出番がきました。\n$ kubectl get svc -n istio-system -l app=istio-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.108.149.40 10.108.149.40 15020:30271/TCP,80:30723/TCP,443:32691/TCP,15029:30831/TCP,15030:30169/TCP,15031:32095/TCP,15032:30604/TCP,15443:30854/TCP 29h 何も設定しない状態では中のサービス名を指定してもアクセスできません。\n$ curl --resolve httpbin-service:80:10.108.149.40 -sv http://httpbin-service/ * Added httpbin-service:80:10.108.149.40 to DNS cache * Hostname httpbin-service was found in DNS cache * Trying 10.108.149.40... * TCP_NODELAY set * Connected to httpbin-service (10.108.149.40) port 80 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: httpbin-service \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 404 Not Found \u0026lt; date: Sun, 08 Mar 2020 13:28:28 GMT \u0026lt; server: istio-envoy \u0026lt; content-length: 0 \u0026lt; * Connection #0 to host httpbin-service left intact * Closing connection 0 istio-ingressgateway のログ\n{ \u0026quot;authority\u0026quot;: \u0026quot;httpbin-service\u0026quot;, \u0026quot;bytes_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes_sent\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;downstream_local_address\u0026quot;: \u0026quot;172.17.0.6:80\u0026quot;, \u0026quot;downstream_remote_address\u0026quot;: \u0026quot;192.168.64.1:52499\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;istio_policy_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request_id\u0026quot;: \u0026quot;edfbfa06-60ba-4500-b80a-069d68967877\u0026quot;, \u0026quot;requested_server_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;response_code\u0026quot;: \u0026quot;404\u0026quot;, \u0026quot;response_flags\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;route_name\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;start_time\u0026quot;: \u0026quot;2020-03-08T13:28:28.477Z\u0026quot;, \u0026quot;upstream_cluster\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream_host\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream_local_address\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream_service_time\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream_transport_failure_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user_agent\u0026quot;: \u0026quot;curl/7.64.1\u0026quot;, \u0026quot;x_forwarded_for\u0026quot;: \u0026quot;192.168.64.1\u0026quot; } 404 が返されるのは blackhole 設定によるものです。\n$ istioctl -n istio-system proxy-config route istio-ingressgateway-757f454bff-57l8j --name http.80 -o json [ { \u0026quot;name\u0026quot;: \u0026quot;http.80\u0026quot;, \u0026quot;virtualHosts\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;blackhole:80\u0026quot;, \u0026quot;domains\u0026quot;: [ \u0026quot;*\u0026quot; ], \u0026quot;routes\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; }, \u0026quot;directResponse\u0026quot;: { \u0026quot;status\u0026quot;: 404 } } ] } ], \u0026quot;validateClusters\u0026quot;: false } ] Gateway の登録 # 次のようにして Gateway を登録します。servers 内の hosts は Host Header を見てどれを対象とするかの定義です。ここでは httpbin.local という DNS 登録がされているということにします。この hosts には FQDN を指定する必要があります。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: httpbin-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026quot;httpbin.local\u0026quot; EOF これだけではまだ httpbin.local なんてどこに流せば良いのか定義されていないためたどり着けません。\nVirtualService と Gateway の紐付け # それではたどり着けるようにするためにどうすれば良いかというと、VirtualService と紐づけるのです。VirtualService はすでにこれまでに設定していますが、まだ使っていない gateways という項目がありました。\ngateway を設定していない状態ではこんな感じです。\n$ kubectl get vs NAME GATEWAYS HOSTS AGE httpbin-virtual-service [httpbin-service] 22h ここで gateways を追加します。他の設定は前回のままで、ここでは特に意味はない。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin-virtual-service spec: spec: hosts: - httpbin-service **gateways:** **- httpbin-gateway** **- mesh** http: - retries: attempts: 10 route: - destination: host: httpbin-service subset: v1 weight: 0 - destination: host: httpbin-service subset: v2 weight: 100 EOF 未定義の場合は暗黙的に mesh が設定されいます。定義するとクラスタ内でのサービス間通信にも使う場合は mesh も明示する必要があります。\n$ kubectl get vs NAME GATEWAYS HOSTS AGE httpbin-virtual-service [httpbin-gateway mesh] [httpbin-service] 22h gateways が表示されるようになりました。httpbin-gateway と紐づけられました。これで外部からアクセス出来るぞ。\nと思いきや\u0026hellip;\n$ curl --resolve httpbin.local:80:10.108.149.40 -sI http://httpbin.local/ip HTTP/1.1 404 Not Found date: Sun, 08 Mar 2020 13:55:57 GMT server: istio-envoy transfer-encoding: chunked なぜか？\nVirtualService にも hosts という定義があるのを忘れていました。ここにマッチするトラフィックが対象となるのです。httpbin.local なんてホストは知らないのです。\nGateway 側の hosts に httpbin-service.default.svc.cluster.local を登録してやればこの名前でアクセスは出来るけれども、外から変こんな名前でアクセスさせることはないでしょう。\nで、VirtualService を再度更新します。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin-virtual-service spec: spec: hosts: - httpbin-service - **httpbin.local** gateways: - httpbin-gateway - mesh http: - retries: attempts: 10 route: - destination: host: httpbin-service subset: v1 weight: 0 - destination: host: httpbin-service subset: v2 weight: 100 EOF やっとアクセスできました 🎉\n❯ curl --resolve httpbin.local:80:10.108.149.40 -sv http://httpbin.local/headers * Added httpbin.local:80:10.108.149.40 to DNS cache * Hostname httpbin.local was found in DNS cache * Trying 10.108.149.40... * TCP_NODELAY set * Connected to httpbin.local (10.108.149.40) port 80 (#0) \u0026gt; GET /headers HTTP/1.1 \u0026gt; Host: httpbin.local \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; server: istio-envoy \u0026lt; date: Sun, 08 Mar 2020 14:05:34 GMT \u0026lt; content-type: application/json \u0026lt; content-length: 586 \u0026lt; access-control-allow-origin: * \u0026lt; access-control-allow-credentials: true \u0026lt; x-envoy-upstream-service-time: 10 \u0026lt; { \u0026quot;headers\u0026quot;: { \u0026quot;Accept\u0026quot;: \u0026quot;*/*\u0026quot;, \u0026quot;Content-Length\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Host\u0026quot;: \u0026quot;httpbin.local\u0026quot;, \u0026quot;User-Agent\u0026quot;: \u0026quot;curl/7.64.1\u0026quot;, \u0026quot;X-B3-Parentspanid\u0026quot;: \u0026quot;980378ff536f223a\u0026quot;, \u0026quot;X-B3-Sampled\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;X-B3-Spanid\u0026quot;: \u0026quot;9b9d2d304765951f\u0026quot;, \u0026quot;X-B3-Traceid\u0026quot;: \u0026quot;5253e2d7d92631ca980378ff536f223a\u0026quot;, \u0026quot;X-Envoy-Internal\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;X-Forwarded-Client-Cert\u0026quot;: \u0026quot;By=spiffe://cluster.local/ns/default/sa/default;Hash=347ea3e62512fc49669288a4f10b74ecaba25d194f3bf57f55e14985077d780b;Subject=\\\u0026quot;\\\u0026quot;;URI=spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\u0026quot; } } * Connection #0 to host httpbin.local left intact * Closing connection 0 ingress gateway のログです\n{ \u0026quot;authority\u0026quot;: \u0026quot;httpbin.local\u0026quot;, \u0026quot;bytes_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes_sent\u0026quot;: \u0026quot;586\u0026quot;, \u0026quot;downstream_local_address\u0026quot;: \u0026quot;172.17.0.6:80\u0026quot;, \u0026quot;downstream_remote_address\u0026quot;: \u0026quot;192.168.64.1:54868\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;10\u0026quot;, \u0026quot;istio_policy_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/headers\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request_id\u0026quot;: \u0026quot;d1c07002-8a22-4d89-97c8-862603926368\u0026quot;, \u0026quot;requested_server_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;response_code\u0026quot;: \u0026quot;200\u0026quot;, \u0026quot;response_flags\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;route_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;start_time\u0026quot;: \u0026quot;2020-03-08T14:05:34.372Z\u0026quot;, \u0026quot;upstream_cluster\u0026quot;: \u0026quot;outbound|80|v2|httpbin-service.default.svc.cluster.local\u0026quot;, \u0026quot;upstream_host\u0026quot;: \u0026quot;172.17.0.8:80\u0026quot;, \u0026quot;upstream_local_address\u0026quot;: \u0026quot;172.17.0.6:40834\u0026quot;, \u0026quot;upstream_service_time\u0026quot;: \u0026quot;10\u0026quot;, \u0026quot;upstream_transport_failure_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user_agent\u0026quot;: \u0026quot;curl/7.64.1\u0026quot;, \u0026quot;x_forwarded_for\u0026quot;: \u0026quot;192.168.64.1\u0026quot; } istioctl コマンドで確認 # $ istioctl proxy-config listener $(kubectl get pods -n istio-system -l istio=ingressgateway -o=jsonpath='{.items[0].metadata.name}') -n istio-system ADDRESS PORT TYPE 0.0.0.0 80 HTTP 0.0.0.0 15090 HTTP $ istioctl proxy-config route $(kubectl get pods -n istio-system -l istio=ingressgateway -o=jsonpath='{.items[0].metadata.name}') -n istio-system -o json | jq . [ { \u0026quot;name\u0026quot;: \u0026quot;http.80\u0026quot;, \u0026quot;virtualHosts\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;httpbin.local:80\u0026quot;, \u0026quot;domains\u0026quot;: [ \u0026quot;httpbin.local\u0026quot;, \u0026quot;httpbin.local:80\u0026quot; ], \u0026quot;routes\u0026quot;: [ { \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; }, \u0026quot;route\u0026quot;: { \u0026quot;cluster\u0026quot;: \u0026quot;outbound|80|v2|httpbin-service.default.svc.cluster.local\u0026quot;, \u0026quot;timeout\u0026quot;: \u0026quot;0s\u0026quot;, \u0026quot;retryPolicy\u0026quot;: { \u0026quot;retryOn\u0026quot;: \u0026quot;connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\u0026quot;, \u0026quot;numRetries\u0026quot;: 10, \u0026quot;retryHostPredicate\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;envoy.retry_host_predicates.previous_hosts\u0026quot; } ], \u0026quot;hostSelectionRetryMaxAttempts\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;retriableStatusCodes\u0026quot;: [ 503 ] }, \u0026quot;maxGrpcTimeout\u0026quot;: \u0026quot;0s\u0026quot; }, \u0026quot;metadata\u0026quot;: { \u0026quot;filterMetadata\u0026quot;: { \u0026quot;istio\u0026quot;: { \u0026quot;config\u0026quot;: \u0026quot;/apis/networking.istio.io/v1alpha3/namespaces/default/virtual-service/httpbin-virtual-service\u0026quot; } } }, \u0026quot;decorator\u0026quot;: { \u0026quot;operation\u0026quot;: \u0026quot;httpbin-service.default.svc.cluster.local:80/*\u0026quot; } } ] } ], \u0026quot;validateClusters\u0026quot;: false }, { \u0026quot;virtualHosts\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;backend\u0026quot;, \u0026quot;domains\u0026quot;: [ \u0026quot;*\u0026quot; ], \u0026quot;routes\u0026quot;: [ { \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/stats/prometheus\u0026quot; }, \u0026quot;route\u0026quot;: { \u0026quot;cluster\u0026quot;: \u0026quot;prometheus_stats\u0026quot; } } ] } ] } ] インデントのスペース4つはちょっと横にのびるので jq をかましてみた。\n今回は Host ヘッダーで紐付けましたが、http 以外では port 番号で紐付けたりします。それはまた別途。\n続く\n Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月8日","permalink":"/2020/03/istio-part6/","section":"Posts","summary":"Istio シリーズです。 いよいよ Ingress Gateway を試します。Istio でクラスタ外からのリクエストをサービスに流すためにはこれが必要です。 Ingress Gateway の確認 # Istio のインス","title":"Istio 導入への道 - Ingress Gatway 編"},{"content":"Istio シリーズです。\nそういえば Ingress Gateway になかなか辿りつかないな。\nOutlierDetection 設定 # OutlierDetection は DestinationRule に設定するものでドキュメントもそこにあります。\n一旦 VirtualService での転送先を v2 だけにします。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin-virtual-service spec: hosts: - httpbin-service http: - route: - destination: host: httpbin-service subset: v1 weight: 0 - destination: host: httpbin-service subset: v2 weight: 100 EOF OutlierDetection で Target を Evict するには転送先にいくつかの Pod が必要なので replica 数を増やします。\n$ kubectl scale --replicas=5 deployment/httpbin-deployment-v2 次に、DestinationRule に OutlierDetection を設定します。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: httpbin-destination-rule spec: host: httpbin-service trafficPolicy: outlierDetection: consecutiveErrors: 3 interval: 10s baseEjectionTime: 1m maxEjectionPercent: 100 minHealthPercent: 1 subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 EOF しかし、これはどうやって確認すれば良いのだろうか？ envoy の 15000/stats へアクセスしてみるのかな？後でわかります。\nhttpbin.org は /status/500 とか /status/503 にアクセスすれば任意の status code を返させることができます。/status/503 にアクセスしてみると curl を実行する側の Pod の istio-proxy のログには1つしか出ませんが、転送先は3つの Pod にアクセスがありました。500 や 502, 504 ではどれも転送先でも1つしかログは出ませんでしたし、response_flags は空でした。retry しても 503 の場合は response_flag が URX になっています。response_flag の意味は Envoy のドキュメントにあります。URX の意味は \u0026ldquo;The request was rejected because the upstream retry limit (HTTP) or maximum connect attempts (TCP) was reached.\u0026rdquo; \u0026ldquo;response_flags\u0026rdquo;: \u0026ldquo;UF,URX\u0026rdquo; と、カンマ区切りで複数入っていることもありました。\n{ \u0026#34;authority\u0026#34;: \u0026#34;httpbin-service\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;10.109.118.31:80\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;172.17.0.9:54610\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;79\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/status/503\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;682de904-a1f8-45ec-bc7e-d755a47a130d\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;503\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;URX\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-08T06:51:32.094Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;outbound|80|v2|httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;172.17.0.13:80\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;172.17.0.9:58194\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;78\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;curl/7.58.0\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;-\u0026#34; } 転送先のログ\n{ \u0026#34;authority\u0026#34;: \u0026#34;httpbin-service\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;172.17.0.8:80\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;172.17.0.9:36276\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/status/503\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;682de904-a1f8-45ec-bc7e-d755a47a130d\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;outbound_.80_.v2_.httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;503\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-08T06:51:32.094Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;inbound|80|http|httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;127.0.0.1:80\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;127.0.0.1:57664\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;curl/7.58.0\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;-\u0026#34; } { \u0026#34;authority\u0026#34;: \u0026#34;httpbin-service\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;172.17.0.10:80\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;172.17.0.9:44944\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/status/503\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;682de904-a1f8-45ec-bc7e-d755a47a130d\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;outbound_.80_.v2_.httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;503\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-08T06:51:32.117Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;inbound|80|http|httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;127.0.0.1:80\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;127.0.0.1:57666\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;curl/7.58.0\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;-\u0026#34; } { \u0026#34;authority\u0026#34;: \u0026#34;httpbin-service\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;172.17.0.13:80\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;172.17.0.9:58194\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/status/503\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;682de904-a1f8-45ec-bc7e-d755a47a130d\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;outbound_.80_.v2_.httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;503\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-08T06:51:32.170Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;inbound|80|http|httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;127.0.0.1:80\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;127.0.0.1:57668\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;curl/7.58.0\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;-\u0026#34; } Retry について # Retry の回数などは VirtualService の http.retries.attempts などで設定することができます。Outlier の話から外れちゃうけどここで retries をいじってみます。\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin-virtual-service spec: hosts: - httpbin-service http: - route: - destination: host: httpbin-service subset: v1 weight: 0 - destination: host: httpbin-service subset: v2 weight: 100 retries: attempts: 10 EOF curl を実行する Pod で http://localhost:15000/config_dump を確認してみると \u0026quot;num_retries\u0026quot;: 10 になりました。\u0026quot;retry_on\u0026quot;: \u0026quot;connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\u0026quot; ということなので、 x-envoy-retriable-status-codes ヘッダーで指定すれば 503 以外でも retry してくれそうです。\n \u0026quot;routes\u0026quot;: [ { \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; }, \u0026quot;route\u0026quot;: { \u0026quot;cluster\u0026quot;: \u0026quot;outbound|80|v2|httpbin-service.default.svc.cluster.local\u0026quot;, \u0026quot;timeout\u0026quot;: \u0026quot;0s\u0026quot;, \u0026quot;retry_policy\u0026quot;: { \u0026quot;retry_on\u0026quot;: \u0026quot;connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\u0026quot; , \u0026quot;num_retries\u0026quot;: 10, \u0026quot;retry_host_predicate\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;envoy.retry_host_predicates.previous_hosts\u0026quot; } ], \u0026quot;host_selection_retry_max_attempts\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;retriable_status_codes\u0026quot;: [ 503 ] }, \u0026quot;max_grpc_timeout\u0026quot;: \u0026quot;0s\u0026quot; }, \u0026quot;metadata\u0026quot;: { \u0026quot;filter_metadata\u0026quot;: { \u0026quot;istio\u0026quot;: { \u0026quot;config\u0026quot;: \u0026quot;/apis/networking.istio.io/v1alpha3/namespaces/default/virtual-service/httpbin-virtual-service\u0026quot; } } }, \u0026quot;decorator\u0026quot;: { \u0026quot;operation\u0026quot;: \u0026quot;httpbin-service.default.svc.cluster.local:80/*\u0026quot; } } ] \u0026quot;num_retries\u0026quot;: 10 になったことで、合計のアクセス回数が11回になりました。デフォルトは2だったということですね。\n次に 502 でも retry されるかどうか x-envoy-retriable-status-codes ヘッダーを試しましたが、、期待の動作にならなりませんでした\u0026hellip; 🤔\nOutlierDetection を状態を確認しながらテスト # 話を OutlierDetection に戻します。istioctl proxy-config コマンドで OUTLIER CHECK の状態が確認できることがわかりました。\n$ istioctl proxy-config endpoint ubuntu-deployment-cc86cc647-vsvbh | egrep '^ENDPOINT|v2\\|httpbin' ENDPOINT STATUS OUTLIER CHECK CLUSTER 172.17.0.10:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.11:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.12:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.13:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.8:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 次のようにして状態を確認しながら /status/503 にアクセスをしてみます。\n$ while : ; do date; istioctl proxy-config endpoint ubuntu-deployment-cc86cc647-vsvbh | egrep '^ENDPOINT|v2\\|httpbin'; sleep 3; done Sun Mar 8 17:49:37 JST 2020 ENDPOINT STATUS OUTLIER CHECK CLUSTER 172.17.0.10:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.11:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.12:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.13:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.8:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local Sun Mar 8 17:49:40 JST 2020 ENDPOINT STATUS OUTLIER CHECK CLUSTER 172.17.0.10:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.11:80 HEALTHY FAILED outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.12:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.13:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.8:80 HEALTHY OK outbound|80|v2|httpbin-service.default.svc.cluster.local Sun Mar 8 17:49:43 JST 2020 ENDPOINT STATUS OUTLIER CHECK CLUSTER 172.17.0.10:80 HEALTHY FAILED outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.11:80 HEALTHY FAILED outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.12:80 HEALTHY FAILED outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.13:80 HEALTHY FAILED outbound|80|v2|httpbin-service.default.svc.cluster.local 172.17.0.8:80 HEALTHY FAILED outbound|80|v2|httpbin-service.default.svc.cluster.local 全部 FAILED になった状態でもアクセスできました。一部だけ FAILED の時は OK の Endpoint にだけ送られました。\nFAILED となって外されている期間は baseEjectionTime かける eject された回数時間になるようです。\n続く。。\n Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月8日","permalink":"/2020/03/istio-part5/","section":"Posts","summary":"Istio シリーズです。 そういえば Ingress Gateway になかなか辿りつかないな。 OutlierDetection 設定 # OutlierDetection は DestinationRule に設定するものでドキュメントもそこにあります。 一旦 VirtualService での転送先を v2 だけ","title":"Istio 導入への道 - OutlierDetection と Retry 編"},{"content":"Istio シリーズです。\n今回は Fault Injection です。前回の VirtualService に設定を入れることでわざと 503 とか 500 エラーを返したり、delay を入れたりすることができます。\n500 Internal Server Error を返す # 現在の設定を確認。QueryString に v=1 があれば v1 に、それ意外は v2 に送られます。\n$ kubectl get vs httpbin-virtual-service -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: annotations: ... (省略) ... spec: hosts: - httpbin-service http: - match: - name: v1 queryParams: v: exact: \u0026quot;1\u0026quot; route: - destination: host: httpbin-service subset: v1 - route: - destination: host: httpbin-service subset: v2 route のレベルに fault を入れます。v1 のところに入れてみます。(HTTPFaultInjection.Abort)\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin-virtual-service spec: hosts: - httpbin-service http: - match: - name: v1 queryParams: v: exact: \u0026quot;1\u0026quot; **fault:** **abort:** **httpStatus: 500** **percentage:** **value: 50** route: - destination: host: httpbin-service subset: v1 - route: - destination: host: httpbin-service subset: v2 EOF v1 の場合に、50% の割合で 500 Internal Server Error を返すようにしました。\nroot@ubuntu-deployment-cc86cc647-vsvbh:/# curl -sv http://httpbin-service/headers\\\\?v=1 \\* Trying 10.109.118.31... \\* TCP\\_NODELAY set \\* Connected to httpbin-service (10.109.118.31) port 80 (#0) \u0026gt; GET /headers?v=1 HTTP/1.1 \u0026gt; Host: httpbin-service \u0026gt; User-Agent: curl/7.58.0 \u0026gt; Accept: \\*/\\* \u0026gt; \u0026lt; HTTP/1.1 500 Internal Server Error \u0026lt; content-length: 18 \u0026lt; content-type: text/plain \u0026lt; date: Sun, 08 Mar 2020 01:53:15 GMT \u0026lt; server: envoy \u0026lt; \\* Connection #0 to host httpbin-service left intact fault filter abort curl を実行した Pod 側で istio-proxy のログを確認してみると response_flags に FI (Fault Injection) と入っています。また、宛先の Pod 側ではログが出ていないため、リクエストは送られていないようです。まあそうでしょう。curl 側のログも upstream の項目が空です。\n{ \u0026quot;authority\u0026quot;: \u0026quot;httpbin-service\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;18\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;10.109.118.31:80\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.9:60170\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/headers?v=1\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;76435924-cdea-4fdb-ae52-bc5db43946fc\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, **\u0026quot;response\\_code\u0026quot;: \u0026quot;500\u0026quot;**, **\u0026quot;response\\_flags\u0026quot;: \u0026quot;FI\u0026quot;**, \u0026quot;route\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-08T01:53:15.360Z\u0026quot;, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } percentage で 50 と指定しているため全てが 500 Error になるわけではなく、リクエストの半分です。percentage の型は double で小数で 1% 未満も指定可能です。もちろんこの設定では v2 側へのアクセス時には 500 Error は返されません。本当に Upstream 側で返されれば別でしょうが。\nDelay を挿入する # 次に v2 側に delay を入れてみます。(HTTPFaultInjection.Delay)\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin-virtual-service spec: hosts: - httpbin-service http: - match: - name: v1 queryParams: v: exact: \u0026quot;1\u0026quot; fault: abort: httpStatus: 500 percentage: value: 50 route: - destination: host: httpbin-service subset: v1 - route: - destination: host: httpbin-service subset: v2 **fault:** **delay:** **fixedDelay: 5s** **percentage:** **value: 50** EOF これで v2 へリクエストを送る際に 50% の割合で5秒の delay が挿入されます。ログを確認してみます。\ncurl を実行している側の Pod の istio-proxy のログです。\u0026quot;response_flags\u0026quot;: \u0026quot;DI\u0026quot; となっており、Delay が挿入されていることがわかります。\u0026quot;start_time\u0026quot;: \u0026quot;2020-03-08T02:11:04.369Z\u0026quot; を宛先側のログと比較してみます。\n{ \u0026quot;authority\u0026quot;: \u0026quot;httpbin-service\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;521\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;10.109.118.31:80\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.9:47376\u0026quot;, **\u0026quot;duration\u0026quot;: \u0026quot;5004\u0026quot;**, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/headers\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;71da7efc-29a0-4e03-b23b-bfbf1638c273\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;response\\_code\u0026quot;: \u0026quot;200\u0026quot;, **\u0026quot;response\\_flags\u0026quot;: \u0026quot;DI\u0026quot;**, \u0026quot;route\\_name\u0026quot;: \u0026quot;-\u0026quot;, **\u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-08T02:11:04.369Z\u0026quot;**, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;outbound|80|v2|httpbin-service.default.svc.cluster.local\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;172.17.0.8:80\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;172.17.0.9:54576\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } 宛先側の Pod の istio-proxy のログです。\u0026quot;start_time\u0026quot;: \u0026quot;2020-03-08T02:11:09.371Z\u0026quot; と開始が5秒遅れていることが確認できます。送信元側で待ってから送っているようです。\n{ \u0026quot;authority\u0026quot;: \u0026quot;httpbin-service\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;521\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;172.17.0.8:80\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.9:54576\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/headers\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;71da7efc-29a0-4e03-b23b-bfbf1638c273\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;outbound\\_.80\\_.v2\\_.httpbin-service.default.svc.cluster.local\u0026quot;, \u0026quot;response\\_code\u0026quot;: \u0026quot;200\u0026quot;, \u0026quot;response\\_flags\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;route\\_name\u0026quot;: \u0026quot;default\u0026quot;, **\u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-08T02:11:09.371Z\u0026quot;**, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;inbound|80|http|httpbin-service.default.svc.cluster.local\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;127.0.0.1:80\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;127.0.0.1:50500\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } 次は OutlierDetection / 異常値検出 かな。\n Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月8日","permalink":"/2020/03/istio-part4/","section":"Posts","summary":"Istio シリーズです。 今回は Fault Injection です。前回の VirtualService に設定を入れることでわざと 503 とか 500 エラーを返したり、delay を入れたりすることができます。 500 Internal Server Error","title":"Istio 導入への道 - Fault Injection 編"},{"content":"Istio シリーズです。\n今回は VirtualService です。これを利用することで、コネクションプーリングの設定をしたり、レートリミットを入れたり、振り分け方法を指定したり、同じホスト名でアクセスしても条件によって振り分けを行えたり、指定の HTTP レスポンス (400 とか 500 Internal Server Error とか) を返したり、delay を入れたりすることができるようになります。また、後でやる Ingress Gateway からアクセスできるようになったりします。\n複数 Version の Deployment を用意する # Version 違いを出し分けたりするテストを行うため、一旦今の httpbin-deployment を削除します。\n$ kubectl delete deployment httpbin-deployment 本当はレスポンスが異なる Pod を用意すればわかりやすいのですが、ログでアクセスを確認するってことで、v1 と v2 と version label だけが異なる Deployment を2つ deploy します。\napiVersion: apps/v1 kind: Deployment metadata: name: httpbin-deployment-v1 labels: app: httpbin version: v1 spec: replicas: 1 selector: matchLabels: app: httpbin version: v1 template: metadata: labels: app: httpbin version: v1 spec: containers: - name: httpbin image: kennethreitz/httpbin:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 readinessProbe: httpGet: path: /status/200 port: 80 initialDelaySeconds: 10 periodSeconds: 5 livenessProbe: httpGet: path: /status/200 port: 80 initialDelaySeconds: 15 periodSeconds: 5 --- apiVersion: apps/v1 kind: Deployment metadata: name: httpbin-deployment-v2 labels: app: httpbin version: v2 spec: replicas: 1 selector: matchLabels: app: httpbin version: v2 template: metadata: labels: app: httpbin version: v2 spec: containers: - name: httpbin image: kennethreitz/httpbin:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 readinessProbe: httpGet: path: /status/200 port: 80 initialDelaySeconds: 10 periodSeconds: 5 livenessProbe: httpGet: path: /status/200 port: 80 initialDelaySeconds: 15 periodSeconds: 5 httpbin-service として作成ずみの Service は selector が app: httpbin だけであるため、v1 も v2 も両方とも対象となり、この状態でも v1, v2 両方にリクエストが振り分けられる状況ですが、振り分け方法を細かく制限したり Fault injection を行えるようにするため VirtualService を定義します。\nv1 も v2 も一つの Service に含まれている様子。Endpoints に両方入っています。\n$ kubectl get pods -o wide -l app=httpbin NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES httpbin-deployment-v1-7d95bdc6f6-5f69g 2/2 Running 0 13m 172.17.0.7 m01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; httpbin-deployment-v2-ccd49cc9c-lgvjs 2/2 Running 0 12m 172.17.0.8 m01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ kubectl describe svc httpbin-service Name: httpbin-service Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Service\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;httpbin-service\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;ports\u0026quot;:\\[{\u0026quot;name\u0026quot;:... Selector: app=httpbin Type: ClusterIP IP: 10.109.118.31 Port: http 80/TCP TargetPort: 80/TCP Endpoints: 172.17.0.7:80,172.17.0.8:80 Session Affinity: None Events: \u0026lt;none\u0026gt; DestinationRule の作成 # v1, v2 それぞれにアクセスするための DestinationRule を作成します。\n# # httpbin-service という Service 宛 traffic で # VirtualService により、subset: v1 と指定された場合は # version label が v1 の Endpoint へ転送し、 # subnet: v2 の場合は label が v2 の Endpoint へ転送する # apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: httpbin-destination-rule spec: host: httpbin-service subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 VirtualService # 上で作った DestinationRule を使って VirtualServive を設定します。\n重みづけで振り分ける # 一番単純な例\n# # httpbin-service 宛ての http リクエストを httpbin-service の v1 か v2 に半々の割合で転送する # apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin-virtual-service spec: hosts: - httpbin-service http: - route: - destination: host: httpbin-service subset: v1 weight: 50 - destination: host: httpbin-service subset: v2 weight: 50 wegith を 100:0 に変更して試すと全部片方にしかリクエストが送られないことが確認できる。\nHTTP Header を使って振り分ける # HTTPMatchRequest を使うことで HTTP の Request の内容によって振り分けを行うことができる。\n# # queryString に v=1 があれば v1 へ、v=2 があれば v2 へ転送する # apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin-virtual-service spec: hosts: - httpbin-service http: - match: - name: v1 queryParams: v: exact: \u0026quot;1\u0026quot; route: - destination: host: httpbin-service subset: v1 - match: - name: v2 queryParams: v: exact: \u0026quot;2\u0026quot; route: - destination: host: httpbin-service subset: v2 この例の様に `match` に `name` を設定しておけば、送信側の istio-proxy のログの `route_name` に subset 名が入っている。\n{ \u0026quot;authority\u0026quot;: \u0026quot;httpbin-service\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;521\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;10.109.118.31:80\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.9:50168\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/headers?v=1\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;c463717a-a722-47c6-8ec1-ea8f90b9ea58\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;response\\_code\u0026quot;: \u0026quot;200\u0026quot;, \u0026quot;response\\_flags\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;route\\_name\u0026quot;: \u0026quot;.v1\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-07T16:06:58.480Z\u0026quot;, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;outbound|80|v1|httpbin-service.default.svc.cluster.local\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;172.17.0.7:80\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;172.17.0.9:60574\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } { \u0026quot;authority\u0026quot;: \u0026quot;httpbin-service\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;521\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;10.109.118.31:80\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.9:50224\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/headers?v=2\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;b5091908-4806-465a-b755-59141e3acd25\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;response\\_code\u0026quot;: \u0026quot;200\u0026quot;, \u0026quot;response\\_flags\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;route\\_name\u0026quot;: \u0026quot;.v2\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-07T16:07:02.565Z\u0026quot;, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;outbound|80|v2|httpbin-service.default.svc.cluster.local\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;172.17.0.8:80\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;172.17.0.9:54576\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } { \u0026quot;authority\u0026quot;: \u0026quot;httpbin-service\u0026quot;, \u0026quot;bytes\\_received\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;bytes\\_sent\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;downstream\\_local\\_address\u0026quot;: \u0026quot;10.109.118.31:80\u0026quot;, \u0026quot;downstream\\_remote\\_address\u0026quot;: \u0026quot;172.17.0.9:50604\u0026quot;, \u0026quot;duration\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;istio\\_policy\\_status\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/headers?v=3\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;HTTP/1.1\u0026quot;, \u0026quot;request\\_id\u0026quot;: \u0026quot;24312033-fbf6-48d9-936a-bb23e3381d7f\u0026quot;, \u0026quot;requested\\_server\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;response\\_code\u0026quot;: \u0026quot;404\u0026quot;, \u0026quot;response\\_flags\u0026quot;: \u0026quot;NR\u0026quot;, \u0026quot;route\\_name\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;start\\_time\u0026quot;: \u0026quot;2020-03-07T16:07:28.632Z\u0026quot;, \u0026quot;upstream\\_cluster\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_host\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_local\\_address\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_service\\_time\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;upstream\\_transport\\_failure\\_reason\u0026quot;: \u0026quot;-\u0026quot;, \u0026quot;user\\_agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;x\\_forwarded\\_for\u0026quot;: \u0026quot;-\u0026quot; } 3つ目のログは v=3 で、その定義はしていなかったため 404 が返されている。次の様に `match` をつけないで `route` を最後に書いておけばマッチしなかったものが全てそこに送られる。\n# # queryString に v=1 があれば v1 へ、そうでなければ v2 へ転送する # apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: httpbin-virtual-service spec: hosts: - httpbin-service http: - match: - name: v1 queryParams: v: exact: \u0026quot;1\u0026quot; route: - destination: host: httpbin-service subset: v1 - route: - destination: host: httpbin-service subset: v2 注意点として、VirtualService の振り分けは最初にマッチしたところで宛先が決まってしまう点。条件の厳しいものから順に書いておく必要がある。\n次回は Fault Injection にしよう。\n Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月7日","permalink":"/2020/03/istio-part3/","section":"Posts","summary":"Istio シリーズです。 今回は VirtualService です。これを利用することで、コネクションプーリングの設定をしたり、レートリミットを入れたり、振り分け方法を指定したり","title":"Istio 導入への道 - VirtualService 編"},{"content":"前回の続きです。\nIstio でのサービス間通信 # まあ、ただサービス間で通信するだけなら Istio は不要なわけだけれども、まずはここから。\nhttpbin をサービスとして deploy # httpbin.org のコンテナは Request Header をそのまま返してくれたりして便利なのでこれをサービスとして deploy します。\napiVersion:apps/v1kind:Deploymentmetadata:name:httpbin-deploymentlabels:app:httpbinspec:replicas:2selector:matchLabels:app:httpbintemplate:metadata:labels:app:httpbinspec:containers:- name:httpbinimage:kennethreitz/httpbin:latestimagePullPolicy:IfNotPresentports:- containerPort:80readinessProbe:httpGet:path:/status/200port:80initialDelaySeconds:10periodSeconds:5livenessProbe:httpGet:path:/status/200port:80initialDelaySeconds:15periodSeconds:5---apiVersion:v1kind:Servicemetadata:name:httpbin-servicespec:selector:app:httpbinports:- name:httpprotocol:TCPport:80targetPort:80不要なんだけど、全部同じ名前にしちゃうとどれとどれが連動しているのかわかりにくくなるので -service とか -deployment とかを名前に入れています。これを httpbin.yaml として保存します。\nこれに Istio の sidecar を inject するのが istioctl kube-inject です。次のように実行すれば inject 済みの manifest が出力されます。\n$ istioctl kube-inject -f httpbin.yaml apiVersion:apps/v1kind:Deploymentmetadata:creationTimestamp:nulllabels:app:httpbinname:httpbin-deploymentspec:replicas:2selector:matchLabels:app:httpbinstrategy:{}template:metadata:annotations:sidecar.istio.io/interceptionMode:REDIRECTsidecar.istio.io/status:\u0026#39;{\u0026#34;version\u0026#34;:\u0026#34;1cdb312e0b39910b7401fa37c42113f6436e281598036cb126f9692adebf1545\u0026#34;,\u0026#34;initContainers\u0026#34;:[\u0026#34;istio-init\u0026#34;],\u0026#34;containers\u0026#34;:[\u0026#34;istio-proxy\u0026#34;],\u0026#34;volumes\u0026#34;:[\u0026#34;istio-envoy\u0026#34;,\u0026#34;podinfo\u0026#34;,\u0026#34;istiod-ca-cert\u0026#34;],\u0026#34;imagePullSecrets\u0026#34;:null}\u0026#39;traffic.sidecar.istio.io/excludeInboundPorts:\u0026#34;15020\u0026#34;traffic.sidecar.istio.io/includeInboundPorts:\u0026#34;80\u0026#34;traffic.sidecar.istio.io/includeOutboundIPRanges:\u0026#39;*\u0026#39;creationTimestamp:nulllabels:app:httpbinsecurity.istio.io/tlsMode:istiospec:containers:- image:kennethreitz/httpbin:latestimagePullPolicy:IfNotPresentlivenessProbe:httpGet:path:/status/200port:80initialDelaySeconds:15periodSeconds:5name:httpbinports:- containerPort:80readinessProbe:httpGet:path:/status/200port:80initialDelaySeconds:10periodSeconds:5resources:{}- args:- proxy- sidecar- --domain- $(POD_NAMESPACE).svc.cluster.local- --configPath- /etc/istio/proxy- --binaryPath- /usr/local/bin/envoy- --serviceCluster- httpbin.$(POD_NAMESPACE)- --drainDuration- 45s- --parentShutdownDuration- 1m0s- --discoveryAddress- istiod.istio-system.svc:15012- --zipkinAddress- zipkin.istio-system:9411- --proxyLogLevel=warning- --proxyComponentLogLevel=misc:error- --connectTimeout- 10s- --proxyAdminPort- \u0026#34;15000\u0026#34;- --concurrency- \u0026#34;2\u0026#34;- --controlPlaneAuthPolicy- NONE- --dnsRefreshRate- 300s- --statusPort- \u0026#34;15020\u0026#34;- --trust-domain=cluster.local- --controlPlaneBootstrap=falseenv:- name:JWT_POLICYvalue:first-party-jwt- name:PILOT_CERT_PROVIDERvalue:istiod- name:CA_ADDRvalue:istio-pilot.istio-system.svc:15012- name:POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.name- name:POD_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespace- name:INSTANCE_IPvalueFrom:fieldRef:fieldPath:status.podIP- name:SERVICE_ACCOUNTvalueFrom:fieldRef:fieldPath:spec.serviceAccountName- name:HOST_IPvalueFrom:fieldRef:fieldPath:status.hostIP- name:ISTIO_META_POD_PORTSvalue:|-[ {\u0026#34;containerPort\u0026#34;:80} ]- name:ISTIO_META_CLUSTER_IDvalue:Kubernetes- name:ISTIO_META_POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.name- name:ISTIO_META_CONFIG_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespace- name:ISTIO_META_INTERCEPTION_MODEvalue:REDIRECT- name:ISTIO_META_WORKLOAD_NAMEvalue:httpbin-deployment- name:ISTIO_META_OWNERvalue:kubernetes://apis/apps/v1/namespaces/default/deployments/httpbin-deployment- name:ISTIO_META_MESH_IDvalue:cluster.localimage:docker.io/istio/proxyv2:1.5.0imagePullPolicy:IfNotPresentname:istio-proxyports:- containerPort:15090name:http-envoy-promprotocol:TCPreadinessProbe:failureThreshold:30httpGet:path:/healthz/readyport:15020initialDelaySeconds:1periodSeconds:2resources:limits:cpu:\u0026#34;2\u0026#34;memory:1Girequests:cpu:100mmemory:128MisecurityContext:allowPrivilegeEscalation:falsecapabilities:drop:- ALLprivileged:falsereadOnlyRootFilesystem:truerunAsGroup:1337runAsNonRoot:truerunAsUser:1337volumeMounts:- mountPath:/var/run/secrets/istioname:istiod-ca-cert- mountPath:/etc/istio/proxyname:istio-envoy- mountPath:/etc/istio/podname:podinfoinitContainers:- command:- istio-iptables- -p- \u0026#34;15001\u0026#34;- -z- \u0026#34;15006\u0026#34;- -u- \u0026#34;1337\u0026#34;- -m- REDIRECT- -i- \u0026#39;*\u0026#39;- -x- \u0026#34;\u0026#34;- -b- \u0026#39;*\u0026#39;- -d- 15090,15020image:docker.io/istio/proxyv2:1.5.0imagePullPolicy:IfNotPresentname:istio-initresources:limits:cpu:100mmemory:50Mirequests:cpu:10mmemory:10MisecurityContext:allowPrivilegeEscalation:falsecapabilities:add:- NET_ADMIN- NET_RAWdrop:- ALLprivileged:falsereadOnlyRootFilesystem:falserunAsGroup:0runAsNonRoot:falserunAsUser:0securityContext:fsGroup:1337volumes:- emptyDir:medium:Memoryname:istio-envoy- downwardAPI:items:- fieldRef:fieldPath:metadata.labelspath:labels- fieldRef:fieldPath:metadata.annotationspath:annotationsname:podinfo- configMap:name:istio-ca-root-certname:istiod-ca-certstatus:{}---apiVersion:v1kind:Servicemetadata:name:httpbin-servicespec:selector:app:httpbinports:- name:httpprotocol:TCPport:80targetPort:80よって、これを pipe で kubectl apply に食わせることで deploy できます。kubectl apply -f \u0026lt;(istioctl kube-inject -f ...) でも良いし、もちろんファイルに一旦書き出しても大丈夫。\n$ istioctl kube-inject -f httpbin.yaml | kubectl apply -f - default namespace に deploy しました。\n$ istioctl kube-inject -f httpbin.yaml | kubectl apply -f - deployment.apps/httpbin-deployment created service/httpbin-service created $ kubectl get pods,deployments,services NAME READY STATUS RESTARTS AGE pod/httpbin-deployment-9bfd96975-px5lv 2/2 Running 0 88s pod/httpbin-deployment-9bfd96975-v6mgg 2/2 Running 0 88s NAME READY UP-TO-DATE AVAILABLE AGE deployment.extensions/httpbin-deployment 2/2 2 2 88s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/httpbin-service ClusterIP 10.109.118.31 80/TCP 88s service/kubernetes ClusterIP 10.96.0.1 443/TCP 4h24m 自動で inject されるようにする # 毎回 Deployment を作る度に kube-inject をするのは面倒なので自動化することができる。自動 inject 対象としたい namespace に対して istio-injection=enabled という label をつけるだけで良い。\n$ kubectl label namespace default istio-injection=enabled $ kubectl get ns default -o yaml apiVersion: v1 kind: Namespace metadata: creationTimestamp: \u0026quot;2020-03-07T06:44:22Z\u0026quot; labels: istio-injection: enabled name: default resourceVersion: \u0026quot;12273\u0026quot; selfLink: /api/v1/namespaces/default uid: 1c02a69b-5ab5-433a-90be-6c6e3b785867 spec: finalizers: - kubernetes status: phase: Active 先ほど deploy した httpbin に対してクラスタ内からアクセスするための ubuntu コンテナを Deployment として deploy します。\napiVersion:apps/v1kind:Deploymentmetadata:name:ubuntu-deploymentlabels:app:ubuntuspec:replicas:1selector:matchLabels:app:ubuntutemplate:metadata:labels:app:ubuntuspec:containers:- name:ubuntuimage:ubuntu:latestimagePullPolicy:IfNotPresentcommand:- sleep- infinityこの manifest を ubuntu.yaml として保存し、今度は istioctl kube-inject を使わずにそのまま kubectl apply します。それでも自動で inject されるはずです。\n$ kubectl apply -f ubuntu.yaml deployment.apps/ubuntu-deployment created Deployment の spec には ubuntu コンテナしか書いてなかったのにコンテナの数が2になっています。\n$ kubectl get pods -l app=ubuntu NAME READY STATUS RESTARTS AGE ubuntu-deployment-cc86cc647-vsvbh 2/2 Running 0 21s istio-proxy というコンテナが追加されています。長いので名前だけ表示。\n$ kubectl get pods -l app=ubuntu -o json | jq '.items[].spec.containers[].name' \u0026quot;ubuntu\u0026quot; \u0026quot;istio-proxy\u0026quot; curl でアクセスしてみる # ubuntu Pod から httpbin Service にアクセスしてみます。\n$ kubectl exec -it ubuntu-deployment-cc86cc647-vsvbh -c ubuntu -- bash root@ubuntu-deployment-cc86cc647-vsvbh:/# curl が入っていないのでインストールする。\nroot@ubuntu-deployment-cc86cc647-vsvbh# apt-get update \u0026amp;\u0026amp; apt-get install -y curl httpbin-service にアクセスしてみる。\nroot@ubuntu-deployment-cc86cc647-vsvbh:/# curl -sv http://httpbin-service/headers * Trying 10.109.118.31... * TCP_NODELAY set * Connected to httpbin-service (10.109.118.31) port 80 (#0) \u0026gt; GET /headers HTTP/1.1 \u0026gt; Host: httpbin-service \u0026gt; User-Agent: curl/7.58.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; server: envoy \u0026lt; date: Sat, 07 Mar 2020 11:33:28 GMT \u0026lt; content-type: application/json \u0026lt; content-length: 521 \u0026lt; access-control-allow-origin: * \u0026lt; access-control-allow-credentials: true \u0026lt; x-envoy-upstream-service-time: 26 \u0026lt; { \u0026quot;headers\u0026quot;: { \u0026quot;Accept\u0026quot;: \u0026quot;*/*\u0026quot;, \u0026quot;Content-Length\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Host\u0026quot;: \u0026quot;httpbin-service\u0026quot;, \u0026quot;User-Agent\u0026quot;: \u0026quot;curl/7.58.0\u0026quot;, \u0026quot;X-B3-Parentspanid\u0026quot;: \u0026quot;361390f32cd55bdc\u0026quot;, \u0026quot;X-B3-Sampled\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;X-B3-Spanid\u0026quot;: \u0026quot;f95fbc3875ab93e5\u0026quot;, \u0026quot;X-B3-Traceid\u0026quot;: \u0026quot;3a3e0cd03f1ded4e361390f32cd55bdc\u0026quot;, \u0026quot;X-Forwarded-Client-Cert\u0026quot;: \u0026quot;By=spiffe://cluster.local/ns/default/sa/default;Hash=b87885c2542d1279071454ae1ce34cea21b7a265095bb23297ff44542009a304;Subject=\\\u0026quot;\\\u0026quot;;URI=spiffe://cluster.local/ns/default/sa/default\u0026quot; } } * Connection #0 to host httpbin-service left intact できました。Response Header に server: envoy や x-envoy-upstream-service-time: 26 が入っています。また、X- プレフィックスのついた curl では送っていないヘッダーが httpbin 側に届いているようです。Envoy が間に入っているようです。X-Forwarded-Client-Cent も送られているのでクライアント証明書も送られたっぽいですね。これは Istio 1.5 からデフォルトになったのだろうか。1.4.6 の時は追加の設定が必要だったのだが。\n$ kubectl get -n istio-system cm istio -o yaml | grep -v '{' | grep enableAutoMtls: enableAutoMtls: true true になってる。前回の記事に戻って default プロファイルの値を確認してみよう。\n$ istioctl manifest generate --set profile=default | grep enableAutoMtls enableAutoMtls: false あれ？どこで変わったんだ？ 🤔 後で確認してみよう。\n # If true, automatically configure client side mTLS settings to match the corresponding service's # server side mTLS authentication policy, when destination rule for that service does not specify # TLS settings. enableAutoMtls: true っていう値。\nEnvoy のログを確認してみる # 前回の istio のインストール時に Envoy のログを有効にしておいたので istio-proxy の出力を確認してみます。\nまずは送信元の ubuntu 側。 grep を pipe に繋げると buffering されて全然 jq まで渡ってこないので --line-buffered をつけています。\n$ kubectl logs -l app=ubuntu -c istio-proxy -f --tail 0 | grep --line-buffered '^{' | jq . { \u0026#34;authority\u0026#34;: \u0026#34;httpbin-service\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;521\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;10.109.118.31:80\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;172.17.0.9:52178\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/headers\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;a93e2afa-cc27-4bb7-897c-5282c754d382\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-07T12:00:58.425Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;outbound|80||httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;172.17.0.8:80\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;172.17.0.9:43422\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;curl/7.58.0\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;-\u0026#34; } 172.17.0.9 は ubuntu Pod の IP アドレス。10.109.118.31 は httpbin-service Serivce の IP アドレス。172.17.0.8 は httpbin Pod の IP アドレス。\nistio-proxy が 172.17.0.9:43422 \u0026ndash;\u0026gt; 172.17.0.8:80 でリクエストを投げて、10.109.118.31:80 から 172.17.0.9:52178 に返したことにしてるってことなのか？？\n次に httpbin 側のログ\nkubectl logs -l app=httpbin -c istio-proxy -f --tail 0 | grep --line-buffered '^{' | grep --line-buffered -v kube-probe | jq . { \u0026#34;authority\u0026#34;: \u0026#34;httpbin-service\u0026#34;, \u0026#34;bytes_received\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;bytes_sent\u0026#34;: \u0026#34;521\u0026#34;, \u0026#34;downstream_local_address\u0026#34;: \u0026#34;172.17.0.8:80\u0026#34;, \u0026#34;downstream_remote_address\u0026#34;: \u0026#34;172.17.0.9:43422\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;istio_policy_status\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/headers\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;a93e2afa-cc27-4bb7-897c-5282c754d382\u0026#34;, \u0026#34;requested_server_name\u0026#34;: \u0026#34;outbound_.80_._.httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;response_code\u0026#34;: \u0026#34;200\u0026#34;, \u0026#34;response_flags\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;route_name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;start_time\u0026#34;: \u0026#34;2020-03-07T12:00:58.425Z\u0026#34;, \u0026#34;upstream_cluster\u0026#34;: \u0026#34;inbound|80|http|httpbin-service.default.svc.cluster.local\u0026#34;, \u0026#34;upstream_host\u0026#34;: \u0026#34;127.0.0.1:80\u0026#34;, \u0026#34;upstream_local_address\u0026#34;: \u0026#34;127.0.0.1:55232\u0026#34;, \u0026#34;upstream_service_time\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;upstream_transport_failure_reason\u0026#34;: \u0026#34;-\u0026#34;, \u0026#34;user_agent\u0026#34;: \u0026#34;curl/7.58.0\u0026#34;, \u0026#34;x_forwarded_for\u0026#34;: \u0026#34;-\u0026#34; } 172.17.0.9:43422 (ubuntu:istio-proxy) \u0026ndash;\u0026gt; 172.17.0.8:80 (httpbin:istio-proxy) 127.0.0.1:55232 \u0026ndash;\u0026gt; 127.0.0.1:80 (httpbin:httpbin)\n各 Pod と Serivce の IP アドレスは次の通り。\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES httpbin-deployment-9bfd96975-px5lv 2/2 Running 0 60m 172.17.0.8 m01 httpbin-deployment-9bfd96975-v6mgg 2/2 Running 0 60m 172.17.0.7 m01 ubuntu-deployment-cc86cc647-vsvbh 2/2 Running 0 45m 172.17.0.9 m01 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE httpbin-service ClusterIP 10.109.118.31 80/TCP 60m kubernetes ClusterIP 10.96.0.1 443/TCP 5h22m 続く\n Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月7日","permalink":"/2020/03/istio-part2/","section":"Posts","summary":"前回の続きです。 Istio でのサービス間通信 # まあ、ただサービス間で通信するだけなら Istio は不要なわけだけれども、まずはここから。 httpbin をサービスとして deploy #","title":"Istio 導入への道 - サービス間通信編"},{"content":"Istio 導入に向けて一歩一歩やっていき。リリースされたばかりの 1.5 を使ってみようと思います。\nMinikube の起動 # まずは Kubernetes クラスタの作成。いつものように minikube です。こちらのページによると Istio 1.5 は Kubernetes 1.14, 1.15, 1.16 でテストされているそうです。1.17 はまだ早いみたいです。特に理由はないですがここではひとまず 1.15 を使うことにしました。\n$ minikube start --kubernetes-version=v1.15.10 --cpus=4 --memory=8gb 後で type: LoadBalancer の Service が作られるので別ターミナルで minikube tunnel を実行しておきます。\n$ minikube tunnel Istio のインストール # istioctl のダウンロード # まずはダウンロードです。最新版をダウンロードする場合は ISTIO_VERSION の指定は不要かと思いますが一応。\n$ curl -L https://istio.io/downloadIstio | ISTIO\\_VERSION=1.5.0 sh - これでカレントディレクトリに istio-1.5.0 というディレクトリができているはずです。(github の releases から環境に合わせた tar.gz をダウンロードして展開してくれるだけ)\nistio-1.5.0/bin にある istioctl コマンドを使います。PATH に入れるか PATH の通ってるところに置いてしまいましょう。\nprofile の確認 # istioctl は複数の preset profile を持っています。\n$ istioctl profile list Istio configuration profiles: default demo empty minimal remote separate それぞれがどんな構成を作るのかは istioctl profile dump PROFILE-NAME で確認できます。istioctl profile diff PROFILE-NAME1 PROFILE-NAME2 で profile 間の差分も表示してくれます。kubectl で適用する manifest は istioctl manifest generate で生成できるため、この出力を比較することもできます。\nした manifest を diff で確認した方がわかりやすいです。\n$ diff -u \u0026lt;(istioctl manifest generate --set profile=default) \\ \u0026lt;(istioctl manifest generate --set profile=minimal) profile dump で確認できる変数を --set で上書きすることで細かいカスタマイズも可能でその影響も manifest generage で確認することができます。(オプション設定一覧)\nEnvoy proxy でアクセスログを出力するには values.global.proxy.accessLogFile=/dev/stdout とします。また、そのフォーマット（エンコーディング？）を JSON にするには values.global.proxy.accessLogEncoding=JSON とします。この manifest の差分が次のようにして確認できます。\n$ diff -u \u0026lt;(istioctl manifest generate \\ --set profile=default \\ --set values.global.proxy.accessLogEncoding=JSON \\ --set values.global.proxy.accessLogFile=/dev/stdout) \\ \u0026lt;(istioctl manifest generate --set profile=default) --- /dev/fd/11\t2020-03-07 16:29:46.000000000 +0900 +++ /dev/fd/12\t2020-03-07 16:29:46.000000000 +0900 @@ -7199,11 +7199,11 @@  enableTracing: true # Set accessLogFile to empty string to disable access log. - accessLogFile: \u0026#34;/dev/stdout\u0026#34; + accessLogFile: \u0026#34;\u0026#34;  accessLogFormat: \u0026#34;\u0026#34; - accessLogEncoding: \u0026#39;JSON\u0026#39; + accessLogEncoding: \u0026#39;TEXT\u0026#39;  enableEnvoyAccessLogService: false # reportBatchMaxEntries is the number of requests that are batched before telemetry data is sent to the mixer server @@ -7569,8 +7569,8 @@  \u0026#34;priorityClassName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;prometheusNamespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;proxy\u0026#34;: { - \u0026#34;accessLogEncoding\u0026#34;: \u0026#34;JSON\u0026#34;, - \u0026#34;accessLogFile\u0026#34;: \u0026#34;/dev/stdout\u0026#34;, + \u0026#34;accessLogEncoding\u0026#34;: \u0026#34;TEXT\u0026#34;, + \u0026#34;accessLogFile\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;accessLogFormat\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;autoInject\u0026#34;: \u0026#34;enabled\u0026#34;, \u0026#34;clusterDomain\u0026#34;: \u0026#34;cluster.local\u0026#34;, Egress Gateway を有効にしたい場合は gateways.components.egressGateway.enabled=true を指定します。\nインストール # $ istioctl manifest apply \\ --set profile=default \\ --set values.global.proxy.accessLogEncoding=JSON \\ --set values.global.proxy.accessLogFile=/dev/stdout Detected that your cluster does not support third party JWT authentication. Falling back to less secure first party JWT. See https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens for details. - Applying manifest for component Base... ✔ Finished applying manifest for component Base. - Applying manifest for component Pilot... ✔ Finished applying manifest for component Pilot. Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... Waiting for resources to become ready... - Applying manifest for component IngressGateways... - Applying manifest for component AddonComponents... ✔ Finished applying manifest for component IngressGateways. ✔ Finished applying manifest for component AddonComponents. ✔ Installation complete 後で変更したり削除する時のために generate で manifest を作成して kubectl apply した方が良いのかな。\n追記:\nConfigure third party service account tokens について。\nistio-proxy が認証に使える token には First Party Token と Third Party Token の2つがあり、First Party は無期限で使えて、全ての Pod にマウントされているため、有効期限の管理される Third Party Token が使えればその方が好ましい。クラウドプロバイダーの提供する Kubernetes の多くでは Third Party Token をサポートしているが、ローカルの開発用クラスタではそうではないことが多いらしい。istioctl manifest apply はこれを自動で調べて使える方を使ってくれるが、istioctl manifest generate で別途 apply する場合は --set values.global.jwtPolicy=third-party-jwt か --set values.global.jwtPolicy=first-party-jwt を明示する必要があります。\n続く\n Istio 導入への道シリーズ\n Istio 導入への道 (1) – インストール編 Istio 導入への道 (2) – サービス間通信編 Istio 導入への道 (3) – VirtualService 編 Istio 導入への道 (4) – Fault Injection 編 Istio 導入への道 (5) – OutlierDetection と Retry 編 Istio 導入への道 (6) – Ingress Gatway 編 Istio 導入への道 (7) – 外部へのアクセス / ServiceEntry 編 Istio 導入への道 (8) – 外部へのアクセスでも Fault Injection 編 Istio 導入への道 (9) – gRPC でも Fault Injection 編 Istio 導入への道 (10) – 図解 Istio 導入への道 (11) – Ingress Gateway で TLS Termination 編  ","date":"2020年3月7日","permalink":"/2020/03/istio-part1/","section":"Posts","summary":"Istio 導入に向けて一歩一歩やっていき。リリースされたばかりの 1.5 を使ってみようと思います。 Minikube の起動 # まずは Kubernetes クラスタの作成。いつものように minikube です。","title":"Istio 導入への道 - インストール編"},{"content":"","date":"2020年3月1日","permalink":"/tags/mysql/","section":"Tags","summary":"","title":"MySQL"},{"content":"Orchestracot の CLI 操作 # 前回に引き続き、Presslabs の mysql-operator です。今回は failover 周りの動作を確認します。Orchestrator の操作は Web UI からも操作できますが、コマンドラインで試します。\norchestrator コマンド # kubectl exec -it mysql-operator-0 -c orchestrator sh ちょっとログが邪魔なので stderr は　/dev/null に捨ててます。\n$ /usr/local/orchestrator/orchestrator --ignore-raft-setup -config /etc/orchestrator/orchestrator.conf.json -c clusters 2\u0026gt; /dev/null mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 $ /usr/local/orchestrator/orchestrator --ignore-raft-setup -config /etc/orchestrator/orchestrator.conf.json -c topolog y -i mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 2\u0026gt; /dev/null mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 [0s,ok,5.7.26-29-log,rw,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] ただ、--ignore-raft-setup というちょっと怪しげなオプションを指定する必要があって、Orchestrator をクラスタ構成としている場合は使ってはダメかも。\norchestrator-client コマンド # ということで Web API 経由でアクセスする orchestrator-client を使います。これは bash script です。\ncurl -LO https://raw.githubusercontent.com/openark/orchestrator/master/resources/bin/orchestrator-client chmod +x orchestrator-client export ORCHESTRATOR_API=$(minikube service mysql-operator --url)/api $ ./orchestrator-client -c clusters mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 $ ./orchestrator-client -c topology -i mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 [0s,ok,5.7.26-29-log,rw,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] orchestrator-client -h でコマンドの help が確認できます。\nmaster の takeover / failover # Master の障害が検知されれば自動で replica が昇格します（後半で検証）が、ダウンタイムが発生します。正常な状態で切り替えるには graceful-master-takeover コマンドを使います。1秒未満の瞬断で切り替えられます。トランザクション中だったものがどうなるかは未確認。\n$ ./orchestrator-client -c graceful-master-takeover orchestrator-client[42884]: instance|alias must be provided クラスタの instance か alias を指定する必要があります。clusters か clusters-alias コマンドで確認できます。\n$ ./orchestrator-client -c clusters mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 $ ./orchestrator-client -c clusters-alias mycluster-mysql-cluster-db-mysql-0.mysql.default:3306,mycluster-mysql-cluster-db.default 今回は alias の mycluster-mysql-cluster-db.default で指定してみます。\n$ ./orchestrator-client -c graceful-master-takeover -a mycluster-mysql-cluster-db.default When no target instance indicated, master mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 should only have one replica (making the takeover safe and simple), but has 2. Aborting どれを新しい master にするか指定しないとダメでした。mycluster-mysql-cluster-db-mysql-1 を新しい master にすることにします。\n$ ./orchestrator-client -c graceful-master-takeover -a mycluster-mysql-cluster-db.default -d mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 $ ./orchestrator-client -c topology -a mycluster-mysql-cluster-db.default mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 [0s,ok,5.7.26-29-log,rw,ROW,\u0026gt;\u0026gt;,GTID] - mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 [null,nonreplicating,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID,downtimed] + mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 [1s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] master が切り替わりました。旧 master は外れました。\n $ kubectl get pod --show-labels -l mysql.presslabs.org/cluster=mycluster-mysql-cluster-db NAME READY STATUS RESTARTS AGE LABELS mycluster-mysql-cluster-db-mysql-0 4/4 Running 0 15h app.kubernetes.io/component=database,app.kubernetes.io/instance=mycluster-mysql-cluster-db,app.kubernetes.io/managed-by=mysql.presslabs.org,app.kubernetes.io/name=mysql,app.kubernetes.io/version=5.7.26,controller-revision-hash=mycluster-mysql-cluster-db-mysql-58f489bb77,healthy=no,mysql.presslabs.org/cluster=mycluster-mysql-cluster-db,role=replica,statefulset.kubernetes.io/pod-name=mycluster-mysql-cluster-db-mysql-0 mycluster-mysql-cluster-db-mysql-1 4/4 Running 0 15h app.kubernetes.io/component=database,app.kubernetes.io/instance=mycluster-mysql-cluster-db,app.kubernetes.io/managed-by=mysql.presslabs.org,app.kubernetes.io/name=mysql,app.kubernetes.io/version=5.7.26,controller-revision-hash=mycluster-mysql-cluster-db-mysql-58f489bb77,healthy=yes,mysql.presslabs.org/cluster=mycluster-mysql-cluster-db,role=master,statefulset.kubernetes.io/pod-name=mycluster-mysql-cluster-db-mysql-1 mycluster-mysql-cluster-db-mysql-2 4/4 Running 0 15h app.kubernetes.io/component=database,app.kubernetes.io/instance=mycluster-mysql-cluster-db,app.kubernetes.io/managed-by=mysql.presslabs.org,app.kubernetes.io/name=mysql,app.kubernetes.io/version=5.7.26,controller-revision-hash=mycluster-mysql-cluster-db-mysql-58f489bb77,healthy=yes,mysql.presslabs.org/cluster=mycluster-mysql-cluster-db,role=replica,statefulset.kubernetes.io/pod-name=mycluster-mysql-cluster-db-mysql-2 mycluster-mysql-cluster-db-mysql-0 はもう replica でもありません。\nこの状態で mysql-0 の pod を delete すれば起動時に replication 設定されて replica として復帰するはずですがバグってて mysql-0 だけは復活しないようです。\n試しに mysql-2 pod を delete してみます。\n$ ./orchestrator-client -c topology -a mycluster-mysql-cluster-db.default mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 [0s,ok,5.7.26-29-log,rw,ROW,\u0026gt;\u0026gt;,GTID] - mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 [null,nonreplicating,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID,downtimed] - mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 [unknown,invalid,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] 停止中は unknown,invalid になってしまいますが\n$ ./orchestrator-client -c topology -a mycluster-mysql-cluster-db.default mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 [0s,ok,5.7.26-29-log,rw,ROW,\u0026gt;\u0026gt;,GTID] - mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 [null,nonreplicating,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID,downtimed] + mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] 復活しました 😀\nmysql-2 の起動時には mysql-1 から extrabackup で取得しています。\n$ kubectl logs mycluster-mysql-cluster-db-mysql-2 -c init Create rclone.conf file. 2020-03-01T08:04:04.721Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;MY_SERVER_ID_OFFSET\u0026quot;} 2020-03-01T08:04:04.721Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;INIT_BUCKET_URI\u0026quot;} 2020-03-01T08:04:04.721Z\tINFO\tsidecar\tcloning command\t{\u0026quot;host\u0026quot;: \u0026quot;mycluster-mysql-cluster-db-mysql-2\u0026quot;} 2020-03-01T08:04:04.721Z\tINFO\tsidecar\tcloning from node\t{\u0026quot;host\u0026quot;: \u0026quot;mycluster-mysql-cluster-db-mysql-1.mysql.default\u0026quot;} 2020-03-01T08:04:04.721Z\tINFO\tsidecar\tinitialize a backup\t{\u0026quot;host\u0026quot;: \u0026quot;mycluster-mysql-cluster-db-mysql-1.mysql.default\u0026quot;, \u0026quot;endpoint\u0026quot;: \u0026quot;/xbackup\u0026quot;} xtrabackup: recognized server arguments: --innodb_checksum_algorithm=crc32 --innodb_log_checksum_algorithm=strict_crc32 --innodb_data_file_path=ibdata1:12M:autoextend --innodb_log_files_in_group=2 --innodb_log_file_size=50331648 --innodb_fast_checksum=0 --innodb_page_size=16384 --innodb_log_block_size=512 --innodb_undo_directory=./ --innodb_undo_tablespaces=0 --server-id=101 --redo-log-version=1 ... ... 一方、mysql-0 では nothing to clone or init from となってしまっています 🤔\n$ k logs mycluster-mysql-cluster-db-mysql-0 -c init Create rclone.conf file. 2020-03-01T07:36:30.420Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;MY_SERVER_ID_OFFSET\u0026quot;} 2020-03-01T07:36:30.421Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;INIT_BUCKET_URI\u0026quot;} 2020-03-01T07:36:30.422Z\tINFO\tsidecar\tcloning command\t{\u0026quot;host\u0026quot;: \u0026quot;mycluster-mysql-cluster-db-mysql-0\u0026quot;} 2020-03-01T07:36:30.422Z\tINFO\tsidecar\tnothing to clone or init from 2020-03-01T07:36:30.422Z\tINFO\tsidecar\tconfiguring server\t{\u0026quot;host\u0026quot;: \u0026quot;mycluster-mysql-cluster-db-mysql-0\u0026quot;} 2020-03-01T07:36:30.422Z\tINFO\tsidecar\terror while reading PURGE GTID from xtrabackup info file\t{\u0026quot;error\u0026quot;: \u0026quot;open /var/lib/mysql/xtrabackup_binlog_info: no such file or directory\u0026quot;} で、コードを探してみると v0.3.8 には nothing to clone or init from がありますが、最新の master branch ではもう修正されているっぽいです。現在の仕様は次のようになっているみたいです。\npkg/sidecar/appclone.go#L29-L58\n// RunCloneCommand clones the data from several potential sources. // // There are a few possible scenarios that this function tries to handle: // // Scenario | Action Taken // ------------------------------------------------------------------------------------ // Data already exists | Log an informational message and return without error. // | This permits the pod to continue initializing and mysql // | will use the data already on the PVC. // ------------------------------------------------------------------------------------ // Healthy replicas exist | We will attempt to clone from the healthy replicas. // | If the cloning starts but is interrupted, we will return // | with an error, not trying to clone from the master. The // | assumption is that some intermittent error caused the // | failure and we should let K8S restart the init container // | to try to clone from the replicas again. // ------------------------------------------------------------------------------------ // No healthy replicas; only | We attempt to clone from the master, assuming that this // master exists | is the initialization of the second pod in a multi-pod // | cluster. If cloning starts and is interrupted, we will // | return with an error, letting K8S try again. // ------------------------------------------------------------------------------------ // No healthy replicas; no | If there is a bucket URL to clone from, we will try that. // master; bucket URL exists | The assumption is that this is the bootstrap case: the // | very first mysql pod is being initialized. // ------------------------------------------------------------------------------------ // No healthy replicas; no | If this is the first pod in the cluster, then allow it // master; no bucket URL | to initialize as an empty instance, otherwise, return an // | error to allow k8s to kill and restart the pod. // ------------------------------------------------------------------------------------ 新しい image はあるみたいなので helm repository からではなく git repository の master にある helm chart からインストールして確認してみます。\nおや、新しい version では replica 用の Service が追加されていますね。\n$ k get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 28m mycluster-mysql-cluster-db-mysql ClusterIP 10.111.142.169 3306/TCP 15m mycluster-mysql-cluster-db-mysql-master ClusterIP 10.109.199.43 3306/TCP,8080/TCP 15m mycluster-mysql-cluster-db-mysql-replicas ClusterIP 10.97.48.190 3306/TCP,8080/TCP 15m mysql ClusterIP None 3306/TCP,9125/TCP 15m mysql-operator NodePort 10.98.93.157 80:32120/TCP 21m mysql-operator-0-svc ClusterIP 10.105.168.29 80/TCP,10008/TCP 21m Master 混在の Service は使いづらそうだなと思っていたのでこれは良い進化。\nで、肝心の mysql-0 がきちんと復帰するかどうかの確認です。\n$ k logs mycluster-mysql-cluster-db-mysql-0 -c init Create rclone.conf file. 2020-03-01T11:57:57.579Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;MY_SERVER_ID_OFFSET\u0026quot;} 2020-03-01T11:57:57.579Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;INIT_BUCKET_URI\u0026quot;} 2020-03-01T11:57:57.579Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;RCLONE_EXTRA_ARGS\u0026quot;} 2020-03-01T11:57:57.579Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;XBSTREAM_EXTRA_ARGS\u0026quot;} 2020-03-01T11:57:57.579Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;XTRABACKUP_EXTRA_ARGS\u0026quot;} 2020-03-01T11:57:57.579Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;XTRABACKUP_PREPARE_EXTRA_ARGS\u0026quot;} 2020-03-01T11:57:57.579Z\tINFO\tsidecar\tenvironment is not set\t{\u0026quot;key\u0026quot;: \u0026quot;XTRABACKUP_TARGET_DIR\u0026quot;} 2020-03-01T11:57:57.579Z\tINFO\tsidecar\tcloning command\t{\u0026quot;host\u0026quot;: \u0026quot;mycluster-mysql-cluster-db-mysql-0\u0026quot;} 2020-03-01T11:57:57.581Z\tINFO\tsidecar\tcloning from node\t{\u0026quot;host\u0026quot;: \u0026quot;mycluster-mysql-cluster-db-mysql-replicas\u0026quot;} 2020-03-01T11:57:57.581Z\tINFO\tsidecar\tinitialize a backup\t{\u0026quot;host\u0026quot;: \u0026quot;mycluster-mysql-cluster-db-mysql-replicas\u0026quot;, \u0026quot;endpoint\u0026quot;: \u0026quot;/xbackup\u0026quot;} xtrabackup: recognized server arguments: --innodb_checksum_algorithm=crc32 --innodb_log_checksum_algorithm=strict_crc32 --innodb_data_file_path=ibdata1:12M:autoextend --innodb_log_files_in_group=2 --innodb_log_file_size=50331648 --innodb_fast_checksum=0 --innodb_page_size=16384 --innodb_log_block_size=512 --innodb_undo_directory=./ --innodb_undo_tablespaces=0 --server-id=102 --redo-log-version=1 xtrabackup: recognized client arguments: --prepare=1 --target-dir=/var/lib/mysql xtrabackup version 2.4.18 based on MySQL server 5.7.26 Linux (x86_64) (revision id: 29b4ca5) ... ... 正しく復帰しました。そして、 replica 専用 Service から xtrabackup で取得しています。ヤッタネ 😉\nMaster 障害時の動作確認 # Master になっていた mysql-1 を delete pod すると程なくして mysql client からのアクセスがエラーとなりました。ERROR 2003 (HY000): Can't connect to MySQL server on '10.109.199.43' (111)。これが15秒程度続き、mysql-2 が Master に昇格しました。ここで3秒程度は Read Only 状態でしたが、その後 Writable に変更されました。\nmysql-1 が Master で正常な状態から開始。\n$ ./orchestrator-client -c topology -a mycluster-mysql-cluster-db.default mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 [0s,ok,5.7.26-29-log,rw,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] mysql-1 にアクセスできなくなった初期状態です。\n$ ./orchestrator-client -c topology -a mycluster-mysql-cluster-db.default mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 [unknown,invalid,5.7.26-29-log,rw,ROW,\u0026gt;\u0026gt;,GTID] - mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 [null,nonreplicating,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] - mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 [null,nonreplicating,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] mysql-2 を Master に昇格させたところ。まだ ReadOnly のままです。\n$ ./orchestrator-client -c topology -a mycluster-mysql-cluster-db.default mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 [null,nonreplicating,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 [14s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] mysql-2 は Writable になり、再起動してきた mysql-1 は replica として初期化されました。しかし、新 Master の mysql-2 にはまだ slave としての設定が残っているため、null,noreplicationg となっています。自動で処理されるのはここまでです。\n$ ./orchestrator-client -c topology -a mycluster-mysql-cluster-db.default mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 [null,nonreplicating,5.7.26-29-log,rw,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] mysql-2 から slave 設定を削除するために reset-replica コマンドを実行する。これは手動で実行します。\n$ ./orchestrator-client -c reset-replica -i mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 これでキレイな状態になりました。\n$ ./orchestrator-client -c topology -a mycluster-mysql-cluster-db.default mycluster-mysql-cluster-db-mysql-2.mysql.default:3306 [0s,ok,5.7.26-29-log,rw,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-0.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] + mycluster-mysql-cluster-db-mysql-1.mysql.default:3306 [0s,ok,5.7.26-29-log,ro,ROW,\u0026gt;\u0026gt;,GTID] Master が何かおかしいけど orchestrator で検知できないような場合は手動で force-master-failover か force-master-takeover コマンドを実行する必要があります。\n今回はここまで。\n","date":"2020年3月1日","permalink":"/2020/03/presslabs-mysql-operator-part2/","section":"Posts","summary":"Orchestracot の CLI 操作 # 前回に引き続き、Presslabs の mysql-operator です。今回は failover 周りの動作を確認します。Orchestrator の操作は Web UI からも操作できま","title":"Presslabs の mysql-operator (part2)"},{"content":"Vitess の helm の完成後がまだまだっぽくて Issue を上げたりもしてるけど Shard 不要な要件なら他の選択肢も持っておくべきだろうなということで探してみると mysql-operator が Oracle からと Presslabs から公開されていた。Oracle の方は開発が停滞しているっぽいので除外。Presslabs は WordPress のホスティング業者で wordpress-operator も公開されている。利用者のサイト単位で wordpress と mysql がデプロイされるっぽい。\nデプロイ方法は Getting started with MySQL operator にあります。\nとりあえず動かすだけなら helm で mysql-operator をインストールして\nhelm repo add presslabs https://presslabs.github.io/charts helm install presslabs/mysql-operator --name mysql-operator ユーザー、パスワード用の Secrets (example-cluster-secret.yaml) を登録後に kind: MysqlCluster を apply するだけです。\nkubectl apply -f https://raw.githubusercontent.com/presslabs/mysql-operator/master/examples/example-cluster-secret.yaml kubectl apply -f https://raw.githubusercontent.com/presslabs/mysql-operator/master/examples/example-cluster.yaml example-cluster.yaml はコメントを除くとこれだけです。コメントを見るといろいろ調整できそうなことがわかります。\napiVersion:mysql.presslabs.org/v1alpha1kind:MysqlClustermetadata:name:my-clusterspec:replicas:2secretName:my-secretMinikube への deploy # Minikube の起動 # パラメータは適当に。\nminikube start \\  --kubernetes-version=v1.15.7 \\  --cpus=4 \\  --memory=8gb \\  --disk-size=20gb \\  --vm-driver=hyperkit helm の install # helm 3 にも対応しているようですが、ここでは都合により helm 2 を使っています。\nhelm バイナリのダウンロードは GitHub などから。\nkubectl -n kube-system get serviceaccount tiller \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \\  || kubectl -n kube-system create serviceaccount tiller kubectl get clusterrolebindings tiller \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \\  || kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller helm list \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || helm init --service-account tiller --wait PersistentVolume の作成 # MySQL の Data ディレクトリにも使えますが、mysql-operator 内の orchestrator が SQLite のファイルを保存する先として使います。\nfor i in $(seq 5); do pvname=$(printf pv%04d $i) echo -e \u0026#34;--- apiVersion: v1 kind: PersistentVolume metadata: name: ${pvname}spec: accessModes: - ReadWriteOnce capacity: storage: 10Gi hostPath: path: /data/${pvname}/ storageClassName: standard persistentVolumeReclaimPolicy: Recycle\u0026#34; done | kubectl apply -f - minikube ssh \u0026#39;for i in $(seq 5); do pvname=$(printf pv%04d $i) sudo install -m 0777 -d /mnt/vda1/data/${pvname} done\u0026#39; mysql-operator の deploy # ここでは minikube なので orchestrator の Web UI へのアクセスに NodePort を使うように mysql-operator の deploy 時にちょっとカスタマイズしてみます。次の内容の mysql-operator-values.yaml というファイルを用意して\norchestrator:service:type:NodePortport:80config:Debug:trueRecoveryPeriodBlockSeconds:60次のコマンドで deploy します。\nhelm repo add presslabs https://presslabs.github.io/charts helm install presslabs/mysql-operator --name mysql-operator -f mysql-operator-values.yaml これで mysqlbackups.mysql.presslabs.org と mysqlclusters.mysql.presslabs.org という2つの Custom Resource Definitions (CRD) が作成されています。また、mysql-operator と orchestrator を含む StatefulSets が deploy されています。\nMySQL クラスタの depeloy # 最初に書いたように kubectl apply でも deploy 可能ですが、 mysql-operator を使って MySQL クラスタを deploy するための mysql-cluster chart もあるのでこれを使います。\n次の内容で mysql-cluster-values.yaml というファイルを作成します。（ファイル名は何でも良いですが）\n# MySQL 3つのクラスタreplicas:3rootPassword:\u0026#34;mypass\u0026#34;# また懲りずに wordpress を動かしてみようと思うのでappUser:\u0026#34;wpapp\u0026#34;appPassword:\u0026#34;password\u0026#34;appDatabase:\u0026#34;wordpress\u0026#34;# 動作確認だけなので要求するメモリ量をデフォルトより小さくしておくpodSpec:resources:requests:memory:512Mcpu:200m# S3 へのバックアップ設定、確認のために10分おきに実行# 最新の5個だけ残す# (バックアップしないなら全部未定義で良い)backupSchedule:\u0026#34;0 */10 * * * *\u0026#34;backupScheduleJobsHistoryLimit:5backupURL:s3://YOUR-BUCKET-NAME/backupRemoteDeletePolicy:deletebackupCredentials:AWS_ACCESS_KEY_ID:AKIBAZGQUWZBKTIL5QRBAWS_SECRET_ACCESS_KEY:****************************************AWS_REGION:ap-northeast-1helm install presslabs/mysql-cluster --name mycluster -f mysql-cluster-values.yaml すると、しばらくして次のような状況になります。\n$ kubectl get all,secrets,configmaps NAME READY STATUS RESTARTS AGE pod/mycluster-mysql-cluster-db-mysql-0 4/4 Running 0 6m4s pod/mycluster-mysql-cluster-db-mysql-1 4/4 Running 0 4m33s pod/mycluster-mysql-cluster-db-mysql-2 4/4 Running 0 3m42s pod/mysql-operator-0 2/2 Running 0 52m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 443/TCP 36h service/mycluster-mysql-cluster-db-mysql ClusterIP 10.105.17.16 3306/TCP 6m4s service/mycluster-mysql-cluster-db-mysql-master ClusterIP 10.98.186.169 3306/TCP 6m4s service/mysql ClusterIP None 3306/TCP,9125/TCP 6m4s service/mysql-operator NodePort 10.108.47.3 80:32328/TCP 52m service/mysql-operator-0-svc ClusterIP 10.102.82.83 80/TCP,10008/TCP 52m NAME READY AGE statefulset.apps/mycluster-mysql-cluster-db-mysql 3/3 6m4s statefulset.apps/mysql-operator 1/1 52m NAME TYPE DATA AGE secret/default-token-92lnd kubernetes.io/service-account-token 3 36h secret/mycluster-mysql-cluster-db Opaque 5 6m4s secret/mycluster-mysql-cluster-db-backup Opaque 3 6m4s secret/mycluster-mysql-cluster-db-mysql-operated Opaque 10 6m4s secret/mysql-operator-orc Opaque 2 52m secret/mysql-operator-token-h82jw kubernetes.io/service-account-token 3 52m NAME DATA AGE configmap/mycluster-mysql-cluster-db-mysql 1 6m4s configmap/mysql-operator-leader-election 0 52m configmap/mysql-operator-orc 2 52m Database 確認 # service/mycluster-mysql-cluster-db-mysql-master の 3306/tcp にアクセスすれば Master の MySQL に接続することができます。-master suffix がつかない方の Service は Master も含んだ healthy=yes の Pod の集合です。\nkubectl run mysql-client --rm --restart=Never \\  --image=mysql:5.7 -it --generator=run-pod/v1 --command -- \\  mysql -h mycluster-mysql-cluster-db-mysql-master -u root -pmypass 指定した wordpress というデータベースの他に sys_operator というデータベースが存在します。\nmysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | sys_operator | | wordpress | +--------------------+ 6 rows in set (0.00 sec) この sys_operator は mysql-operator の中で監視やら起動確認に使われています。\nmysql\u0026gt; show tables; +------------------------+ | Tables_in_sys_operator | +------------------------+ | heartbeat | | status | +------------------------+ 2 rows in set (0.00 sec) Orchestrator の Web UI を確認する # mysql-operator はクラスタの topology 管理に Orchestrator を採用しています。Orchestrator については \u0026ldquo;「MySQL High Availability tools」のフォローアップとorchestratorの追加\u0026rdquo; かその原文(作者著)が詳しいです。\nこれには Web UI がついているので次のコマンドでアクセスしてみます。これでアクセス出来るように operator の deploy 時に NodePort に変更しました。\nminikube service mysql-operator Orchestrator の Dashboard にはクラスタの一覧が表示されます。mysql-operator で deploy されるクラスタは一組の orchestrator で管理されるのでここに追加されていきます。\n クラスタを選択すると replication の topology が見えます。これは 3 node クラスタの例。\n mysql-operator の helm chart は replica 数がデフォルトで1ですが、orchestrator は Raft でクラスタが組めるようになっているため冗長構成にもできそうです。\n先ほどの sys_operator.heartbeat テーブルは orchestrator の SlaveLagQuery という設定から参照されています。\n続く # 長くなったので failover とか backup とかは別記事にしよう。\n続き\nおまけ # Vitess は PlanetScale の vitess-operator の方が出来が良さそうです。こちらは彼らが Vitess をサービスとして提供しておりそこで使われているものをベースに彼らの infra に依存する部分を無くして公開しているようです。まだ PRE-RELEASE 状態だということですが。\n","date":"2020年2月29日","permalink":"/2020/02/presslabs-mysql-operator-part1/","section":"Posts","summary":"Vitess の helm の完成後がまだまだっぽくて Issue を上げたりもしてるけど Shard 不要な要件なら他の選択肢も持っておくべきだろうなということで探してみると mysql-operator が Oracle から","title":"Presslabs の mysql-operator (part1)"},{"content":"昔、メールサーバーを管理していた時から使ってた Perl で書いた smtp クライアントがどこに行ったかわからなくなったけど、確か mailx コマンドで出来るっぽかったなということで試してみた。他にも github に何か便利そうなツールがあった気がするけど探し出せない\u0026hellip;\n試した環境は AmazonLinux 2 です。mailx コマンドは mailx というパッケージに入っています。\n$ rpm -qf $(which mailx) mailx-12.5-19.amzn2.x86\\_64 Google アカウントで SMTP サーバーを使ってメールを送る方法は「プリンタ、スキャナ、アプリからのメール送信」にあります。\n送信方法は次の通り。465/tcp の SMTP over SSL/TLS には非対応かな。\necho メッセージ本文 \\\\ | mailx -n -v \\\\ -S smtp=smtp.gmail.com:587 \\\\ -S smtp-auth=plain \\\\ -S smtp-auth-user=username@gmail.com \\\\ -S smtp-auth-password=アプリパスワード\\\\ -S smtp-use-starttls \\\\ -S ssl-verify=ignore \\\\ -S nss-config-dir=/etc/pki/nssdb \\\\ -S from=\u0026quot;表示名 \u0026lt;username@gmail.com\u0026gt;\u0026quot; \\\\ -s \u0026quot;サブジェクト\u0026quot; \\\\ -c CCメールアドレス \\\\ -b BCCメールアドレス -a 添付ファイルのpath \\\\ 宛先メールアドレス 日本語など us-ascii 外のバイト列が含まれる場合は自動で charset を utf-8 にしてくれますし、添付ファイルの Content-Type もバイナリなら octet-stream になるし、テキストファイルなら text/plain になります。ヘッダー内の非 ascii は utf-8 の Base64 encode にしてくれます。ステキ。\nアプリパスワードについては「アプリ パスワードでログイン」を参照\nsmtp.gmail.com の EHLO のレスポンスです。通常の SMTP サーバーでは見かけない認証に対応してますね。\n250-smtp.gmail.com at your service, \\[203.0.113.10\\] 250-SIZE 35882577 250-8BITMIME 250-AUTH LOGIN PLAIN XOAUTH2 PLAIN-CLIENTTOKEN OAUTHBEARER XOAUTH 250-ENHANCEDSTATUSCODES 250-PIPELINING 250-CHUNKING 250 SMTPUTF8 ","date":"2020年2月28日","permalink":"/2020/02/send-a-mail-with-mailx/","section":"Posts","summary":"昔、メールサーバーを管理していた時から使ってた Perl で書いた smtp クライアントがどこに行ったかわからなくなったけど、確か mailx コマンドで出来るっぽかった","title":"mailx コマンドでメール送信テスト"},{"content":"","date":"2020年2月28日","permalink":"/tags/smtp/","section":"Tags","summary":"","title":"SMTP"},{"content":"","date":"2020年2月16日","permalink":"/tags/vitess/","section":"Tags","summary":"","title":"vitess"},{"content":"最近、目にすることの増えた Vitess ですが、Tutorial を試してみてもなかなか分かった気になれません。Sharding するとそれによる制限は受けそうだなというのと、実際にクエリを投げてみて SELECT * すると ORDER BY が使えない（SELECT で列を明示する必要がある）とか Sharding の key とした列を WHERE で指定するとちゃんとそれを持ってる tablet にだけ投げてくれる IN で複数指定してもその tablet のものだけにして投げてくれるとか、tablet を跨ぐ JOIN をすると Nested Loop がだいぶ辛そうだなというのは分かったけど。動かしたいアプリで必要なクエリに vtgate が対応しているのかは実際に動かしてみるしかありません。\nところで手元に動かしたいアプリがありません\u0026hellip;\n私の思いつく最近の OSS では PostgreSQL を採用しているものが多く、WordPress を動かしてみることにしました。（コード読むの辛そうだからもっとシンプルなのが良かったけど）\nまずは Sharding なしで動くかどうかを確認。\nminikube で Kubernetes 環境を用意 # ここでのポイントは vitess の helm chart がまだ Kuberntes 1.16 以降に対応していないため 1.15 を指定している点。\n$ minikube start \\  --kubernetes-version=1.15.7 \\  --cpus=4 \\  --memory=6g minikube の version は 1.7.1 でした。\n$ minikube version minikube version: v1.7.1 commit: 7de0325eedac0fbe3aabacfcc43a63eb5d029fda helm の準備 # helm 2系です。3 系にはまだ未対応のようです。\n$ kubectl -n kube-system create serviceaccount tiller $ kubectl create clusterrolebinding tiller \\  --clusterrole cluster-admin --serviceaccount=kube-system:tiller $ helm init --service-account tiller --wait tiller のセットアップには minikube の addon を使うという手もあります。\netcd-operator の deploy # ZooKeeper と Consul にも対応しているようですが、今のデフォルトは etcd みたいです。helm もそれ前提です。\n$ git clone https://github.com/coreos/etcd-operator.git $ cd etcd-operator $ ./example/rbac/create_role.sh $ kubectl create -f example/deployment.yaml Persistent Volume の作成 # Vitess の tablet で使われる MySQL がデータを置く場所と WordPress 用が必要です。MySQL 用は今回の記事の範囲では Master, Replica, Backup 用の3つ、WordPress 用に1つ。\nfor i in $(seq 4); do pvname=$(printf pv%04d $i) echo -e \u0026#34;--- apiVersion: v1 kind: PersistentVolume metadata: name: ${pvname}spec: accessModes: - ReadWriteOnce capacity: storage: 10Gi hostPath: path: /data/${pvname}/ storageClassName: standard persistentVolumeReclaimPolicy: Recycle\u0026#34; done | kubectl apply -f - これで pv0001 から pv0004 まで作成されます。\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv0001 10Gi RWO Recycle Available standard 7s pv0002 10Gi RWO Recycle Available standard 7s pv0003 10Gi RWO Recycle Available standard 7s pv0004 10Gi RWO Recycle Available standard 7s pv0005 10Gi RWO Recycle Available standard 7s Persistent Volume の Permission 設定 # もっと良い方法が会ったら知りたいのだけれど、tablet の init container が mkdir するところで権限がなくてコケてしまうので、minikube サーバー上の Persistent Volume 用ディレクトリの owner を変更する。\nPersistent Volume Claim を受けて、割り当てる時に owner が root のディレクトリが作成されるのだけれど先に作っておく。MySQL の実行ユーザーの uid が 1000 だったのでディレクトリの owner を 1000 にしておく。\nminikube ssh \u0026#39;for i in $(seq 5); do pvname=$(printf pv%04d $i) sudo install -o 1000 -m 0755 -d /mnt/vda1/data/${pvname} done\u0026#39; WordPress の方は 1000 じゃなくても良いのだけれどどれが割り当てられるかわからないし、1000 でも問題なさそうなので全部 1000 にしておく。\nちなみに minikube じゃなくて EKS とか GKE であればこの作業は不要。\nvtgate 用パスワードのための Secrets 登録 # MySQL クライアントが接続する先は Vitess の vtgate というサーバーで、認証もここで行われます。helm での deploy 時に使われるので Secrets にパスワードを登録する。\n$ cat \u0026gt; wordpress-password-secret.yml \u0026lt;\u0026lt; _EOF_ apiVersion: v1 kind: Secret metadata: name: wordpress-password type: Opaque data: password: aG9nZWhvZ2U= _EOF_ $ kubectl apply -f wordpress-password-secret.yml aG9nZWhvZ2U= は hogehoge の base64 です。echo -n hogehoge | base64\nVitess の deploy # Vitess の tutorial でも使われている helm chart を使います。\nhelm chart は Vitess の git repository に入っているので　clone します。\n$ git clone https://github.com/vitessio/vitess.git $ cd vitess/example/helm この example/helm ディレクトリには tutorial で使う helm の　variables ファイルが置かれています。ここにある 101_initial_cluster.yaml をコピーしてちょっといじって使います。vitess-wordpress-init.yaml というファイル名とします。\n# vitess-wordpress-init.yamltopology:cells:- name:\u0026#34;zone1\u0026#34;etcd:replicas:1vtctld:replicas:1vtgate:replicas:1mysqlProtocol:enabled:trueauthType:\u0026#34;secret\u0026#34;username:wpapppasswordSecret:wordpress-passwordkeyspaces:- name:\u0026#34;wordpress\u0026#34;shards:- name:\u0026#34;0\u0026#34;tablets:- type:\u0026#34;replica\u0026#34;vttablet:replicas:2- type:\u0026#34;rdonly\u0026#34;vttablet:replicas:1etcd:replicas:1resources:vtctld:serviceType:\u0026#34;NodePort\u0026#34;resources:vtgate:serviceType:\u0026#34;NodePort\u0026#34;resources:vttablet:mysqlSize:\u0026#34;prod\u0026#34;resources:mysqlResources:vtworker:resources:pmm:enabled:falseorchestrator:enabled:falsetable 作成は WordPress アプリに任せるので keyspaces 内の schema, vschema は削除しました。keyspace (database) 名は commerce から wordpress に変更しました。vtgate の認証を有効にするため mysqlProtocol の authType を \u0026ldquo;secret\u0026rdquo; にし、username, passwordSecret を追加しました。passwordSecret は先に作成した Kubernets の Secrets の名前です。\nhelm install コマンドで deploy します。\n$ helm install ../../helm/vitess -f vitess-wordpress-init.yaml これで、wordpress という keyspace (database) が作成され、master と semi-synchronous な replica 1つと async な repolica (rdonly) 1つのクラスタが作成されます。\nしばらく、待っていると次のような状態になります。\n$ kubectl get pods,jobs NAME READY STATUS RESTARTS AGE pod/etcd-global-7lhmznmvld 1/1 Running 0 2m21s pod/etcd-operator-866875d5dc-8btrw 1/1 Running 0 18m pod/etcd-zone1-vcjkdtkrdv 1/1 Running 0 2m21s pod/vtctld-8547867c9c-jrmw9 1/1 Running 3 2m21s pod/vtgate-zone1-774b6c87d5-96ngl 1/1 Running 3 2m21s pod/zone1-wordpress-0-init-shard-master-jl7c5 1/1 Running 0 2m21s pod/zone1-wordpress-0-rdonly-0 4/6 Running 0 2m21s pod/zone1-wordpress-0-replica-0 4/6 Running 0 2m21s pod/zone1-wordpress-0-replica-1 4/6 Running 0 2m21s NAME COMPLETIONS DURATION AGE job.batch/zone1-wordpress-0-init-shard-master 0/1 2m21s 2m21ss zone1-wordpress-0-replica という statefulset が master と semi-synchronous な replica です。{cell}-{keyspace}-{shard}-replica という命名規則となっています。\nサービスはこうです。WordPress からの接続先は vtgate-zone1:3306 です。\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE etcd-global ClusterIP None 2379/TCP,2380/TCP 3m23s etcd-global-client ClusterIP 10.99.214.68 2379/TCP 3m23s etcd-zone1 ClusterIP None 2379/TCP,2380/TCP 3m23s etcd-zone1-client ClusterIP 10.96.178.199 2379/TCP 3m23s kubernetes ClusterIP 10.96.0.1 443/TCP 21m vtctld NodePort 10.103.67.88 15000:31292/TCP,15999:32327/TCP 3m23s vtgate-zone1 NodePort 10.104.197.56 15001:31352/TCP,15991:32133/TCP,3306:32651/TCP 3m23s vttablet ClusterIP None 15002/TCP,16002/TCP 3m23s minikube の外からアクセスするには nodeport を確認する必要があります。minikube には service list というコマンドがあります。\n$ minikube service list |-------------|--------------------|--------------------------------|-----| | NAMESPACE | NAME | TARGET PORT | URL | |-------------|--------------------|--------------------------------|-----| | default | etcd-global | No node port | | default | etcd-global-client | No node port | | default | etcd-zone1 | No node port | | default | etcd-zone1-client | No node port | | default | kubernetes | No node port | | default | vtctld | http://192.168.64.11:31292 | | | | http://192.168.64.11:32327 | | default | vtgate-zone1 | http://192.168.64.11:31352 | | | | http://192.168.64.11:32133 | | | | http://192.168.64.11:32651 | | default | vttablet | No node port | | kube-system | kube-dns | No node port | | kube-system | tiller-deploy | No node port | |-------------|--------------------|--------------------------------|-----| が、protocl が不明です。全部 http:// となっていますが、嘘です・・・\n次の様にしてアクセスすることが出来ます。\nhost=$(minikube ip) port=$(kubectl describe service vtgate-zone1 | grep NodePort | grep mysql | awk \u0026#39;{print $3}\u0026#39; | awk -F\u0026#39;/\u0026#39; \u0026#39;{print $1}\u0026#39;) mysql -h $host -P $port -u wpapp -phogehoge wordpress ほぼ、普通の MySQL サーバーの様にアクセスできます。\nmysql\u0026gt; select version(); +---------------+ | version() | +---------------+ | 5.7.26-29-log | +---------------+ 1 row in set (0.01 sec) mysql\u0026gt; show databases; +-----------+ | Databases | +-----------+ | wordpress | +-----------+ 1 row in set (0.01 sec) mysql\u0026gt; WordPress を deploy する # DB の準備ができたので次は WordPress を deploy します。Kubernetes のサイトに Example: Deploying WordPress and MySQL with Persistent Volumes という StatefulSet として WordPress を deploy する例があったので、ここの wordpress-deployment.yaml を参考にします。\n# wordpress-deployment.yaml---apiVersion:v1kind:Servicemetadata:name:wordpresslabels:app:wordpressspec:ports:- port:80selector:app:wordpresstier:frontendtype:NodePort---apiVersion:v1kind:PersistentVolumeClaimmetadata:name:wp-pv-claimlabels:app:wordpressspec:accessModes:- ReadWriteOnceresources:requests:storage:10Gi---apiVersion:apps/v1# for versions before 1.9.0 use apps/v1beta2kind:Deploymentmetadata:name:wordpresslabels:app:wordpressspec:selector:matchLabels:app:wordpresstier:frontendstrategy:type:Recreatetemplate:metadata:labels:app:wordpresstier:frontendspec:containers:- image:wordpress:5.3.2-php7.2-apachename:wordpressenv:- name:WORDPRESS_DB_HOSTvalue:vtgate-zone1- name:WORDPRESS_DB_USERvalue:wpapp- name:WORDPRESS_DB_PASSWORDvalueFrom:secretKeyRef:name:wordpress-passwordkey:passwordports:- containerPort:80name:wordpressvolumeMounts:- name:wordpress-persistent-storagemountPath:/var/www/htmlvolumes:- name:wordpress-persistent-storagepersistentVolumeClaim:claimName:wp-pv-claimimage を Docker Hub の最新のものにしました。環境変数の WORDPRESS_DB_HOST, WORDPRESS_DB_USER を Vitess 側で設定したものにしました。WORDPRESS_DB_PASSWORD は Secrets を参照していますが、これも Vitess 側で使ったものを指定しました。PersistentVolumeClaim はサイズが 20Gi になっていましたが、事前に作成していた PV のサイズを超えているので 10Gi に変更しました。Service の type を LoadBalancer から NodePort に変更しました。\ndeploy します。\n$ kubectl apply -f wordpress-deployment.yaml これで起動を待って minikube service wordpress とすると NodePort の URL をブラウザで開いてくれます。\n$ minikube service wordpress |-----------|-----------|-------------|----------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|-----------|-------------|----------------------------| | default | wordpress | | http://192.168.64.11:30803 | |-----------|-----------|-------------|----------------------------| 🎉 Opening service default/wordpress in default browser... 無事起動しました。\nが、言語選択して、ブログのタイトルやユーザー名、パスワードを入力して先に進むと「成功しました！」という表示と共に見慣れないエラーが\u0026hellip;\n [vtgate: http://vtgate-zone1-774b6c87d5-96ngl:15001/: target: wordpress.0.master, used tablet: zone1-1372366900 (zone1-wordpress-0-replica-0.vttablet): vttablet: rpc error: code = Unimplemented desc = unsupported: cannot identify primary key of statement (CallerID: wpapp)]\nrpc error: code = Unimplemented desc = unsupported: cannot identify primary key of statement Primary key が見つけられないってことか？エラーになったのは次の2つの SQL。見慣れないクエリだ。\nDELETEa,bFROMwp_optionsa,wp_optionsbWHEREa.option_nameLIKE\u0026#39;\\\\_transient\\\\_%\u0026#39;ANDa.option_nameNOTLIKE\u0026#39;\\\\_transient\\\\_timeout\\\\_%\u0026#39;ANDb.option_name=CONCAT(\u0026#39;_transient_timeout_\u0026#39;,SUBSTRING(a.option_name,12))ANDb.option_value\u0026lt;1581760340DELETEa,bFROMwp_optionsa,wp_optionsbWHEREa.option_nameLIKE\u0026#39;\\\\_site\\\\_transient\\\\_%\u0026#39;ANDa.option_nameNOTLIKE\u0026#39;\\\\_site\\\\_transient\\\\_timeout\\\\_%\u0026#39;ANDb.option_name=CONCAT(\u0026#39;_site_transient_timeout_\u0026#39;,SUBSTRING(a.option_name,17))ANDb.option_value\u0026lt;1581760340この SQL を投げているのは delete_expired_transients() でしたが、MySQL に直接投げてみてもマッチするレコードは存在しないのでとりあえず無視して先に進みます。\n次は Dashboard の表示です。\n 画面上にエラーは表示されませんでしたが、apache の error_log に沢山エラーが出てました。\n1行目の PHP Warning: mysqli_query(): Error reading result set's header in /var/www/html/wp-includes/wp-db.php on line 2030 が後続のエラーを引き起こしてるのかな？wp-includes/wp-db.php の 2030行目 という情報からでは追いかけるのが厳しいのでまたの機会に調べてみようかな。\n追記 # vttablet のログに次のものがありました。\ntabletserver.go:1643] Incorrect string value: '\\xF0\\x9F\\x99\\x82\u0026quot; ...' for column 'option_value' at row 1 (errno 1366) (sqlstate HY000) (CallerID: wpapp): Sql: \u0026quot;insert into wp_options(option_name,... 「Incorrect string value: 🙂\u0026quot; \u0026hellip;」character set が utf8mb4 になってない問題か？でも、この文字自体は MySQL に直接 INSERT することは可能だな。\n余談ですが Apache のエラーログはセキュリティのために printable な ASCII 意外はエスケープして \\x と16進のコードで出力されてしまいます。元はなんだったのかな？ってこれを変換するスクリプトでも書こうかと思ったのですが、これ、zsh なら echo に渡すだけで良かったんですね！！ (追記: bash でも echo -e で同じことができました)\n$ echo 'WordPress \\xe3\\x83\\x87\\xe3\\x83\\xbc\\xe3\\x82\\xbf\\xe3\\x83\\x99\\xe3\\x83\\xbc\\xe3\\x82\\xb9\\xe3\\x82\\xa8\\xe3\\x83\\xa9\\xe3\\x83\\xbc' WordPress データベースエラー で、さっきのエラーログも warning と notice だし、致命的ではなかったので先に進んで記事を投稿してみます。\n 無事投稿して表示も確認できました。WordPress を動かすのは難しいかな？なんて思ってたんですが意外にも動きましたね。\nそうそう、管理画面のメディアページでもエラーが出ました。\nSELECTSQL_CALC_FOUND_ROWSwp_posts.IDFROMwp_postsWHERE1=1ANDwp_posts.post_type=\u0026#39;attachment\u0026#39;AND((wp_posts.post_status=\u0026#39;inherit\u0026#39;ORwp_posts.post_status=\u0026#39;private\u0026#39;))ORDERBYwp_posts.post_dateDESCLIMIT0という SQL で syntax error となりました。SQL_CALC_FOUND_ROWS に対応していないようです。それはそうと数を数えるだけなのになんで ORDER BY なんかついてるのかな。\nMySQL との互換性の情報は MySQL Compatibility にありました。\nおまけ # MySQL に直接接続する方法 # 複数コンテナが入っているので -c で mysql コンテナを指定します。\n$ kubectl exec -itc mysql zone1-wordpress-0-replica-0 -- mysql --socket=/vtdataroot/tabletdata/mysql.sock -u root MySQL 側でのクエリ確認 # Vitess の helm で deploy される Pod はファイルに出力される error.log, slow-query.log, general.log をそれぞれ tail -F して stdout に流すコンテナがいる（rotation させるのも別途いる）んですが、general.log は MySQL 側で設定されてないため、起動後に MySQL にアクセスして設定してやる必要があります。\n上の方法で MySQL にアクセスしたら次の設定をします。\nset global general_log_file = '/vtdataroot/tabletdata/general.log'; set global general_log = on; まとめ # Tutorial 試しても楽しくなかったので WordPress を動かしてみました。意外と動きましたね、でもやっぱり vtgate でサポートされてないクエリも使われてますね。ここから Sharding とか Backup や障害復旧などを試していこうかなと。vtctlclient コマンドの使い方とか VReplication とか Topology Service とかまだ全然わからない。\n","date":"2020年2月16日","permalink":"/2020/02/wordpress-with-vitess/","section":"Posts","summary":"最近、目にすることの増えた Vitess ですが、Tutorial を試してみてもなかなか分かった気になれません。Sharding するとそれによる制限は受け","title":"Vitess で WordPress を動かしてみる"},{"content":"","date":"2020年2月11日","permalink":"/tags/bash/","section":"Tags","summary":"","title":"bash"},{"content":"Qualys SSL Server Test みたいなコマンドラインツール (testssl.sh) が cron.weekly で紹介されていたのでメモ。\nPublic には公開してない (あるいは公開前の) サイトでのテストに使えるかもね。(1MB 近い bash script 😅)\n","date":"2020年2月11日","permalink":"/2020/02/testssl/","section":"Posts","summary":"Qualys SSL Server Test みたいなコマンドラインツール (testssl.sh) が cron.weekly で紹介されていたのでメモ。 Public には公開してない (あるいは公開前の) サイトでのテストに使えるかもね。(","title":"Qualys の SSL Server Test みたいなコマンドラインツール"},{"content":"","date":"2020年1月25日","permalink":"/tags/gcloud/","section":"Tags","summary":"","title":"gcloud"},{"content":"Google Cloud SDK の gcloud で複数のプロジェクトを切り替えたり、さらには複数アカウントを切り替えながら使う方法を調べた。\ngcloud init # gcloud init としてブラウザで OAuth2 認証して、プロジェクトとデフォルトのゾーンを選択して設定が作成される。\nこれは default という configuration が作られていて gcloud config configurations list で確認することができる。gcloud の設定ファイルは ~/.config/gcloud 配下に作られている。\n$ gcloud config configurations list NAME IS_ACTIVE ACCOUNT PROJECT DEFAULT_ZONE DEFAULT_REGION default True **********@gmail.com my-project-****** asia-northeast1-b asia-northeast1 ここで表示されている格値は gcloud config get-value で確認できる。\n$ gcloud config get-value account **********@gmail.com $ gcloud config get-value project my-project-**** $ gcloud config get-value compute/zone asia-northeast1-b $ gcloud config get-value compute/region asia-northeast1 region や zone はサービスごとの設定の様で上の DEFAULT_ZOONE, DEFAULT_REGION は compute のものである。他は us になってたり未指定だったりする。\n$ gcloud config get-value functions/region us-central1 $ gcloud config get-value run/region (unset) アカウント # 認証済みのアカウントは gcloud auth list で確認できる。\n$ gcloud auth list Credentialed Accounts ACTIVE ACCOUNT * **********@gmail.com To set the active account, run: $ gcloud config set account `ACCOUNT` ここで gcloud auth login とすれば別のアカウントも追加することができ、その後アカウントを切り替えながら使うことができる。\ngcloud auth list Credentialed Accounts ACTIVE ACCOUNT **********@gmail.com * xxxxxxxxxx@example.com To set the active account, run: $ gcloud config set account `ACCOUNT` gcloud auth login すると次のような warning が表示された。gsutil とかアプリで使うための json のやつが必要な場合は別途コマンド実行してねということのよう。\nWARNING: `gcloud auth login` no longer writes application default credentials. If you need to use ADC, see: gcloud auth application-default --help ところで、gcloud auth login で新たに別アカウントでログインしたら default 設定のアカウントが切り替わってしまった。\n$ gcloud config configurations list NAME IS_ACTIVE ACCOUNT PROJECT DEFAULT_ZONE DEFAULT_REGION default True xxxxxxxxxx@example.com my-project-****** asia-northeast1-b asia-northeast1 これは gcloud config set で変更することができる。\n$ gcloud config set account **********@gmail.com Updated property [core/account]. が、私の欲しいのはこれじゃないんですね。アカウントとプロジェクトをセットにして切り替えたい。\n使わなくなったアカウントは revoke で削除することができます。\ngcloud auth revoke [ACCOUNTS ...] [--all] [GCLOUD_WIDE_FLAG ...] アカウントを省略すると今 active になっているアカウントが revoke されます。\ngcloud config configurations # 前項の「アカウントとプロジェクトをセットにして切り替えたい」を実現するための仕組みが configurations でした。\n$ gcloud config configurations ERROR: (gcloud.config.configurations) Command name argument expected. Available commands for gcloud config configurations: activate Activates an existing named configuration. create Creates a new named configuration. delete Deletes a named configuration. describe Describes a named configuration by listing its properties. list Lists existing named configurations. For detailed information on this command and its flags, run: gcloud config configurations --help create で新しいのを作って activate で切り替えます。\ntutorial という configuration を作ってみます。\n$ gcloud config configurations create tutorial Created [tutorial]. Activated [tutorial]. 新しく作られた物が active になっていますが、--no-activate をつけて実行すればこれは抑制できます。\n$ gcloud config configurations list NAME IS_ACTIVE ACCOUNT PROJECT DEFAULT_ZONE DEFAULT_REGION default False **********@gmail.com my-project-****** asia-northeast1-b asia-northeast1 tutorial True でも新しく作った方でアカウントやプロジェクトを指定するので active になってくれた方が便利かな。\n$ gcloud auth list $ gcloud config set account xxxxxxxxxx@example.com $ gcloud projects list | grep tutorial $ gcloud config set project tutorial-xxxxxxx $ gcloud compute zones list | grep asia-northeast $ gcloud config set compute/zone asia-northeast1-b $ gcloud config set compute/region asia-northeast1 これで次の様になる。\n$ gcloud config configurations list NAME IS_ACTIVE ACCOUNT PROJECT DEFAULT_ZONE DEFAULT_REGION default False **********@gmail.com my-project-****** asia-northeast1-b asia-northeast1 tutorial True xxxxxxxxxx@example.com tutorial-xxxxxxx asia-northeast1-b asia-northeast1 後は gcloud config configurations activate xxx で切り替えることができる。project id が覚えにくくても好きな名前をつけて切り替えられるし、プロジェクトによって region が違っても一緒に切り替えられる。\n","date":"2020年1月25日","permalink":"/2020/01/gcloud-switch-account-and-project/","section":"Posts","summary":"Google Cloud SDK の gcloud で複数のプロジェクトを切り替えたり、さらには複数アカウントを切り替えながら使う方法を調べた。 gcloud init # gcloud init としてブラウザで OAuth2 認証して、","title":"gcloud でアカウントやプロジェクトを切り替える"},{"content":"","date":"2020年1月11日","permalink":"/tags/eks/","section":"Tags","summary":"","title":"EKS"},{"content":"eksctl が何をやってくれるのが、何ができるのかを確認します。\neksctl create cluster # いきなり eksctl create cluster を実行するだけでクラスタが作れるっぽいのでひとまず試してみる。\n$ eksctl create cluster \\[ℹ\\] eksctl version 0.12.0 \\[ℹ\\] using region ap-northeast-1 \\[ℹ\\] setting availability zones to \\[ap-northeast-1c ap-northeast-1d ap-northeast-1b\\] \\[ℹ\\] subnets for ap-northeast-1c - public:192.168.0.0/19 private:192.168.96.0/19 \\[ℹ\\] subnets for ap-northeast-1d - public:192.168.32.0/19 private:192.168.128.0/19 \\[ℹ\\] subnets for ap-northeast-1b - public:192.168.64.0/19 private:192.168.160.0/19 \\[ℹ\\] nodegroup \u0026quot;ng-bb178f10\u0026quot; will use \u0026quot;ami-07296175bc6b826a5\u0026quot; \\[AmazonLinux2/1.14\\] \\[ℹ\\] using Kubernetes version 1.14 \\[ℹ\\] creating EKS cluster \u0026quot;beautiful-badger-1578666058\u0026quot; in \u0026quot;ap-northeast-1\u0026quot; region with un-managed nodes \\[ℹ\\] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup \\[ℹ\\] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-1 --cluster=beautiful-badger-1578666058' \\[ℹ\\] CloudWatch logging will not be enabled for cluster \u0026quot;beautiful-badger-1578666058\u0026quot; in \u0026quot;ap-northeast-1\u0026quot; \\[ℹ\\] you can enable it with 'eksctl utils update-cluster-logging --region=ap-northeast-1 --cluster=beautiful-badger-1578666058' \\[ℹ\\] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \u0026quot;beautiful-badger-1578666058\u0026quot; in \u0026quot;ap-northeast-1\u0026quot; \\[ℹ\\] 2 sequential tasks: { create cluster control plane \u0026quot;beautiful-badger-1578666058\u0026quot;, create nodegroup \u0026quot;ng-bb178f10\u0026quot; } \\[ℹ\\] building cluster stack \u0026quot;eksctl-beautiful-badger-1578666058-cluster\u0026quot; \\[ℹ\\] deploying stack \u0026quot;eksctl-beautiful-badger-1578666058-cluster\u0026quot; VPC から一式全部作る CloudFormation の stack (ここでは eksctl-beautiful-badger-1578666058-cluster という名前) が作成されて deploy が開始されました。AWS Console で CloudFormation の stack を確認すれば状況がわかります。ありゃ？エラーが\u0026hellip;\n\\[✖\\] unexpected status \u0026quot;ROLLBACK\\_IN\\_PROGRESS\u0026quot; while waiting for CloudFormation stack \u0026quot;eksctl-beautiful-badger-1578666058-nodegroup-ng-bb178f10\u0026quot; \\[ℹ\\] fetching stack events in attempt to troubleshoot the root cause of the failure \\[!\\] AWS::EC2::SecurityGroupEgress/EgressInterClusterAPI: DELETE\\_IN\\_PROGRESS \\[!\\] AWS::EC2::SecurityGroupIngress/IngressInterClusterAPI: DELETE\\_IN\\_PROGRESS \\[!\\] AWS::EC2::SecurityGroupIngress/IngressInterClusterCP: DELETE\\_IN\\_PROGRESS \\[!\\] AWS::EC2::SecurityGroupEgress/EgressInterCluster: DELETE\\_IN\\_PROGRESS \\[!\\] AWS::AutoScaling::AutoScalingGroup/NodeGroup: DELETE\\_IN\\_PROGRESS \\[!\\] AWS::EC2::SecurityGroupIngress/IngressInterCluster: DELETE\\_IN\\_PROGRESS \\[✖\\] AWS::AutoScaling::AutoScalingGroup/NodeGroup: CREATE\\_FAILED – \u0026quot;AWS was not able to validate the provided access credentials (Service: AmazonAutoScaling; Status Code: 400; Error Code: ValidationError; Request ID: a31c5a54-33b6-11ea-9896-5308bff4139a)\u0026quot; \\[ℹ\\] 1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console \\[ℹ\\] to cleanup resources, run 'eksctl delete cluster --region=ap-northeast-1 --name=beautiful-badger-1578666058' \\[✖\\] waiting for CloudFormation stack \u0026quot;eksctl-beautiful-badger-1578666058-nodegroup-ng-bb178f10\u0026quot;: ResourceNotReady: failed waiting for successful resource state Error: failed to create cluster \u0026quot;beautiful-badger-1578666058\u0026quot; NodeGroup の作成でこけたらしいことは分かったが\u0026hellip; 😣\nAWS was not able to validate the provided access credentials (Service: AmazonAutoScaling; Status Code: 400; Error Code: ValidationError; Request ID: a31c5a54-33b6-11ea-9896-5308bff4139a) 再チャレンジしたら今度は別のところでコケた\u0026hellip;\n\\[✖\\] AWS::EC2::Subnet/SubnetPublicAPNORTHEAST1A: CREATE\\_FAILED – \u0026quot;Value (ap-northeast-1a) for parameter availabilityZone is invalid. Subnets can currently only be created in the following availability zones: ap-northeast-1d, ap-northeast-1c, ap-northeast-1b. (Service: AmazonEC2; Status Code: 400; Error Code: InvalidParameterValue; Request ID: 28f3bfed-2939-4efd-ad9c-7f6152d6b041)\u0026quot; \\[✖\\] AWS::EC2::Subnet/SubnetPrivateAPNORTHEAST1A: CREATE\\_FAILED – \u0026quot;Value (ap-northeast-1a) for parameter availabilityZone is invalid. Subnets can currently only be created in the following availability zones: ap-northeast-1b, ap-northeast-1c, ap-northeast-1d. (Service: AmazonEC2; Status Code: 400; Error Code: InvalidParameterValue; Request ID: 0e402395-2b61-4b7f-ac5b-6e9f01625f0c)\u0026quot; この後、数回試したが、全部この AZ の問題でコケた\u0026hellip;\nそれでもあと1回、あと1回と思って試してたら成功した。(eksctl create cluster \u0026ndash;zones=ap-northeast-1b,ap-northeast-1c,ap-northeast-1d と \u0026ndash;zones を指定すれば良いのかな) しかし、AZ の問題はわかるんだけど、初回のエラーは必ず再発するのかと思ってたので意外\n$ eksctl create cluster \\[ℹ\\] eksctl version 0.12.0 \\[ℹ\\] using region ap-northeast-1 \\[ℹ\\] setting availability zones to \\[ap-northeast-1c ap-northeast-1d ap-northeast-1b\\] \\[ℹ\\] subnets for ap-northeast-1c - public:192.168.0.0/19 private:192.168.96.0/19 \\[ℹ\\] subnets for ap-northeast-1d - public:192.168.32.0/19 private:192.168.128.0/19 \\[ℹ\\] subnets for ap-northeast-1b - public:192.168.64.0/19 private:192.168.160.0/19 \\[ℹ\\] nodegroup \u0026quot;ng-c384e850\u0026quot; will use \u0026quot;ami-07296175bc6b826a5\u0026quot; \\[AmazonLinux2/1.14\\] \\[ℹ\\] using Kubernetes version 1.14 \\[ℹ\\] creating EKS cluster \u0026quot;wonderful-painting-1578669435\u0026quot; in \u0026quot;ap-northeast-1\u0026quot; region with un-managed nodes \\[ℹ\\] will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup \\[ℹ\\] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-northeast-1 --cluster=wonderful-painting-1578669435' \\[ℹ\\] CloudWatch logging will not be enabled for cluster \u0026quot;wonderful-painting-1578669435\u0026quot; in \u0026quot;ap-northeast-1\u0026quot; \\[ℹ\\] you can enable it with 'eksctl utils update-cluster-logging --region=ap-northeast-1 --cluster=wonderful-painting-1578669435' \\[ℹ\\] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \u0026quot;wonderful-painting-1578669435\u0026quot; in \u0026quot;ap-northeast-1\u0026quot; \\[ℹ\\] 2 sequential tasks: { create cluster control plane \u0026quot;wonderful-painting-1578669435\u0026quot;, create nodegroup \u0026quot;ng-c384e850\u0026quot; } \\[ℹ\\] building cluster stack \u0026quot;eksctl-wonderful-painting-1578669435-cluster\u0026quot; \\[ℹ\\] deploying stack \u0026quot;eksctl-wonderful-painting-1578669435-cluster\u0026quot; \\[ℹ\\] building nodegroup stack \u0026quot;eksctl-wonderful-painting-1578669435-nodegroup-ng-c384e850\u0026quot; \\[ℹ\\] --nodes-min=2 was set automatically for nodegroup ng-c384e850 \\[ℹ\\] --nodes-max=2 was set automatically for nodegroup ng-c384e850 \\[ℹ\\] deploying stack \u0026quot;eksctl-wonderful-painting-1578669435-nodegroup-ng-c384e850\u0026quot; \\[✔\\] all EKS cluster resources for \u0026quot;wonderful-painting-1578669435\u0026quot; have been created \\[✔\\] saved kubeconfig as \u0026quot;/Users/teraoka/.kube/config\u0026quot; \\[ℹ\\] adding identity \u0026quot;arn:aws:iam::949160801735:role/eksctl-wonderful-painting-1578669-NodeInstanceRole-18QXALFB6Y0W2\u0026quot; to auth ConfigMap \\[ℹ\\] nodegroup \u0026quot;ng-c384e850\u0026quot; has 0 node(s) \\[ℹ\\] waiting for at least 2 node(s) to become ready in \u0026quot;ng-c384e850\u0026quot; \\[ℹ\\] nodegroup \u0026quot;ng-c384e850\u0026quot; has 2 node(s) \\[ℹ\\] node \u0026quot;ip-192-168-12-245.ap-northeast-1.compute.internal\u0026quot; is ready \\[ℹ\\] node \u0026quot;ip-192-168-94-158.ap-northeast-1.compute.internal\u0026quot; is ready \\[ℹ\\] kubectl command should work with \u0026quot;/Users/teraoka/.kube/config\u0026quot;, try 'kubectl get nodes' \\[✔\\] EKS cluster \u0026quot;wonderful-painting-1578669435\u0026quot; in \u0026quot;ap-northeast-1\u0026quot; region is ready ~/.kube/config に保存したよって出てるから kubectl コマンドを試してみる。\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-12-245.ap-northeast-1.compute.internal Ready 3m25s v1.14.8-eks-b8860f ip-192-168-94-158.ap-northeast-1.compute.internal Ready 3m24s v1.14.8-eks-b8860f $ kubectl get svc \u0026ndash;all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.100.0.1 443/TCP 16m kube-system kube-dns ClusterIP 10.100.0.10 53/UDP,53/TCP 16m\n$ kubectl get ns NAME STATUS AGE default Active 17m kube-node-lease Active 17m kube-public Active 17m kube-system Active 17m $ kubectl get pods \u0026ndash;all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-b74p7 1/1 Running 0 11m kube-system aws-node-xx89x 1/1 Running 0 11m kube-system coredns-58986cd576-nc45g 1/1 Running 0 17m kube-system coredns-58986cd576-wcxcj 1/1 Running 0 17m kube-system kube-proxy-cfqcd 1/1 Running 0 11m kube-system kube-proxy-kzlgz 1/1 Running 0 11m\n 動いてる 😍 さて、どんなリソースが作られているのか。 VPC VPC から新しく作られる Subnet Public と Private という2種類の Subnet がそれぞれ3つの Availability zone に作成される Internet Gateway 新しい VPC を作って Public subnet を作成するので当然 Internet Gateway も必要になる Role ServiceRole (Controle Plane 用)、NodeInstanceRole (EC2 Workder Node 用)、FargatePodExecutionRole の3つが作成される。ServiceRole は EKS のドキュメントにある AmazonEKSClusterPolicy、AmazonEKSServicePolicy と eksctl が作る PutMetrics 用 Policy と NLB 管理用の Policy が紐付けられている。NodeInstanceRole は EKS のドキュメントにある AmazonEKSWorkerNodePolicy、AmazonEC2ContainerRegistryReadOnly、AmazonEKS\\_CNI\\_Policy の3つが、FargatePodExecutionRole には AmazonEKSFargatePodExecutionRolePolicy が紐付けられている NAT Gateway Private subnet があるので NAT Gateway も作成される。デフォルトでは1つの Availability zone にしか作られないが、--vpc-nat-mode で HighlyAvailable, Single, Disable から選択して指定することが可能。デフォルトは Single SecurityGroup クラスタ内の node 間での通信用に作成される ControlePlane もちろん Kubernetes の Controle Plane が作られる AutoScalingGroup Worker node 用の AutoScalingGroup が作られる LaunchTemplate Worker node 用の AutoScalingGroup で使われる LaunchTemplate で AMI Image や InstanceType、InstanceProflie、Userdata が設定されている Worker node として EC2 インスタンスが作成されたらログインしてみたいはず。でも eksctl create cluster に --ssh-access (ファイルを指定するなら --ssh-public-key も) をつけておかないと Public Key が設定されないため worker node へのアクセスには [EC2 Instance Connect](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-methods.html) で接続する必要がある。[aws-instance-connect-cli](https://github.com/aws/aws-ec2-instance-connect-cli) を使えば `mssh \u0026lt;instance-id\u0026gt;` だけで接続できる。 しかし Instance Connect は Instance Metadata Service v1 (IDMSv1) しかサポートしていないらしい。接続先インスタンスは Global IP address を持っている必要があり、SSH (22/tcp) が SecurityGroup で許可されている必要がある。Web Console から AWS のアドレスからの接続を許可しておく必要がある。また、接続する人は IAM Policy で [ec2-instance-connect:SendSSHPublicKey](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-set-up.html#ec2-instance-connect-configure-IAM-role) 権限を持っている必要がある。公開鍵を送りつけて一時的にログインできるようにしてくれるんですね。 ところで、worker node にログインしてみたら Pod ごとに2つのコンテナが起動していました。複数コンテナが指定された Pod じゃないのになんでだろ？って思ったらどれも \u0026quot;/pause\u0026quot; が実行されてるものと、名前に合ったプログラムが実行されているコンテナの2つでした。 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES df061216904c 602401143452.dkr.ecr.ap-northeast-1.amazonaws.com/eks/coredns \u0026ldquo;/coredns -conf /etc…\u0026rdquo; 2 hours ago Up 2 hours k8s_coredns_coredns-58986cd576-6kl57_kube-system_00aec73c-3449-11ea-a7e3-069c17bfdc00_0 379bc8e2844a 602401143452.dkr.ecr.ap-northeast-1.amazonaws.com/eks/pause-amd64:3.1 \u0026ldquo;/pause\u0026rdquo; 2 hours ago Up 2 hours k8s_POD_coredns-58986cd576-6kl57_kube-system_00aec73c-3449-11ea-a7e3-069c17bfdc00_0 09ee64a82565 602401143452.dkr.ecr.ap-northeast-1.amazonaws.com/amazon-k8s-cni \u0026ldquo;/bin/sh -c /app/ins…\u0026rdquo; 2 hours ago Up 2 hours k8s_aws-node_aws-node-7l99l_kube-system_6f2001d2-344e-11ea-a7e3-069c17bfdc00_0 79eef819a1f4 602401143452.dkr.ecr.ap-northeast-1.amazonaws.com/eks/kube-proxy \u0026ldquo;kube-proxy \u0026ndash;v=2 \u0026ndash;…\u0026rdquo; 2 hours ago Up 2 hours k8s_kube-proxy_kube-proxy-d599c_kube-system_6f202763-344e-11ea-a7e3-069c17bfdc00_0 40ae949a2e0c 602401143452.dkr.ecr.ap-northeast-1.amazonaws.com/eks/pause-amd64:3.1 \u0026ldquo;/pause\u0026rdquo; 2 hours ago Up 2 hours k8s_POD_aws-node-7l99l_kube-system_6f2001d2-344e-11ea-a7e3-069c17bfdc00_0 eeed9d5bf85f 602401143452.dkr.ecr.ap-northeast-1.amazonaws.com/eks/pause-amd64:3.1 \u0026ldquo;/pause\u0026rdquo; 2 hours ago Up 2 hours k8s_POD_kube-proxy-d599c_kube-system_6f202763-344e-11ea-a7e3-069c17bfdc00_0\n [The Almighty Pause Container](https://www.ianlewis.org/en/almighty-pause-container) ってのを見つけた。これが Pod という複数コンテナをまとめるキモだったんですね。 Worker node は起動時に Kubernetes のクラスタに参加する必要があるが、これは AutoScalingGroup の LaunchTemplate に userdata 設定があります。テキストが gzip されて base64 エンコードされていました、デコードすると次の内容でした。 userdata``` #cloud-config packages: null runcmd: - - /var/lib/cloud/scripts/per-instance/bootstrap.al2.sh write\\_files: - content: | # eksctl-specific systemd drop-in unit for kubelet, for Amazon Linux 2 (AL2) \\[Service\\] # Local metadata parameters: REGION, AWS\\_DEFAULT\\_REGION EnvironmentFile=/etc/eksctl/metadata.env # Global and static parameters: CLUSTER\\_DNS, NODE\\_LABELS, NODE\\_TAINTS EnvironmentFile=/etc/eksctl/kubelet.env # Local non-static parameters: NODE\\_IP, INSTANCE\\_ID EnvironmentFile=/etc/eksctl/kubelet.local.env ExecStart= ExecStart=/usr/bin/kubelet \\\\ --node-ip=${NODE\\_IP} \\\\ --node-labels=${NODE\\_LABELS},alpha.eksctl.io/instance-id=${INSTANCE\\_ID} \\\\ --max-pods=${MAX\\_PODS} \\\\ --register-node=true --register-with-taints=${NODE\\_TAINTS} \\\\ --allow-privileged=true \\\\ --cloud-provider=aws \\\\ --container-runtime=docker \\\\ --network-plugin=cni \\\\ --cni-bin-dir=/opt/cni/bin \\\\ --cni-conf-dir=/etc/cni/net.d \\\\ --pod-infra-container-image=${AWS\\_EKS\\_ECR\\_ACCOUNT}.dkr.ecr.${AWS\\_DEFAULT\\_REGION}.amazonaws.com/eks/pause-amd64:3.1 \\\\ --kubeconfig=/etc/eksctl/kubeconfig.yaml \\\\ --config=/etc/eksctl/kubelet.yaml owner: root:root path: /etc/systemd/system/kubelet.service.d/10-eksclt.al2.conf permissions: \u0026quot;0644\u0026quot; - content: |- NODE\\_LABELS=alpha.eksctl.io/cluster-name=hilarious-hideout-1578729265,alpha.eksctl.io/nodegroup-name=ng-b50c30b5 NODE\\_TAINTS= owner: root:root path: /etc/eksctl/kubelet.env permissions: \u0026quot;0644\u0026quot; - content: | address: 0.0.0.0 apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/eksctl/ca.crt authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s cgroupDriver: cgroupfs clusterDNS: - 10.100.0.10 clusterDomain: cluster.local featureGates: RotateKubeletServerCertificate: true kind: KubeletConfiguration serverTLSBootstrap: true owner: root:root path: /etc/eksctl/kubelet.yaml permissions: \u0026quot;0644\u0026quot; - content: | -----BEGIN CERTIFICATE----- (省略) -----END CERTIFICATE----- owner: root:root path: /etc/eksctl/ca.crt permissions: \u0026quot;0644\u0026quot; - content: | apiVersion: v1 clusters: - cluster: certificate-authority: /etc/eksctl/ca.crt server: https://7950C8F5B4E86FAF920EB443279E8896.sk1.ap-northeast-1.eks.amazonaws.com name: hilarious-hideout-1578729265.ap-northeast-1.eksctl.io contexts: - context: cluster: hilarious-hideout-1578729265.ap-northeast-1.eksctl.io user: kubelet@hilarious-hideout-1578729265.ap-northeast-1.eksctl.io name: kubelet@hilarious-hideout-1578729265.ap-northeast-1.eksctl.io current-context: kubelet@hilarious-hideout-1578729265.ap-northeast-1.eksctl.io kind: Config preferences: {} users: - name: kubelet@hilarious-hideout-1578729265.ap-northeast-1.eksctl.io user: exec: apiVersion: client.authentication.k8s.io/v1alpha1 args: - token - -i - hilarious-hideout-1578729265 command: aws-iam-authenticator env: null owner: root:root path: /etc/eksctl/kubeconfig.yaml permissions: \u0026quot;0644\u0026quot; - content: | m5.24xlarge 737 g4dn.4xlarge 29 i3.xlarge 58 ... (省略) ... t2.nano 4 c3.xlarge 58 c5.large 29 owner: root:root path: /etc/eksctl/max\\_pods.map permissions: \u0026quot;0644\u0026quot; - content: |- AWS\\_DEFAULT\\_REGION=ap-northeast-1 AWS\\_EKS\\_CLUSTER\\_NAME=hilarious-hideout-1578729265 AWS\\_EKS\\_ENDPOINT=https://7950C8F5B4E86FAF920EB443279E8896.sk1.ap-northeast-1.eks.amazonaws.com AWS\\_EKS\\_ECR\\_ACCOUNT=602401143452 owner: root:root path: /etc/eksctl/metadata.env permissions: \u0026quot;0644\u0026quot; - content: | #!/bin/bash set -o errexit set -o pipefail set -o nounset function get\\_max\\_pods() { while read instance\\_type pods; do if \\[\\[ \u0026quot;${instance\\_type}\u0026quot; == \u0026quot;${1}\u0026quot; \\]\\] \u0026amp;\u0026amp; \\[\\[ \u0026quot;${pods}\u0026quot; =~ ^\\[0-9\\]+$ \\]\\] ; then echo ${pods} return fi done \u0026lt; /etc/eksctl/max\\_pods.map } NODE\\_IP=\u0026quot;$(curl --silent http://169.254.169.254/latest/meta-data/local-ipv4)\u0026quot; INSTANCE\\_ID=\u0026quot;$(curl --silent http://169.254.169.254/latest/meta-data/instance-id)\u0026quot; INSTANCE\\_TYPE=\u0026quot;$(curl --silent http://169.254.169.254/latest/meta-data/instance-type)\u0026quot; source /etc/eksctl/kubelet.env # this can override MAX\\_PODS cat \u0026gt; /etc/eksctl/kubelet.local.env \u0026lt;\u0026lt;EOF NODE\\_IP=${NODE\\_IP} INSTANCE\\_ID=${INSTANCE\\_ID} INSTANCE\\_TYPE=${INSTANCE\\_TYPE} MAX\\_PODS=${MAX\\_PODS:-$(get\\_max\\_pods \u0026quot;${INSTANCE\\_TYPE}\u0026quot;)} EOF systemctl daemon-reload systemctl enable kubelet systemctl start kubelet owner: root:root path: /var/lib/cloud/scripts/per-instance/bootstrap.al2.sh permissions: \u0026quot;0755\u0026quot; でも今は Managed Node Group ってのがあるのでこれを使うのが良いのかな。eksctl create cluster に \u0026ndash;managed をつけるだけで良さそう。また、\u0026ndash;fargate をつければ Fargate で実行する設定が入るっぽい。この辺りを確認して terraform で EKS 環境を構築できるようにしてみることにする。\n","date":"2020年1月11日","permalink":"/2020/01/eksctl/","section":"Posts","summary":"eksctl が何をやってくれるのが、何ができるのかを確認します。 eksctl create cluster # いきなり eksctl create cluster を実行するだけでクラスタが作れるっぽいのでひとまず試してみる。 $","title":"eksctl で何ができるのか"},{"content":"前回の続き。今回は Istio を導入してみる。\nクラスタの作成 # Control Plane 1台、Worker Node 2台のクラスタを作成する。\n$ cat cluster.yaml # three node (two workers) cluster kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker - role: worker $ kind create cluster --config cluster.yaml MetalLB のインストール # なくても Istio は使えるのですが、MetalLB で type: LoadBalancer なサービスも作成されるので使えるようにしておく。\nKubernetes in docker (kind) on Mac に MetalLB 入れて type:LoadBalancer もマルチノードもお手軽に遊ぶを参考にさせてもらいました。\n$ kubectl apply \\ -f https://raw.githubusercontent.com/danderson/metallb/v0.8.3/manifests/metallb.yaml namespace/metallb-system created podsecuritypolicy.policy/speaker created serviceaccount/controller created serviceaccount/speaker created clusterrole.rbac.authorization.k8s.io/metallb-system:controller created clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created role.rbac.authorization.k8s.io/config-watcher created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created rolebinding.rbac.authorization.k8s.io/config-watcher created daemonset.apps/speaker created deployment.apps/controller created $ kubectl get ns NAME STATUS AGE default Active 5m8s kube-node-lease Active 5m9s kube-public Active 5m9s kube-system Active 5m9s metallb-system Active 9s Pod を確認してみる。controller は Deployment で、speaker は DaemonSet で作成されている。\n$ kubectl get pods -n metallb-system NAME READY STATUS RESTARTS AGE controller-65895b47d4-zsw5d 1/1 Running 0 37s speaker-cd8rm 1/1 Running 0 37s speaker-kbn97 1/1 Running 0 37s speaker-w24jv 1/1 Running 0 37s L2 モード用の ConfigMap を作成する。IPアドレスのレンジは自分の環境で被ってなければ何でも良い。\n$ kubectl apply -f - \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.100.2-192.168.100.254 EOF 参考にした Qiita のページにあるようにサービスを作って試してみると良い。\nIstio のセットアップ # Customizable Install with Istioctl を見ながら istioctl コマンドでインストールします。\ncurl -L https://istio.io/downloadIstio | sh - と実行するとカレントディレクトリに istio-1.4.2 ディレクトリができて tar.gz が展開されています。この中の bin/istioctl を使います。インストール後に使い道があるのかどうかよく分からないのでとりあえず、ここに置いたまま進めます。\nIstio には多くのコンポーネントがあり、profile という形でセットになっています。\n$ istio-1.4.2/bin/istioctl profile list Istio configuration profiles: default demo minimal remote sds 今回は見栄えのする Kiali を使うために demo profile でインストールします。\n$ istio-1.4.2/bin/istioctl manifest apply --set profile=demo Preparing manifests for these components: - Telemetry - Injector - PrometheusOperator - EgressGateway - Cni - Citadel - Pilot - Policy - Kiali - CoreDNS - Tracing - Grafana - NodeAgent - Prometheus - IngressGateway - Galley - CertManager - Base Applying manifest for component Base Finished applying manifest for component Base Applying manifest for component Tracing Applying manifest for component Policy Applying manifest for component EgressGateway Applying manifest for component Kiali Applying manifest for component Citadel Applying manifest for component Galley Applying manifest for component IngressGateway Applying manifest for component Prometheus Applying manifest for component Pilot Applying manifest for component Injector Applying manifest for component Telemetry Applying manifest for component Grafana Finished applying manifest for component Kiali Finished applying manifest for component Prometheus Finished applying manifest for component Citadel Finished applying manifest for component Tracing Finished applying manifest for component Galley Finished applying manifest for component Injector Finished applying manifest for component EgressGateway Finished applying manifest for component IngressGateway Finished applying manifest for component Policy Finished applying manifest for component Pilot Finished applying manifest for component Grafana Finished applying manifest for component Telemetry Component NodeAgent installed successfully: =========================================== Component Kiali installed successfully: ======================================= Component CoreDNS installed successfully: ========================================= Component Tracing installed successfully: ========================================= Component Grafana installed successfully: ========================================= Component Base installed successfully: ====================================== Component Prometheus installed successfully: ============================================ Component IngressGateway installed successfully: ================================================ Component Galley installed successfully: ======================================== Component CertManager installed successfully: ============================================= Component Telemetry installed successfully: =========================================== Component Injector installed successfully: ========================================== Component PrometheusOperator installed successfully: ==================================================== Component Policy installed successfully: ======================================== Component EgressGateway installed successfully: =============================================== Component Cni installed successfully: ===================================== Component Citadel installed successfully: ========================================= Component Pilot installed successfully: ======================================= istio-system という namespace が追加されました。\n$ kubectl get ns NAME STATUS AGE default Active 25m istio-system Active 59s kube-node-lease Active 25m kube-public Active 25m kube-system Active 25m metallb-system Active 20m サービスの確認。istio-ingressgateway は LoadBalaner ですが MetalLB のおかげで EXTERNAL-IP がセットされています。\n$ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.97.73.72 3000/TCP 2m38s istio-citadel ClusterIP 10.102.98.80 8060/TCP,15014/TCP 2m41s istio-egressgateway ClusterIP 10.106.154.29 80/TCP,443/TCP,15443/TCP 2m38s istio-galley ClusterIP 10.101.223.137 443/TCP,15014/TCP,9901/TCP,15019/TCP 2m40s istio-ingressgateway LoadBalancer 10.106.135.139 192.168.100.2 15020:32290/TCP,80:30252/TCP,443:32462/TCP,15029:30985/TCP,15030:32647/TCP,15031:30814/TCP,15032:30392/TCP,15443:31688/TCP 2m38s istio-pilot ClusterIP 10.103.79.179 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m38s istio-policy ClusterIP 10.101.34.3 9091/TCP,15004/TCP,15014/TCP 2m38s istio-sidecar-injector ClusterIP 10.104.69.225 443/TCP 2m40s istio-telemetry ClusterIP 10.105.96.88 9091/TCP,15004/TCP,15014/TCP,42422/TCP 2m37s jaeger-agent ClusterIP None 5775/UDP,6831/UDP,6832/UDP 2m42s jaeger-collector ClusterIP 10.101.71.42 14267/TCP,14268/TCP,14250/TCP 2m42s jaeger-query ClusterIP 10.110.22.1 16686/TCP 2m42s kiali ClusterIP 10.109.39.104 20001/TCP 2m41s prometheus ClusterIP 10.105.127.123 9090/TCP 2m41s tracing ClusterIP 10.103.102.176 80/TCP 2m42s zipkin ClusterIP 10.110.0.16 9411/TCP 2m41s Istio が自動でサイドカー設定を差し込んでくれるように namespace に label を設定します。まずは、default namespace をそのまま使っても良いのですが、テスト用の namespace を作ってそこに設定してみます。あとで bookinfo アプリをデプロイするので名前を bookinfo としておく。\n$ kubectl create namespace bookinfo namespace/bookinfo created $ kubectl label namespace bookinfo istio-injection=enabled namespace/bookinfo labeled Bookinfo アプリのデプロイ # $ kubectl apply -n bookinfo \\ -f https://raw.githubusercontent.com/istio/istio/release-1.4/samples/bookinfo/platform/kube/bookinfo.yaml service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created $ kubectl get svc -n bookinfo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.103.184.201 9080/TCP 13s productpage ClusterIP 10.96.161.43 9080/TCP 11s ratings ClusterIP 10.104.176.147 9080/TCP 12s reviews ClusterIP 10.99.9.14 9080/TCP 12s $ kubectl apply -n bookinfo \\ -f https://raw.githubusercontent.com/istio/istio/release-1.4/samples/bookinfo/networking/bookinfo-gateway.yaml gateway.networking.istio.io/bookinfo-gateway created virtualservice.networking.istio.io/bookinfo created $ kubectl get gateway -n bookinfo NAME AGE bookinfo-gateway 22s これで istio-ingressgateway にアクセスすれば bookinfo アプリにルーティングされるようになっている。次のように port forwarding して localhost:8080 にアクセスすると bookinfo アプリのページが表示される。\nkubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80 localhost:8080/productpage\nKiali で可視化 # 同様に kiali に対しても port forwarding してアクセスする。\nkubectl -n istio-system port-forward svc/kiali 20001:20001 今度は localhost:20001 にアクセスする。ログイン用のID、パスワードは kubernetes の Secrets に入っているのでのぞけばわかるが、 admin / admin となっていた。\nブラウザで Kiali にアクセスしてログインすると Overview で次のような表示となる。\n curl で productpage に何度もアクセスしながら kiali の Graph ページを見ると次のような表示を確認することができる。カッケー！\n Bookinfo は次のようなサービスで構成されており\nkubectl get svc -n bookinfo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.103.184.201 9080/TCP 28m productpage ClusterIP 10.96.161.43 9080/TCP 28m ratings ClusterIP 10.104.176.147 9080/TCP 28m reviews ClusterIP 10.99.9.14 9080/TCP 28m reviews には3つのバージョンが混在しているため、上のグラフの用に reviews のところから3つに矢印がのびていた。そして、v2, v3 は ratings サービスにアクセスして星の数を表示するが v1 はアクセスしない。v2 と v3 の違いは星の色。\n$ kubectl get deployment -n bookinfo NAME READY UP-TO-DATE AVAILABLE AGE details-v1 1/1 1 1 28m productpage-v1 1/1 1 1 28m ratings-v1 1/1 1 1 28m reviews-v1 1/1 1 1 28m reviews-v2 1/1 1 1 28m reviews-v3 1/1 1 1 28m 各矢印のところに Requests per second や Requests percentage, Response time を表示させることも可能でした。\n以上、Istio を入れて Kiali で可視化するまででした。トラフィックの制御という大事な部分はまだこれから。\nお掃除 # $ kind delete cluster ","date":"2020年1月7日","permalink":"/2020/01/kind-part-2/","section":"Posts","summary":"前回の続き。今回は Istio を導入してみる。 クラスタの作成 # Control Plane 1台、Worker Node 2台のクラスタを作成する。 $ cat cluster.yaml # three node (two workers) cluster kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role:","title":"kind で kubernetes に触れる (2) - Istio"},{"content":"続 Tutorials を順に試す。（前回）\nBest Practices for Operating Containers # Best Practices for Operating Containers\n前回の最後のやつと同じようなベストプラクティス紹介です。今回は運用編。\nUse the native logging mechanisms of containers # 重要度: 高\nログは重要であり、Docker や Kubernetes はこれの扱いに力を入れている。stdout, stderr に出して Docker や Kubernetes の提供する機能を使ってプラットフォームが提供するログ管理システムへ送るのが良い。フォーマットに JSON を使うと検索や集計が容易になる。ただし、アプリケーションによっては容易にログの出力先を変更できない場合がある（例えば Tomcat など）。この場合はサイドカーパターンを利用すると良い。このパターンでは Pod 内の複数のコンテナで emptyDir ボリュームを共有し、アプリコンテナが書き出したログをログエージェントコンテナで読み出して集約先に送る。ログエージェントがファイルのローテーションに対応していなければこれについても考える必要がある。\nEnsure that your containers are stateless and immutable # 重要度: 高\nコンテナはステートレスでイミュータブルでなければならない。脆弱性があっても直接コンテナ内でパッチを当てるのではなく新しいイメージを作成して入れ替えることで対応する。\nStatelessness # ステートレスとは状態（データ）が外部に保存されることを意味する。\n ファイルであれば Cloud Storage などのオブジェクトストレージに保存することが望ましい セッションデータは Redis や Memcached など、外部の低遅延なキーバリューストアに保存することが望ましい ブロックストレージが必要な場合は GKE では persistent disks の使用が望ましい  こうすることでデータを失うことなく、いつでもコンテナをシャットダウンすることができるし、更新も容易となる。\nImmutability # イミュータブルとは実行中のコンテナに変更を加えないこと。設定変更やパッチ適用はコンテナイメージを作り直して入れ替えることで対応する。こうすることでロールバックも古いイメージを再度デプロイするだけで完了する。また全ての環境を同一に保つことを容易にする。\n実行環境によって切り替えたい設定は Secrets や ConfigMaps を使って指定できるようにしておく。\nAvoid privileged containers # 重要度: 高\n仮想サーバーでアプリケーションを root ユーザーで実行しないのと同じように、乗っ取られた場合の危険度を考慮し、コンテナに特権を与えてはいけない。\n特権が必要だと思った場合には次の代替案を検討すべき\n Kubernetes の securityContext option や Docker の \u0026ndash;cap-add フラグで特定の capabilities を与える。Docker のドキュメントにデフォルトで有効な capabilities と明示的に追加の必要な capabilities のリストがある どうしても host の設定を変更する必要がある場合はサイドカーや init container を使う。このコンテナは直接外部からのアクセスを受けないためより安全になる Kubernetes で sysctl を変更する必要がある場合は dedicated annotation を使う  Kubernetes では Pod Security Policy によって特権コンテナの実行を禁止することができる。\nMake your application easy to monitor # 重要度: 高\nロギングと同様に、モニタリングはアプリケーションの運用に不可欠である。\nコンテナ環境は動的に監視対象が変わるため、それに適したモニタリングシステムが必要であり、メトリクス収集では Prometheus が人気。Stackdriver は Kubernetes クラスタとアプリを Prometheus でモニタリングが可能。(enable Stackdriver Kubernetes Monitoring on GKE)\nPrometheus や Stackdriver Kubernetes Monitoring でアプリのモニタリングを行うには Prometheus 形式のメトリクスを提供する必要がある。これにはアプリケーション自身にメトリクス用の HTTP エンドポイントを実装する方法とサイドカーを使ってサイドカー側で提供する方法がある。後者は例えば jmx_exporter などをサイドカーで実行する。\nExpose the health of your application # 重要度: 中\nKubernetes には Liveness probe と Readiness probe の2種類のヘルスチェックがある。詳細は Kubernetes documentation を参照。\nLiveness probe # Liveness probe は /health という HTTP エンドポイントを使うことが推奨される。正常な場合は 200 OK を返す。正常でない結果が返った場合、Kubernetes はコンテナのリスタートをかけたりする。\nReadiness probe # Readiness probe は /ready という HTTP エンドポイントを使うことが推奨される。正常な場合は 200 OK を返す。これはリクエストを受けられる状態であることを意味し、Kubernetes はリクエストトラフィックを流し始める。\nDeployment の更新時、Readiness が成功するまでまって次の Pod の更新に移る。\n多くのアプリケーションでは Liveness と Readiness の状態に違いがなく、どちらも同じエンドポイントを使用する。\nAvoid running as root # 重要度: 中\nコンテナはホストと kernel を共有しているため、未知の脆弱性への対策としてコンテナ内のアプリを root で実行しないことが推奨される。Kubernetes では PodSecurityPolicy で root で実行させないように強制したり、runAsUser option で Dockerfile の USER を上書きすることができるが、多くのメジャーなアプリの公式イメージが root で実行していたりして、ファイルシステムの権限周りで問題があるかもしれない。\nExternal volume の場合は fsGroup Kubernetes option で権限問題を回避可能。\nroot でないと 1024 より小さなポート番号で Listen できないことは Kubernetes のサービスが redirect してくれるため問題ではない。\nCarefully choose the image version # 重要度: 中\nとにかく latest タグは使うべきでない。Dockerfile で FROM に指定するイメージの tag はセマンティックバージョニングされていればパッチバージョンを省略すれば、新たにイメージをビルドする際にはパッチ適用済みのものが使われるのでこの辺りも要検討。\n続く\u0026hellip;\n","date":"2020年1月6日","permalink":"/2020/01/gke-tutorials-part2/","section":"Posts","summary":"続 Tutorials を順に試す。（前回） Best Practices for Operating Containers # Best Practices for Operating Containers 前回の最後のやつと同じようなベストプラクティス紹介です。今回は運用編。 Use the native logging mechanisms of containers # 重要度:","title":"GKE Tutorials (2)"},{"content":"Tutorials を順に試す。\nDeploying a containerized web application # Deploying a containerized web application\nサンプルアプリを git clone して docker build, push して YAML を使わず kubectl でコンテナをデプロイして、expose して pod の数を増やしたり、イメージを入れ替えたりする。\nCreate a Guestbook with Redis and PHP # Create a Guestbook with Redis and PHP\nDeployment リソースができる前の初期からある PHP + Redis のゲストブックですね。Redis の master サービスと replica サービスを作ります。これはなんかもう古くさい手順かな？こっちのやつはまだ ReplicationController って書いてある。\nDeploying WordPress on GKE with Persistent Disks and Cloud SQL # Deploying WordPress on GKE with Persistent Disks and Cloud SQL Persistent Volume と Cloud SQL を使った WordPress をデプロイする。Deploy 用の YAML は wordpress-persistent-disks にある。\n Cloud SQL の MySQL インスタンスを作成 MySQL のユーザー作成 Cloud SQL Proxy 用の Service Account を作成 作成した Service Account に cloudsql.client ロールを紐付ける Service Account のクレデンシャルを取得して Kubernetes の Secrets に登録 MySQL ユーザーのパスワードを Kubernetes の Secrets に登録 WordPress の Deployment を作成  Pod に Cloud SQL Proxy コンテナも相乗りして MySQL へのアクセスはこれを経由させる Persistent Volume を /var/www/html にマウント\n（/var/www/html に index.php とかが存在しない場合はコピーされる）   type: LoadBalancer で Service を作成し、外部公開  Authenticating to Cloud Platform with Service Accounts # Authenticating to Cloud Platform with Service Accounts\nGKE Workload (container) から Google API へアクセスするサンプル。Service Account を作ってクレデンシャルを Secrets に登録して Cloud Pub/Sub へアクセスします。\nCloud Pub/Sub の topic を作成するのに gcloud pubsub topics create echo と gcloud コマンドを使う方法とは別に YAML を書いて kubectl で適用する（kubernetes のリソースのように宣言型の管理が可能となる）Config Connector ってのが登場して、これはなんぞや？って調べ始めると Workload Identity というまた知らないものが出てくる\u0026hellip;😩\nGKE で実行する Workload から Google API にアクセスする場合は Google Service Accounts (GSAs) のクレデンシャルを Secrets などで別途指定するのではなく、Pod に割り当てる Kubernetes Service Accounts (KSAs) で Google API にもアクセスできるようにすればよりセキュアだということらしい。（Workload Identity は 2020-01-04 時点ではまだベータ）\nWorkload Identity を有効にするにはクラスタ作成時に指定すれば良いみたい。ネームスペースがプロジェクト単位であるため、同一プロジェクト内に用途の違うクラスタは相乗りさせない方が良いらしい。開発用と本番用のクラスタを同じプロジェクトに作るとおそらくハマると。\nConfig Connector / Workload Identiry は手順をなぞってみたけどうまくいかなかったので別途調査が必要。\nBest practices for building containers # Best practices for building containers\nこれはチュートリアルなのか？？？\nPackage a single app per container # 重要度: 高\nコンテナを仮想マシンのように複数のアプリを同居させるのは良くある間違い。Apache, PHP, MySQL であれば2つ(mod_php)か3つ(PHP-FPM)のコンテナに分ける。アプリが止まればコンテナも止まる、コンテナが止まればアプリも止まる。ライフサイクルの違うものは混ぜるな危険。\nPublic Image や有名 Vendor の提供する Image にも同居させているものがありますが、安易にそれを真似しないこと。\nProperly handle PID 1, signal handling, and zombie processes # 重要度: 高\nPID 1 のシグナルハンドリングやゾンビプロセスを正しく扱う。\nProblem 1: How the Linux kernel handles signals # Linux では PID 1 のプロセスは特別扱いされ、SIGTERM や SIGINT でプロセスの終了という他の PID での処理が行われないため、コンテナ内で PID 1 として起動されるプロセスが自身でシグナルハンドリングしてやる必要がある。\nProblem 2: How classic init systems handle orphaned processes # systemd などの従来の init システムはゾンビプロセスの削除も担っているし、親プロセスを失ったプロセスの親にもなります。コンテナではこの処理を PID 1 として起動するプロセスが担う必要がある。これをきちんと行わないとメモリやその他のリソースが不足する要因となる。\nSolution 1: Run as PID 1 and register signal handlers # シグナルハンドラを実装したアプリを PID 1 として実行する。ENTRYPOINT や CMD で直接そのアプリを指定する、前処理などのためにシェルスクリプトから起動させる場合は exec を使って PID 1 がアプリの PID となるようにする。\nSolution 2: Enable process namespace sharing in Kubernetes # Pod (複数コンテナを含むことが出来る) 内のプロセスネームスペースを一つにする process namespace sharing を有効にすると Kubernetes Pod infrastructure container が PID 1 となり、親のいなくなったプロセスを引き取る。\nSolution 3: Use a specialized init system # 通常の Linux サーバーの PID 1 のような処理をするプログラムを使うことで回避することも可能だが、systemd はコンテナ向けとしては高機能で複雑すぎるため tini などがおすすめ。Docker では \u0026ndash;init オプションを指定することで tini を使った docker-init プロセスが PID 1 となる。docker-compose.yml でも init という設定項目がある。\nOptimize for the Docker build cache # 重要度: 高\nDocker の build cache の仕組みをきちんと理解して有効活用するべし。誤って使うと古いキャッシュを使い続けることになってしまうため要注意。\nRemove unnecessary tools # 重要度: 中\n攻撃対象となる不要なツールをインストールしないようにする。例えば Reverse Shell として使われることの多い netcat をインストールしないなど。これはコンテナに限った話ではないが、コンテナの方が容易である。これを十分に推進するとデバッグツールの類も入れられなくなるため、ログの管理やトレーシング、プロファイリングのシステムが必然的に必要になってくる。\nコンテナ内のファイルは可能な限り少なくする。スタティックにリンクしたバイナリだけであれば scratch image を使うことも出来る。\nイメージにツールがインストールされていないだけでは不十分であり、インストールもできないようにする必要があるため root でアプリを実行しないようにします。docker run の \u0026ndash;read-only フラグで書き込みを禁止することが可能。Kubernetes の場合は readOnlyRootFilesystem オプションが使え、PodSecurityPolicy で強制することも可能である。\nBuild the smallest image possible # 重要度: 中\n小さなイメージファイルはアップロードもダウンロードも早く終わるので起動も速くなる。小さなベースイメージを使おう。スタティックリンクされたバイナリなら scratch から作るのも簡単だが、そうではないことが多い。distroless は言語別にランタイムで必要なものだけを含んだイメージを提供している。shell とかパケージマネージャーなどは入っていない。不要なファイルを別レイヤーで削除してもイメージサイズは小さくならないため、一つの RUN コマンドに削除まで全部含めるのが良いが、Docker 17.05 で追加された Multi-staged builds を使うと良い。同じ node で実行するコンテナであればベースイメージを共通化することでレイヤーのキャッシュが有効に使える。\nUse vulnerability scanning in Container Registry # 重要度: 中\nコンテナレジストリの持つ脆弱性スキャナを使うと便利です。Cloud Pub/Sub 経由で patch の適用された新しいイメージの作成を自動化することも出来る。(Getting vulnerabilities and metadata for images)\nProperly tag your images # 重要度: 中\nコンテナイメージには適切な Tag をつけるべき、Semantic Versioning か Git の commit hash を使うなど。\nCarefully consider whether to use a public image # 重要度: N/A\n公開されているイメージを使わない場合、ベースイメージのビルドの自動化や、それを使ったイメージのビルドの自動化まで考えておくべき。Cloud Build の Build triggers はこれを助ける手段となるし、Google はメジャーなディストリビューションの base image も提供している。container-diff はコンテナイメージ間の差分を確認できる。container-structure-test は ServerSpec 的な感じでイメージのテストが行える。Grafeas は metadata の API で保存したイメージの metadata を後からチェックすることができる。Kubernetes の場合はデプロイ前に必須条件を確認する admission controllers もあるし、pod security policies でセキュリティオプションを強制することが可能。\nサードパーティ製ライブラリやパッケージを含める場合にはライセンスにも注意すること。\nチュートリアルはまだまだ続く\u0026hellip; 次もベストプラクティス (Best Practices for Operating Containers) って書いてあるんだよなあ\n","date":"2020年1月5日","permalink":"/2020/01/gke-tutorials-part1/","section":"Posts","summary":"Tutorials を順に試す。 Deploying a containerized web application # Deploying a containerized web application サンプルアプリを git clone して docker build, push して YAML を使わず kubectl でコンテナをデプロイして、expose して pod の数を増やした","title":"GKE Tutorials (1)"},{"content":"GCP の Cloud Shell を触ってみて、ふとどんな環境で動いてるんだろう？と気になったのでちょっと調べてみた。\nCloud Shell とは # Cloud Shell はブラウザから shell にアクセス可能な Linux コンテナ環境で多くの言語 (Java, Go, Python, Node.js, Ruby, PHP, .NET Core) と gcloud や gsutil などはもちろん他にも 多くのツール（Emacs も Vim も入ってる）を含んでおり、イメージは毎週更新されるようです。\nLinux Distribution は debian で、apt でパッケージを追加することも可能。常に必要なら後述の ~/.customize_environment に書いておく。\n$ cat /etc/os-release PRETTY\\_NAME=\u0026quot;Debian GNU/Linux 9 (stretch)\u0026quot; NAME=\u0026quot;Debian GNU/Linux\u0026quot; VERSION\\_ID=\u0026quot;9\u0026quot; VERSION=\u0026quot;9 (stretch)\u0026quot; VERSION\\_CODENAME=stretch ID=debian HOME\\_URL=\u0026quot;https://www.debian.org/\u0026quot; SUPPORT\\_URL=\u0026quot;https://www.debian.org/support\u0026quot; BUG\\_REPORT\\_URL=\u0026quot;https://bugs.debian.org/\u0026quot; root 権限あるし、任意の Docker コンテナ実行できるしファイルのアップロード、ダウンロードもできるしエディタとして VS Code みたいな Theia も使える便利環境です。インスタンスは通常 g1-small （0.5 vCPU, 1.7GB メモリ）ですが、ブーストモードで n1-standard-1 （1 vCPU, 3.75GB メモリ）にすることも可能。ブーストモードの詳細は知らない。\nOpen in Cloud Shell # Open in Cloud Shell 機能を使えばワンクリックで指定の Docker コンテナ環境に git repository を clone してブラウザ内のエディタで開くことが出来ます。\n  \nCloud Shell を立ち上げて次のコマンドを実行するのと同じした。\ncloudshell\\_open --repo\\_url \u0026quot;https://github.com/yteraoka/flask-sample.git\u0026quot; --page \u0026quot;editor\u0026quot; --open\\_in\\_editor \u0026quot;app/app.py\u0026quot; ちなみに alias edit='cloudshell edit-files' が設定されているため edit some-exist-file とするだけで Theia で開くことが出来ます。dl という alias もあって簡単にファイルをダウンロード出来ます。\nWeb Preview # Cloud Shell 内で 8080/tcp を listen したサーバーを起動させて Web Preview 機能を使えばブラウザでアクセスすることが出来ます。\nドキュメントにあるようにアプリを書いて実行することもできるし、次のように docker container で 8080 をマップすることでも対応可能。\ndocker run -d -p 8080:80 nginx ストレージ # 5GB ある /home 配下は永続化され、新しいコンテナに切り替わってもデータは引き継がれます。120日間アクセスが無いとこのストレージは削除されます。\n料金 # 無料らしい。\nプロセスを見てみる # さて、どんな環境かということで ps コマンドで確認してみると次のようになっていました。\nusername@cloudshell:~ (project-id)$ ps auxwwf USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.1 17976 2876 ? Ss 12:23 0:00 /bin/bash /google/scripts/onrun.sh sleep infinity root 8 4.6 0.1 250116 2200 ? Ssl 12:23 0:00 /usr/sbin/rsyslogd root 81 0.0 0.1 69960 3172 ? S\u0026lt;s 12:23 0:00 /usr/sbin/sshd -p 22 -o AuthorizedKeysFile=/etc/ssh/keys/authorized\\_keys root 294 0.0 0.3 86552 6476 ? S\u0026lt;s 12:23 0:00 \\\\\\_ sshd: username \\[priv\\] username 298 0.0 0.2 86552 4352 ? S\u0026lt; 12:23 0:00 \\\\\\_ sshd: username@pts/0 username 299 0.0 0.1 11176 3044 pts/0 S\u0026lt;s+ 12:23 0:00 \\\\\\_ bash -c if \\[ -f /google/devshell/start-shell.sh \\]; then /google/devshell/start-shell.sh 38771 'project-id' '' '1363912993' false else touch /var/run/google/devshell/38771 \u0026amp;\u0026amp; bash --login fi username 301 0.0 0.1 11192 2896 pts/0 S\u0026lt;+ 12:23 0:00 | \\\\\\_ /bin/bash /google/devshell/start-shell.sh 38771 project-id 1363912993 false username 306 0.0 0.1 19332 3064 pts/0 S\u0026lt;+ 12:23 0:00 | \\\\\\_ tmux new-session -A -D -n cloudshell -s 1363912993 username 300 0.0 0.0 12688 1724 ? S\u0026lt;s 12:23 0:00 \\\\\\_ /usr/lib/openssh/sftp-server root 156 1.6 3.8 376044 66772 ? Sl 12:23 0:00 /usr/bin/dockerd -p /var/run/docker.pid --mtu=1460 --registry-mirror=https://asia-mirror.gcr.io root 170 0.5 2.4 496136 42304 ? Ssl 12:23 0:00 \\\\\\_ containerd --config /var/run/docker/containerd/containerd.toml --log-level info username 289 3.6 1.1 58580 20368 ? S 12:23 0:00 /usr/bin/python /usr/bin/supervisord -n -c /google/devshell/supervisord.conf -u username --pidfile=/var/run/supervisor.pid --logfile=/var/log/supervisord.log username 365 2.2 0.9 43128 17304 ? S 12:23 0:00 \\\\\\_ /usr/bin/python /google/devshell/send\\_heartbeats.py root 290 0.0 0.0 25384 1504 ? S 12:23 0:00 logger -t supervisord root 293 0.0 0.0 4188 612 ? S 12:23 0:00 sleep infinity username 308 0.0 0.1 27940 3280 ? S\u0026lt;s 12:23 0:00 tmux new-session -A -D -n cloudshell -s 1363912993 username 309 1.8 0.3 23080 6712 pts/1 S\u0026lt;s 12:23 0:00 \\\\\\_ -bash username 383 0.0 0.1 38304 3200 pts/1 R\u0026lt;+ 12:24 0:00 \\\\\\_ ps auxwwf username@cloudshell:~ (project-id)$ PID 1 で onrun.sh が実行されており何らかのコンテナで実行されてるっぽい。sleep コマンドの引数に infinity なんて使い方ができるんですね。(sleep infinity で無限に待つ - @tmtms のメモ)\nonrun.sh の中では rsyslogd を実行した後に環境変数 ONRUM で指定されたコマンドを順に実行し、その後、引数で指定されたコマンド（sleep infinity）が実行されます。環境変数 ONRUN には /google/devshell/startup.sh /google/scripts/wrapdocker.sh /google/devshell/start-supervisord.sh が入っていました。（以下、環境変数による細かい振る舞いは省略）\nstartup.sh #  ログイン関連で必要となるファイルを vmtouch コマンドでキャッシュに乗っける ~/.customize_environment が存在すれば実行する（これは初回起動時のみ）\nEnvironment customization useradd コマンドでユーザーを作成する（docker, adm, sudo グループに所属） sshd の起動\nAuthorizedKeysFile=/etc/ssh/keys/authorized_keys と指定されており、ここには Theia 用とおぼしき公開鍵も入っていました /etc/environment への環境変数の設定 Google Cloud SDK 設定  wrapdocker.sh # 環境変数 DISABLE_DIND が空でなければ何もしない。\n stdin / stdout / stderr 以外の file descripter を閉じる（fork で引き継いだやつとかかな？） DOCKER_OPTS を /etc/default/docker に追記 service docker start で docker daemon を起動させる  start-supervisord.sh #  supervisord を起動させる  supervisord では /google/devshell/send_heartbeats.py を実行する。send_heartbeats.py は1分おきに ssh.cloud.google.com にあるエンドポイントに対してハートビートリクエストを送る。\nその他 # /google/devshell/start-shell.sh ってどこから実行してるのかな？って悩んでたのだけれどこれは単に ssh してから実行してるのかな。\nまとめ # セッションは最長12時間であるとか、いくつかの制限はありますが、ブラウザさえあれば使える便利な環境が無料で使えるんです。もちろん GCP 関係ない作業にも使える。\n調査することで sleep に infinity が指定可能であるとか vmtouch や mountpoint コマンドというものがあるということを知ることができました。やったね！\n","date":"2019年12月31日","permalink":"/2019/12/processes-in-cloudshell/","section":"Posts","summary":"GCP の Cloud Shell を触ってみて、ふとどんな環境で動いてるんだろう？と気になったのでちょっと調べてみた。 Cloud Shell とは # Cloud Shell はブラウザから shell にアクセス可能な Linux","title":"CloudShell のプロセス"},{"content":"ちょいちょいランダムな文字列が欲しくなるので Perl で書いたスクリプト使ってたのだけど Bash で出来そうだなということで書いてみたのだが (記号の有無オプションとか追加しても良い)\n#!/bin/bash CHARS='23456789abcdefghjkmnpqrstuvwxyzABCDEFGHJKLMNPQRSTUVWXYZ' #CHARS='23456789abcdefghjkmnpqrstuvwxyzABCDEFGHJKLMNPQRSTUVWXYZ\\_-=+%#' LEN=${1:-16} i=0 while \\[ $i -le $LEN \\] ; do pos=$(( $RANDOM % ${#CHARS} )) echo -n ${CHARS:$pos:1} i=$(( $i + 1 )) done echo わざわざスクリプト書かなくても pwgen ってコマンドがあるらしい（そりゃそうだ）。apt や yum でも入るらしい。mac なら brew で。16文字を4個生成する例（-B は見分けづらい文字を除く、-s は厳密に random にする）\n$ pwgen -Bs 16 4 CgoTwgbJTpw9nCsg zRpWKdboyY7yFhEF Ajn4RTVpPVxyWaEq Hos9XNJfAkJfhjbX /dev/random, /dev/urandom から読み出して base64 とかもあるけど openssl コマンドでもできる\n$ openssl rand -base64 32 | head -c 16 EfU893c//rqfnMJS ","date":"2019年12月24日","permalink":"/2019/12/generationg-password/","section":"Posts","summary":"ちょいちょいランダムな文字列が欲しくなるので Perl で書いたスクリプト使ってたのだけど Bash で出来そうだなということで書いてみたのだが (記号の有無オプ","title":"bash でパスワード生成"},{"content":"初めての Mac を手にいれました。macOS は Catalina (10.15.2) でした。Control と Command キーの使い分けに慣れない。\n$ sw_vers ProductName:\tMac OS X ProductVersion:\t10.15.2 BuildVersion:\t19C57 Google Chrome のインストール # Safari で Chrome (googlechrome.dmg) をダウンロードし、アプリケーションフォルダにコピーする。\nbrew cask でインストールするのとどっちが良いのかな？\nHomebrew のインストール # 個人で入れるものはホームディレクトリ配下に入れるのが良いのかなと思い、Homebrew をホームディレクトリ以下にインストール を参考にインストール。\nmkdir $HOME/.homebrew \\ \u0026amp;\u0026amp; curl -L https://github.com/Homebrew/brew/tarball/master \\ | tar xz --strip 1 -C $HOME/.homebrew PATH=$HOME/.homebrew/bin:$PATH export HOMEBREW_CACHE=$HOME/.homebrew/cache brew update Homebrew cask # homebrew-cask で GUI アプリのインストール（管理）もできるらしい。\nbrew cask install visual-studio-code ~/Applications/ ではなく /Applications/ にインストールされました。\nBrew でいくつかインストール # Google Cloud SDK # brew cask install google-cloud-sdk bq, docker-credential-gcloud, gcloud, git-credential-gcloud.sh, gsutil が homebrew の bin/ に symlink される\njq # brew install jq tfenv # brew install tfenv goenv # brew install goenv が、古くて 1.11 までしか出てこないので手動でインストールすることにした\u0026hellip;\nkubectl # brew install kubernetes-cli $ kubectl version --short --client Client Version: v1.14.8 古っ！と思ったらこれは docker といっしょにインストールされた /usr/local/bin/kubectl だった\u0026hellip; 邪魔だな。brew で入れたのはこの時点 (2019-12-23) では 1.17.0 だった。\nGNU tools # Linux のつもりで使ってるとハマるので Gnu のツールを入れる。通常の path には gsed などと g prefix がついたものが置かれる。$(brew --prefix)/opt/gnu-sed/libexec/gnubin を PATH の手前の方に入れておけば g 無しで使える。\nbrew install coreutils brew install gnu-sed brew install gawk brew install gnu-tar direnv # github.com/direnv/direnv\nbrew install direnv ~/.zshrc に次の行を追加して hook を設定する\neval \u0026quot;$(direnv hook zsh)\u0026quot; wakeonlan # 家の別の PC を起こすため\nbrew install wakeonlan Terminal の設定 # Iceberge プロファイルをインストール\nプロファイルをダウンロードして、ターミナルアプリの環境設定からそれを読み込んで、自分好みにちょっといじる。フォントサイズとかウインドウサイズとか背景の透過とか。\nDocker のインストール # Homebrew でインストールする\nbrew install docker brew cask install docker open /Applications/Docker.app DockerをHomebrewでMac OSに導入する方法\nZsh # デフォルトの Login Shell は zsh でした。使ったことない。~/.zlogin, ~/.zshrc を書けば良いようだ。\nzplug # Zsh のプラグインマネージャーとして github.com/zplug/zplug をインストール\nbrew install zplug export ZPLUG_HOME=/Users/teraoka/.homebrew/opt/zplug source $ZPLUG_HOME/init.zsh # pure prompt 設定 zplug mafredri/zsh-async, from:github zplug sindresorhus/pure, use:pure.zsh, from:github, as:theme if ! zplug check --verbose; then printf \u0026#34;Install? [y/N]: \u0026#34; if read -q ; then echo; zplug install fi fi #zplug load --verbose zplug load pure-prompt # プロンプトは Powerline を入れようかと思ったけれども面倒だから github.com/sindresorhus/pure を使うことにしました。\nREADME には npm で入れる方法が書いてあったのですが、インストールに npm を使うだけで prompt としての動作には不要っぽいのでマニュアルインストールを選択\u0026hellip; したんだけど、その後 zplug で入れられることがわかった\nマニュアルインストール # mkdir -p \u0026quot;$HOME/.zsh\u0026quot; git clone https://github.com/sindresorhus/pure.git \u0026quot;$HOME/.zsh/pure\u0026quot; .zshrc への追加\nfpath+=(\u0026quot;$HOME/.zsh/pure\u0026quot;) autoload -U promptinit; promptinit prompt pure zplug でのインストール # マニュアルインストールのやつは全部不要\n.zshrc に入れる\nzplug mafredri/zsh-async, from:github zplug sindresorhus/pure, use:pure.zsh, from:github, as:theme Zsh の補完 # Homebrew でインストールしたコマンドの補完用スクリプトは $(brew --prefix)/share/zsh/site-functions/ 配下に symbolic link が張られるため、.zshrc でここを fpath に追加するなどする\nfpath+=($(brew --prefix)/share/zsh/site-functions) autoload -U compinit compinit -u Git # システムワイドな .gitignore 設定 # git config --global core.excludesfile ~/.gitignore_global グローバルで.gitignoreを適応する\ngithub.com/github/gitignore\ngit-secrets # brew install git-secrets git secrets --register-aws --global クラウド破産しないように git-secrets を使う\nRemote Desktop クライアント # Microsoft Remote Desktop 10\n","date":"2019年12月22日","permalink":"/2019/12/mac/","section":"Posts","summary":"初めての Mac を手にいれました。macOS は Catalina (10.15.2) でした。Control と Command キーの使い分けに慣れない。 $ sw_vers ProductName: Mac OS X ProductVersion: 10.15.2 BuildVersion: 19C57 Google Chrome のインストール # Safari で","title":"mac のセットアップ"},{"content":"kind のインストール # $ curl -Lo ~/bin/kind ¥ https://github.com/kubernetes-sigs/kind/releases/download/v0.6.1/kind-$(uname)-amd64 $ chmod +x ~/bin/kind $ kind version クラスタの作成・削除 # クラスタの作成 # $ kind create cluster Creating cluster \u0026quot;kind\u0026quot; ... ✓ Ensuring node image (kindest/node:v1.16.3) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \u0026quot;kind-kind\u0026quot; You can now use your cluster with: kubectl cluster-info --context kind-kind Not sure what to do next? 😅 Check out [https://kind.sigs.k8s.io/docs/user/quick-start/](https://kind.sigs.k8s.io/docs/user/quick-start/) kubectl コマンドのインストール\n$ k8sversion=v1.16.3 $ curl -Lo ~/bin/kubectl ¥ https://storage.googleapis.com/kubernetes-release/release/$k8sversion/bin/$(uname | tr A-Z a-z)/amd64/kubectl $ chmod +x ~/bin/kubectl $ kubectl cluster-info --context kind-kind Kubernetes master is running at https://127.0.0.1:39585 KubeDNS is running at https://127.0.0.1:39585/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 別名のクラスタを追加で作成 # $ kind create cluster --name kind2 (snip) kubectl cluster-info --context kind-kind2 クラスタの一覧を確認 # $ kind get clusters kind kind2 複数のクラスタは kubectl コマンドで context を指定することで切り替えられる。後述の kubectx を使うと便利\nクラスタの削除 # $ kind delete cluster Deleting cluster \u0026quot;kind\u0026quot; ... $ kind delete cluster --name kind2 Deleting cluster \u0026quot;kind2\u0026quot; ... 作成、削除で ~/.kube/config が更新されるので context を指定するだけでクラスタにアクセスできる。\nマルチノードクラスタ # 1 Control Plane + 2 Workers # $ cat 1cp-2wk-cluster.yml # three node (two workers) cluster kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker - role: worker $ kind create cluster --config 1cp-2wk-cluster.yml Creating cluster \u0026quot;kind\u0026quot; ... ✓ Ensuring node image (kindest/node:v1.16.3) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜 Set kubectl context to \u0026quot;kind-kind\u0026quot; You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! 👋 3 Control Planes + 3 Workers # $ cat 3cp-3wk-cluster.yml # a cluster with 3 control-plane nodes and 3 workers kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: control-plane - role: control-plane - role: worker - role: worker - role: worker $ kind create cluster --config 3cp-3wk-cluster.yml Creating cluster \u0026quot;kind\u0026quot; ... ✓ Ensuring node image (kindest/node:v1.16.3) 🖼 ✓ Preparing nodes 📦 ✓ Configuring the external load balancer ⚖️ ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining more control-plane nodes 🎮 ✓ Joining worker nodes 🚜 Set kubectl context to \u0026quot;kind-kind\u0026quot; You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! 👋 $ kubectl get pods --namespace kube-system NAME READY STATUS RESTARTS AGE coredns-5644d7b6d9-hznzl 1/1 Running 0 9m55s coredns-5644d7b6d9-p79sd 1/1 Running 0 9m55s etcd-kind-control-plane 1/1 Running 0 7m20s etcd-kind-control-plane2 1/1 Running 0 9m8s etcd-kind-control-plane3 1/1 Running 0 8m11s kindnet-9gppd 1/1 Running 0 9m9s kindnet-ht8rw 1/1 Running 0 7m34s kindnet-lc8l7 1/1 Running 0 7m34s kindnet-q78sq 1/1 Running 0 7m35s kindnet-xncxc 1/1 Running 0 9m35s kindnet-zvkcs 1/1 Running 0 8m12s kube-apiserver-kind-control-plane 1/1 Running 0 9m9s kube-apiserver-kind-control-plane2 1/1 Running 0 8m kube-apiserver-kind-control-plane3 1/1 Running 1 8m1s kube-controller-manager-kind-control-plane 1/1 Running 1 9m10s kube-controller-manager-kind-control-plane2 1/1 Running 0 8m1s kube-controller-manager-kind-control-plane3 1/1 Running 0 7m22s kube-proxy-5vbrr 1/1 Running 0 7m35s kube-proxy-6rqgc 1/1 Running 0 7m34s kube-proxy-dks2d 1/1 Running 0 9m55s kube-proxy-hd9dg 1/1 Running 0 7m34s kube-proxy-jkxvd 1/1 Running 0 8m12s kube-proxy-k546b 1/1 Running 0 9m9s kube-scheduler-kind-control-plane 1/1 Running 1 8m55s kube-scheduler-kind-control-plane2 1/1 Running 0 7m55s kube-scheduler-kind-control-plane3 1/1 Running 0 6m57s クラスタのカスタマイズ # Pod Subnet # kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 networking: podSubnet: \u0026quot;10.244.0.0/16\u0026quot; Pod Subnet\nService Subnet # kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 networking: serviceSubnet: \u0026quot;10.96.0.0/12\u0026quot; Service Subnet\nデフォルト CNI を無効にする # デフォルトでは kindnetd という CNI がセットアップされるが、Calico など別の CNI を使いたい場合はこれを無効にしておく。\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 networking: # the default CNI will not be installed disableDefaultCNI: true Calico を使う # default CNI (kindnetd) を無効にした Cluster の作成 # 前項の手順で default CNI を無効にした cluster を作成する\n$ kind create cluster --config disable-default-cni.yml Creating cluster \u0026quot;kind\u0026quot; ... ✓ Ensuring node image (kindest/node:v1.16.3) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing StorageClass 💾 Set kubectl context to \u0026quot;kind-kind\u0026quot; You can now use your cluster with: kubectl cluster-info --context kind-kind Not sure what to do next? 😅 Check out https://kind.sigs.k8s.io/docs/user/quick-start/ $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-5644d7b6d9-4fvl4 0/1 Pending 0 62s kube-system coredns-5644d7b6d9-8rskp 0/1 Pending 0 62s kube-system etcd-kind-control-plane 1/1 Running 0 17s kube-system kube-apiserver-kind-control-plane 1/1 Running 0 24s kube-system kube-controller-manager-kind-control-plane 0/1 Pending 0 2s kube-system kube-proxy-pcnt4 1/1 Running 0 62s kube-system kube-scheduler-kind-control-plane 0/1 Pending 0 8s CNI を無効にしているため CoreDNS が Pending 状態となっている\nPod Subnet を確認\n$ kubectl cluster-info dump | grep -- --cluster-cidr \u0026quot;--cluster-cidr=10.244.0.0/16\u0026quot;, calico.yaml の CALICO_IPV4POOL_CIDR をクラスタの Pod Subnet に合わせる\n$ curl -s https://docs.projectcalico.org/v3.11/manifests/calico.yaml | sed 's,192.168.0.0/16,10.244.0.0/16,' \u0026gt; calico.yaml $ kubectl apply -f calico.yaml configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.apps/calico-node created serviceaccount/calico-node created deployment.apps/calico-kube-controllers created serviceaccount/calico-kube-controllers created しばらく待つと Calico の Pod と CoreDNS の Pod が Running になっていることが確認できる\n$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-648f4868b8-ghzdt 1/1 Running 0 79s kube-system calico-node-mnftc 1/1 Running 0 79s kube-system coredns-5644d7b6d9-4fvl4 1/1 Running 0 6m29s kube-system coredns-5644d7b6d9-8rskp 1/1 Running 0 6m29s kube-system etcd-kind-control-plane 1/1 Running 0 5m44s kube-system kube-apiserver-kind-control-plane 1/1 Running 0 5m51s kube-system kube-controller-manager-kind-control-plane 1/1 Running 0 5m29s kube-system kube-proxy-pcnt4 1/1 Running 0 6m29s kube-system kube-scheduler-kind-control-plane 1/1 Running 0 5m35s Krew # kubectl plugin の Package manager krew.dev. kubectx や kubens をインストールするのに使う。\n( set -x; cd \u0026quot;$(mktemp -d)\u0026quot; \u0026amp;\u0026amp; curl -fsSLO \u0026quot;https://github.com/kubernetes-sigs/krew/releases/download/v0.3.3/krew.{tar.gz,yaml}\u0026quot; \u0026amp;\u0026amp; tar zxvf krew.tar.gz \u0026amp;\u0026amp; KREW=./krew-\u0026quot;$(uname | tr '[:upper:]' '[:lower:]')_amd64\u0026quot; \u0026amp;\u0026amp; \u0026quot;$KREW\u0026quot; install --manifest=krew.yaml --archive=krew.tar.gz \u0026amp;\u0026amp; \u0026quot;$KREW\u0026quot; update ) Installing plugin: krew Installed plugin: krew \\ | Use this plugin: | kubectl krew | Documentation: | https://sigs.k8s.io/krew | Caveats: | \\ | | krew is now installed! To start using kubectl plugins, you need to add | | krew's installation directory to your PATH: | | | | * macOS/Linux: | | - Add the following to your ~/.bashrc or ~/.zshrc: | | export PATH=\u0026quot;${KREW_ROOT:-$HOME/.krew}/bin:$PATH\u0026quot; | | - Restart your shell. | | | | * Windows: Add %USERPROFILE%\\.krew\\bin to your PATH environment variable | | | | To list krew commands and to get help, run: | | $ kubectl krew | | For a full list of available plugins, run: | | $ kubectl krew search | | | | You can find documentation at https://sigs.k8s.io/krew. | / / WARNING: You installed a plugin from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. + ./krew-linux_amd64 update Updated the local copy of plugin index. PATH=\u0026quot;${KREW_ROOT:-$HOME/.krew}/bin:$PATH\u0026quot; $ kubectl krew krew is the kubectl plugin manager. You can invoke krew through kubectl: \u0026quot;kubectl krew [command]...\u0026quot; Usage: krew [command] Available Commands: help Help about any command info Show information about a kubectl plugin install Install kubectl plugins list List installed kubectl plugins search Discover kubectl plugins uninstall Uninstall plugins update Update the local copy of the plugin index upgrade Upgrade installed plugins to newer versions version Show krew version and diagnostics Flags: -h, --help help for krew -v, --v Level number for the log level verbosity Use \u0026quot;krew [command] --help\u0026quot; for more information about a command. kubectx, kubens # Cluster と Namespace の切り替えを楽ちんにするツール。kubectx.dev.\nインストール # 上記の krew を使ってインストールする\nkubectl krew install ctx Updated the local copy of plugin index. Installing plugin: ctx Installed plugin: ctx \\ | Use this plugin: | kubectl ctx | Documentation: | https://github.com/ahmetb/kubectx | Caveats: | \\ | | If fzf is installed on your machine, you can interactively choose | | between the entries using the arrow keys, or by fuzzy searching | | as you type. | | | | See https://github.com/ahmetb/kubectx for customization and details. | / / WARNING: You installed a plugin from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. $ kubectl krew install ns Updated the local copy of plugin index. Installing plugin: ns Installed plugin: ns \\ | Use this plugin: | kubectl ns | Documentation: | https://github.com/ahmetb/kubectx | Caveats: | \\ | | If fzf is installed on your machine, you can interactively choose | | between the entries using the arrow keys, or by fuzzy searching | | as you type. | | | | See https://github.com/ahmetb/kubectx for customization and details. | / / WARNING: You installed a plugin from the krew-index plugin repository. These plugins are not audited for security by the Krew maintainers. Run them at your own risk. krew でインストールした場合は kubectx コマンドではなく kubectl ctx として実行する\nkubectl tree # Twitter で kubectl tree という便利ツールを知ったのでこれもインストール。\nkubectl krew install tree  I\u0026rsquo;m announcing a new fun kubectl subcommand:\n🌲 kubectl tree\nIt lets you explore Kubernetes object ownerships in a visual tree view.\nGet it today: https://t.co/mhVfAH1ila pic.twitter.com/gqPaYb4HeQ\n— ahmet alp balkan (@ahmetb) January 2, 2020\n ","date":"2019年12月20日","permalink":"/2019/12/kind-part-1/","section":"Posts","summary":"kind のインストール # $ curl -Lo ~/bin/kind ¥ https://github.com/kubernetes-sigs/kind/releases/download/v0.6.1/kind-$(uname)-amd64 $ chmod +x ~/bin/kind $ kind version クラスタの作成・削除 # クラスタの作成 # $ kind create cluster Creating cluster \u0026quot;kind\u0026quot; ... ✓ Ensuring node image (kindest/node:v1.16.3) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing","title":"kind で kubernetes に触れる (1)"},{"content":"","date":"2019年11月1日","permalink":"/tags/python/","section":"Tags","summary":"","title":"python"},{"content":"先日から go-chromecast で遊んでいますが、これは息子の目覚まし時計代わりに Google Home を使ってるからなのですね。で、適当なメッセージを喋らせていたんですが考えるのも面倒なので今日が何の日かを喋らせてみることにしてみました。\nで、そんな API を提供してくれてるところはないかとググってみたけどなさそうで、Wikipedia の「できごと」ってところを取ってくれば良さそうだとうことはわかりました。Scraping するなら Python の BeautifulSoup だろうということで書いてみた。できごとのリストから3つ取ってきて表示します。\npip install requests beautifulsoup4 lxml で、試しに実行してみるとこんなことになって、10月31日に飛行機墜落しすぎだろ！！（たまたまこの3つが出てきてびっくり）\n1994年 - アメリカン・イーグル航空4184便墜落事故。68名全員死亡。 1999年 - エジプト航空990便墜落事故。203名死亡。 2000年 - シンガポール航空006便墜落事故。乗員乗客83名が死亡。 まあ、私なら本田翼に起こしてほしいわけですけど\n","date":"2019年11月1日","permalink":"/2019/11/kyohanannohi/","section":"Posts","summary":"先日から go-chromecast で遊んでいますが、これは息子の目覚まし時計代わりに Google Home を使ってるからなのですね。で、適当なメッセージを喋らせていたんですが考えるの","title":"Wikipedia から今日は何の日かを取得する"},{"content":"先日、go-chromecast で Google Home Mini に任意のメッセージを喋らせる で Google Home に Google の Text-to-Speech を使って任意のメッセージを喋らせましたが、Google のサービスには日本語の声色が1種類しかなく楽しくありません。そこで、Amazon Polly にも喋ってもらうことにしました。\nまた go-chromecast に手を入れても良いのですが、Amazon Polly は awscli でも簡単に音声ファイルを取得可能だったので、shell script でさくっと試しました。Polly は日本語でも女性の声と男性の声の2種類がありました。（いまのところ、しゃべりの滑らかさは Google の方が良さそうです）\nawscli でテキストから音声ファイルを作成するには次のように実行するだけです。voice-id は女性なら Mizuki を、男性なら Takumi を指定するだけです。\naws polly synthesize-speech \\\\ --output-format mp3 \\\\ --voice-id $voice\\_id \\\\ --text \u0026quot;$message\u0026quot; \\\\ voice.mp3 こうして作成した音声ファイルを再生するには、次のように load サブコマンドを実行するだけです\ngo-chromecast load \\\\ /some/where/audio.mp3 \\\\ -n $NODE\\_NAME -i $INTERFACE 次は LINE Bot を使って外から喋らせようかなと。固定電話をやめたら外から家族に電話しても気付けなかったりするので「電話に出てね」と喋らせるのは有効なんじゃないかと思ってる。\nと書いた後で、これは Google Home アプリを使えば外出先からもブロードキャスト機能が使えるし、家族を招待することが出来ることに気付いた。\n","date":"2019年10月21日","permalink":"/2019/10/play-polly-voice-audio-on-google-home-with-go-chromecast/","section":"Posts","summary":"先日、go-chromecast で Google Home Mini に任意のメッセージを喋らせる で Google Home に Google の Text-to-Speech を使って任意のメッセージを喋らせましたが、Google のサ","title":"Google Home に Amazon Polly の声で喋らせる"},{"content":"","date":"2019年10月21日","permalink":"/tags/ubuntu/","section":"Tags","summary":"","title":"Ubuntu"},{"content":"","date":"2019年10月17日","permalink":"/tags/keyboard/","section":"Tags","summary":"","title":"keyboard"},{"content":"Laptop でしばらく Windows 10 を使っていたのですが、WSL2 を使おうと思っていじってたらいつまで経ってもシャットダウンが完了せず、毎回強制的に電源を切らなければならない不自由なことになってしまい、もう嫌になったのでまた Linux にもどしました。で、私の Laptop はもうガタが来ており、右矢印キーが壊れていて反応しません。でもたまに使うことがあるので、その上に位置する右シフトキーに右矢印キーとして機能してもらうことにします。Windows では ChangeKey というフリーソフトで対応してました。\nCapsLock と Ctrl キーを入れ替えるのは簡単にできるようになっているのですが、細かなカスタマイズは自分で頑張る必要があるみたいです。幸い先人が方法を公開してくれてるのでググるだけですけど。\n今回は「Ubuntu 16.04 で XKB を使ってキーマップをカスタマイズする」を参考にさせていただきました。\n$ mkdir -p .xkb/{keymap,symbols} $ setxkbmap -print \u0026gt; ~/.xkb/keymap/mykbd $ cat \u0026gt; ~/.xkb/symbols/custom \u0026lt;\u0026lt;\\_EOF\\_ xkb\\_symbols \u0026quot;myright\u0026quot; { replace key \u0026lt;RTSH\u0026gt; { \\[ Right \\] }; }; \\_EOF\\_ ```として、こんな感じのファイルを作ります。``` $ tree .xkb .xkb ├── keymap │ └── mykbd └── symbols └── custom 2 directories, 2 files ```なんだか Qiita の記事のコピーみたいになってきたが将来の私のためにメモっておく``` $ mkdir ~/bin $ cat \u0026gt; ~/bin/load\\_xkbmap.sh \u0026lt;\u0026lt;\\_EOF\\_ #!/bin/bash if \\[ -s $HOME/.xkb/keymap/mykbd \\] ; then sleep 1 xkbcomp -I$HOME/.xkb $HOME/.xkb/keymap/mykbd $DISPLAY 2\u0026gt; /dev/null fi \\_EOF\\_ $ chmod +x ~/bin/load\\_xkbmap.sh でもって、gnome-session-properties を起動して Startup Programs に登録しておきます。\n以上\n","date":"2019年10月17日","permalink":"/2019/10/customize-keymap-on-ubuntu-18-04-xkb-using-xkb/","section":"Posts","summary":"Laptop でしばらく Windows 10 を使っていたのですが、WSL2 を使おうと思っていじってたらいつまで経ってもシャットダウンが完了せず、毎回強制的に電源を切らな","title":"Ubuntu 18.04 で xkb を使ってキーマップをカスタマイズする"},{"content":"","date":"2019年10月7日","permalink":"/tags/lightsail/","section":"Tags","summary":"","title":"Lightsail"},{"content":"以前、「AWS Lightsail の snapshot 取得を自動化する」という記事を書きましたが、先日 Lightsail の自動スナップショット管理機能がリリースされました「Amazon Lightsail now provides automatic snapshots」。日次でスナップショットを取得し、7日分キープしてくれます。\nということで、lightsail-auto-snapshots を使ってセットアップしたものを削除して、ネイティブの Automatic Snapshots 機能を有効にすることにします。\nlightsail-auto-snapshots は CloudFormation を使って CloudWatch Evnets をトリガーに Lambda を実行するようになっているので、CloudFormation のスタックを削除すれば丸っと消すことができます。自動削除も停止してしまうので取得済みの snapshot は別途削除が必要です。\n次に Automatic Snapshots を有効にします\nLightsail の Snapshots タグにある次のトグルをオンにします # 確認ダイアログが表示されるので読んで有効にします # 有効になりました # 東京リージョンの場合、デフォルトでは日本時間の朝10時に snapshot が取得されるようです\n取得時刻は変更可能です # 試しに朝4時にしてみます\n以上です。簡単ですね\n","date":"2019年10月7日","permalink":"/2019/10/lightsail-automatic-snapshots/","section":"Posts","summary":"以前、「AWS Lightsail の snapshot 取得を自動化する」という記事を書きましたが、先日 Lightsail の自動スナップショット管理機能がリリースされました「Amazon Lightsail now provides","title":"Lightsail の snapshot を自動で取得する"},{"content":"Google Home Mini に任意のメッセージを喋らせる OSS は昔からいくつかあるし、過去にそれをいじってブラウザから任意のメッセージを与えて喋らせるアプリを書いたりもしていました（go の source code を紛失）が、今回は cron などで目覚まし時計代わりに使おうと思って Google の Text-to-Speech 使って何か書こうかなって思ったわけですが、go-chroemcast が良くできていたので、これをいじって遊びました。私に必要だったコードは Pull Request しておきました。Merge してもらいました。\n私の感じる go-chromecast の良い点は、デバイスを名前や UUID で指定できること。mDNS で探して名前で選んでくれます（デバイス名を日本語（マルチバイト）にしていると対応できない、使いたかったらプルリクしましょう）。未指定だと一覧を表示して選択を求められます。\nハマった点 # go-chromecast は一時的に HTTP サーバーを立ち上げて、Google Home にその URL を伝えてメディアファイルをダウンロードさせるわけですが、Network Interface が複数ある場合はどの Interface を Listen させるかを指定してあげないと、Google Home からアクセスできなくて再生させられません。我が家のラズパイは VPN サーバーとしても使っているため、この VPN で払い出すサブネットのIPアドレスを Listen してることに気づかずにしばらくハマりました。でもちゃんと Interface 名を指定するオプションがありました。すばらしい。\nいじった点 # Text-to-Speech が使えるようはなっていたのですが、言語が en-US 固定だったので、コマンドラインオプションで指定可能にしました。また、gocui を使った ui からでないと volume のコントロールができなかったので volume コマンドを追加しました。\n次のようにして喋らせることができるようになりました。\n#!/bin/bash NODE_NAME=Bedroom INTERFACE=wlan0 MESSAGE=\u0026quot;起きてください、遅刻します\u0026quot; volume=$(go-chromecast -n $NODE_NAME volume) go-chromecast -n $NODE_NAME volume 0.8 go-chromecast tts \\ \u0026quot;$MESSAGE\u0026quot; \\ --google-service-account ~/account.json \\ --language-code ja-JP \\ -n $NODE_NAME -i $INTERFACE go-chromecast -n $NODE_NAME volume $volume 課題 # 声が味気ない・・・。でも go-chroemcast は音声ファイルを再生させることもできるので、目覚ましボイスを YouTube で探そうかなと思ってます。ちらっと見てたら「世にも奇妙な物語」のテーマが出てきました、ひどく恐ろしい寝起きとなりそうです 😱\n息子が目覚ましかけないから、面白メッセージで起こしてやろうとしてるのに嫌だと言って Google Home Mini の電源を切ってしまうのが一番の問題\u0026hellip;\nおまけ # $ go-chromecast Control your Google Chromecast or Google Home Mini from the command line. Usage: go-chromecast [command] Available Commands: help Help about any command load Load and play media on the chromecast ls List devices next Play the next available media pause Pause the currently playing media on the chromecast playlist Load and play media on the chromecast previous Play the previous available media restart Restart the currently playing media rewind Rewind by seconds the currently playing media seek Seek by seconds into the currently playing media status Current chromecast status stop Stop casting tts text-to-speech ui Run the UI unpause Unpause the currently playing media on the chromecast volume Get or set volume watch Watch all events sent from a chromecast device Flags: -a, --addr string Address of the chromecast device --debug debug logging -d, --device string chromecast device, ie: 'Chromecast' or 'Google Home Mini' -n, --device-name string chromecast device name --disable-cache disable the cache -h, --help help for go-chromecast -i, --iface string Network interface to use when looking for a local address to use for the http server -p, --port string Port of the chromecast device if 'addr' is specified (default \u0026quot;8009\u0026quot;) -u, --uuid string chromecast device uuid --with-ui run with a UI Use \u0026quot;go-chromecast [command] --help\u0026quot; for more information about a command. Amazon Polly (追記) # GCP の Text-to-Speech には日本語の声の種類がひとつしかなく、機械っぽい声なので楽しくありません。AWS には Amazon Polly というサービスがあり、こちらには男女1つずつの声がありました。（結局のところどっちも残念な感じであったが）\nAmazon Polly は awscli でも簡単に音声ファイルを取得することが可能です。Mizuki が女性で Takumi が男性の声です。\naws polly synthesize-speech \\ --output-format mp3 \\ --voice-id Mizuki \\ --text 'テストです' \\ mizuki.mp3 aws polly synthesize-speech \\ --output-format mp3 \\ --voice-id Takumi \\ --text 'テストです' \\ takumi.mp3 音声ファイルが生成できたら go-chromecast の load コマンドで再生させることが可能です。\ngo-chromecast load /path/to/mizuki.mp3 -n $NODE_NAME -i $INTERFACE ","date":"2019年10月7日","permalink":"/2019/10/google-home-mini-and-text-to-speech/","section":"Posts","summary":"Google Home Mini に任意のメッセージを喋らせる OSS は昔からいくつかあるし、過去にそれをいじってブラウザから任意のメッセージを与えて喋らせるアプリを書いたり","title":"go-chromecast で Google Home Mini に任意のメッセージを喋らせる"},{"content":"","date":"2019年10月7日","permalink":"/tags/googlehome/","section":"Tags","summary":"","title":"GoogleHome"},{"content":"","date":"2019年9月14日","permalink":"/tags/ansible/","section":"Tags","summary":"","title":"ansible"},{"content":"外部のサービスとの連携などで IP アドレスを使った設定をすることがあります。この場合に、その相手がテキストや JSON で IP アドレスのリストを公開してくれていれば、Ansible ではその URL から直接変数に取り込むことが可能です。\nLookup Plugin の url というやつを使います。試してみましょう。\nStripe は Domains and IP Addresses というページで IP アドレスの情報を公開していますが https://stripe.com/files/ips/ips_webhooks.txt や https://stripe.com/files/ips/ips_webhooks.json でテキストや JSON で公開してくれています。\n次の Playbook を site.yml として保存して実行してみます。\n\\- hosts: all gather\\_facts: no vars: text1: \u0026quot;{{ lookup('url', 'https://stripe.com/files/ips/ips\\_webhooks.txt') }}\u0026quot; text2: \u0026quot;{{ lookup('url', 'https://stripe.com/files/ips/ips\\_webhooks.txt', wantlist=True) }}\u0026quot; json1: \u0026quot;{{ lookup('url', 'https://stripe.com/files/ips/ips\\_webhooks.json') }}\u0026quot; json2: \u0026quot;{{ lookup('url', 'https://stripe.com/files/ips/ips\\_webhooks.json', split\\_lines=False) }}\u0026quot; tasks: - name: text1 debug: var: text1 - name: text2 debug: var: text2 - name: json1 debug: var: json1 - name: json2 debug: var: json2 $ ansible-playbook -i localhost, site.yml \u0026ndash;connection=local\nPLAY [all] *****************************************************************************************\nTASK [text1] *************************************************************************************** ok: [localhost] =\u0026gt; text1: 54.187.174.169,54.187.205.235,54.187.216.72,54.241.31.99,54.241.31.102,54.241.34.107\nTASK [text2] *************************************************************************************** ok: [localhost] =\u0026gt; text2:\n 54.187.174.169 54.187.205.235 54.187.216.72 54.241.31.99 54.241.31.102 54.241.34.107  TASK [json1] *************************************************************************************** ok: [localhost] =\u0026gt; json1: \u0026ldquo;{,\\t\\\u0026ldquo;WEBHOOKS\\\u0026quot;:[,\\t\\t\\\u0026ldquo;54.187.174.169\\\u0026rdquo;,,\\t\\t\\\u0026ldquo;54.187.205.235\\\u0026rdquo;,,\\t\\t\\\u0026ldquo;54.187.216.72\\\u0026rdquo;,,\\t\\t\\\u0026ldquo;54.241.31.99\\\u0026rdquo;,,\\t\\t\\\u0026ldquo;54.241.31.102\\\u0026rdquo;,,\\t\\t\\\u0026ldquo;54.241.34.107\\\u0026rdquo;,\\t],}\u0026rdquo;\nTASK [json2] *************************************************************************************** ok: [localhost] =\u0026gt; json2: WEBHOOKS: - 54.187.174.169 - 54.187.205.235 - 54.187.216.72 - 54.241.31.99 - 54.241.31.102 - 54.241.34.107\nPLAY RECAP ***************************************************************************************** localhost : ok=4 changed=0 unreachable=0 failed=0\n * テキストファイルは1行1アドレスのリストですが、text1 の方法では各行がカンマ区切りで1つの文字列として取り込まれました * text2 は `wantlist=True` を指定したことで各行を要素としたリストとして取り込まれました * json1 は JSON という形式を無視して text1 と同様に各行がカンマ区切りで1つの文字列として取り込まれました * json3 は `split_lines=False` と指定したことで JSON の構造を維持した状態で取り込まれました 以上","date":"2019年9月14日","permalink":"/2019/09/ansible-lookup-plugin-url/","section":"Posts","summary":"外部のサービスとの連携などで IP アドレスを使った設定をすることがあります。この場合に、その相手がテキストや JSON で IP アドレスのリストを公開してくれ","title":"Ansible で変数を URL から読み込む"},{"content":"Ansible で一連の処理を実行する際に、途中で失敗したらそれまでの変更も元に戻したいといったことがあるかもしれません。そんな場合に使えるのが Block 機能です。Ansible 2.3 （2017年4月）から使える結構古い機能です。\n存在は知っていたけど使ったことなかったのでどんなものか試してみます。\nblock 内の task が失敗した場合に、block 内の以降の処理をスキップして rescue 内の task が実行されます。 rescue 内の task が失敗するとそこでそのホストへの処理は中断されます。\n次のように書けば step 2 でコケて rescue の task が実行される\n\\- hosts: all gather\\_facts: no tasks: - name: some procedure block: - name: step 1 command: /bin/true - name: step 2 \\# 失敗する command: /bin/false - name: step 3 \\# 前の task が失敗しているため実行されない command: /bin/true rescue: \\# block 内がすべて成功していれば rescue 内は実行されない - name: rescue 1 command: /bin/false \\# 失敗すると継続の処理が実行されないため false にする failed\\_when: false - name: rescue 2 debug: msg: \u0026quot;Rescue 2\u0026quot; - name: failed task debug: \\# 失敗した task の情報が ansible\\_failed\\_task 変数に入っている var: ansible\\_failed\\_task.name - name: failed result debug: \\# 失敗した task の結果が ansible\\_failed\\_result 変数入っている、register で登録されるのと同じ内容 var: ansible\\_failed\\_result 上記の Playbook を site.yml として保存し、実際に実行してみると次のようになります。（環境変数 ANSIBLE_STDOUT_CALLBACK を yaml にしているため出力が JSON ではなく YAML になっています）\n$ ansible-playbook -i localhost, site.yml --connection=local PLAY \\[all\\] \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* TASK \\[step 1\\] \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* changed: \\[localhost\\] TASK \\[step 2\\] \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* fatal: \\[localhost\\]: FAILED! =\u0026gt; changed=true cmd: - /bin/false delta: '0:00:00.007922' end: '2019-09-14 08:51:21.915700' msg: non-zero return code rc: 1 start: '2019-09-14 08:51:21.907778' stderr: '' stderr\\_lines: \\[\\] stdout: '' stdout\\_lines: TASK \\[rescue 1\\] \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* changed: \\[localhost\\] TASK \\[rescue 2\\] \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* ok: \\[localhost\\] =\u0026gt; msg: Rescue 2 TASK \\[failed task\\] \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* ok: \\[localhost\\] =\u0026gt; ansible\\_failed\\_task.name: step 2 TASK \\[failed result\\] \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* ok: \\[localhost\\] =\u0026gt; ansible\\_failed\\_result: changed: true cmd: - /bin/false delta: '0:00:00.007922' end: '2019-09-14 08:51:21.915700' failed: true invocation: module\\_args: \\_raw\\_params: /bin/false \\_uses\\_shell: false argv: null chdir: null creates: null executable: null removes: null stdin: null warn: true msg: non-zero return code rc: 1 start: '2019-09-14 08:51:21.907778' stderr: '' stderr\\_lines: \\[\\] stdout: '' stdout\\_lines: \\[\\] PLAY RECAP \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\* localhost : ok=5 changed=2 unreachable=0 failed=1 ","date":"2019年9月14日","permalink":"/2019/09/ansible-block-error-handling/","section":"Posts","summary":"Ansible で一連の処理を実行する際に、途中で失敗したらそれまでの変更も元に戻したいといったことがあるかもしれません。そんな場合に使えるのが Block 機能です","title":"Ansible の block でエラーハンドリング"},{"content":"Kubernetes in docker (kind) を使えるようになっておこうと思います。今回は DigitalOcean の CentOS 7 で試す。\nDocker CE のインストール # Get Docker Engine - Community for CentOS\nsudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli containerd.io sudo usermod -a -G docker centos sudo systemctl start docker sudo systemctl enable docker kind のインストール # sudo curl -Lo /usr/bin/kind https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-linux-amd64 sudo chmod +x /usr/bin/kind kubectl のインストール # sudo curl -Lo /usr/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl sudo chmod +x /usr/bin/kubectl kind の実行 # kind create cluster と実行するだけで Kubernetes クラスタが起動する。\n$ kind create cluster Creating cluster \u0026quot;kind\u0026quot; ... ✓ Ensuring node image (kindest/node:v1.15.3) 🖼 ✓ Preparing nodes 📦 ✓ Creating kubeadm config 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Cluster creation complete. You can now use the cluster with: export KUBECONFIG=\u0026quot;$(kind get kubeconfig-path --name=\u0026quot;kind\u0026quot;)\u0026quot; kubectl cluster-info けど、docker ps では1コンテナ起動してるだけだな\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 260a0db4daf7 kindest/node:v1.15.3 \u0026quot;/usr/local/bin/entr…\u0026quot; 13 minutes ago Up 13 minutes 37630/tcp, 127.0.0.1:37630-\u0026gt;6443/tcp kind-control-plane docker exec で ps してみると次のようになっている\n# ps auxwwf USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 9013 0.2 0.0 4040 2076 pts/1 Ss 14:25 0:00 bash root 9122 0.0 0.0 5972 1464 pts/1 R+ 14:25 0:00 \\_ ps auxwwf root 1 0.0 0.0 17648 6260 ? Ss 14:10 0:00 /sbin/init root 34 0.0 0.0 24640 6648 ? S\u0026lt;s 14:10 0:00 /lib/systemd/systemd-journald root 45 2.1 0.6 2360932 48788 ? Ssl 14:10 0:19 /usr/bin/containerd root 283 0.0 0.0 10732 3768 ? Sl 14:10 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/03e619f69de43ddc43b1641fb24ac9b0b0c362aa6018999b81b6e894995b72bb -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 349 0.0 0.0 1012 4 ? Ss 14:10 0:00 | \\_ /pause root 294 0.0 0.0 9324 3308 ? Sl 14:10 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/6fd68362e810b18a2356142f473499626842e55fd1e38dc60dd602c2b4f918c6 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 329 0.0 0.0 1012 4 ? Ss 14:10 0:00 | \\_ /pause root 316 0.0 0.0 10732 3796 ? Sl 14:10 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/9f055513abd1d8723515ad210dfcd3b31ce8d262d3aacbe48bbe575c4e46ac31 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 358 0.0 0.0 1012 4 ? Ss 14:10 0:00 | \\_ /pause root 317 0.0 0.0 10732 3564 ? Sl 14:10 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/14df61c598ebf1f6fed8bebc9c4f80b1f11a37a16c29f175d66136525c9e6b60 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 363 0.0 0.0 1012 4 ? Ss 14:10 0:00 | \\_ /pause root 491 0.0 0.0 10732 3540 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/d71810b32bd0ae25b4406449803f7ef496f5ca3cbd6f784557538b654871b709 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 508 0.3 0.3 141480 29196 ? Ssl 14:11 0:02 | \\_ kube-scheduler --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true root 542 0.0 0.0 10732 3716 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/b9dcfe2ef38c93bbc0cb9dbde378b4a5824ccc208c86d39629d668dc4cd58489 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 574 1.8 0.9 217568 72104 ? Ssl 14:11 0:16 | \\_ kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --enable-hostpath-provisioner=true --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --node-cidr-mask-size=24 --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --use-service-account-credentials=true root 551 0.2 0.0 11788 4804 ? Sl 14:11 0:02 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/973aec49f3de28367798e25733cd4863397cccc9aab775d9dbcbe145381f787c -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 580 1.9 0.4 10537600 34244 ? Ssl 14:11 0:17 | \\_ etcd --advertise-client-urls=https://172.17.0.2:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --initial-advertise-peer-urls=https://172.17.0.2:2380 --initial-cluster=kind-control-plane=https://172.17.0.2:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://172.17.0.2:2379 --listen-peer-urls=https://172.17.0.2:2380 --name=kind-control-plane --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt root 639 0.0 0.0 10732 3628 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/af9f6dfbc2bb2a8b807e0028de9020c77dea646339b835cc90be74884e6264bd -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 661 3.8 2.8 402920 224844 ? Ssl 14:11 0:34 | \\_ kube-apiserver --advertise-address=172.17.0.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --insecure-port=0 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key root 862 0.0 0.0 9324 3536 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/4879d4a5e3d75990f15f95fb7e54c1afb80396fe59fcfcab4f43efc47e9103ff -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 879 0.0 0.0 1012 4 ? Ss 14:11 0:00 | \\_ /pause root 884 0.0 0.0 9324 3400 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/e9f36ddb48b6571ba9998ac5a5625b92c2b9908b9b7070d8566af02860295d74 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 907 0.0 0.0 1012 4 ? Ss 14:11 0:00 | \\_ /pause root 963 0.0 0.0 9324 3240 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/f6fd15e55e522bc4b48ba3b1672963e2ad652f3d04c69a60fae9bfa0a026924a -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 1002 0.1 0.3 139724 24172 ? Ssl 14:11 0:01 | \\_ /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=kind-control-plane root 967 0.0 0.0 10732 5264 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/bf7aea0752d89b06394670a2ce49da52096604beff7cfeddaf057eb83d6c030d -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 995 0.0 0.1 130224 15516 ? Ssl 14:11 0:00 | \\_ /bin/kindnetd root 1246 0.0 0.0 10796 3500 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/828fb3f0c098eb67209152d7697c468bfe9fa508a440f3bb9b56120ae6336f3e -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 1263 0.0 0.0 1012 4 ? Ss 14:11 0:00 | \\_ /pause root 1297 0.0 0.0 9324 3544 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/3c1bbc921022e0151d4d32ca680462ea14130fd5e823cdfbf8317c275f90b4d3 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 1314 0.3 0.3 142788 24596 ? Ssl 14:11 0:02 | \\_ /coredns -conf /etc/coredns/Corefile root 1375 0.0 0.0 9324 3628 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/11963940dfda62ca5eea86ebaeffc1441113b059423b9a1e4515165fe5d58b92 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 1392 0.0 0.0 1012 4 ? Ss 14:11 0:00 | \\_ /pause root 1430 0.0 0.0 10732 3612 ? Sl 14:11 0:00 \\_ containerd-shim -namespace k8s.io -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/k8s.io/bd01f1f3ae96c3b6a392c0055ac4a10afdadff7695078c7b34ce8a6de2decca7 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd root 1447 0.2 0.3 142788 24136 ? Ssl 14:11 0:02 \\_ /coredns -conf /etc/coredns/Corefile root 239 2.4 0.8 1623832 68848 ? Ssl 14:10 0:22 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --fail-swap-on=false --node-ip=172.17.0.2 --fail-swap-on=false ふむふむ、Docker in Docker ですね。\nkubectl で確認 # $ export KUBECONFIG=\u0026quot;$(kind get kubeconfig-path --name=\u0026quot;kind\u0026quot;)\u0026quot; $ kubectl cluster-info Kubernetes master is running at https://127.0.0.1:37630 KubeDNS is running at https://127.0.0.1:37630/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 33m [centos@kind ~]$ kubectl get services --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 443/TCP 33m kube-system kube-dns ClusterIP 10.96.0.10 53/UDP,53/TCP,9153/TCP 33m あ、たまたま kubectl と同じバージョンだったけどバージョン合わせるように気を付ける必要があるか\n$ kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;15\u0026quot;, GitVersion:\u0026quot;v1.15.3\u0026quot;, GitCommit:\u0026quot;2d3c76f9091b6bec110a5e63777c332469e0cba2\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2019-08-19T11:13:54Z\u0026quot;, GoVersion:\u0026quot;go1.12.9\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;15\u0026quot;, GitVersion:\u0026quot;v1.15.3\u0026quot;, GitCommit:\u0026quot;2d3c76f9091b6bec110a5e63777c332469e0cba2\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2019-08-20T18:57:36Z\u0026quot;, GoVersion:\u0026quot;go1.12.9\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} Multi-node cluster # kind create に \u0026ndash;config でコンフィグを渡せば node 数を指定することができる。kind-example-config.yaml\ncurl -LO https://raw.githubusercontent.com/kubernetes-sigs/kind/master/site/content/docs/user/kind-example-config.yaml $ kind create cluster --config kind-example-config.yaml Creating cluster \u0026quot;kind\u0026quot; ... ✓ Ensuring node image (kindest/node:v1.15.3) 🖼 ✓ Preparing nodes 📦📦📦📦 ✓ Creating kubeadm config 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜 Cluster creation complete. You can now use the cluster with: export KUBECONFIG=\u0026quot;$(kind get kubeconfig-path --name=\u0026quot;kind\u0026quot;)\u0026quot; kubectl cluster-info $ kubectl get nodes NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 2m40s v1.15.3 kind-worker Ready \u0026lt;none\u0026gt; 2m4s v1.15.3 kind-worker2 Ready \u0026lt;none\u0026gt; 2m4s v1.15.3 kind-worker3 Ready \u0026lt;none\u0026gt; 2m3s v1.15.3 Control Plane も複数台にするには kind-example-config.yaml の nodes を次のように変更して kind create すれば Control Plane が3台になる\nnodes: - role: control-plane - role: control-plane - role: control-plane - role: worker - role: worker - role: worker $ kubectl get nodes NAME STATUS ROLES AGE VERSION kind-control-plane Ready master 2m36s v1.15.3 kind-control-plane2 Ready master 2m2s v1.15.3 kind-control-plane3 Ready master 86s v1.15.3 kind-worker Ready 60s v1.15.3 kind-worker2 Ready 60s v1.15.3 kind-worker3 Ready 60s v1.15.3 便利ツールだ。kind のドキュメントは https://kind.sigs.k8s.io/ にある。\n","date":"2019年8月26日","permalink":"/2019/08/kubernetes-in-docker/","section":"Posts","summary":"Kubernetes in docker (kind) を使えるようになっておこうと思います。今回は DigitalOcean の CentOS 7 で試す。 Docker CE のインストール # Get Docker Engine - Community for CentOS sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine sudo","title":"Kubernetes in docker の使い方"},{"content":"","date":"2019年8月6日","permalink":"/tags/deco/","section":"Tags","summary":"","title":"deco"},{"content":"Deco M5 の新しい firmware が公開されていたので更新しました。全然関係なけど今「TCP 技術入門」読んでます。\n今回の更新内容 # Modifications and Bug Fixes: #  Added smart home hub feature and supported to manage kasa devices (lights/plugs/switches) and philips hue lights (hue bridge required) in Deco app. Added shortcut and automation features. Displayed which client is connected to which deco and which Wi-Fi band. Displayed how Decos are connected to each other. Added support for IPTV bridge mde and custom mode. Added more options for notification feature. Added IPv6 firewall rules feature. Supported up to 64 entries each for Address Reseervation and Port Forwarding.  Notes: #  Please update your Deco App to the latest version in order to use new features. If there are other Deco models in your Deco network, you may miss some new features till other models release updates in the near fu\u0026hellip;  最後なぜか省略されてしまっていますが future でしょう。\nうれしい更新 # まあ、うまいこと機能してくれてるのであまり大した意味はないのですが、今回の更新でついにどの端末がどの AP (Deco) に接続されているのかが確認できるようになりました。部屋を移動したときにほんとにちゃんとハンドオーバーされてるのかな？というのが確認できるようになりました。また、どの端末がどの周波数 (2.4GHz なのか 5GHz なのか) で接続しているのかも確認できます。さらに、Deco 同士がどのように接続されているのかも確認できます。これも、導入時に Ethernet Backhaul を設定したのだけれども果たしてそれが意図したとおりになっているのだろうかというのが確認できるようになりました。\nどの AP に接続されているのか # 「現在接続されているDeco」という欄に「寝室」とあります。移動前は「リビング」でした。ちゃんと機能しているようです。 AP にどの端末が接続されているか、AP 同士はどうか # 先ほどの「寝室」というところをタップするとその AP (Deco) の情報が確認できます。上位回線と接続されているのはリビングの Deco なので「接続元」が「リビング」となっており、その右に「有線LAN」と表示されており Ethernet Backhaul が機能していることが確認できます。「接続中のクライアント」欄でこの Deco に接続されている端末一覧が確認できます。 その他の機能 # オートメーション # わが家にスマートライトやスマートプラグは無いので使い道がないのですが、特定の端末が接続されたら電源を入れるといったオートメーション設定ができるようになっていました。「自分のスマホが切断されたら照明をOFFにする」とかが出来ると便利なのかな？デバイスなしでも使えるのは「セキュリティの脅威」が検知されたら「通知」するくらいかな？\nショートカット # こちらもスマートライトやスマートプラグがあれば、特定の操作をショートカットとして登録して簡単に実行することができるようです。アプリからリモートで操作できるのかな？\nその他 # IPTV (おそらく日本では関係ない) 関連とか、IPv6 の firewall ルールが設定可能になった。DHCP の IP アドレス予約とポートフォワード設定が 64 個まで指定できるようになったよと。\n以上です。更新しましょう。\nDeco M5 2個セットは2,500円 OFF のクーポンがあるみたいですね。\n","date":"2019年8月6日","permalink":"/2019/08/deco-m5-firmware-1-3-1-build-20190617-rel-47476/","section":"Posts","summary":"Deco M5 の新しい firmware が公開されていたので更新しました。全然関係なけど今「TCP 技術入門」読んでます。 今回の更新内容 # Modifications and Bug Fixes: # Added smart home hub feature and supported to manage kasa","title":"Deco M5 の firmware 更新 1.3.1 Build 20190617 Rel. 47476"},{"content":"","date":"2019年8月6日","permalink":"/tags/wifi/","section":"Tags","summary":"","title":"WiFi"},{"content":"","date":"2019年7月24日","permalink":"/tags/curl/","section":"Tags","summary":"","title":"curl"},{"content":"curl には -w, \u0026ndash;write-out というオプションがあり、HTTP のコードやどのフェーズに何秒かかったかなどを出力することができます。ときどき調査で使うのですが、毎回 man curl することになるのでコピペで使えるようにメモっておく\ncurl -so /dev/nul -w \u0026quot;http\\_code: %{http\\_code}\\\\ntime\\_namelookup: %{time\\_namelookup}\\\\ntime\\_connect: %{time\\_connect}\\\\ntime\\_appconnect: %{time\\_appconnect}\\\\ntime\\_pretransfer: %{time\\_pretransfer}\\\\ntime\\_starttransfer: %{time\\_starttransfer}\\\\ntime\\_total: %{time\\_total}\\\\n\u0026quot; https://www.google.com/ これで次のような出力が得られる\nhttp\\_code: 200 time\\_namelookup: 0.002 time\\_connect: 0.005 time\\_appconnect: 0.054 time\\_pretransfer: 0.054 time\\_starttransfer: 0.126 time\\_total: 0.126 他にも次のような変数が用意されている\ncontent_type\nレスポンスの Content-Type\nfilename_effective\ncurl が書き出すファイル名。--remote-name か --output とともに使う場合にのみ意味を持つ。--remote-header-name と使うのが最も有用 (7.25.1 で追加された)\nftp_entry_path\nFTP でログインした際の最初のディレクトリ path (7.15.4 で追加された)\nhttp_code\nレスポンスのコード 200 とか 404 とか。最後のレスポンスのコードなので、リダイレクト先へもアクセスする場合はリダイレクト後のレスポンス。7.18.2 で response_code という alias が追加された\nhttp_connect\nCONNECT に対するレスポンスのコード (7.12.4 で追加された)\nlocal_ip\n接続時のローカル側のIPアドレス (7.29.0 で追加された)\nlocal_port\n接続時のローカル側のポート番号 (7.29.0 で追加された)\nnum_connects\nNumber of new connects made in the recent transfer. (7.12.3 で追加された)\nnum_redirects\nリダイレクトされた回数 (7.12.3 で追加された)\nredirect_url\n-L を使わず、redirect 先にアクセスしない場合に redirect 先が入っている (7.18.2 で追加された)\nremote_ip\n接続先のIPアドレス (7.29.0 で追加された)\nremote_port\n接続先のポート番号 (7.29.0 で追加された)\nsize_download\nダウンロードしたバイト数。ヘッダーを含む\nsize_header\nレスポンスヘッダーのバイト数\nsize_request\nリクエストのバイト数\nsize_upload\nリクエストボディのバイト数\nspeed_download\nダウンロードの平均 Bytes per second\nspeed_upload\nアップロードの平均 Bytes per second\nssl_verify_result\n証明書の検証結果、0 が成功 (7.19.0 で追加された)\ntime_appconnect\nTLS ハンドシェイクが完了するまでにかかった時間（秒）\ntime_connect\nTCP の connect が完了するまでの時間（秒）\ntime_namelookup\n名前解決が完了するまでにかかった時間（秒）\ntime_pretransfer\nファイル転送が始まるまでにかかった時間（秒）\ntime_redirect\nリダイレクトを辿った最後のリクエストまでの時間（秒）\ntime_starttransfer\nレスポンスの最初のバイトを受け取るまでの時間（秒）\ntime_total\nダウンロードが完了するまでの時間（秒）\nurl_effective\n最後にリクエストした URL\n「時間（秒）」は単位は秒だが制度はミリ秒\n毎度長ったらしい引数を指定するのは煩わしいです。そんな場合は -w @filename のようにしてファイルで渡すことができます。次の内容のテキストファイルを curl.out という名前（任意）で作成し、\ncontent\\_type: %{content\\_type}\\\\n filename\\_effective: %{filename\\_effective}\\\\n ftp\\_entry\\_path: %{ftp\\_entry\\_path}\\\\n http\\_code: %{http\\_code}\\\\n http\\_connect: %{http\\_connect}\\\\n local\\_ip: %{local\\_ip}\\\\n local\\_port: %{local\\_port}\\\\n num\\_connects: %{num\\_connects}\\\\n num\\_redirects: %{num\\_redirects}\\\\n redirect\\_url: %{redirect\\_url}\\\\n remote\\_ip: %{remote\\_ip}\\\\n remote\\_port: %{remote\\_port}\\\\n size\\_download: %{size\\_download}\\\\n size\\_header: %{size\\_header}\\\\n size\\_request: %{size\\_request}\\\\n size\\_upload: %{size\\_upload}\\\\n speed\\_download: %{speed\\_download}\\\\n speed\\_upload: %{speed\\_upload}\\\\n ssl\\_verify\\_result: %{ssl\\_verify\\_result}\\\\n time\\_appconnect: %{time\\_appconnect}\\\\n time\\_connect: %{time\\_connect}\\\\n time\\_namelookup: %{time\\_namelookup}\\\\n time\\_pretransfer: %{time\\_pretransfer}\\\\n time\\_redirect: %{time\\_redirect}\\\\n time\\_starttransfer: %{time\\_starttransfer}\\\\n time\\_total: %{time\\_total}\\\\n url\\_effective: %{url\\_effective}\\\\n -w @curl.out で指定すれば次のような出力が得られます。-w @- と、ファイル名を「-」にすれば標準入力から渡すこともできます\n$ curl -so /dev/null -w @curl.out https://www.google.com/ content\\_type: text/html; charset=ISO-8859-1 filename\\_effective: /dev/null ftp\\_entry\\_path: http\\_code: 200 http\\_connect: 000 local\\_ip: 172.26.45.140 local\\_port: 43760 num\\_connects: 1 num\\_redirects: 0 redirect\\_url: remote\\_ip: 172.217.25.228 remote\\_port: 443 size\\_download: 12618 size\\_header: 772 size\\_request: 78 size\\_upload: 0 speed\\_download: 98443.000 speed\\_upload: 0.000 ssl\\_verify\\_result: 0 time\\_appconnect: 0.052 time\\_connect: 0.004 time\\_namelookup: 0.001 time\\_pretransfer: 0.052 time\\_redirect: 0.000 time\\_starttransfer: 0.128 time\\_total: 0.128 url\\_effective: https://www.google.com/ JSON 出力\n2020年4月29日リリース予定の 7.70.0 では --write-out '%{json}' とすることで全部入りの json を取得できるようになるそうです。「CURL WRITE-OUT JSON」\n{ \u0026quot;url\\_effective\u0026quot;: \u0026quot;https://example.com/\u0026quot;, \u0026quot;http\\_code\u0026quot;: 200, \u0026quot;response\\_code\u0026quot;: 200, \u0026quot;http\\_connect\u0026quot;: 0, \u0026quot;time\\_total\u0026quot;: 0.44054, \u0026quot;time\\_namelookup\u0026quot;: 0.001067, \u0026quot;time\\_connect\u0026quot;: 0.11162, \u0026quot;time\\_appconnect\u0026quot;: 0.336415, \u0026quot;time\\_pretransfer\u0026quot;: 0.336568, \u0026quot;time\\_starttransfer\u0026quot;: 0.440361, \u0026quot;size\\_header\u0026quot;: 347, \u0026quot;size\\_request\u0026quot;: 77, \u0026quot;size\\_download\u0026quot;: 1256, \u0026quot;size\\_upload\u0026quot;: 0, \u0026quot;speed\\_download\u0026quot;: 0.002854, \u0026quot;speed\\_upload\u0026quot;: 0, \u0026quot;content\\_type\u0026quot;: \u0026quot;text/html; charset=UTF-8\u0026quot;, \u0026quot;num\\_connects\u0026quot;: 1, \u0026quot;time\\_redirect\u0026quot;: 0, \u0026quot;num\\_redirects\u0026quot;: 0, \u0026quot;ssl\\_verify\\_result\u0026quot;: 0, \u0026quot;proxy\\_ssl\\_verify\\_result\u0026quot;: 0, \u0026quot;filename\\_effective\u0026quot;: \u0026quot;saved\u0026quot;, \u0026quot;remote\\_ip\u0026quot;: \u0026quot;93.184.216.34\u0026quot;, \u0026quot;remote\\_port\u0026quot;: 443, \u0026quot;local\\_ip\u0026quot;: \u0026quot;192.168.0.1\u0026quot;, \u0026quot;local\\_port\u0026quot;: 44832, \u0026quot;http\\_version\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;scheme\u0026quot;: \u0026quot;HTTPS\u0026quot;, \u0026quot;curl\\_version\u0026quot;: \u0026quot;libcurl/7.69.2 GnuTLS/3.6.12 zlib/1.2.11 brotli/1.0.7 c-ares/1.15.0 libidn2/2.3.0 libpsl/0.21.0 (+libidn2/2.3.0) nghttp2/1.40.0 librtmp/2.3\u0026quot; } ","date":"2019年7月24日","permalink":"/2019/07/curl-write-out-format/","section":"Posts","summary":"curl には -w, \u0026ndash;write-out というオプションがあり、HTTP のコードやどのフェーズに何秒かかったかなどを出力することができます。ときどき調査で使うのですが、毎","title":"Curl で時間計測"},{"content":"","date":"2019年6月22日","permalink":"/tags/postgresql/","section":"Tags","summary":"","title":"PostgreSQL"},{"content":"以前、WAL-E について書きました「WAL-E で PostgreSQL の Backup / Restore」。今回はその後継っぽい WAL-G を試してみます。WAL-E の4倍速いとのこと (Introducing WAL-G by Citus: Faster Disaster Recovery for Postgres)。その後、2019-04-22 に 0.2 (0.2.9) がリリースされてより便利になっているようです (WAL-G v0.2 released)。WAL-G はGo言語で書かれているためインストールが楽ですね。\nWAL-G の特徴 # 圧縮 # WAL-E は圧縮に LZO が使われていましたが、WAL-G では LZ4, LZMA, Brotli から選択可能になっています。Brotli が圧縮率と速度のバランスが良く、LZMA は遅すぎとのこと。\n対応ストレージ # WAL-E と同様に多くのオブジェクトストレージに対応しています\n S3 (SSE にも対応) とその互換 Azure Storage Google Cloud Storage Swift Local Filesystem     (adsbygoogle = window.adsbygoogle || []).push({});  帯域制限 # Backup 時のDBファイルの読み出しレート、オブジェクトストレージへの転送レート、ダウンロードの並列度などを制御することが可能です。\nDelta Backup # WAL-delta backups (a.k.a. fast block-level incremental backup). This feature enables scanning of WAL during archivation, the gathered information is used to make delta backup much faster.\n1つの Full Backup に対していくつの Delta Backup を持たせるかという環境変数 WALG_DELTA_MAX_STEPS をデフォルトの 0 から 1 以上に増やすと機能するようです。\n暗号化 # S3 の SSE (KMS 対応) も使えるが、GPG で暗号化して保存することも可能で、外部の gpg コマンドも必要ない。\n試してみる # サーバー環境 # 2台の CentOS 7 サーバーにそれぞれ PostgreSQL 11 をインストールし、Warm スタンバイ構成とする。 スタンバイ(レプリカ)側で Minio サーバーを起動させて WAL-G のファイル保存先とする。\nPostgreSQL のインストール # PGDG のリポジトリから PostgreSQL 11 をインストールする\n$ sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm $ sudo yum install -y postgresql11-server wal-g コマンドのインストール # $ curl -LO https://github.com/wal-g/wal-g/releases/download/v0.2.9/wal-g.linux-amd64.tar.gz $ sudo tar -C /usr/local/bin -xvf wal-g.linux-amd64.tar.gz $ sudo chmod +x /usr/local/bin/wal-g Minio server のダウンロードと起動 # レプリカ側のサーバーで起動させます\n$ curl -LO https://dl.min.io/server/minio/release/linux-amd64/minio $ chmod +x minio $ mkdir data $ ./minio server data \u0026amp; minio を起動させると AccessKey と SecretKey が表示されます。(data/.minio.sys/config/config.json にも保存されてます)\nMinio client のダウンロードと設定 # $ sudo curl -Lo /usr/local/bin/mc https://dl.min.io/client/mc/release/linux-amd64/mc $ sudo chmod +x /usr/local/bin/mc 認証設定 # Bucket 作成や bucket 内のオブジェクト確認のために使います\n$ mc config host add local http://localhost:9000 NRZJHAVRWVWPN8X8SYGZ iwJLitwBL32iO1lWD7FtfegZ2P6XBPf601x5s67H WAL-G 用 Bucket 作成 # $ mc mb local/wal-g Bucket created successfully `local/wal-g`. $ mc ls local/ [2019-06-22 06:55:50 UTC] 0B wal-g/ PostgreSQL の initdb と起動 # $ sudo PGSETUP_INITDB_OPTIONS=\u0026quot;-E utf8 --no-locale --data-checksums\u0026quot; \\ /usr/pgsql-11/bin/postgresql-11-setup initdb $ sudo systemctl start postgresql-11 pgbench を実行してみる # 特に意味はないけど DB のデータ更新ツールとして\n$ sudo -iu postgres createdb pgbench $ sudo -iu postgres /usr/pgsql-11/bin/pgbench -i pgbench dropping old tables... NOTICE: table \u0026quot;pgbench_accounts\u0026quot; does not exist, skipping NOTICE: table \u0026quot;pgbench_branches\u0026quot; does not exist, skipping NOTICE: table \u0026quot;pgbench_history\u0026quot; does not exist, skipping NOTICE: table \u0026quot;pgbench_tellers\u0026quot; does not exist, skipping creating tables... generating data... 100000 of 100000 tuples (100%) done (elapsed 0.16 s, remaining 0.00 s) vacuuming... creating primary keys... done. Local Filesystem へ WAL-G でバックアップしてみる # バックアップ先ディレクトリを作成\n$ sudo install -o postgres -g postgres -m 0700 -d /backup バックアップの一覧確認コマンドを実行してみる。まだバックアップしてないので \u0026ldquo;No backups found\u0026rdquo;\n$ sudo -iu postgres WALE_FILE_PREFIX=/backup /usr/local/bin/wal-g backup-list ERROR: 2019/06/22 07:08:35.529498 No backups found wal-g backup-push でフルバックアップを取得。ここでは Unix Domain Socket で通信するために PGHOST に socket ファイルのパスを指定している\n$ sudo -iu postgres PGHOST=/tmp/.s.PGSQL.5432 WALE_FILE_PREFIX=/backup /usr/local/bin/wal-g backup-push /var/lib/pgsql/11/data INFO: 2019/06/22 07:14:45.762189 Doing full backup. INFO: 2019/06/22 07:14:45.877402 Walking ... INFO: 2019/06/22 07:14:45.877928 Starting part 1 ... INFO: 2019/06/22 07:14:46.183716 Finished writing part 1. INFO: 2019/06/22 07:14:46.183757 Starting part 2 ... INFO: 2019/06/22 07:14:46.183768 /global/pg_control INFO: 2019/06/22 07:14:46.185711 Finished writing part 2. INFO: 2019/06/22 07:14:47.218344 Starting part 3 ... INFO: 2019/06/22 07:14:47.221441 backup_label INFO: 2019/06/22 07:14:47.221920 tablespace_map INFO: 2019/06/22 07:14:47.222044 Finished writing part 3. PostgreSQL のログから次のクエリが実行されたことがわかります。pg_start_backup() や pg_stop_backup() が実行されています。\n2019-06-22 07:14:45.772 UTC [12227] LOG: execute \u0026lt;unnamed\u0026gt;: select t.oid, case when nsp.nspname in ('pg_catalog', 'public') then t.typname else nsp.nspname||'.'||t.typname end from pg_type t left join pg_type base_type on t.typelem=base_type.oid left join pg_namespace nsp on t.typnamespace=nsp.oid where ( t.typtype in('b', 'p', 'r', 'e') and (base_type.oid is null or base_type.typtype in('b', 'p', 'r')) ) 2019-06-22 07:14:45.773 UTC [12227] LOG: execute \u0026lt;unnamed\u0026gt;: select t.oid, t.typname from pg_type t join pg_type base_type on t.typelem=base_type.oid where t.typtype = 'b' and base_type.typtype = 'e' 2019-06-22 07:14:45.774 UTC [12227] LOG: execute \u0026lt;unnamed\u0026gt;: select t.oid, t.typname, t.typbasetype from pg_type t join pg_type base_type on t.typbasetype=base_type.oid where t.typtype = 'd' and base_type.typtype = 'b' 2019-06-22 07:14:45.774 UTC [12227] LOG: execute \u0026lt;unnamed\u0026gt;: show archive_mode 2019-06-22 07:14:45.774 UTC [12227] LOG: execute \u0026lt;unnamed\u0026gt;: show archive_command 2019-06-22 07:14:45.774 UTC [12227] LOG: execute \u0026lt;unnamed\u0026gt;: select (current_setting('server_version_num'))::int 2019-06-22 07:14:45.775 UTC [12227] LOG: execute \u0026lt;unnamed\u0026gt;: SELECT case when pg_is_in_recovery() then '' else (pg_walfile_name_offset(lsn)).file_name end, lsn::text, pg_is_in_recovery() FROM pg_start_backup($1, true, false) lsn 2019-06-22 07:14:45.775 UTC [12227] DETAIL: parameters: $1 = '2019-06-22 07:14:45.77465243 +0000 UTC m=+0.018118250' 2019-06-22 07:14:45.876 UTC [12227] LOG: execute \u0026lt;unnamed\u0026gt;: select timeline_id, bytes_per_wal_segment from pg_control_checkpoint(), pg_control_init() 2019-06-22 07:14:46.187 UTC [12227] LOG: execute \u0026lt;unnamed\u0026gt;: select (current_setting('server_version_num'))::int 2019-06-22 07:14:46.187 UTC [12227] LOG: statement: begin 2019-06-22 07:14:46.187 UTC [12227] LOG: statement: SET statement_timeout=0; 2019-06-22 07:14:46.187 UTC [12227] LOG: execute \u0026lt;unnamed\u0026gt;: SELECT labelfile, spcmapfile, lsn FROM pg_stop_backup(false) 2019-06-22 07:14:47.217 UTC [12227] LOG: statement: commit wal-g backup-list でバックアップの一覧を確認\n$ sudo -iu postgres WALE_FILE_PREFIX=/backup /usr/local/bin/wal-g backup-list name last_modified wal_segment_backup_start base_000000010000000000000006 2019-06-22T07:14:47Z 000000010000000000000006 バックアップ先ディレクトリには次のようなファイルができていました\n$ sudo find /backup -type f /backup/basebackups_005/base_000000010000000000000006/tar_partitions/part_001.tar.lz4 /backup/basebackups_005/base_000000010000000000000006/tar_partitions/pg_control.tar.lz4 /backup/basebackups_005/base_000000010000000000000006/tar_partitions/part_003.tar.lz4 /backup/basebackups_005/base_000000010000000000000006_backup_stop_sentinel.json lz4 で圧縮された tar ファイルは次のようにしてファイルのリストを確認できます (gz, bz2, xz なんかは GNU Tar は自分で判断して処理してくれますけど lz4 には対応してないみたい)\nsudo lz4cat /backup/basebackups_005/base_000000010000000000000006/tar_partitions/part_001.tar.lz4 | tar tf - WAL を minio にアーカイブする # wal-g を環境変数して実行するために wrapper を作ります\n$ cat | sudo tee /usr/local/bin/wal-g.sh \u0026lt;\u0026lt;'EOF' #!/bin/bash export AWS_ACCESS_KEY_ID=\u0026quot;NRZJHAVRWVWPN8X8SYGZ\u0026quot; export AWS_SECRET_ACCESS_KEY=\u0026quot;iwJLitwBL32iO1lWD7FtfegZ2P6XBPf601x5s67H\u0026quot; export WALE_S3_PREFIX=\u0026quot;s3://wal-g\u0026quot; export AWS_ENDPOINT=\u0026quot;http://10.130.59.155:9000\u0026quot; export AWS_S3_FORCE_PATH_STYLE=\u0026quot;true\u0026quot; export AWS_REGION=\u0026quot;us-east-1\u0026quot; exec /usr/local/bin/wal-g \u0026quot;$@\u0026quot; EOF $ sudo chmod +x /usr/local/bin/wal-g.sh archive_mode = on archive_command = '/usr/local/bin/wal-g.sh wal-push \u0026quot;%p\u0026quot;' archive_timeout = 30 PostgreSQL のログに次のように出ました\nINFO: 2019/06/22 07:46:02.628958 FILE PATH: 000000010000000000000009.lz4 minio 側でもファイルが確認できました\n$ mc ls local/wal-g [2019-06-22 07:46:11 UTC] 0B wal_005/ $ mc ls local/wal-g/wal_005/ [2019-06-22 07:46:02 UTC] 65KiB 000000010000000000000009.lz4 次に、この状態で replica を作ります。近くにあるサーバー同士であれば pg_basebackup でデータをコピーすれば良いのですがここではあえて WAL-G の backup-push と backup-fetch を使っている。オンプレにある DB のレプリカをクラウドに作るのに使えるのではないかと\n (master) replication のための設定 streaming replication しない場合は不要だった (master) WAL-G の backup-push で minio に backup を保存 (replica) WAL-G の backup-fetch で /var/lib/pgsql/11/data にリストア (replica) recovery.conf を設定して起動 (restore_command に wal-g wal-fetch を設定)  バックアップを minio に保存 # $ sudo -iu postgres PGHOST=/tmp/.s.PGSQL.5432 /usr/local/bin/wal-g.sh backup-push /var/lib/pgsql/11/data INFO: 2019/06/22 08:00:49.706362 Doing full backup. INFO: 2019/06/22 08:00:49.858714 Walking ... INFO: 2019/06/22 08:00:49.859470 Starting part 1 ... INFO: 2019/06/22 08:00:50.130802 Finished writing part 1. INFO: 2019/06/22 08:00:50.375493 Starting part 2 ... INFO: 2019/06/22 08:00:50.375529 /global/pg_control INFO: 2019/06/22 08:00:50.407612 Finished writing part 2. INFO: 2019/06/22 08:00:51.437201 Starting part 3 ... INFO: 2019/06/22 08:00:51.452981 backup_label INFO: 2019/06/22 08:00:51.453106 tablespace_map INFO: 2019/06/22 08:00:51.453188 Finished writing part 3. minio の bucket 内を確認\n$ mc ls local/wal-g [2019-06-22 08:00:57 UTC] 0B basebackups_005/ [2019-06-22 08:00:57 UTC] 0B wal_005/ $ mc ls local/wal-g/basebackups_005/ [2019-06-22 08:00:51 UTC] 126KiB base_00000001000000000000000C_backup_stop_sentinel.json [2019-06-22 08:01:06 UTC] 0B base_00000001000000000000000C/ $ mc ls local/wal-g/basebackups_005/base_00000001000000000000000C/ [2019-06-22 08:01:14 UTC] 0B tar_partitions/ $ mc ls local/wal-g/basebackups_005/base_00000001000000000000000C/tar_partitions/ [2019-06-22 08:00:50 UTC] 11MiB part_001.tar.lz4 [2019-06-22 08:00:51 UTC] 536B part_003.tar.lz4 [2019-06-22 08:00:50 UTC] 488B pg_control.tar.lz4 minio からバックアップを取り出す # LATEST と指定すれば最新のバックアップが取り出せる\n$ sudo -iu postgres /usr/local/bin/wal-g.sh backup-fetch /var/lib/pgsql/11/data LATEST INFO: 2019/06/22 08:07:16.142053 LATEST backup is: 'base_00000001000000000000000C' INFO: 2019/06/22 08:07:16.290391 Finished decompression of part_003.tar.lz4 INFO: 2019/06/22 08:07:16.290412 Finished extraction of part_003.tar.lz4 INFO: 2019/06/22 08:07:19.771957 Finished decompression of part_001.tar.lz4 INFO: 2019/06/22 08:07:19.772172 Finished extraction of part_001.tar.lz4 INFO: 2019/06/22 08:07:19.785737 Finished decompression of pg_control.tar.lz4 INFO: 2019/06/22 08:07:19.785761 Finished extraction of pg_control.tar.lz4 INFO: 2019/06/22 08:07:19.785770 Backup extraction complete. 指定した /var/lib/pgsql/11/data に展開された\n$ sudo ls -l /var/lib/pgsql/11/data total 56 -rw-------. 1 postgres postgres 253 Jun 22 08:07 backup_label drwx------. 7 postgres postgres 71 Jun 22 08:07 base -rw-------. 1 postgres postgres 30 Jun 22 08:07 current_logfiles drwx------. 2 postgres postgres 4096 Jun 22 08:07 global drwx------. 2 postgres postgres 32 Jun 22 08:07 log drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_commit_ts drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_dynshmem -rw-------. 1 postgres postgres 4269 Jun 22 08:07 pg_hba.conf -rw-------. 1 postgres postgres 1636 Jun 22 08:07 pg_ident.conf drwx------. 4 postgres postgres 68 Jun 22 08:07 pg_logical drwx------. 4 postgres postgres 36 Jun 22 08:07 pg_multixact drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_notify drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_replslot drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_serial drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_snapshots drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_stat drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_stat_tmp drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_subtrans drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_tblspc drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_twophase -rw-------. 1 postgres postgres 3 Jun 22 08:07 PG_VERSION drwx------. 2 postgres postgres 6 Jun 22 08:07 pg_wal drwx------. 2 postgres postgres 18 Jun 22 08:07 pg_xact -rw-------. 1 postgres postgres 88 Jun 22 08:07 postgresql.auto.conf -rw-------. 1 postgres postgres 23854 Jun 22 08:07 postgresql.conf -rw-------. 1 postgres postgres 0 Jun 22 08:07 tablespace_map $ recovery.conf を作成して PostgreSQL を起動 # $ cat | sudo -u postgres tee /var/lib/pgsql/11/data/recovery.conf \u0026lt;\u0026lt;'EOF' standby_mode = on restore_command = '/usr/local/bin/wal-g.sh wal-fetch \u0026quot;%f\u0026quot; \u0026quot;%p\u0026quot;' EOF $ sudo systemctl start postgresql-11 ps コマンドで確認。WAL 待ちになっていることがわかる\n$ ps -ef | grep postg postgres 11821 1 0 08:21 ? 00:00:00 /usr/pgsql-11/bin/postmaster -D /var/lib/pgsql/11/data/ postgres 11823 11821 0 08:21 ? 00:00:00 postgres: logger postgres 11824 11821 0 08:21 ? 00:00:00 postgres: startup waiting for 00000001000000000000000E postgres 11835 11821 0 08:21 ? 00:00:00 postgres: checkpointer postgres 11836 11821 0 08:21 ? 00:00:00 postgres: background writer postgres 11838 11821 0 08:21 ? 00:00:00 postgres: stats collector centos 11865 11243 0 08:21 pts/0 00:00:00 grep --color=auto postg ログ確認\n2019-06-22 08:21:18.339 UTC [11824] LOG: database system was interrupted; last known up at 2019-06-22 08:00:49 UTC 2019-06-22 08:21:18.339 UTC [11824] LOG: creating missing WAL directory \u0026quot;pg_wal/archive_status\u0026quot; 2019-06-22 08:21:18.363 UTC [11824] LOG: entering standby mode 2019-06-22 08:21:18.580 UTC [11824] LOG: restored log file \u0026quot;00000001000000000000000C\u0026quot; from archive 2019-06-22 08:21:18.675 UTC [11824] LOG: redo starts at 0/C000028 2019-06-22 08:21:18.677 UTC [11824] LOG: consistent recovery state reached at 0/C000130 2019-06-22 08:21:18.678 UTC [11821] LOG: database system is ready to accept read only connections 2019-06-22 08:21:18.983 UTC [11824] LOG: restored log file \u0026quot;00000001000000000000000D\u0026quot; from archive ERROR: 2019/06/22 08:21:19.022706 Archive '00000001000000000000000E' does not exist. ERROR: 2019/06/22 08:21:19.043264 Archive '00000001000000000000000E' does not exist. ... 2019-06-22 08:25:55.821 UTC [11824] LOG: restored log file \u0026quot;00000001000000000000000E\u0026quot; from archive ... 掃除 # wal-push で WAL を送り続けるとひたすらたまり続けてしまうので掃除してやる必要がある。wal-g delete コマンドで指定の basebackup より前のものを削除することができる、残す世代数を指定しいて削除することも可能\nUsage: wal-g delete [command] Available Commands: before retain Usage: wal-g delete before [FIND_FULL] backup_name|timestamp [flags] Examples: before base_0123 keep everything after base_0123 including itself before FIND_FULL base_0123 keep everything after the base of base_0123 Usage: wal-g delete retain [FULL|FIND_FULL] backup_count [flags] Examples: retain 5 keep 5 backups retain FULL 5 keep 5 full backups and all deltas of them retain FIND_FULL 5 find necessary full for 5th and keep everything after it $ /usr/local/bin/wal-g.sh backup-list name last_modified wal_segment_backup_start base_00000001000000000000000C 2019-06-22T08:00:51Z 00000001000000000000000C base_00000001000000000000001B 2019-06-22T08:36:29Z 00000001000000000000001B base_00000001000000000000001D 2019-06-22T08:40:45Z 00000001000000000000001D 圧縮率確認 # 圧縮アルゴリズムの変更は環境変数 WALG_COMPRESSION_METHOD に lz4, lzma, brotli のいずれかを設定します。デフォルトは lz4 です。\n負荷は全然見ていないけれども圧縮後のサイズを見るにこれは brotli が良さそう\nbasebackup は元データにほぼ差がない状態での比較\n$ mc ls local/wal-g/basebackups_005/base_00000001000000000000001D/tar_partitions/ [2019-06-22 08:40:44 UTC] 14MiB part_001.tar.lz4 [2019-06-22 08:40:45 UTC] 538B part_003.tar.lz4 [2019-06-22 08:40:44 UTC] 488B pg_control.tar.lz4 $ mc ls local/wal-g/basebackups_005/base_000000010000000000000021/tar_partitions/ [2019-06-22 08:59:18 UTC] 5.4MiB part_001.tar.br [2019-06-22 08:59:20 UTC] 270B part_003.tar.br [2019-06-22 08:59:18 UTC] 264B pg_control.tar.br WAL は内容によるけど、ほぼ変更がない場合の brotli がとても小さい\n[2019-06-22 08:26:22 UTC] 66KiB 00000001000000000000000F.lz4 [2019-06-22 08:26:52 UTC] 2.0MiB 000000010000000000000010.lz4 [2019-06-22 08:27:22 UTC] 2.3MiB 000000010000000000000011.lz4 [2019-06-22 08:27:52 UTC] 2.2MiB 000000010000000000000012.lz4 [2019-06-22 08:28:22 UTC] 2.0MiB 000000010000000000000013.lz4 [2019-06-22 08:28:52 UTC] 3.4MiB 000000010000000000000014.lz4 [2019-06-22 08:29:22 UTC] 3.5MiB 000000010000000000000015.lz4 [2019-06-22 08:29:53 UTC] 3.7MiB 000000010000000000000016.lz4 [2019-06-22 08:30:23 UTC] 1.4MiB 000000010000000000000017.lz4 [2019-06-22 08:31:23 UTC] 80KiB 000000010000000000000018.lz4 [2019-06-22 08:33:23 UTC] 64KiB 000000010000000000000019.lz4 [2019-06-22 08:36:27 UTC] 64KiB 00000001000000000000001A.lz4 [2019-06-22 08:36:28 UTC] 269B 00000001000000000000001B.00000028.backup.lz4 [2019-06-22 08:36:28 UTC] 65KiB 00000001000000000000001B.lz4 [2019-06-22 08:40:44 UTC] 64KiB 00000001000000000000001C.lz4 [2019-06-22 08:40:44 UTC] 268B 00000001000000000000001D.00000028.backup.lz4 [2019-06-22 08:40:44 UTC] 65KiB 00000001000000000000001D.lz4 [2019-06-22 08:45:44 UTC] 179B 00000001000000000000001E.br [2019-06-22 08:58:53 UTC] 111B 00000001000000000000001F.br [2019-06-22 08:59:18 UTC] 110B 000000010000000000000020.br [2019-06-22 08:59:19 UTC] 198B 000000010000000000000021.00000028.backup.br [2019-06-22 08:59:19 UTC] 178B 000000010000000000000021.br [2019-06-22 09:02:03 UTC] 2.1MiB 000000010000000000000022.br [2019-06-22 09:02:33 UTC] 2.2MiB 000000010000000000000023.br [2019-06-22 09:03:03 UTC] 1.9MiB 000000010000000000000024.br [2019-06-22 09:03:33 UTC] 2.0MiB 000000010000000000000025.br [2019-06-22 09:04:03 UTC] 1.2MiB 000000010000000000000026.br [2019-06-22 09:04:33 UTC] 2.2KiB 000000010000000000000027.br その他 # MySQL 対応？ # WAL-G は MySQL にも対応しようとしてるみたいです。MySQL ナニモワカラナイ..\n古い OS で実行する場合の注意 # 今のところ、古い OS (glibc) では GitHub で公開されてるバイナリが動かない。CentOS 6 などで実行する必要がある場合は自前でビルドする必要がある\nhttps://github.com/wal-g/wal-g/issues/300\n他のツール # PostgreSQL のバックアップツール沢山あって困る\u0026hellip;\n WAL-E WAL-G pgBackRest  Efficiently Backing up Terabytes of Data with pgBackRest (PGDay Russia 2017) High Performance pgBackRest (PGCon 2019) How to backup PostgreSQL in mass volume production environments | Inceptum   pg_rman  pg_rman (PostgreSQL のバックアップ/リストア管理ツール) - SRA OSS, Inc. 日本支社 PostgreSQLの周辺ツール ～ pg_rmanでバックアップ・リカバリーを管理する ～：PostgreSQLインサイド : 富士通   Barman  Barman（PostgreSQL PITR 補助ツール）    まだ他にもあるけど pgBackRest が良さそうなのかな\n          ","date":"2019年6月22日","permalink":"/2019/06/wal-g/","section":"Posts","summary":"以前、WAL-E について書きました「WAL-E で PostgreSQL の Backup / Restore」。今回はその後継っぽい WAL-G を試してみます。WAL-E の4倍速いとのこと","title":"WAL-G で PostgreSQL の Backup や Replica 作成"},{"content":"「NGINX Tokyoハッピーアワー - API：インテリジェントルーティング、セキュリティと管理」というイベントがありまして\n API作成、バックエンド基盤のセキュリティ保護、トラフィック管理、継続的モニタリンングなど、万全なライフサイクル管理におけるすべての要素について APIコールの処理にマイクロゲートウェイの追加を必要としない革新的なアーキテクチャについて  という内容に惹かれて参加してきました。NGINX さん主催ですから製品紹介ではあるのですが、”なるほど、言われてみればそうだわ” という気づきがあったので、OSS 版でも使える API 管理設定をメモっておきます。内容は NGINX Controller という製品の GUI を使ったデモから私が想像した設定です。\nNGINX Controller による API Management # NGINX Controller には API Management Module for NGINX Controller というモジュールがあり\n 1. APIの定義と公開 2. レート制限 3. 認証と権限付与 4. リアルタイムのモニタリングとアラート作成 5. ダッシュボード  という機能をもち、NGINX Controller で管理する複数の NGINX サーバーを一つの管理コンソールから管理できます（設定の配布は管理される側に入れた Agent が pollling します）。これを OSS 版でやろうと思うと、1 の「APIの定義と公開」は Reverse Proxy 設定ですから特に問題ないですね（NGINX Plus であればアクティブヘルスチェックとか DNS キャッシュの更新ができて便利ではある）。2 の「レート制限」は ngx_http_limit_req_module で実現可能です。3 の「認証と権限付与」は任意のヘッダーの値でアクセスを許可したり、JWT での許可設定が可能というものでした。JWT は njs か lua-nginx-module を使えばできそうではありますがこの記事ではパス。4, 5 は Prometheus + Grafana とかかな。ここでは 3 の「認証と権限付与」の設定について OSS 版でどのように設定すると良さそうかを試してみます。2 のレート制限については以前「ngx_http_limit_req_module でリクエストレートをコントロール」という記事を書きました。また「Rate Limiting with NGINX and NGINX Plus」にも詳しい説明がありました。\nAPI Key をうまく管理する # 複数の API Key を発行する場合、if 文を並べるのは良くありません。If Is Evil です。こういう場合は map が使えます。\nmap $http_api_key $api_client_name { ad6621ea-9632-4fdd-9fb4-64a2a191c9e0 blue; 2c202c32-b33f-4de2-9d46-60c2509e67ef red; 990e1f99-35e5-4d5c-a019-5e8f940a84db green; 2bcbd78f-bf28-4c3c-90d4-64cf8008f982 green; } if ($api_client_name = \u0026#34;\u0026#34;) { return 403; } 上記のように設定することで map で定義済みの API-KEY ヘッダーが送られてきた場合のみアクセスを許可することが可能です。blue, red, green は任意の名前ですが、ここをうまく使えばクライアントを識別することが可能です。log_format で $api_client_name をログに出力するように設定も可能です。また、右辺は重複可能ですから API-KEY のローテーションも可能です。\n注意点として map は http コンテキストにしか書けないため virtual host 単位で別に設定することはできません。map する変数名を別にすることは可能です。また、正規表現を使わない場合のマッチングは case-insensitive です。\nAPI Key ごとにレート制限する # 先の設定に加え、次のように設定することで $api_client_name ごとに秒間5リクエストまでに制限することがかのうです。burst とか delay といった設定もありますが、それは上にあるリンクを参照してください。複数台の NGINX を並べる場合、アクセスの振り分け方法によっては全体ではこの制限を超えることが可能ですが、要件によってはそれは問題ではないかもしれません。\nlimit_req_zone $api_client_name zone=zone1:100m rate=5r/s; limit_req_status 429; limit_req zone=zone1 まとめ # API Management というと Kong などを導入する必要があると思い込んでしまっていましたが、要件によっては今回書いたような設定で問題ないですし、ずっとシンプルです。思い込みは怖いですね。「あー、これを API Management と呼んでも良かったんだ！！」というのが今回の収穫（気付き）です。NGINX さんありがとうございました。\nTシャツもいただきました！！ありがとうございます。前に SIOS さんからもらった紺色のやつもまだ部屋着として着てます。            ","date":"2019年6月8日","permalink":"/2019/06/nginx-api-management/","section":"Posts","summary":"「NGINX Tokyoハッピーアワー - API：インテリジェントルーティング、セキュリティと管理」というイベントがありまして API作成、バック","title":"Nginx で API Management"},{"content":"Push 通知が来なくて気づいていませんでしたが。Deco M5 の新しい firmware が公開されていたので更新しました。\nダウンロードサイトの表記が謎でよくわかりませんが、2つバージョンが上がったみたいです。\n Added switch for SIP ALG feature. Added switch for Beamforming feature. Added support for IPTV. Added Privilege options for manager accounts. Optimized the QoS settings. Improved the networking stability. Enhanced system security. Improved total performance.  管理用アプリから見える違いとしては「詳細設定」に「IPTV」、「MACクローン」、「SIP ALG」、「ビームフォーミング」という項目が追加されていました。「ビームフォーミング」はデフォオルトで有効になっていました。\nIPTV # 「こちらを有効にすると、メイン Deco の他ポートはIPTVサービスをサポートし使用可能になります。」とあり、IPTV 用の VLAN が追加されるっぽいです。ただし、「IPTV モード」の選択肢が「Singapore-Singtel」、「Malaysia-Unifi」、「Malaysia-Maxis1」、「Malaysia-Maxis2」となっており、日本で使うものではなさそう。\nMAC クローン # ISP によっては特定の MAC アドレスでしか接続できないようになっているらしく、これに対応するための機能のようです。Deco 配下の選択したデバイスの MAC アドレスか指定のカスタム MAC アドレスを使って ISP と通信する機能のようです。日本ではあまり聞きませんね。あ、MAC アドレス制限された LAN に接続するという使い方が・・・\nSIP ALG # 「こちらを有効にすると、クライアントは NAT 経由で SIP (セッション開始プロトコル) サーバーと通信できます。」とあります。IP 電話を使う場合にうれしい機能なのかな？\nビームフォーミング # 「ビームフォーミングは接続されたデバイスに WiFi 信号を集中させ、より強い接続を提供いたします。」とあり、デフォルトで有効となっていました。\nアップロード速度が改善した？？ # 今日は「ソフトバンク光 (フレッツ)」から「auひかり マンションタイプ」に切り替えたのですが、ダウンロード速度はいずれも VDSL の限界っぽい 78Mbps くらいで違いがありませんでした。アップロードは 27Mbps から 38Mbps へと 11Mbps 改善しました。この状態で firmware を更新したら、アップロードがなんと 52Mbps にまで上がりました。「Improved the networking stability」か「Improved total performance」の影響でしょうか。しかし、この計測、Deco のアプリで行っているのだけどスマホからの通信速度なんだろうか Deco からの有線での速度なのだろうか？？\n履歴表示\n","date":"2019年5月21日","permalink":"/2019/05/deco-m5-firmware-1-2-8-build-20190403-rel-40244/","section":"Posts","summary":"Push 通知が来なくて気づいていませんでしたが。Deco M5 の新しい firmware が公開されていたので更新しました。 ダウンロードサイトの表記が謎でよくわかりませ","title":"Deco M5 の firmware 更新 1.2.8 Build 20190403 Rel. 40244"},{"content":"https://github.com/amazon-archives/lightsail-auto-snapshots を使うと非常に簡単。\nawscli がインストール済みで、認証設定もされているとすると、README に書いてあるように次のように実行すれば CloudFormation で lambda と CloudWatch Event がセットアップされて、すべての Lightsail インスタンスを定期的に取得し、古いものを削除してくれる。\ngit clone https://github.com/amazon-archives/lightsail-auto-snapshots.git cd lightsail-auto-snapshots AWS\\_PROFILE=default REGION=ap-northeast-1 DAYS=15 SCHEDULE=\u0026quot;cron(0 19 \\* \\* ? \\*)\u0026quot; bin/deploy SCHEDULE は UTC なので、上記の例では日本時間の早朝4時となる。DAYS=15 で取得後15日を過ぎると削除される。\n","date":"2019年5月15日","permalink":"/2019/05/aws-lightsail-auto-snapshots/","section":"Posts","summary":"https://github.com/amazon-archives/lightsail-auto-snapshots を使うと非常に簡単。 awscli がインストール済みで、認証設定もされているとすると、README に書いてあるように次のように実行すれば CloudFormation で lambda と CloudWatch Event が","title":"AWS Lightsail の snapshot 取得を自動化する"},{"content":"","date":"2019年5月12日","permalink":"/tags/centos/","section":"Tags","summary":"","title":"CentOS"},{"content":"Remote Development with VS Code が発表されました、大変便利そうなので早速試そうと、Vagrant で起動している CentOS 7 へ SSH でアクセスするように設定してみました。が、Git 2.0 以降をインストールしてねと表示されてしまったので、さてどうやってインストールしようかなと確認したメモ。Source から自前で compile して入れるのではなく yum で入れたいなと。\nThe Software Collections ( SCL ) Repository と IUS Community Project が候補となるようです。SCL はインストール先がちょっと特殊で使いづらいので IUS かな。\nIUS # IUS の git をインストールするには先に標準 repository から入れている git を削除する必要があります。\nCentOS 7 # sudo yum -y install https://centos7.iuscommunity.org/ius-release.rpm sudo yum -y remove git git-\\\\\\* sudo yum -y install git2u CentOS 6 # sudo yum -y install https://centos6.iuscommunity.org/ius-release.rpm sudo yum -y remove git git-\\\\\\* sudo yum -y install git2u SCL # SCL の package をインストールするためにはまず centos-release-scl package で repository を追加する。\nsudo yum -y install centos-release-scl するとインストール可能な複数のバージョンがみつかる。CentOS 7 では centos-sclo-sclo repository に sclo-git25 (git-2.5.0) と sclo-git212 (git-2.12)、centos-sclo-rh repository に rh-git218 (git-2.18) と rh-git29 (git-2.9) と git19 (git-1.9.4) がありました。\nしかし、SCL は使い方が面倒。rh-git218 を入れたとすると次のように scl enable を使う\n$ scl -l httpd24 rh-git218 $ scl enable rh-git218 \u0026quot;git --version\u0026quot; git version 2.18.1 scl enable rh-git218 bash とすれば環境変数のセットされた状態の bash が起動するが、あまりやりたくない。セットされる環境変数はこれ。\n$ cat /opt/rh/rh-git218/enable export PATH=/opt/rh/rh-git218/root/usr/bin${PATH:+:${PATH}} export MANPATH=/opt/rh/rh-git218/root/usr/share/man:${MANPATH} export PERL5LIB=/opt/rh/rh-git218/root/usr/share/perl5/vendor\\_perl${PERL5LIB:+:${PERL5LIB}} export LD\\_LIBRARY\\_PATH=/opt/rh/httpd24/root/usr/lib64${LD\\_LIBRARY\\_PATH:+:${LD\\_LIBRARY\\_PATH}} source scl_source enable rh-git218 を .bashrc とかに書くこともできるけど、うーむ\u0026hellip; (scl_source には \u0026ldquo;Don\u0026rsquo;t use this script outside of SCL scriptlets!\u0026rdquo; とコメントが書いてある)\nというわけで、IUS の git を入れるのが便利。\nまとめ # Visual Studio Code Remote Development は便利！！\n","date":"2019年5月12日","permalink":"/2019/05/install-git-2-on-centos/","section":"Posts","summary":"Remote Development with VS Code が発表されました、大変便利そうなので早速試そうと、Vagrant で起動している CentOS 7 へ SSH でアクセスするように設定してみました。が、G","title":"CentOS に Git 2.x をインストールする方法"},{"content":"","date":"2019年5月12日","permalink":"/tags/git/","section":"Tags","summary":"","title":"git"},{"content":"","date":"2019年4月22日","permalink":"/tags/nomad/","section":"Tags","summary":"","title":"Nomad"},{"content":"前回 Nomad クラスタを構築しました。今回はこのクラスタで Job を実行します。\nnomad job init # job init コマンドを実行することでスケルトンファイルを生成することができます。\n# nomad job init Example job file written to example.nomad これで example.nomad ファイルが生成されています。多くのコメントが含まれていますが、コメント行と空行を除くと次のようになっています。\n# egrep -v '^ *#|^$' example.nomad job \u0026quot;example\u0026quot; { datacenters = [\u0026quot;dc1\u0026quot;] type = \u0026quot;service\u0026quot; update { max_parallel = 1 min_healthy_time = \u0026quot;10s\u0026quot; healthy_deadline = \u0026quot;3m\u0026quot; progress_deadline = \u0026quot;10m\u0026quot; auto_revert = false canary = 0 } migrate { max_parallel = 1 health_check = \u0026quot;checks\u0026quot; min_healthy_time = \u0026quot;10s\u0026quot; healthy_deadline = \u0026quot;5m\u0026quot; } group \u0026quot;cache\u0026quot; { count = 1 restart { attempts = 2 interval = \u0026quot;30m\u0026quot; delay = \u0026quot;15s\u0026quot; mode = \u0026quot;fail\u0026quot; } ephemeral_disk { size = 300 } task \u0026quot;redis\u0026quot; { driver = \u0026quot;docker\u0026quot; config { image = \u0026quot;redis:3.2\u0026quot; port_map { db = 6379 } } resources { cpu = 500 # 500 MHz memory = 256 # 256MB network { mbits = 10 port \u0026quot;db\u0026quot; {} } } service { name = \u0026quot;redis-cache\u0026quot; tags = [\u0026quot;global\u0026quot;, \u0026quot;cache\u0026quot;] port = \u0026quot;db\u0026quot; check { name = \u0026quot;alive\u0026quot; type = \u0026quot;tcp\u0026quot; interval = \u0026quot;10s\u0026quot; timeout = \u0026quot;2s\u0026quot; } } } } }  type = \u0026quot;servie\u0026quot; ですからどこかの node でずっと動かすプロセスを起動させます。 実行するのは task \u0026quot;redis\u0026quot; 内にあり、driver = \u0026quot;docker\u0026quot; で image = \u0026quot;redis:3.2\u0026quot; を実行します。 group \u0026quot;cache\u0026quot; で count = 1 と指定されているので1コンテナを維持します。 [resources](https://www.nomadproject.io/docs/job-specification/resources.html) 内でこの Task を実行するのに必要なリソースを指定してあります。port は固定した場合にその port が使用可能な node が選択されます。port \u0026quot;db\u0026quot; {} は動的割り当てを意味します。転送先は config の port_map で定義されています。 [update](https://www.nomadproject.io/docs/job-specification/update.html) 設定ではローリングアップデートやカナリアリリースのための設定です。省略するとこれらの機能は無効になります。 node を停止する場合などに task を別 node へ移動させますが、これに関する設定が [migrate](https://www.nomadproject.io/docs/job-specification/migrate.html) にあります。(Workload Migration)  nomad job run # example.nomad を実行してみます。\n# nomad job run example.nomad ==\u0026gt; Monitoring evaluation \u0026quot;a1ebcac6\u0026quot; Evaluation triggered by job \u0026quot;example\u0026quot; Evaluation within deployment: \u0026quot;4f9cf929\u0026quot; Allocation \u0026quot;4fef9fed\u0026quot; created: node \u0026quot;4c1bbcfa\u0026quot;, group \u0026quot;cache\u0026quot; Evaluation status changed: \u0026quot;pending\u0026quot; -\u0026gt; \u0026quot;complete\u0026quot; ==\u0026gt; Evaluation \u0026quot;a1ebcac6\u0026quot; finished with status \u0026quot;complete\u0026quot; # nomad status example ID = example Name = example Submit Date = 2019-04-22T12:55:16Z Type = service Priority = 50 Datacenters = dc1 Status = running Periodic = false Parameterized = false Summary Task Group Queued Starting Running Failed Complete Lost cache 0 0 1 0 0 0 Latest Deployment ID = 4f9cf929 Status = running Description = Deployment is running Deployed Task Group Desired Placed Healthy Unhealthy Progress Deadline cache 1 1 0 0 2019-04-22T13:05:16Z Allocations ID Node ID Task Group Version Desired Status Created Modified 4fef9fed 4c1bbcfa cache 0 run running 17s ago 7s ago Allocations にある ID を指定して nomad alloc status コマンドを実行すると次のような情報を確認することができます。\n# nomad alloc status 4fef9fed ID = 4fef9fed Eval ID = a1ebcac6 Name = example.cache[0] Node ID = 4c1bbcfa Job ID = example Job Version = 0 Client Status = running Client Description = Tasks are running Desired Status = run Desired Description = Created = 1h4m ago Modified = 1h4m ago Deployment ID = 4f9cf929 Deployment Health = healthy Task \u0026quot;redis\u0026quot; is \u0026quot;running\u0026quot; Task Resources CPU Memory Disk Addresses 3/500 MHz 6.3 MiB/256 MiB 300 MiB db: 68.183.xxx.xxx:23800 Task Events: Started At = 2019-04-22T12:55:27Z Finished At = N/A Total Restarts = 0 Last Restart = N/A Recent Events: Time Type Description 2019-04-22T12:55:27Z Started Task started by client 2019-04-22T12:55:16Z Driver Downloading image 2019-04-22T12:55:16Z Task Setup Building Task Directory 2019-04-22T12:55:16Z Received Task received by client nomad alloc logs コマンドでログを確認することができます。\n# nomad alloc logs 4fef9fed 1:C 22 Apr 12:55:27.013 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 3.2.12 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 1 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' ... docker コマンドで確認してみます。\n$ sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 749c8f2f23bb redis:3.2 \u0026quot;docker-entrypoint...\u0026quot; 9 minutes ago Up 9 minutes 68.183.xxx.xxx:23800-\u0026gt;6379/tcp, 68.183.xxx.xxx:23800-\u0026gt;6379/udp redis-4fef9fed-d03e-cbac-d480-8f4d4bc45ecb $ sudo docker inspect 749c8f2f23bb [ { \u0026quot;Id\u0026quot;: \u0026quot;749c8f2f23bb662ac3f7ddc03cf212281732d3bb06bbc14d2d493ccb640d8b21\u0026quot;, \u0026quot;Created\u0026quot;: \u0026quot;2019-04-22T12:55:26.242480047Z\u0026quot;, \u0026quot;Path\u0026quot;: \u0026quot;docker-entrypoint.sh\u0026quot;, \u0026quot;Args\u0026quot;: [ \u0026quot;redis-server\u0026quot; ], \u0026quot;State\u0026quot;: { \u0026quot;Status\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;Running\u0026quot;: true, \u0026quot;Paused\u0026quot;: false, \u0026quot;Restarting\u0026quot;: false, \u0026quot;OOMKilled\u0026quot;: false, \u0026quot;Dead\u0026quot;: false, \u0026quot;Pid\u0026quot;: 16282, \u0026quot;ExitCode\u0026quot;: 0, \u0026quot;Error\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;StartedAt\u0026quot;: \u0026quot;2019-04-22T12:55:26.847576628Z\u0026quot;, \u0026quot;FinishedAt\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot; }, \u0026quot;Image\u0026quot;: \u0026quot;sha256:87856cc39862cec77541d68382e4867d7ccb29a85a17221446c857ddaebca916\u0026quot;, \u0026quot;ResolvConfPath\u0026quot;: \u0026quot;/var/lib/docker/containers/749c8f2f23bb662ac3f7ddc03cf212281732d3bb06bbc14d2d493ccb640d8b21/resolv.conf\u0026quot;, \u0026quot;HostnamePath\u0026quot;: \u0026quot;/var/lib/docker/containers/749c8f2f23bb662ac3f7ddc03cf212281732d3bb06bbc14d2d493ccb640d8b21/hostname\u0026quot;, \u0026quot;HostsPath\u0026quot;: \u0026quot;/var/lib/docker/containers/749c8f2f23bb662ac3f7ddc03cf212281732d3bb06bbc14d2d493ccb640d8b21/hosts\u0026quot;, \u0026quot;LogPath\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;/redis-4fef9fed-d03e-cbac-d480-8f4d4bc45ecb\u0026quot;, \u0026quot;RestartCount\u0026quot;: 0, \u0026quot;Driver\u0026quot;: \u0026quot;overlay2\u0026quot;, \u0026quot;MountLabel\u0026quot;: \u0026quot;system_u:object_r:svirt_sandbox_file_t:s0:c198,c202\u0026quot;, \u0026quot;ProcessLabel\u0026quot;: \u0026quot;system_u:system_r:svirt_lxc_net_t:s0:c198,c202\u0026quot;, \u0026quot;AppArmorProfile\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;ExecIDs\u0026quot;: null, \u0026quot;HostConfig\u0026quot;: { \u0026quot;Binds\u0026quot;: [ \u0026quot;/opt/nomad/alloc/4fef9fed-d03e-cbac-d480-8f4d4bc45ecb/alloc:/alloc\u0026quot;, \u0026quot;/opt/nomad/alloc/4fef9fed-d03e-cbac-d480-8f4d4bc45ecb/redis/local:/local\u0026quot;, \u0026quot;/opt/nomad/alloc/4fef9fed-d03e-cbac-d480-8f4d4bc45ecb/redis/secrets:/secrets\u0026quot; ], \u0026quot;ContainerIDFile\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;LogConfig\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;journald\u0026quot;, \u0026quot;Config\u0026quot;: {} }, \u0026quot;NetworkMode\u0026quot;: \u0026quot;bridge\u0026quot;, \u0026quot;PortBindings\u0026quot;: { \u0026quot;6379/tcp\u0026quot;: [ { \u0026quot;HostIp\u0026quot;: \u0026quot;68.183.xxx.xxx\u0026quot;, \u0026quot;HostPort\u0026quot;: \u0026quot;23800\u0026quot; } ], \u0026quot;6379/udp\u0026quot;: [ { \u0026quot;HostIp\u0026quot;: \u0026quot;68.183.xxx.xxx\u0026quot;, \u0026quot;HostPort\u0026quot;: \u0026quot;23800\u0026quot; } ] }, \u0026quot;RestartPolicy\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;MaximumRetryCount\u0026quot;: 0 }, \u0026quot;AutoRemove\u0026quot;: false, \u0026quot;VolumeDriver\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;VolumesFrom\u0026quot;: null, \u0026quot;CapAdd\u0026quot;: null, \u0026quot;CapDrop\u0026quot;: null, \u0026quot;Dns\u0026quot;: null, \u0026quot;DnsOptions\u0026quot;: null, \u0026quot;DnsSearch\u0026quot;: null, \u0026quot;ExtraHosts\u0026quot;: null, \u0026quot;GroupAdd\u0026quot;: null, \u0026quot;IpcMode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Cgroup\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Links\u0026quot;: null, \u0026quot;OomScoreAdj\u0026quot;: 0, \u0026quot;PidMode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Privileged\u0026quot;: false, \u0026quot;PublishAllPorts\u0026quot;: false, \u0026quot;ReadonlyRootfs\u0026quot;: false, \u0026quot;SecurityOpt\u0026quot;: null, \u0026quot;UTSMode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;UsernsMode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;ShmSize\u0026quot;: 67108864, \u0026quot;Runtime\u0026quot;: \u0026quot;docker-runc\u0026quot;, \u0026quot;ConsoleSize\u0026quot;: [ 0, 0 ], \u0026quot;Isolation\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;CpuShares\u0026quot;: 500, \u0026quot;Memory\u0026quot;: 268435456, \u0026quot;NanoCpus\u0026quot;: 0, \u0026quot;CgroupParent\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;BlkioWeight\u0026quot;: 0, \u0026quot;BlkioWeightDevice\u0026quot;: null, \u0026quot;BlkioDeviceReadBps\u0026quot;: null, \u0026quot;BlkioDeviceWriteBps\u0026quot;: null, \u0026quot;BlkioDeviceReadIOps\u0026quot;: null, \u0026quot;BlkioDeviceWriteIOps\u0026quot;: null, \u0026quot;CpuPeriod\u0026quot;: 0, \u0026quot;CpuQuota\u0026quot;: 0, \u0026quot;CpuRealtimePeriod\u0026quot;: 0, \u0026quot;CpuRealtimeRuntime\u0026quot;: 0, \u0026quot;CpusetCpus\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;CpusetMems\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Devices\u0026quot;: null, \u0026quot;DiskQuota\u0026quot;: 0, \u0026quot;KernelMemory\u0026quot;: 0, \u0026quot;MemoryReservation\u0026quot;: 0, \u0026quot;MemorySwap\u0026quot;: 268435456, \u0026quot;MemorySwappiness\u0026quot;: -1, \u0026quot;OomKillDisable\u0026quot;: false, \u0026quot;PidsLimit\u0026quot;: 0, \u0026quot;Ulimits\u0026quot;: null, \u0026quot;CpuCount\u0026quot;: 0, \u0026quot;CpuPercent\u0026quot;: 0, \u0026quot;IOMaximumIOps\u0026quot;: 0, \u0026quot;IOMaximumBandwidth\u0026quot;: 0 }, \u0026quot;GraphDriver\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;overlay2\u0026quot;, \u0026quot;Data\u0026quot;: { \u0026quot;LowerDir\u0026quot;: \u0026quot;/var/lib/docker/overlay2/74aca4e9bfab9d8e9a4cf278f5ef8f7e1631b10f1367fb50832c1b9763f450d2-init/diff:/var/lib/docker/overlay2/595ed0c413560d4b9e7167b9283704a2b04bb1e0704d7b9e92a8ab71789c79c6/diff:/var/lib/docker/overlay2/b3ce76020c10eb942c5e8166860c190b240e539a70271eddaec4a92fac947f07/diff:/var/lib/docker/overlay2/fc8771b79bcdb2806e86fb5f4510a8b3aed84a7ce33f2e2a252f7a0bb17fd42a/diff:/var/lib/docker/overlay2/345168c12d66c16cbdf0d6895741cfd2e727e2b31850d784d3ca3418b9d2d7cc/diff:/var/lib/docker/overlay2/1ffa8ff2fbc5f980e89308e6213d81470d816de9fb05dae8460842516a2efc1c/diff:/var/lib/docker/overlay2/77a0bab9ba43a0e0d426a9dee1751c6c5f280da21748012b691b4ef5c45335b9/diff\u0026quot;, \u0026quot;MergedDir\u0026quot;: \u0026quot;/var/lib/docker/overlay2/74aca4e9bfab9d8e9a4cf278f5ef8f7e1631b10f1367fb50832c1b9763f450d2/merged\u0026quot;, \u0026quot;UpperDir\u0026quot;: \u0026quot;/var/lib/docker/overlay2/74aca4e9bfab9d8e9a4cf278f5ef8f7e1631b10f1367fb50832c1b9763f450d2/diff\u0026quot;, \u0026quot;WorkDir\u0026quot;: \u0026quot;/var/lib/docker/overlay2/74aca4e9bfab9d8e9a4cf278f5ef8f7e1631b10f1367fb50832c1b9763f450d2/work\u0026quot; } }, \u0026quot;Mounts\u0026quot;: [ { \u0026quot;Type\u0026quot;: \u0026quot;bind\u0026quot;, \u0026quot;Source\u0026quot;: \u0026quot;/opt/nomad/alloc/4fef9fed-d03e-cbac-d480-8f4d4bc45ecb/alloc\u0026quot;, \u0026quot;Destination\u0026quot;: \u0026quot;/alloc\u0026quot;, \u0026quot;Mode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;RW\u0026quot;: true, \u0026quot;Propagation\u0026quot;: \u0026quot;rprivate\u0026quot; }, { \u0026quot;Type\u0026quot;: \u0026quot;bind\u0026quot;, \u0026quot;Source\u0026quot;: \u0026quot;/opt/nomad/alloc/4fef9fed-d03e-cbac-d480-8f4d4bc45ecb/redis/local\u0026quot;, \u0026quot;Destination\u0026quot;: \u0026quot;/local\u0026quot;, \u0026quot;Mode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;RW\u0026quot;: true, \u0026quot;Propagation\u0026quot;: \u0026quot;rprivate\u0026quot; }, { \u0026quot;Type\u0026quot;: \u0026quot;bind\u0026quot;, \u0026quot;Source\u0026quot;: \u0026quot;/opt/nomad/alloc/4fef9fed-d03e-cbac-d480-8f4d4bc45ecb/redis/secrets\u0026quot;, \u0026quot;Destination\u0026quot;: \u0026quot;/secrets\u0026quot;, \u0026quot;Mode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;RW\u0026quot;: true, \u0026quot;Propagation\u0026quot;: \u0026quot;rprivate\u0026quot; }, { \u0026quot;Type\u0026quot;: \u0026quot;volume\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;0ac66640f3e7fd443ed25809edd9a20b63174f40c0927f53e2d3164838bcd06b\u0026quot;, \u0026quot;Source\u0026quot;: \u0026quot;/var/lib/docker/volumes/0ac66640f3e7fd443ed25809edd9a20b63174f40c0927f53e2d3164838bcd06b/_data\u0026quot;, \u0026quot;Destination\u0026quot;: \u0026quot;/data\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Mode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;RW\u0026quot;: true, \u0026quot;Propagation\u0026quot;: \u0026quot;\u0026quot; } ], \u0026quot;Config\u0026quot;: { \u0026quot;Hostname\u0026quot;: \u0026quot;749c8f2f23bb\u0026quot;, \u0026quot;Domainname\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;User\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;AttachStdin\u0026quot;: false, \u0026quot;AttachStdout\u0026quot;: false, \u0026quot;AttachStderr\u0026quot;: false, \u0026quot;ExposedPorts\u0026quot;: { \u0026quot;6379/tcp\u0026quot;: {}, \u0026quot;6379/udp\u0026quot;: {} }, \u0026quot;Tty\u0026quot;: false, \u0026quot;OpenStdin\u0026quot;: false, \u0026quot;StdinOnce\u0026quot;: false, \u0026quot;Env\u0026quot;: [ \u0026quot;NOMAD_ADDR_db=68.183.xxx.xxx:23800\u0026quot;, \u0026quot;NOMAD_ALLOC_DIR=/alloc\u0026quot;, \u0026quot;NOMAD_ALLOC_ID=4fef9fed-d03e-cbac-d480-8f4d4bc45ecb\u0026quot;, \u0026quot;NOMAD_ALLOC_INDEX=0\u0026quot;, \u0026quot;NOMAD_ALLOC_NAME=example.cache[0]\u0026quot;, \u0026quot;NOMAD_CPU_LIMIT=500\u0026quot;, \u0026quot;NOMAD_DC=dc1\u0026quot;, \u0026quot;NOMAD_GROUP_NAME=cache\u0026quot;, \u0026quot;NOMAD_HOST_PORT_db=23800\u0026quot;, \u0026quot;NOMAD_IP_db=68.183.xxx.xxx\u0026quot;, \u0026quot;NOMAD_JOB_NAME=example\u0026quot;, \u0026quot;NOMAD_MEMORY_LIMIT=256\u0026quot;, \u0026quot;NOMAD_PORT_db=23800\u0026quot;, \u0026quot;NOMAD_REGION=global\u0026quot;, \u0026quot;NOMAD_SECRETS_DIR=/secrets\u0026quot;, \u0026quot;NOMAD_TASK_DIR=/local\u0026quot;, \u0026quot;NOMAD_TASK_NAME=redis\u0026quot;, \u0026quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026quot;, \u0026quot;GOSU_VERSION=1.10\u0026quot;, \u0026quot;REDIS_VERSION=3.2.12\u0026quot;, \u0026quot;REDIS_DOWNLOAD_URL=http://download.redis.io/releases/redis-3.2.12.tar.gz\u0026quot;, \u0026quot;REDIS_DOWNLOAD_SHA=98c4254ae1be4e452aa7884245471501c9aa657993e0318d88f048093e7f88fd\u0026quot; ], \u0026quot;Cmd\u0026quot;: [ \u0026quot;redis-server\u0026quot; ], \u0026quot;ArgsEscaped\u0026quot;: true, \u0026quot;Image\u0026quot;: \u0026quot;redis:3.2\u0026quot;, \u0026quot;Volumes\u0026quot;: { \u0026quot;/data\u0026quot;: {} }, \u0026quot;WorkingDir\u0026quot;: \u0026quot;/data\u0026quot;, \u0026quot;Entrypoint\u0026quot;: [ \u0026quot;docker-entrypoint.sh\u0026quot; ], \u0026quot;OnBuild\u0026quot;: null, \u0026quot;Labels\u0026quot;: {} }, \u0026quot;NetworkSettings\u0026quot;: { \u0026quot;Bridge\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;SandboxID\u0026quot;: \u0026quot;5e94da3afc88eff1fd99102d762a81e861bf0ec5764850a746788fc1b4f10549\u0026quot;, \u0026quot;HairpinMode\u0026quot;: false, \u0026quot;LinkLocalIPv6Address\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;LinkLocalIPv6PrefixLen\u0026quot;: 0, \u0026quot;Ports\u0026quot;: { \u0026quot;6379/tcp\u0026quot;: [ { \u0026quot;HostIp\u0026quot;: \u0026quot;68.183.xxx.xxx\u0026quot;, \u0026quot;HostPort\u0026quot;: \u0026quot;23800\u0026quot; } ], \u0026quot;6379/udp\u0026quot;: [ { \u0026quot;HostIp\u0026quot;: \u0026quot;68.183.xxx.xxx\u0026quot;, \u0026quot;HostPort\u0026quot;: \u0026quot;23800\u0026quot; } ] }, \u0026quot;SandboxKey\u0026quot;: \u0026quot;/var/run/docker/netns/5e94da3afc88\u0026quot;, \u0026quot;SecondaryIPAddresses\u0026quot;: null, \u0026quot;SecondaryIPv6Addresses\u0026quot;: null, \u0026quot;EndpointID\u0026quot;: \u0026quot;46921b2b1d86852e283570ac5fb3d0c0ac1305de8939dce3a707cbaf7d57bef0\u0026quot;, \u0026quot;Gateway\u0026quot;: \u0026quot;172.17.0.1\u0026quot;, \u0026quot;GlobalIPv6Address\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;GlobalIPv6PrefixLen\u0026quot;: 0, \u0026quot;IPAddress\u0026quot;: \u0026quot;172.17.0.2\u0026quot;, \u0026quot;IPPrefixLen\u0026quot;: 16, \u0026quot;IPv6Gateway\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;MacAddress\u0026quot;: \u0026quot;02:42:ac:11:00:02\u0026quot;, \u0026quot;Networks\u0026quot;: { \u0026quot;bridge\u0026quot;: { \u0026quot;IPAMConfig\u0026quot;: null, \u0026quot;Links\u0026quot;: null, \u0026quot;Aliases\u0026quot;: null, \u0026quot;NetworkID\u0026quot;: \u0026quot;34694538c86fc73431d4e1f42c0d84708ebb6c69b4fb3eb19183f7e5f4a7da67\u0026quot;, \u0026quot;EndpointID\u0026quot;: \u0026quot;46921b2b1d86852e283570ac5fb3d0c0ac1305de8939dce3a707cbaf7d57bef0\u0026quot;, \u0026quot;Gateway\u0026quot;: \u0026quot;172.17.0.1\u0026quot;, \u0026quot;IPAddress\u0026quot;: \u0026quot;172.17.0.2\u0026quot;, \u0026quot;IPPrefixLen\u0026quot;: 16, \u0026quot;IPv6Gateway\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;GlobalIPv6Address\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;GlobalIPv6PrefixLen\u0026quot;: 0, \u0026quot;MacAddress\u0026quot;: \u0026quot;02:42:ac:11:00:02\u0026quot; } } } } ] Modifying a Job # 次は Job の変更です。example.nomad ファイルを編集して group \u0026quot;cache\u0026quot; 内の count を 1 から 3 に変更してみます。\n編集したら nomad job plan コマンドで差分を確認します。同じ Hashicorp 製の Terraform っぽいですね。\n# nomad job plan example.nomad +/- Job: \u0026quot;example\u0026quot; +/- Task Group: \u0026quot;cache\u0026quot; (2 create, 1 in-place update) +/- Count: \u0026quot;1\u0026quot; =\u0026gt; \u0026quot;3\u0026quot; (forces create) Task: \u0026quot;redis\u0026quot; Scheduler dry-run: - All tasks successfully allocated. Job Modify Index: 49 To submit the job with version verification run: nomad job run -check-index 49 example.nomad When running the job with the check-index flag, the job will only be run if the server side version matches the job modify index returned. If the index has changed, another user has modified the job and the plan's results are potentially invalid. nomad job run コマンドで適用します。-check-index を指定することで、サーバー側でバージョンが一致していないと適用されないようにします。plan で確認した後に別の変更が入っていたら適用されないということになります。\n# nomad job run -check-index 49 example.nomad ==\u0026gt; Monitoring evaluation \u0026quot;2e842c28\u0026quot; Evaluation triggered by job \u0026quot;example\u0026quot; Evaluation within deployment: \u0026quot;e300014c\u0026quot; Allocation \u0026quot;f5abe160\u0026quot; created: node \u0026quot;623aadcd\u0026quot;, group \u0026quot;cache\u0026quot; Allocation \u0026quot;4fef9fed\u0026quot; modified: node \u0026quot;4c1bbcfa\u0026quot;, group \u0026quot;cache\u0026quot; Allocation \u0026quot;be184b8e\u0026quot; created: node \u0026quot;2bb2e01c\u0026quot;, group \u0026quot;cache\u0026quot; Evaluation status changed: \u0026quot;pending\u0026quot; -\u0026gt; \u0026quot;complete\u0026quot; ==\u0026gt; Evaluation \u0026quot;2e842c28\u0026quot; finished with status \u0026quot;complete\u0026quot; nomad job status コマンドで allocation (コンテナ) が 3 つになっていることが確認できます。\n# nomad job status example ID = example Name = example Submit Date = 2019-04-22T14:16:07Z Type = service Priority = 50 Datacenters = dc1 Status = running Periodic = false Parameterized = false Summary Task Group Queued Starting Running Failed Complete Lost cache 0 0 3 0 0 0 Latest Deployment ID = e300014c Status = successful Description = Deployment completed successfully Deployed Task Group Desired Placed Healthy Unhealthy Progress Deadline cache 3 3 3 0 2019-04-22T14:26:31Z Allocations ID Node ID Task Group Version Desired Status Created Modified be184b8e 2bb2e01c cache 1 run running 3m27s ago 3m7s ago f5abe160 623aadcd cache 1 run running 3m27s ago 3m2s ago 4fef9fed 4c1bbcfa cache 1 run running 1h24m ago 3m16s ago 次にアプリケーションの更新として redis イメージのバージョンを変更します。image = \u0026quot;redis:3.2\u0026quot; を image = \u0026quot;redis:4.0\u0026quot; にします。\n# nomad job plan example.nomad +/- Job: \u0026quot;example\u0026quot; +/- Task Group: \u0026quot;cache\u0026quot; (1 create/destroy update, 2 ignore) +/- Task: \u0026quot;redis\u0026quot; (forces create/destroy update) +/- Config { +/- image: \u0026quot;redis:3.2\u0026quot; =\u0026gt; \u0026quot;redis:4.0\u0026quot; port_map[0][db]: \u0026quot;6379\u0026quot; } Scheduler dry-run: - All tasks successfully allocated. Job Modify Index: 141 To submit the job with version verification run: nomad job run -check-index 141 example.nomad When running the job with the check-index flag, the job will only be run if the server side version matches the job modify index returned. If the index has changed, another user has modified the job and the plan's results are potentially invalid. 1つの task だけ更新して2つはそのままだと表示されています。これは update 設定で max_parallel = 1 となっているためで、1つずつ順に更新されます。\n# nomad job run -check-index 141 example.nomad ==\u0026gt; Monitoring evaluation \u0026quot;f143cc8c\u0026quot; Evaluation triggered by job \u0026quot;example\u0026quot; Evaluation within deployment: \u0026quot;7459aa07\u0026quot; Allocation \u0026quot;80f319e8\u0026quot; created: node \u0026quot;623aadcd\u0026quot;, group \u0026quot;cache\u0026quot; Evaluation status changed: \u0026quot;pending\u0026quot; -\u0026gt; \u0026quot;complete\u0026quot; ==\u0026gt; Evaluation \u0026quot;f143cc8c\u0026quot; finished with status \u0026quot;complete\u0026quot; この後、待っていると順に3つ更新されて、完了後は次のようになりました。\n# nomad job status example ID = example Name = example Submit Date = 2019-04-22T14:30:29Z Type = service Priority = 50 Datacenters = dc1 Status = running Periodic = false Parameterized = false Summary Task Group Queued Starting Running Failed Complete Lost cache 0 0 3 0 3 0 Latest Deployment ID = 7459aa07 Status = successful Description = Deployment completed successfully Deployed Task Group Desired Placed Healthy Unhealthy Progress Deadline cache 3 3 3 0 2019-04-22T14:41:45Z Allocations ID Node ID Task Group Version Desired Status Created Modified 3ee0f6a7 4c1bbcfa cache 2 run running 34s ago 17s ago ae380133 4c1bbcfa cache 2 run running 1m5s ago 36s ago 80f319e8 623aadcd cache 2 run running 1m32s ago 1m7s ago be184b8e 2bb2e01c cache 1 stop complete 15m55s ago 34s ago f5abe160 623aadcd cache 1 stop complete 15m55s ago 1m5s ago 4fef9fed 4c1bbcfa cache 1 stop complete 1h36m ago 1m32s ago GUI で確認 # GUI で Jobs を開くとまずこの画面で Job の一覧が表示されます\nJobs  Overview ページはこんな感じ\nOverview  次に Definition で nomad job inspect コマンドの出力が確認できます\nDefinition  Versions では各 version 間での差分も確認できます\nVersion  Deployments でも各 version 時の deployment が確認できます\nDeployments  Allocations で task の配置が確認できますが、これは Overview でも見れますね\nAllocations  Evaluations は何だかまだよく知らない\nEvaluations  Client 画面では Worker node のホスト情報が確認できます。CPU と Memory はリアルタイムにグラフ表示されます\nClient  Stopping a Job # Job を停止します。GUI にも Stop ボタンがありますが nomad job stop コマンドを実行します。\n# nomad job stop example ==\u0026gt; Monitoring evaluation \u0026quot;858e1662\u0026quot; Evaluation triggered by job \u0026quot;example\u0026quot; Evaluation within deployment: \u0026quot;7459aa07\u0026quot; Evaluation status changed: \u0026quot;pending\u0026quot; -\u0026gt; \u0026quot;complete\u0026quot; ==\u0026gt; Evaluation \u0026quot;858e1662\u0026quot; finished with status \u0026quot;complete\u0026quot; 停止されました。\n# nomad status example ID = example Name = example Submit Date = 2019-04-22T14:30:29Z Type = service Priority = 50 Datacenters = dc1 Status = dead (stopped) Periodic = false Parameterized = false Summary Task Group Queued Starting Running Failed Complete Lost cache 0 0 0 0 6 0 Latest Deployment ID = 7459aa07 Status = successful Description = Deployment completed successfully Deployed Task Group Desired Placed Healthy Unhealthy Progress Deadline cache 3 3 3 0 2019-04-22T14:41:45Z Allocations ID Node ID Task Group Version Desired Status Created Modified 3ee0f6a7 4c1bbcfa cache 2 stop complete 28m4s ago 3m32s ago ae380133 4c1bbcfa cache 2 stop complete 28m35s ago 3m32s ago 80f319e8 623aadcd cache 2 stop complete 29m2s ago 3m32s ago be184b8e 2bb2e01c cache 1 stop complete 43m25s ago 28m4s ago f5abe160 623aadcd cache 1 stop complete 43m25s ago 28m35s ago 4fef9fed 4c1bbcfa cache 1 stop complete 2h4m ago 29m2s ago GUI の Versions で確認すると Stop が false から true に変化していました。\n+/- Job: \u0026quot;example\u0026quot; +/- Stop:\t\u0026quot;false\u0026quot; =\u0026gt; \u0026quot;true\u0026quot; Task Group: \u0026quot;cache\u0026quot; Task: \u0026quot;redis\u0026quot; 停止した Job を再開するには nomad job run example.nomad すれば良いですし、plan で確認することもできます。\n# nomad job plan example.nomad +/- Job: \u0026quot;example\u0026quot; +/- Stop: \u0026quot;true\u0026quot; =\u0026gt; \u0026quot;false\u0026quot; Task Group: \u0026quot;cache\u0026quot; (3 create) Task: \u0026quot;redis\u0026quot; Scheduler dry-run: - All tasks successfully allocated. Job Modify Index: 229 To submit the job with version verification run: nomad job run -check-index 229 example.nomad When running the job with the check-index flag, the job will only be run if the server side version matches the job modify index returned. If the index has changed, another user has modified the job and the plan's results are potentially invalid. 以上、Getting Started の Jobs の内容でした。Job の種類が Service の他に Batch と System がありますが、Service でできることをもう少し確認したいですね。\n","date":"2019年4月22日","permalink":"/2019/04/run-nomad-service-job/","section":"Posts","summary":"前回 Nomad クラスタを構築しました。今回はこのクラスタで Job を実行します。 nomad job init # job init コマンドを実行することでスケルトンファイルを生成することができ","title":"Nomad で Service Job を実行する"},{"content":"","date":"2019年4月14日","permalink":"/tags/consul/","section":"Tags","summary":"","title":"Consul"},{"content":"","date":"2019年4月14日","permalink":"/tags/hashicorp/","section":"Tags","summary":"","title":"Hashicorp"},{"content":"口にするとマサカリならぬ船のホイールが飛んできそうですが、話題の Hashicorp Nomad のクラスタをセットアップしてみました。下の図のような構成です。3台のサーバーで構築した Consul クラスタとこれまた3台のサーバーで Nomad のサーバークラスタを構築し、そこへ3台の Nomad クライアント(worker)を参加させます。公式の図を拝借したらこうなっていたのですが、Nomad サーバーとクライアントが特定のもの同士で紐づいているように見えますがそういうわけではなく、クライアントはクラスタに対して参加しています。DigitalOcean に簡単に構築するための Terraform \u0026amp; Ansible を https://github.com/yteraoka/nomad-cluster-do に置いてあります。\n  \nNomad は Kubernetes の Cluster IP みたいなものがなく、コンテナの公開するポート情報を Consul に登録し、その Consul の情報を元に動的に振り分け先を更新するロードバランサー（Fabio というのが Consul を直接参照できるらしいし、HAProxy は 1.8 から DNS を使った動的更新ができ、この DNS サーバーとして Consul も使えるらしい）を使ったり consul-template で動的に HAProxy や nginx の設定を更新することによってサービスを公開するようにします。これは昔 (Docker 1.11 以前) の Docker Swarm の構成に似ています (参考: 小さく始める Docker container の deploy)。もちろん、そんな昔のやつより高機能です。ちゃんと指定したコンテナの数を維持してくれたり、ローリングアップデートの仕組みもありますし、DaemonSet みたいなものも、batch 実行にも対応しています。Job についてはまだ詳しく見れていないけれど batch が不要なら Nomad よりも Docker Swarm mode のほうが構築はずっと楽です。Docker だけで構築できますし。\nLoad Balancing Strategies for Consul\nConsul Cluster # Consul は3台のサーバーでクラスタを組み、Nomad を実行する各サーバー (Server も Worker も) には agent を起動させてクラスタに参加させます。各 Nomad は localhost の Consul agent とやり取りすることになります。Consul サーバーに誰でも自由にアクセスできては困るので TLS のクライアント証明書で認証するようにしてあります。Web UI へのアクセスにもクライアント証明書が必要なのでブラウザで使えるように登録します。いつの間にか consul には tls サブコマンドが実装されており、簡単に CA や証明書を作成することができるようになっていました。便利。(Creating and Configuring TLS Certificates)\n通信内容の暗号化だけであれば encrypt 設定があり、TLS が無効でも暗号化することができます。\nConsul には ACL 機能もありますが、まだ理解不足なので有効にしていません。\nTLS 認証必須にするとサーバー上で consul コマンドで確認するときにも証明書や鍵の path や URL 指定が必要でこれをいちいちコマンドラインオプションで指定するのは大変なので環境変数を使うと便利です。\n# consul members Error retrieving members: Get http://127.0.0.1:8500/v1/agent/members?segment=_all: dial tcp 127.0.0.1:8500: connect: connection refused export CONSUL_HTTP_ADDR=https://127.0.0.1:8501 export CONSUL_CACERT=/etc/consul.d/ca.pem export CONSUL_CLIENT_CERT=/etc/consul.d/cert.pem export CONSUL_CLIENT_KEY=/etc/consul.d/key.pem # consul members Node Address Status Type Build Protocol DC Segment consul-0 10.130.22.70:8301 alive server 1.4.4 2 dc1 consul-1 10.130.83.30:8301 alive server 1.4.4 2 dc1 consul-2 10.130.53.222:8301 alive server 1.4.4 2 dc1 nomad-client-0 10.130.76.121:8301 alive client 1.4.4 2 dc1 nomad-client-1 10.130.90.138:8301 alive client 1.4.4 2 dc1 nomad-client-2 10.130.83.99:8301 alive client 1.4.4 2 dc1 nomad-server-0 10.130.90.148:8301 alive client 1.4.4 2 dc1 nomad-server-1 10.130.90.224:8301 alive client 1.4.4 2 dc1 nomad-server-2 10.130.82.224:8301 alive client 1.4.4 2 dc1 Consul の Web UI は次のような感じ (consul サーバーの 8501 ポートに https でアクセスします)\nサービス一覧  ノード一覧  Nomad # Nomad も Consul と似たような構成ですね。中で Serf が使われてるのも同じですし。Consul と同じように TLS のクライアント証明書認証をするようにしました。今回構築したものは Nomad のテスト用だから当然ですが Consul は Nomad 専用なので同じ CA と証明書を使いまわすことにしました。TLS なしでの暗号化についても Consul と同じです。\nNomad も TLS のクライアント証明書認証を強制している場合は環境変数を指定しておく。nomad daemon 用には設定ファイルで指定してある。consul も同様。\n# nomad server members Error querying servers: Get http://127.0.0.1:4646/v1/agent/members: net/http: HTTP/1.x transport connection broken: malformed HTTP response \u0026quot;\\x15\\x03\\x01\\x00\\x02\\x02\u0026quot; export NOMAD_ADDR=https://127.0.0.1:4646 export NOMAD_CACERT=/etc/consul.d/ca.pem export NOMAD_CLIENT_CERT=/etc/consul.d/cert.pem export NOMAD_CLIENT_KEY=/etc/consul.d/key.pem # nomad server members Name Address Port Status Leader Protocol Build Datacenter Region nomad-server-0.global 10.130.90.148 4648 alive true 2 0.9.0 dc1 global nomad-server-1.global 10.130.90.224 4648 alive false 2 0.9.0 dc1 global nomad-server-2.global 10.130.82.224 4648 alive false 2 0.9.0 dc1 global # nomad node status ID DC Name Class Drain Eligibility Status 063515e8 dc1 nomad-client-2 false eligible ready 0977c339 dc1 nomad-client-0 false eligible ready cc10bd14 dc1 nomad-client-1 false eligible ready Nomad の Web UI は次のような感じ (nomad サーバーの 4646 ポートに https でアクセスします)\nサーバー一覧  クライアント一覧  Job はまだ登録してないので空っぽ\nNomad は Kubernetes よりも簡単なのか？ # 微妙\u0026hellip;\nOverlay ネットワークとかないし、Calico, Flannel, Weave ?? どれ使えば良いの？😧 とか Istio, Cilium, Linkerd 😵 ??? という悩みは減りますね。\n用途によっては Docker Swarm mode がやっぱり一番お手軽\n続く・・・ # せっかくなのでそのうち Job についても調べてみます。Job が Deployment で Group が ReplicaSet で Task が Pod っぽいのかな。\nコンテナの実行に限らず、サーバー上のコマンドをそのまま実行したりもできる。\nService, Batch, System という種類があって System は DaemonSet っぽい (Schedulers)\n","date":"2019年4月14日","permalink":"/2019/04/setup-nomad-cluster/","section":"Posts","summary":"口にするとマサカリならぬ船のホイールが飛んできそうですが、話題の Hashicorp Nomad のクラスタをセットアップしてみました。下の図のような構成です。3台のサー","title":"Nomad cluster のセットアップ"},{"content":"","date":"2019年4月1日","permalink":"/tags/openvpn/","section":"Tags","summary":"","title":"OpenVPN"},{"content":"前回の設定で外出先からスマホでおうちに VPN 接続してリモートデスクトップで PC にアクセスできるようになりました。しかし、まだ問題が残っていました。PC は未使用時にはスリープ状態になっているのでした。でもおじさんなので知っています、こんな時のために Wake-on-LAN という機能があるのです。ラズパイサーバーからマジックパケットを送ってあげましょう。（私は root としてログインしてたので以下はその前提になっています。必要であれば sudo とかを足してやってください）\nラズパイから Wake on LAN パケットを送る # ググると etherwake というコマンドを使えば良さそうだとわかります。前回使った dietpi-software コマンドではそれらしいパッケージは見つかりませんでした。でも DietPi では apt が使えるのでこれでインストールできました。\napt install etherwake etherwake の使い方はこんな感じ\nusage: etherwake \\[-i \\] \\[-p aa:bb:cc:dd\\[:ee:ff\\]\\] 00:11:22:33:44:55 Use '-u' to see the complete set of options. 起動させたい PC の MAC アドレスを調べて引数でわたせば OK. 早速試しましたが起動しない\u0026hellip; 私のラズパイには有線LANポートもあるので -i wlan0 とインターフェースを指定してやる必要がありました。\nVPN 接続時に自動で PC を起こしたい # さて、コマンドを実行すれば起動させられることはわかりました。でも VPN 接続後にわざわざ SSH してコマンドを打つのも面倒です。WebUI 作って実行ですらやりたくありません。\nOpenVPN 接続時に任意のコマンドをサーバー側で実行させる機能はないものかと調べたらありました。\n--client-connect で接続時のコマンドを --client-disconnect で切断時のコマンドを実行できるようです。危険なコマンドを不用意に実行してしまわないようになっており、任意の外部コマンドを実行する場合は --script-security 2 と指定する必要があるようです。\n 0 \u0026ndash; Strictly no calling of external programs. 1 \u0026ndash; (Default) Only call built-in executables such as ifconfig, ip, route, or netsh. 2 \u0026ndash; Allow calling of built-in executables and user-defined scripts. 3 \u0026ndash; Allow passwords to be passed to scripts via environmental variables (potentially unsafe).  OpenVPN の起動コマンドの引数を変更する必要があります。私の使っている DietPi は systemd が採用されているためこれのカスタマイズには systemctl edit コマンドを使います。Unit は openvpn@server.service だったので次のようにします。\nsystemctl edit openvpn@server.service これで、エディタが立ち上がるので次のように入力して保存して終了します。\n\\[Service\\] ExecStart= ExecStart=/usr/sbin/openvpn --daemon ovpn-%i --status /run/openvpn/%i.status 10 --cd /etc/openvpn --config /etc/openvpn/%i.conf --writepid /run/openvpn/%i.pid --client-connect /etc/openvpn/on-connect.sh --script-security 2 これで /etc/systemd/system/openvpn@server.service.d/override.conf というファイルに保存されます。\netherwake は root で実行する必要があるため sudo の設定も行います。sudoedit /etc/sudoers.d/openvpn\nnobody ALL=(root) NOPASSWD: /usr/sbin/etherwake 後は /etc/openvpn/on-connect.sh にコマンドを書いて実行権限をつけれやり、OpenVPN を再起動すれば完了！\n接続時のコマンド実行時は環境変数でクライアント名や接続元IPアドレスなど多くの情報を取得可能でした。\n参考サイト #  OpenVPN の接続・切断時に Slack に通知する  ","date":"2019年4月1日","permalink":"/2019/04/wake-on-lan-on-vpn-connected/","section":"Posts","summary":"前回の設定で外出先からスマホでおうちに VPN 接続してリモートデスクトップで PC にアクセスできるようになりました。しかし、まだ問題が残っていました。","title":"VPN 接続時に PC を起こしてやる"},{"content":"","date":"2019年3月30日","permalink":"/tags/dietpi/","section":"Tags","summary":"","title":"DietPi"},{"content":"家の PC にリモートデスクトップでアクセスしたくなったので環境構築したお話。\nChrome の Remote Desktop 機能使えば簡単にできるのかと思ったけれども Android から接続した場合に Ctrl キーなどを入力する方法がなかったのでしかたなく VPN サーバーをセットアップすることにしました。サーバーとなるのは以前の記事にもした「kano を DietPi でサーバーにした」ラズパイです。\n     （ラズパイイメージ図）\nDietPi に PiVPN をインストールする # PiVPN (Simplest OpenVPN setup and configuration, designed for Raspberry Pi.) というものを見つけたのでこれを使います。\nとは言っても、DietPi 使いこなしてないので作法を知らないのですけど\u0026hellip;\nDietPi はログインすると次のような表示となります。\n なんとなく dietpi-software というやつを使えば良さそうです。実行してみましょう。\n Search で検索します。\n 検索ワードは vpn で。\n PiVPN があったので選択して Ok で戻ります。\n Install でさっき選択した PiVPN をインストールします。\n インストール中にサービスが止まったりするよと注意書き\n OpenVPN もインストールするよ。\n サーバーだからIPアドレスを固定する必要があるよ。\n 今のアドレスで固定しちゃってよいですか？と、DHCP サーバーで予約アドレスにしてあるので Ok\n DHCP の IP プールだから DHCP サーバー側をうまいことやっといてね。\n OpenVPN 用のユーザー選んでね。\n ここでは pivpn を選択することにしました。\n インターネットにポートを公開するサービスだから unattended-upgrades 機能を有効にすることを推奨しますよと、でも reboot は自分でやってね。\n unattended upgrades を有効にしますか？はい。\n OpenVPN で使うポートを UDP にするか TCP にするかを選択する。UDP にします。\n ポート番号はデフォルトの 1194 のままにする。\n  OpenVPN 2.4 からよりセキュアな認証と鍵交換をサポートしているので有効にしますか？ モバイルアプリの OepnVPN Connect はサポートしてます。はい有効にします。\n 暗号の鍵長選択。うちのラズパイは貧弱なので 256 ビットを選択。\n クライアントが接続する先のIPアドレス or DNS 名指定。我が家は固定IPアドレスではないので DNS を選択する。\n DNS 名指定。我が家の WiFi ルーターは TP-Link Deco M5 なので簡単に DDNS 設定できました。\n 確認。\n VPN クライアントに配る DNS サーバーの IP アドレスリストを指定します。有名所の Public DNS サーバーは選択肢から選べます。スクロールして見えなくなっていますが Google の 8.8.8.8 のやつも1番目にありました。VPN での接続先にあるサーバーを指定したりする場合は Custom を選択します。\n Quad9 (9.9.9.9) を指定してみました。\n 確認。\n インストール完了。pivpn add でクライアント用プロファイル作ってね。ログは /etc/pivpn ディレクトリにあるよ。\n インストール後は reboot をオススメします。reboot しますか？はい。\n reboot します。\n Ok で reboot するよ。\nインストール完了しました。次はクライアント用プロファル作成です。\nVPN クライアント用プロファイル作成 # インストール中にも表示されていたように pivpn add コマンドで作成できます。\n# pivpn -h ::: Control all PiVPN specific functions! ::: ::: Usage: pivpn \u0026lt;command\u0026gt; [option] ::: ::: Commands: ::: -a, add [nopass] Create a client ovpn profile, optional nopass ::: -c, clients List any connected clients to the server ::: -d, debug Start a debugging session if having trouble ::: -l, list List all valid and revoked certificates ::: -r, revoke Revoke a client ovpn profile ::: -h, help Show this help dialog ::: -u, uninstall Uninstall PiVPN from your system! キーペアの秘密鍵をパスフレーズで保護しない場合は pivpn add nopass と実行します。 実行したら Enter a Name for the Client: と、クライアント名の入力を求められるので入力したら /home/pivpn/ovpns ディレクトリに クライアント名.ovpn ファイルが作成されます。このファイルをメールなどでスマホ (スマホじゃなくても良いけど) に送って OpenVPN Connect アプリを起動して読み込めば完了です。\nあ、ポート転送忘れてた。\nルーターでのポート転送 # 我が家は Softbank のルーターの下にさらに WiFi ルーターの TP-Link Deco M5 が NAT しているのでそれぞれで 1194/udp を転送しました。\n[Softbank Router] --\u0026gt; [Deco M5] --\u0026gt; [OpenVPN on DietPi] これで Microsoft Remote Desktop アプリを使えば Ctrl キーとかも使えます。やったネ！(Beta版もある)\n","date":"2019年3月30日","permalink":"/2019/03/pivpn/","section":"Posts","summary":"家の PC にリモートデスクトップでアクセスしたくなったので環境構築したお話。 Chrome の Remote Desktop 機能使えば簡単にできるのかと思ったけれども Android から接続した場合","title":"PiVPN で外からおうちに VPN 接続する"},{"content":"","date":"2019年3月27日","permalink":"/tags/rancher/","section":"Tags","summary":"","title":"Rancher"},{"content":"「続 Rancher 2.0 の HA 構成を試す」で DigitalOcean に Rancher の HA 環境を1コマンドでセットアップするスクリプトの紹介をしていたのですが、そこで使っていた RKE Add-on 方式はもう古いということが前回判明したので helm でセットアップするように書き換えました。\nRKE Add-on 版では Rancher 2.0.8 までにしか対応していなかったのですが、helm 対応により GA になったばかりの 2.2 も使えます。スクリプトの中では rancher-latest を使うようになっているので今実行すると 2.2 がセットアップされるはずです。2.0.8 と比べた構成の違いとして大きいのは以前は Kubernentes クラスタ内に1コンテナしか起動していなかったのが、複数コンテナに対応していることですね。3台のサーバーで3つのコンテナが起動してました。\n自分が使いたいから書いただけですが Github にあります。\nhttps://github.com/yteraoka/rancher-ha-tf-do\n","date":"2019年3月27日","permalink":"/2019/03/rancher-ha-helm/","section":"Posts","summary":"「続 Rancher 2.0 の HA 構成を試す」で DigitalOcean に Rancher の HA 環境を1コマンドでセットアップするスクリプトの紹介をしていたのですが、そこで使っていた RKE Add-on 方式はもう古い","title":"Rancher HA 1コマンドセットアップを Helm 版にした"},{"content":"","date":"2019年3月24日","permalink":"/tags/helm/","section":"Tags","summary":"","title":"Helm"},{"content":"RKE Add-on での Rancher セットアップはもう古い # 過去に試していた RKE での Rancher の HA セットアップ (続 Rancher 2.0 の HA 構成を試す) はどうやらもう古いらしい。2.0.8 までしかサポートしてないよというのには気づいていたのですが、改めてインストールページ(RKE Add-On Install)を確認するとこの方法はもう過去のものらしい。Helm 使ってねと。\n Important: RKE add-on install is only supported up to Rancher v2.0.8\nPlease use the Rancher helm chart to install HA Rancher. For details, see the HA Install - Installation Outline.\nIf you are currently using the RKE add-on install method, see Migrating from an HA RKE Add-on Install for details on how to move to using the helm chart.\n Helm を使ったセットアップへの変更 # そんなわけで、構築済みの HA Rancher を RKE add-on から変更する手順をなぞってみます。Migrating from an HA RKE Add-on Install\nkubectl を rancher cluster に向ける # 私のセットアップでは RKE に rke.yml というファイルを渡したので kube_config_rke.yml が生成されています。環境変数 KUBECONFIG でこれを指定します。\nexport KUBECONFIG=$(pwd)/kube\\_config\\_rke.yml Kubernetes は 1.11.6 だったので同じバージョンの kubectl コマンドを curl で取得します。とりあえずカレントディレクトリに置いておくので以降 ./kubectl として実行します。(後で出てくる helm コマンドも同様)\nInstall kubectl binary using curl\n次のコマンドでここまでの設定が正しく行えているかを確認します。\n./kubectl config view -o=jsonpath='{.clusters\\[\\*\\].cluster.server}' 出力が https://NODE:6443 の様に1つのノードの 6443 ポートを指していれば正しいです。\n証明書の保存 # Rancher クラスタの Ingress で TLS Termination を行っている場合は Helm でのインストール時に必要となるため、次のコマンドで取り出します。後で Kubernetes Secrets に登録します。\n証明書\n./kubectl -n cattle-system get secret cattle-keys-ingress \\\\ -o jsonpath --template='{ .data.tls\\\\.crt }' | base64 -d \u0026gt; tls.crt 秘密鍵\n./kubectl -n cattle-system get secret cattle-keys-ingress \\\\ -o jsonpath --template='{ .data.tls\\\\.key }' | base64 -d \u0026gt; tls.key プライベート CA を使っている場合は次のコマンドで CA の証明書も取得します。\n./kubectl -n cattle-system get secret cattle-keys-server \\\\ -o jsonpath --template='{ .data.cacerts\\\\.pem }' | base64 -d \u0026gt; cacerts.pem 古い Kubernetes オブジェクトを削除 # RKE でのインストールで作られた Kubernetes オブジェクトを削除する。\n./kubectl -n cattle-system delete ingress cattle-ingress-http ./kubectl -n cattle-system delete service cattle-service ./kubectl -n cattle-system delete deployment cattle ./kubectl -n cattle-system delete clusterrolebinding cattle-crb ./kubectl -n cattle-system delete serviceaccount cattle-admin これらのコンポーネントを削除しても Rancher の設定やデータベースに影響はありませんが、何かメンテナンスを行う場合にバックアップを取得しておくのは良いことです。バックアップの取得方法は Creating Backups—High Availability Installs にあります。\nAddon の削除 # RKE で使う YAML (ここでは rke.yml) には Rancher で必要なリソースが全て入っています。今後の RKE 操作のためにここから addons セクションのまるっと削除しておきます。\naddons: |- --- kind: Namespace apiVersion: v1 metadata: name: cattle-system --- kind: ServiceAccount apiVersion: v1 metadata: name: cattle-admin namespace: cattle-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cattle-crb namespace: cattle-system subjects: - kind: ServiceAccount name: cattle-admin namespace: cattle-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io --- apiVersion: v1 kind: Secret metadata: name: cattle-keys-ingress namespace: cattle-system type: Opaque data: tls.crt: \u0026lt;証明書の Base64\u0026gt; tls.key: \u0026lt;秘密鍵の Base64\u0026gt; --- apiVersion: v1 kind: Service metadata: namespace: cattle-system name: cattle-service labels: app: cattle spec: ports: - port: 80 targetPort: 80 protocol: TCP name: http - port: 443 targetPort: 443 protocol: TCP name: https selector: app: cattle --- apiVersion: extensions/v1beta1 kind: Ingress metadata: namespace: cattle-system name: cattle-ingress-http annotations: nginx.ingress.kubernetes.io/proxy-connect-timeout: \u0026quot;30\u0026quot; nginx.ingress.kubernetes.io/proxy-read-timeout: \u0026quot;1800\u0026quot; # Max time in seconds for ws to remain shell window open nginx.ingress.kubernetes.io/proxy-send-timeout: \u0026quot;1800\u0026quot; # Max time in seconds for ws to remain shell window open spec: rules: - host: rancher.do.teraoka.me # FQDN to access cattle server http: paths: - backend: serviceName: cattle-service servicePort: 80 tls: - secretName: cattle-keys-ingress hosts: - rancher.do.teraoka.me # FQDN to access cattle server --- kind: Deployment apiVersion: extensions/v1beta1 metadata: namespace: cattle-system name: cattle spec: replicas: 1 template: metadata: labels: app: cattle spec: serviceAccountName: cattle-admin containers: # Rancher install via RKE addons is only supported up to v2.0.8 - image: rancher/rancher:v2.0.8 args: - --no-cacerts imagePullPolicy: Always name: cattle-server livenessProbe: httpGet: path: /ping port: 80 initialDelaySeconds: 60 periodSeconds: 60 readinessProbe: httpGet: path: /ping port: 80 initialDelaySeconds: 20 periodSeconds: 10 ports: - containerPort: 80 protocol: TCP - containerPort: 443 protocol: TCP 削ると残るのはこれだけ\nnodes: - address: 192.168.100.1 # hostname or IP to access nodes user: rancher # root user (usually 'root') role: \\[controlplane,etcd,worker\\] # K8s roles for node ssh\\_key\\_path: id\\_rsa # path to PEM file - address: 192.168.100.2 user: rancher role: \\[controlplane,etcd,worker\\] ssh\\_key\\_path: id\\_rsa - address: 192.168.100.3 user: rancher role: \\[controlplane,etcd,worker\\] ssh\\_key\\_path: id\\_rsa services: etcd: snapshot: true creation: 6h retention: 24h Helm の初期化 # ここからは Helm を使った通常のインストールする手順 (3. Initialize Helm (Install Tiller)) に進みます。Helm を使うためにはサーバーサイドコンポーネントの Tiller をインストールする必要があるようです。\nhelm コマンドは各種パッケージマネージャからもインストールできると思いますが、ここでは Github の releases ページからダウンロードします。cert-manager との組み合わせの問題で v2.12.1 以降が必要です。\nまず、tiller という名前で Tiller 用のサービスアカウントを作成する。\n./kubectl -n kube-system create serviceaccount tiller 次に、作成したサービスアカウントに cluster-admin ロールを付与します。\n./kubectl create clusterrolebinding tiller \\\\ --clusterrole=cluster-admin \\\\ --serviceaccount=kube-system:tiller Helm の初期化 # ./helm init --service-account tiller Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. と表示されていれば Tiller のインストールが完了しているはず。\ninit 時に次のメッセージも出てました。必要であれば --tiller-tls-verify も付けることも検討。\n Please note: by default, Tiller is deployed with an insecure \u0026lsquo;allow unauthenticated users\u0026rsquo; policy. To prevent this, run `helm init` with the \u0026ndash;tiller-tls-verify flag. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\n Tiller の確認 # Tiller が正しくインストールされているかどうかを次のコマンドで確かめます。\n./kubectl -n kube-system rollout status deploy/tiller-deploy deployment \u0026quot;tiller-deploy\u0026quot; successfully rolled out と表示されれば OK.\nバージョン確認\n$ ./helm version Client: \u0026amp;version.Version{SemVer:\u0026quot;v2.13.1\u0026quot;, GitCommit:\u0026quot;618447cbf203d147601b4b9bd7f8c37a5d39fbb4\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} Server: \u0026amp;version.Version{SemVer:\u0026quot;v2.13.1\u0026quot;, GitCommit:\u0026quot;618447cbf203d147601b4b9bd7f8c37a5d39fbb4\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;} Rancher のインストール # 次の手順はこちら 4. Install Rancher.\nHelm Chart repository の追加 # latest, stable, alpha から選んで追加します。Production 環境であれば stable を選択。(alpha は upgrade がサポートされていません)(Helm Chart Repositories)\n./helm repo add rancher-latest https://releases.rancher.com/server-charts/latest ./helm repo add rancher-stable https://releases.rancher.com/server-charts/stable ./helm repo add rancher-alpha https://releases.rancher.com/server-charts/alpha SSL/TLS 設定の選択 # 証明書には3つの推奨設定があります。上2つには cert-manager が必要。\nRancher が生成 (default)\n(chart option) ingress.tls.source=rancher\nLet\u0026rsquo;s Encrypt\n(chart option) ingress.tls.source=letsEncrypt\nファイルからの読み込み (Kubernetes の Secrets に入れる)\n(chart option) ingress.tls.source=secret\ncert-manager のインストール (必要な場合のみ) # Kubernetes Helm chart repository からインストールする。\n./helm install stable/cert-manager \\\\ --name cert-manager \\\\ --namespace kube-system \\\\ --version v0.5.2 インストールが完了するまで待つ。次のコマンドで deployment \u0026quot;cert-manager\u0026quot; successfully rolled out と表示されるようになれば完了。\n./kubectl -n kube-system rollout status deploy/cert-manager Rancher のデプロイ (Rancher Generated Certificates) # デフォルトが Rancer Generated なので特に証明書に関するオプションは指定されていない。(この例はリポジトリが rancher-latest になっていることに注意)\n./helm install rancher-latest/rancher \\\\ --name rancher \\\\ --namespace cattle-system \\\\ --set hostname=rancher.example.com デプロイの状況を確認する。\n./kubectl -n cattle-system rollout status deploy/rancher Rancher のデプロイ (Let’s Encrypt) # Let\u0026rsquo;s Encrypt での証明書発行のためにメールアドレスを指定する必要がある。(この例はリポジトリが rancher-latest になっていることに注意)\n./helm install rancher-latest/rancher \\\\ --name rancher \\\\ --namespace cattle-system \\\\ --set hostname=rancher.example.com \\\\ --set ingress.tls.source=letsEncrypt \\\\ --set letsEncrypt.email=me@example.org デプロイの状況を確認する。\n./kubectl -n cattle-system rollout status deploy/rancher Rancher のデプロイ (Certificates from Files) # 今回は RKE でのセットアップ時に使っていたものを取り出したファイルを使うのでこの手順で進めます。(この例はリポジトリが rancher-latest になっていることに注意)\n./helm install rancher-latest/rancher \\\\ --name rancher \\\\ --namespace cattle-system \\\\ --set hostname=rancher.example.com \\\\ --set ingress.tls.source=secret 実行すると次のような出力がある。\nNAME: rancher LAST DEPLOYED: Sun Mar 24 22:21:35 2019 NAMESPACE: cattle-system STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/ClusterRoleBinding NAME AGE rancher 1s ==\u0026gt; v1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE rancher 0/3 3 0 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE rancher-f7467f757-9mpdp 0/1 ContainerCreating 0 0s rancher-f7467f757-m5v49 0/1 ContainerCreating 0 0s rancher-f7467f757-tq4jg 0/1 ContainerCreating 0 0s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE rancher ClusterIP 10.43.23.29 80/TCP 1s ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE rancher 1 1s ==\u0026gt; v1beta1/Ingress NAME HOSTS ADDRESS PORTS AGE rancher rancher.example.com 80, 443 0s NOTES: Rancher Server has been installed. NOTE: Rancher may take several minutes to fully initialize. Please standby while Certificates are being issued and Ingress comes up. Check out our docs at https://rancher.com/docs/rancher/v2.x/en/ Browse to https://rancher.example.com Happy Containering! Kubernetes Secrets に証明書を tls という secret タイプで tls-rancher-ingress という名前で登録します。\n./kubectl -n cattle-system create secret tls tls-rancher-ingress \\\\ --cert=tls.crt \\\\ --key=tls.key プライベート CA の場合はそれも登録する。こちらは generic タイプ。\n./kubectl -n cattle-system create secret generic tls-ca \\\\ --from-file=cacerts.pem デプロイの状況を確認する。\n./kubectl -n cattle-system rollout status deploy/rancher Rancher 2.1.7 に更新されました。次は https://github.com/yteraoka/rancher-ha-tf-do を Helm でのセットアップに変更しようと思います。\n","date":"2019年3月24日","permalink":"/2019/03/rancher-migrating-from-an-ha-rke-add-on-install/","section":"Posts","summary":"RKE Add-on での Rancher セットアップはもう古い # 過去に試していた RKE での Rancher の HA セットアップ (続 Rancher 2.0 の HA 構成を試す) はどうやらもう古いらしい。2.0.8 までし","title":"Rancher: Migrating from an HA RKE Add-on Install"},{"content":"Rancher 2.x の HA 構成を RKE でセットアップし、Kubernetes クラスタを追加して、その中にコンテナをデプロイし、nginx-ingress-controller 経由で外からアクセスできるようにします。\nRKE での HA 構成セットアップは過去の投稿 (続 Rancher 2.0 の HA 構成を試す) と同じです。1コマンドで構築できます。現在のところ RKE でセットアップできるのは 2.0.8 までのようでちょっと古めで、選択できる Kubernetes も 1.11.2 まで (Hardcode Rancher version to v2.0.8 in rke templates)。その後の更新はできるんじゃないかとは思うものの調べられていない。\nKubernetes クラスタの追加 # Rancher セットアップ後のクラスタ一覧画面です\nクラスタ一覧  ここで右上の「Add Cluster」ボタンから新しい Kubernetes クラスタを追加します。(すでに存在する local というクラスタ内で Rancher サーバーが起動しています)\nクラスタ追加画面\nクラスタの追加  メジャーなクラウドサービスを使っている場合は認証情報を渡せばサーバーの作成から全部やってくれますが、ここでは別途 Docker のインストールされたサーバーを用意することにします。そのため一番上のアイコンが並んでいるところでは「Custom」を選択します。他はデフォルトのままで次に進みます。(Rancher はクラウドのマネージド Kubernetes の管理もできますし、オンプレにすでに Kubernetes が存在すればそれの管理をすることもできます。)\nKubernetes クラスタへの Node の追加 # ノードの追加コマンド確認画面\n ノードの登録  まずは etcd と Control Plane 用の3台を登録します。表示されてる docker run コマンドをコピペすれば rancher-agent がノード内で起動して必要なことを全部やってくれます。完了までにはしばらく時間かかります。(Node Role で Worker にもチェックを入れておけば worker としても使えるノードとなりますが、ここでは分けることにします)\nノード一覧画面\nノード一覧(1)  Roles 列に etcd と Control Plane が入った3つのノードが確認できます。黄色い Unschedulable という表示はこれが Worker ノードでは無いためこのノードにコンテナが割り当てられないという意味です。\nノードの追加コマンド確認画面\nWorker ノードの追加  次に Worker ノードを追加します。Node Role で Worker だけを選択してコマンドをコピペします。この環境はもう削除済みなので token もそのまま載っけちゃってますが公開しちゃダメです。\n再びノード一覧画面\nノード一覧(2)  Worker が追加されました。\nクラスタのダッシュボードはこんな感じ\ncluster dashboard  選んだサーバーの CPU 数が少なかったかな\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION cp1 Ready controlplane,etcd 34m v1.11.2 cp2 Ready controlplane,etcd 34m v1.11.2 cp3 Ready controlplane,etcd 40m v1.11.2 wk1 Ready worker 3m v1.11.2 wk2 Ready worker 3m v1.11.2 wk3 Ready worker 3m v1.11.2 プロジェクトの作成 # Rancher 2.x では Kubernetes のクラスタとネームスペースの間にプロジェクトという層が入り、複数のネームスペースを束ねて権限管理を行うことができるようになっています。このあたりも過去の投稿 (Understanding Authentication \u0026amp; Authorization In Rancher 2.0) で書きました。\n初期状態で Default と System というプロジェクトが存在します。\nprojects  自分のコンテナをデプロイするなら Default を使っても良さそうですが、ここでは myproject というプロジェクトを作ってみます。\nプロジェクト追加画面\nプロジェクトの追加  クラスタ作成時に Pod Security Policy を有効にするかどうか、デフォルトを何にするかという選択項目がありましたが、ここでも出てきました。コンテナに特権の付与を許可するかどうかなどを制御することができます。\nネームスペースの作成 # Kubernetes ではネームスペース単位で権限を管理し、コンテナはネームスペース内にデプロイすることになるため、新しく作ったプロジェクト内にネームスペースを作成します。\nネームスペース追加の画面\nネームスペースの追加  ネームスペースの名前を指定するだけです。ここでは myns とします。\nワークロードのデプロイ # Kubernetes 用語では Deployment と Service を作成することになるのかな。\nデプロイ前のワークロードページ\nワークロードページ  Deploy ボタンからワークロードをデプロイします\nワークロードをデプロイするための入力画面\nワークロードのデプロイ  名前、Docker Image、Pod の数、Port マッピング、スケーリングポリシーやボリューム、環境変数、ヘルスチェックなどを指定します。Show advanced options をクリックするともっと多くの項目が表示されます。Workload Type で More options をクリックすると DaemonSet や StatefulSet、CronJob、Job を選択できます。Port マッピングでは NodePort (On every node)、HostPort (Nodes running a pod)、Cluster IP (Internal only)、Layer-4 Load Balancer を選べます。ここでは Cluster IP (Internal only) を選択します。\nワークロード一覧ページ\nWorkload 一覧  今回は設定しませんが サイドカー設定はこのページの右の︙ボタンから行います。\n上のキャプチャのようにして httpbin.org のコンテナをデプロイしてみました。(kennethreitz/httpbin)\nhttpbin workload  Cluster IP で作成しているのでこのままでは外部からアクセスできません\n$ kubectl get services -n myns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE httpbin ClusterIP 10.43.74.133 80/TCP 16m $ kubectl get pods -n myns NAME READY STATUS RESTARTS AGE httpbin-578b587f58-9xgwb 1/1 Running 0 16m httpbin-578b587f58-bz27f 1/1 Running 0 16m httpbin-578b587f58-tbnnd 1/1 Running 0 16m nginx ingress controller で外部公開する # デプロイしたコンテナに外部からアクセスできるように Workloads タブの Load Balance のページから Ingress 設定を追加します\nLoad Balancing  Ingress 設定入力ページ\nIngress の追加  Hostname と Path ごとにどこに proxy するかを設定します。TLS の証明書設定も可能です。\n追加後の Load Balancing 画面\nLoad Balancing(2)  これでしばらく待つとブラウザでインターネット越しにアクセスできるようになります。\nnginx-ingress-controller が System プロジェクトの ingress-nginx ネームスペースに DaemonSet として各 Worker Node にデプロイされており、Worker Node に Ingress で設定した Host ヘッダーをつけてアクセスすることで作成したコンテナにルーティングされます。\n$ kubectl get ingress --all-namespaces NAMESPACE NAME HOSTS ADDRESS PORTS AGE myns httpbin httpbin.do.teraoka.me 178.128.24.17,178.128.24.30,206.189.94.233 80 14m ブラウザで /anything にアクセスしました。\n{ \u0026#34;args\u0026#34;: {}, \u0026#34;data\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;files\u0026#34;: {}, \u0026#34;form\u0026#34;: {}, \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\u0026#34;, \u0026#34;Accept-Encoding\u0026#34;: \u0026#34;gzip, deflate\u0026#34;, \u0026#34;Accept-Language\u0026#34;: \u0026#34;ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7\u0026#34;, \u0026#34;Connection\u0026#34;: \u0026#34;close\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;httpbin.do.teraoka.me\u0026#34;, \u0026#34;Upgrade-Insecure-Requests\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\u0026#34;, \u0026#34;X-Forwarded-Host\u0026#34;: \u0026#34;httpbin.do.teraoka.me\u0026#34;, \u0026#34;X-Original-Uri\u0026#34;: \u0026#34;/anything\u0026#34;, \u0026#34;X-Scheme\u0026#34;: \u0026#34;http\u0026#34; }, \u0026#34;json\u0026#34;: null, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;origin\u0026#34;: \u0026#34;我が家のグローバルIPアドレス\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://httpbin.do.teraoka.me/anything\u0026#34; } Origin として Global IP が認識されているので X-Forwarded-For がちゃんとわたっているようです。\nYAML 一個も書いてねー！\n","date":"2019年3月10日","permalink":"/2019/03/rancher2-walk-through/","section":"Posts","summary":"Rancher 2.x の HA 構成を RKE でセットアップし、Kubernetes クラスタを追加して、その中にコンテナをデプロイし、nginx-ingress-cont","title":"Rancher2 の構築からサービス公開まで"},{"content":"","date":"2019年2月23日","permalink":"/tags/dns/","section":"Tags","summary":"","title":"DNS"},{"content":"海外滞在中に Amazon Prime Video の日本向けコンテンツを見たかったり、帰りの飛行機の中で見るためにダウンロードしておきたくなったりしますよね。しか～し、普通にアプリを開いても地域制限がかかっているため視聴もダウンロードもできません 😱 残念。\n海外で日本向けの動画を見ようとすると次のように表示されて再生できません。\n ただ、簡単に諦めてしまうのもつまらないので、なんとかホテルの無線LAN環境からスマホで視聴可能にする方法はないかともがいたログです。\nVPN # まず思いつくのは VPN ですね、日本の IP からのアクセスとなるように日本の VPS とかを使って VPN サーバーをセットアップします。\nIDCF クラウドは VPN 環境をさくっと作れるような何かがあった気がするけど個人向けサービスは終了するということなので対象外。安価で、通信量制限もないらしい ConoHa VPS を使ってみました。VPN サーバーとして使うだけなので 512MB の月額630円(または1時間1円)で十分です。\nLinux Distribution は何でも良いのですが今回は CentOS 7 にした。前は Ubuntu で構築した けど慣れてるし。\nHow To Set Up and Configure an OpenVPN Server on CentOS 7 を参考にしたけどまんまではうまくいかなかったのでちょっと調整。（詳細は後で追記するかも）\nスマホからの接続には OpenVPN Connect を使いました。\n「よし、これで見れるぞ」と思ったのですが\u0026hellip;\n amazon.jp/vpn\n ぐぬぬ、、撃沈\u0026hellip; 😭\nDNS だけ日本のサーバーを使う # VPN 使用時のメッセージから HTTP プロキシは検出されてしまいそうなので次は DNS サーバーだけ日本にあるものをつかうことにします。先ほどの VPN サーバーを DNS サーバーとして使えるように Unbound をセットアップしました。\n無線LANで割り当てられたIPアドレスを確認し、固定IPでそれを指定し、DNS サーバーだけ変更しました。\nしかし、名前解決ができません。DNS サーバー側でログを見てもクエリが届いてないし、tcpdump をしてみても問い合わせの packet が届きません。ホテルの無線LAN環境は MAC アドレスからするとどうやら Ruckus Networks のコントローラが導入されているようです。使い始める際に認証画面が表示されるタイプだったのでおそらく DNS のリクエストはそこでインターセプトされてしまっているようです 😢\nまたしても撃沈\u0026hellip; 😭😭\n会社から貸与されているポータブル WiFi ルーターがあるので、これを使って DNS サーバーさえ日本のものが使えれば見れるのか？を確認してみました。仕事と関係ない動画のダウンロードに使うのはよろしくないので動作確認にとどめます。\n結果は成功！日本の DNS サーバーさえ使えれば見れることが確認できました。一歩前進。\nDNS over HTTPS を使おう # 普通に DNS で問い合わせると無線LANコントローラに横取りされてしまうので、これを避けるために使えるのは DNS over HTTPS じゃないか？ということでこれが可能な Proxy を探してみまたらありました。cloudflared でできるようです、これは Go で書かれていてシングルバイナリで Windows 版もあり、ここからダウンロードできます。Cloudflare は 1.1.1.1 を提供しているところですね。\nhttps://developers.cloudflare.com/argo-tunnel/downloads/ からバイナリをダウンロードできます。\n使い方は次のようになっており、proxy-dns コマンドを使えば通常の DNS で受けた問い合わせを DNS over HTTPS で任意の対応サーバーにプロキシしてくれます。\n$ ./cloudflared --help NAME: cloudflared - Cloudflare's command-line tool and agent USAGE: cloudflared.exe [global options] command [command options] origin-url VERSION: 2019.1.0 (built 2019-01-28-2336 UTC) DESCRIPTION: cloudflared connects your machine or user identity to Cloudflare's global network. You can use it to authenticate a session to reach an API behind Access, route web traffic to thismachine, and configure access control. COMMANDS: update Update the agent if a new version exists version Print the version proxy-dns Run a DNS over HTTPS proxy server. service Manages the Argo Tunnel Windows service help, h Shows a list of commands or help for one command Access (BETA): access access \u0026lt;subcommand\u0026gt; Tunnel: tunnel Make a locally-running web service accessible over the internet using Argo Tunnel. GLOBAL OPTIONS: --help, -h show help (default: false) --version, -v, -V Print the version (default: false) COPYRIGHT: (c) 2019 Cloudflare Inc. Use is subject to the license agreement at https://developers.cloudflare.com/argo-tunnel/license/ $ ./cloudflared proxy-dns --help NAME: cloudflared.exe proxy-dns - Run a DNS over HTTPS proxy server. USAGE: cloudflared.exe proxy-dns [command options] OPTIONS: --metrics value Listen address for metrics reporting. (default: \u0026quot;localhost:\u0026quot;) [%TUNNEL_METRICS%] --address value Listen address for the DNS over HTTPS proxy server. (default: \u0026quot;localhost\u0026quot;) [%TUNNEL_DNS_ADDRESS%] --port value Listen on given port for the DNS over HTTPS proxy server. (default: 53) [%TUNNEL_DNS_PORT%] --upstream value Upstream endpoint URL, you can specify multiple endpoints for redundancy. (default: \u0026quot;https://1.1.1.1/dns-query\u0026quot;, \u0026quot;https://1.0.0.1/dns-query\u0026quot;) [%TUNNEL_DNS_UPSTREAM%] --help, -h show help (default: false) さて、Proxy サーバーは見つかりましたが、1.1.1.1 を使ってしまっては日本からのアクセスにならないはずなので、やはり先ほどの DNS サーバーに今度は DNS over HTTPS サーバーを構築します。github.com/m13253/dns-over-https が使えるようです。ググれば何でも見つかってすごい。\nPC で次のように cloudflared を起動します。\n$ ./cloudflared proxy-dns --address 0.0.0.0 --upstream https://dns.example.com/dns-query INFO[0000] Adding DNS upstream url=\u0026quot;https://dns.example.com/dns-query\u0026quot; INFO[0000] Starting metrics server addr=\u0026quot;127.0.0.1:57464\u0026quot; INFO[0000] Starting DNS over HTTPS proxy server addr=\u0026quot;dns://0.0.0.0:53\u0026quot; この例では DNS サーバーの指定が dns.example.com になっています、Windosw PC 自体もこの proxy サーバーを参照するようにしてしまうと dns.example.com の名前解決ができなくなります\u0026hellip;\n今回の構成では Windows は DHCP でもらった DNS サーバーをそのまま使用して問題なし。1.1.1.1 は IP アドレスなので名前解決不要なのですが、通常 IP アドレスに対して公的な証明書は取得できないんじゃないかな。\nで、次のような構成になりました。cloudflared は証明書のチェックもするので Let\u0026rsquo;s Encrypt で取得しました。\nで、結果はというと？\nえ～っと、、何度目だっけ\u0026hellip; 撃沈 😭😭😭\nきっとそうだろうなぁと薄々思いながらやってましたがやっぱり無線の端末同士は通信が許可されていませんでした。まあそうですよね。\nWindows の Mobile Hotspot 機能 # あきらめが悪いのでさらに次の手を考えます。PC を無線の親機にできたら解決じゃね？ということです。Windows 10 にはモバイルホットスポット機能というのがあって簡単に無線LANの親機にできちゃいます。\n「設定」→「ネットワークとインターネット」で\n 「モバイルホットスポット」を有効にするだけです\n これで完璧！！\nのはずでしたが、どうやら会社管理の Windows PC であったため、この通信が許可されないようでした 😵\nさすがにここであきらめました。が、帰宅してから家の PC でこの構成が有効であることは確認しました。ヤッタネ 😉\n（ただ、映画館で予告編とか見てるとこれも見たいあれも見たいってなるけどいざ Prime Video でどれ見ようかなって探すときには見たいものが全然無いんだよなあ）\n帰りの飛行機は行きよりも映画のラインナップがずっと良くて助かりました。\n","date":"2019年2月23日","permalink":"/2019/02/dns-over-https/","section":"Posts","summary":"海外滞在中に Amazon Prime Video の日本向けコンテンツを見たかったり、帰りの飛行機の中で見るためにダウンロードしておきたくなったりしますよね。しか～し、普通","title":"DNS over HTTPS の使いみち"},{"content":"","date":"2019年2月23日","permalink":"/tags/vpn/","section":"Tags","summary":"","title":"VPN"},{"content":"","date":"2019年1月13日","permalink":"/tags/guacamole/","section":"Tags","summary":"","title":"Guacamole"},{"content":"kakakakakku さんのブログの Guacamole 記事を見て、私もやったのになあとこんな tweet をしてしまいました。\nGuacamole を docker compose でって、去年やったわー、でもブログに書かずじまいだったわー。書こう / “001605” https://t.co/DEMJDqM5O5\n\u0026mdash; yteraoka (@yteraoka) January 9, 2019  てなわけで書かざるをえません。そして書こうと思ったら直前に1年ぶりの新バージョン 1.0.0 がリリースされていたのでこれを試します。\nGuacamole とは # （オフィスでご覧の方は ZOZOTOWN Guacamole で検索するといいんじゃないかな 🤭）\nhttps://guacamole.apache.org/ に\n Apache Guacamole is a clientless remote desktop gateway. It supports standard protocols like VNC, RDP, and SSH.\n と書いてあります。\nブラウザさえあれば、Windows へのリモートデスクトップも VNC サーバーへのアクセスも SSH も Telnet もできちゃうすぐれものです。その操作をレコーディングして監査に使うこともできますし、セッションを共有することで別の場所にいる誰かと画面や SSH のセッションを共有することも可能です。参照専用の共有と操作可能な共有から選択できますし、共有用のリンクからであれば Guacamole へのログインも不要です。認証については DB に Guacamole 用ユーザーを作成することも可能ですし、LDAP を参照することも OpenID サーバと連携させることも可能です。1.0.0 では RADIUS にも対応したようですし、LDAP のグループにも対応したっぽい。以前は2要素認証として Duo だけがサポートされていましたが 1.0.0 から Google Authenticator や Authy などで使える TOTP (Time-based One-time Password) も使えるようになりました。\n共有アカウントの接続情報を登録しておき、Guacamole の認証、認可によりその接続を利用させるといった使い方になると思います。\nDocker Compose で実行する # 各機能を使うにはいろいろ依存ライブらいなどを用意する必要があるので Docker を使うのが楽そうです。そこで docker-compose を使います。\n環境 # 使った docker と docker-compose のバージョンは次の通り。\n$ sudo docker version Client: Version: 18.09.1 API version: 1.39 Go version: go1.10.6 Git commit: 4c52b90 Built: Wed Jan 9 19:35:01 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 18.09.1 API version: 1.39 (minimum version 1.12) Go version: go1.10.6 Git commit: 4c52b90 Built: Wed Jan 9 19:06:30 2019 OS/Arch: linux/amd64 Experimental: false $ docker-compose version docker-compose version 1.23.2, build 1110ad01 docker-py version: 3.6.0 CPython version: 3.6.7 OpenSSL version: OpenSSL 1.1.0f 25 May 2017 構成 # Guacamole はサーバーの guacd と、クライアントで Web UI を提供する guacamole と設定やユーザー情報を保存する Database (MySQL, PostgreSQL, SQLServer) で構成されます。この記事では Database に PostgreSQL を使いました。LDAP のスキーマを拡張することで設定も LDAP に保存することができるようですが試していません。後半で LDAP 連携も試します。\ndocker-compose 設定 # docker-compose.yml は次のようにします。PostgreSQL のデータはホストのディレクトリをマウントさせるため pgdata ディレクトリをホスト側に作成しておきます。\ndocker-compose.yml # version:\u0026#39;2\u0026#39;services:postgres:image:postgres:10-alpineenv_file:./envvolumes:- ./pgdata:/var/lib/postgresql/datarestart:alwaysguacd:image:guacamole/guacd:1.0.0restart:alwaysguacamole:image:guacamole/guacamole:1.0.0env_file:./envenvironment:GUACD_HOSTNAME:guacdGUACD_PORT:4822POSTGRES_HOSTNAME:postgresrestart:alwaysnginx:image:nginx:latestports:- \u0026#34;80:80\u0026#34;volumes:- ./nginx.conf:/etc/nginx/nginx.confrestart:alwaysenv # env ファイルに guacamole と postgres コンテナで共有する環境変数を入れてあります。\nPOSTGRES_DATABASE=guacamole POSTGRES_USER=guacamole POSTGRES_PASSWORD=guacamole guacamole の Tomcat に直接アクセスしても良いのですが、実用するには手前の Reverse Proxy で https 終端をすると思うので nginx を入れます。WebSocket に対応する必要があるため nginx.conf を用意してマウントします。\nnginx.conf # user nginx; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; server { listen 80; root /usr/share/nginx/html; location /guacamole/ { proxy_pass http://guacamole:8080/guacamole/; proxy_buffering off; proxy_http_version 1.1; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $http_connection; #access_log off;  } } } Database の初期化 # Database にテーブルを作ったりする必要があります。次のようにして初期化用の SQL を生成します。\nsudo docker run --rm guacamole/guacamole:1.0.0 /opt/guacamole/bin/initdb.sh --postgres \u0026gt; init.sql 次にこの SQL を Database に適用します。\nsudo docker-compose up --no-start postgres sudo docker-compose start postgres cat init.sql | sudo docker-compose exec -T postgres psql -U guacamole -d guacamole 起動 # あとは docker-compose up で起動させれば OK\nsudo docker-compose up ブラウザで /guacamole/ にアクセスして次のようなログインフォームが表示されたらきっと成功。ID: guacadmin Password: guacadmin でログインしてみましょう。\nSSH # ログインできたら右上のメニューから settings に入ります。\nここの Connections から New Connection で接続情報を設定します。SSH では次の設定項目があります。\n Configuring Guacamole SSH\nNameと接続先のHostnameが最低限必要な設定ですが、これではパスワード認証となります。公開鍵認証にするためにはPrivate keyが必要です。Username、Password、Passphrase がブランクの場合は接続時に入力を求められます。ログインに必要な情報がすべて設定されていれば Guacamole にログインして表示される接続名をクリックするだけで何の入力もなしにログインできます。\nPrivate key も Password も Passphrase も plain text で guacamole_connection_parameter テーブルに保存されます。\n次のような OpenSSH の Private key の新しいフォーマットは未対応のようです。\n-----BEGIN OPENSSH PRIVATE KEY----- ... -----END OPENSSH PRIVATE KEY----- ブラウザでの SSH はどんなものかと思っていましたが、思っていた以上に違和感なく使えました。Bash での各種 Ctrl キーとの組み合わせも動作しますし。文字列を選択した状態でマウスの右クリックをすればペーストできます。\nRemote Desktop # Remote Desktop の場合は次の設定項目があります。\n Remote Desktop の場合は Name, Hostname だけではなく、Username, Password などログインするのに必要なものはすべて登録済みである必要があります。これも SSH と同じく guacamole_connection_parameter テーブルに plain text で保存されます。EC2 の Windows Server 2016 では Security Mode は Any にする必要がありました。(Lightsail の 512MB メモリの Windows は遅すぎてまともに使えなかった)\nこれがブラウザ内に表示される画面です。\n  Time-based One-time Password # TOTP を有効にするにはそれ用の jar ファイルを ${GUACAMOLE_HOME}/extensions ディレクトリに置いて起動するようにしてやる必要があります。docker での起動は /opt/guacamole/bin/start.sh が環境変数によって使う Database を判断して毎回 ${GUACAMOLE_HOME} を作り直しています。一旦ディレクトリごと削除した後に ${GUACAMOLE_HOME} で指定したディレクトリをコピーしてから Database 用の jar をコピーするので docker-compose.yml を次のようにして guacamole/extensions に TOTP 用 jar を置いておきます。\nguacamole:image:guacamole/guacamole:1.0.0env_file:./envenvironment:GUACD_HOSTNAME:guacdGUACD_PORT:4822POSTGRES_HOSTNAME:postgresGUACAMOLE_HOME:/guacamole-templatevolumes:- ./guacamole:/guacamole-template:rorestart:alwaysjar の入った tar.gz をダウンロードして展開する。\ncurl -Lo guacamole-auth-totp-1.0.0.tar.gz \\  \u0026#34;http://apache.org/dyn/closer.cgi?action=download\u0026amp;filename=guacamole/1.0.0/binary/guacamole-auth-totp-1.0.0.tar.gz\u0026#34; test -d guacamole/extensions || mkdir -p guacamole/extensions tar -xvf guacamole-auth-totp-1.0.0.tar.gz \\  --strip-components 1 \\  -C guacamole/extensions \\  guacamole-auth-totp-1.0.0/guacamole-auth-totp-1.0.0.jar この状態で docker-compose down \u0026amp; docker-compose up すれば TOTP が使えるようになります。初回、ID, Password を入力した後に登録画面が表示されます。\n 登録が完了すれば次回ログインからは ID, Password 入力後に次のように 6 桁のコード入力を求められます。\n TOTP は Database (MySQL, PostgreSQL, SQLServer) 認証でしかサポートされていないようです。OpenID の場合は OpenID サーバー側でやるべきでしょうしね。\n(Vagrant でやっていたのですが、途中 PC を Suspend していたため、その間、サーバーの時刻が止まっておりずれてしまって。Varidation が通らなくてしばらくハマってました 😅)\nLDAP 認証 # 次にLDAP認証を試します。環境変数で LDAP サーバーの指定をするだけで起動前に jar がコピーされて LDAP アカウントでロおグインできるようになります。https://guacamole.apache.org/doc/gug/ldap-auth.html\nguacamole:image:guacamole/guacamole:1.0.0env_file:./envenvironment:GUACD_HOSTNAME:guacdGUACD_PORT:4822POSTGRES_HOSTNAME:postgresLDAP_HOSTNAME:ldap.example.comLDAP_PORT:636LDAP_ENCRYPTION_METHOD:sslLDAP_USER_BASE_DN:ou=people,dc=example,dc=comrestart:alwaysLDAP がらみで使える環境変数には次のものがあります。\n LDAP_HOSTNAME LDAP_PORT LDAP_ENCRYPTION_METHOD LDAP_USER_BASE_DN LDAP_USERNAME_ATTRIBUTE LDAP_GROUP_BASE_DN LDAP_CONFIG_BASE_DN LDAP_SEARCH_BIND_DN LDAP_SEARCH_BIND_PASSWORD  Ad-hoc Connect # TOTP と同じように guacamole-auth-quickconnect-1.0.0.jar を extensions ディレクトリに置くことで Ad-hoc Connections が使えるようになります。Ad-hoc Connection は登録済みの接続設定を使うのではなく ssh://linux1.example.com/ などとその場で入力して接続できるようになります。\ncurl -Lo guacamole-auth-quickconnect-1.0.0.tar.gz \\ \u0026quot;http://apache.org/dyn/closer.cgi?action=download\u0026amp;filename=guacamole/1.0.0/binary/guacamole-auth-quickconnect-1.0.0.tar.gz\u0026quot; tar -xvf /tmp/guacamole-auth-quickconnect-1.0.0.tar.gz \\ --strip-components 1 \\ -C guacamole/extensions \\ guacamole-auth-quickconnect-1.0.0/guacamole-auth-quickconnect-1.0.0.jar Home 画面に次の用な入力フォームが表示されます。\n 認証情報が保存できないので実際に使える場面は少なそうですけど。\nセッション共有 # URL を共有することで SSH やリモートデスクトップのセッションを複数人で共有することができます。そしてこれは Guacamole にログインする必要がないため、組織外の人と共有できます。参照専用で共有することもできます。\n接続設定に対して New Sharing Profile で共有プロファイルを作成することができます。このキャプチャの ec2-win1 には readonly と share-test という2つの共有プロファイルを作成してあります。作成時に Read-only フラグにチェックを入れておくことで参照専用で共有することができるようになります。\n共有用のリンクを取得するには接続した状態で Ctrl+Alt+Shift で Guacamole menu を表示して、そこの上部にある share ボタンからプロファイルを選択します。\nSSH でも同様です\n表示されたリンクを使って共有セッションにアクセスすることができます。Guacamole menu は共有リンクの表示以外にオンスクリーンキーボードを表示させたり、マウスの挙動をタブレットやスマホ用に切り替えたりもできます。これは個人設定の Preference でも設定可能ですが、セッション中に変更するにはこのメニューを使います。Guacamole menu からホーム画面やセッティング画面に移動すればセッションは継続したまま別のセッションに移ったりできます。\n「今、こんな感じの画面になってて困ってます」って共有する時とかに便利そうですね。\nSharing the connection\nレコーディング # リモートデスクトップや VNC ではグラフィカルなレコーディングが可能です。SSH や Telnet ではそれに加えてテキストでのレコーティングも可能です。保存を行うのはサーバー側の guacd です。\nText session recording (typescripts) # SSH や Telnet の場合は Typescript path に保存先ディレクトリを指定すれば script コマンドでのログが保存されます。Typescript name が未設定の場合は typescript というファイル名で保存されます。Automatically create typescript path にチェックを入れておくと保存先ディレクトリが存在しない場合にディレクトリを作成してくれます。ただし、作成するのは最後のサブディレクトリだけで、その親ディレクトリは予め存在している必要があります。次のように2つ以降のセッションには末尾に連番の数字がつきます。\ntypescript typescript.1 typescript.1.timing typescript.2 typescript.2.timing typescript.timing 再生するには次のように scriptreplay コマンドを使います。\nscriptreplay typescript.timing typescript テキストなので grep したりエディタで開くこともできます。\nScreen Recording # グラフィカルなレコーディングもテキストと同様に Recording path に保存先ディレクトリを指定すればレコーディングされます。Recording name でファイル名を指定しなかった場合は recording というファイル名で保存されます。2つ目以降は末尾に連番の数字がつきます。guacenc コマンドを使うことで保存されたファイルから MP4 ファイルを生成することができます。Include key events を有効にしておけば guaclog コマンドで入力をテキストで書き出すことができます。\nただし、guacenc も guaclog も guacd コンテナに含まれていなかったので試していません。\nSFTP によるファイルのアップロード # SSH の場合は設定の Enable SFTP を有効にして File browser root directory にサーバー側のディレクトリを指定するとそのディレクトリ以下のディレクトリツリーを参照できるようになり、任意の場所にファイルをアップロードしたり、ダウンロードすることができるようになります。\n次のキャプチャのように Guacamole メニュー (Ctrl+Alt+Shift) に Device という項目が表示されます。この例ではディレクトリが未指定なので / で全部が見えるようになっています。\n Device を選択するとファイルブラウザの用なものが表示されるので、ここからたどってファイルをダウンロードしたり、ディレクトリにファイルをアップロードします。\n  リモートデスクトップと VNC にも SFTP 設定はありますが、別途 SFTP サーバーや認証情報を設定する必要があります。リモデ先が Windows であればそこで SFTP サーバーを起動させるか、その Windows からアクセス可能なファイルサーバーに SFTP でアクセスするとかいった使い方になるようです。\nリモートデスクトップでのドライブマウント # Windows ならクリップボードからファイルもコピーできるんだろうか？とも思ったけど、文字列のコピペはできますがファイルはできないようです。リモートデスクトップの設定には Device Redirection というセクションがあり、この中に Enable drive、Drive name、Drive path、Automatically create drive という項目があり、Guacamole サーバーの指定のディレクトリをリモデ先がマウントしてアクセスすることができるようになります。\n次のように設定してみます\n すると、リモデ先では次のようにドライブとしてマウントされています\n このドライブ内に Download という作成した覚えのないフォルダが存在するのですが、この Download フォルダにファイルを置くとブラウザでダウンロードする形でリモデ先からファイルをコピーすることができるのです。ドライブ内の他の場所にファイルを置くとそれは guacd を実行しているサーバー（コンテナ）内の指定のディレクトリ配下にコピーされます\n接続設定のグルーピングと権限設定 # このキャプチャはセッション共有の節で使ったものですが、接続情報が Banana と Orange というフォルダに分かれています。\n このようにグループ分けすることでグループ単位で人やグループにその接続の使用を許可するかどうかの設定が可能です。個別の接続単位でももちろん制御可能です。ユーザーごとに新たな設定を追加可能にするかどうかなどを設定できます。\n このキャプチャは接続設定作成前のものですが、作成済みの場合は一番下に接続設定のツリーが表示され、どれを使用可能とするかを選択できます。ユーザーグループの方でも制御可能です。\nその他 # OpenID 認証や VNC 接続など、ここで試さなかった機能が他にもあります\nい～～～～っじょうぅ！\n","date":"2019年1月13日","permalink":"/2019/01/guacamole-functions/","section":"Posts","summary":"kakakakakku さんのブログの Guacamole 記事を見て、私もやったのになあとこんな tweet をしてしまいました。 Guacamole を docker compose でって、去年やったわー、でもブログに書かずじまいだった","title":"Guacamole の機能紹介"},{"content":"","date":"2019年1月13日","permalink":"/tags/remotedesktop/","section":"Tags","summary":"","title":"remotedesktop"},{"content":"","date":"2019年1月5日","permalink":"/tags/kubeadm/","section":"Tags","summary":"","title":"kubeadm"},{"content":"kubeadm を使って HA な Kubernetes を構築するというのを 1.11 の時に試して Ansible Playbook にしたりしていましたが、kubeadm が GA になったことだし 1.13 での手順に更新してみた(github.com/yteraoka/do-k8s-stacked-etcd)。etcd 同居タイプです。\n CentOS 7 に kubeadm をインストール kubeadm で外部 etcd で HA な Kubernetes クラスタをセットアップする etcd stacked Highly Available Kubernetes cluster を Ansible でセットアップ 外部 etcd な Kubernetes を DigitalOcean に構築する Ansible Playbook  Production-Ready Kubernetes Cluster Creation with kubeadm\nサーバーの準備 # いつもながら DigitalOcean に Control Plane 用サーバー3台と Worker 用サーバー1台を作ります。\n CentOS 7 のサーバー4台 API サーバー用の Load Balancer と DNS レコード  terraform で作れるようになっているので terraform plan と terraform apply するだけです。\nSSH 用の公開鍵は事前に登録されている前提で、ssh_key_ids 変数で指定します。DNS のゾーンも事前に作成されてる前提で、domain_suffix 変数で指定します。\nAnsible Playbook 適用 # Ansible は 2.7 以降で、DigitalOcean の Dynamic Inventory を使うために Python の requests module が必要でした。\nkubeadm init で使う config ファイルは etcd 用設定が不要になってました\nansible-playbook site.yml -e load_balancer_dns=k8s-api.example.com うまくいけばこれで 3 台の Control Plane サーバーがクラスタ化されます。(なぜか2台目以降の kubeadm join 時に etcd の起動に失敗することがある。原因を調べきれていない)\n# kubectl get nodes NAME STATUS ROLES AGE VERSION cp1 Ready master 3h42m v1.13.1 cp2 Ready master 3h38m v1.13.1 cp3 Ready master 3h37m v1.13.1 Worker node をクラスタに追加する # 現状はこの後に worker ノードの join 処理を手動で行う必要がある、当該サーバー上で次の様なコマンドを実行することで join させることができる\nkubeadm join k8s-api.example.com:443 --token e69t47.k98pkcidzvgexwbz \\ --discovery-token-ca-cert-hash \\ sha256:b0e28afb25529ad1405d6adecd4a154ace51b6245ff59477f5ea465e221936de --token に渡すのは Control Plane のいずれかのホストで kubeadm token list することで確認できる。--discovery-token-ca-cert-hash に渡す hash は /etc/kubernetes/pki/ca.crt のあるホスト (Control Plane にはある) で次のコマンドを実行することで得られる。\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt \\ | openssl rsa -pubin -outform der 2\u0026gt;/dev/null \\ | openssl dgst -sha256 -hex | sed 's/^.* //' 正常に追加できたら kubectl get nodes で確認します。しばらくすると status が Ready になるはず\n# kubectl get nodes NAME STATUS ROLES AGE VERSION cp1 Ready master 3h42m v1.13.1 cp2 Ready master 3h38m v1.13.1 cp3 Ready master 3h37m v1.13.1 worker-0 Ready 3h30m v1.13.1 Kubernetes Dashboard のセットアップ # Dashboard を deploy するには次のコマンドを実行するだけ\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 上記、recommended の YAML で deploy するとアクセスするためにユーザーを作成して token を入力します。そのためのユーザー作成には次の YAML を admin-user.yaml として保存し\n---apiVersion:v1kind:ServiceAccountmetadata:name:admin-usernamespace:kube-system---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:admin-userroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- kind:ServiceAccountname:admin-usernamespace:kube-systemkubectl apply -f admin-user.yaml します。次に token を確認します。\nkubectl -n kube-system describe secret \\ $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') これは admin-user を含む secret を grep で取り出していますが、正確な名前を得るには次のようにします。admin-user は先ほど作成した ServiceAccount の名前です\nkubectl get serviceaccounts admin-user -o json -n kube-system | jq -r .secrets[].name Dashboard にクラスタの外からアクセスするためには Ingress を作成して外部へ公開するか kube proxy を使って、クラスタ内へアクセスします。手元の PC からアクセスするために Control Plane のいずれかから /root/.kube/config を ~/.kube/config にコピーして kubectl proxy と実行すれば proxy が起動します\n$ kubectl proxy Starting to serve on 127.0.0.1:8001 これで 127.0.0.1:8001 経由で Dashboard にアクセスできます。http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/\nこの画面でトークンを選択して先ほど確認したトークン文字列を入力します\nこんな画面が確認できます\n課題山積 # https://kubernetes.io/docs/setup/independent/high-availability/ の手順では Weave が使われているが、前回は Calico で試したので今回もと思ってやってみたがうまく動作しなかった、辛い。しかし、どれを選ぶのが正解なのだか\u0026hellip;\nkubeadm join 時に etcd が起動しないことがある問題\nトラブったほうが調査で理解が深まるのだけれどなかなか大変\u0026hellip;\nSelf-hosted っていう話もあったなあ\nひとまず Kubernetes完全ガイド を読むことにします\n","date":"2019年1月5日","permalink":"/2019/01/creating-highly-available-1-13-clusters-with-kubeadm/","section":"Posts","summary":"kubeadm を使って HA な Kubernetes を構築するというのを 1.11 の時に試して Ansible Playbook にしたりしていましたが、kubeadm が GA になったことだし 1.13 での手順に更新してみた(g","title":"kubeadm で kubernetes 1.13 の HA 環境を構築"},{"content":"","date":"2018年12月30日","permalink":"/tags/slony/","section":"Tags","summary":"","title":"slony"},{"content":"Slony-I 2.2.7 のドキュメントからの情報です\n概要の確認とチュートリアルを試してみます。\nSlony-I とは # Slony-I は PostgreSQL のレプリケーションシステムで、複数レプリカの作成、カスケードレプリケーション、プロモーションをサポートしており、次の特徴を持つ\n 異なるメジャーバージョン間でもレプリケーション可能 異なるハードウェアアーキテクチャ、OSの間でもレプリケーション可能 一部のテーブルだけをレプリケーションすることが可能 あるテーブルをレプリカ(A)に、別のテーブルをレプリカ(B)にレプリケーションといったことが可能 テーブル毎にレプリカ元のデータベースサーバーが異なっていてもレプリケーション可能  PostgreSQL は version 10 から logical replication に対応し、今後のバージョンアップにはこれが使えますが、旧バージョンから 10 に上げるには当然ながら使えません(9.4 から logical decoding が使えるようになっているので追加の何かで logical replication できるような気もします)。ということで旧バージョンからの更新を短いダウンタイムで行うための手段として Slony-I は候補となります。\n Slony-I (トリガーによる行単位レプリケーションツール) - SRA OSS, Inc. 日本支社 第3回「ロジカルレプリケーション」 | NTTデータ先端技術株式会社  System Requirements #  PostgreSQL 8.3 以降 (8.3.x, 8.4.x , 9.0.x, 9.1.x, 9.2.x, 9.3.x,9.4.x, 9.5.x での動作が確認されている)。これより前のバージョンでは Slony-I 1.2.x を使う必要がある。2.2.6 の release note に Support for PG10 とあるので 10.x にも対応しているはず  以降は推奨\n NTP などで時刻を同期すること、UTC や GMT といった安定したタイムゾーンを使うこと（夏時間のないタイムゾーン、PostgreSQL が認識できるタイムゾーンが好ましい） 信頼性の高いネットワーク（WAN 越しにレプリケーションする場合、それぞれの slon プロセスはそれぞれのローカルネットワーク内で実行するべし） データベースのエンコーディングは揃えるべし  Slony-I Concepts # セットアップするためには次の概念を理解する必要がある\n Cluster Node Replication Set Origin, Providers and Subscribers slon daemons slonik configuration processor  ロシア語の意味も理解しておくと良い\n slon は象 🐘 slony は象の複数形 🐘🐘🐘 slonik は小さな象  Cluster # Cluster はレプリケーションを組む PostgreSQL インスタンスの集合で、各 Slonik スクリプトで次のように定義する\ncluster name = something; Cluster 名が something だった場合、それぞれのデータベースに _something という schema が作成される\nNode # レプリケーションを構成するうちのひとつひとつのデータベースを Node と呼び、各 Slonik スクリプトの冒頭で次のように定義される\nNODE 1 ADMIN CONNINFO = 'dbname=testdb host=server1 user=slony'; この ADMIN CONNINFO は libpq の PQconnectdb() 関数に渡される\nReplication Set # Node 間でレプリケーションされるテーブルとシーケンスのセット\nOrigin, Providers and Subscribers # 各 Replication Set には Origin node があり、それはアプリケーションによるレコードの変更が唯一許されている場所であり、Master provider とも呼ばれる。Replication Set の他の node は Subscriber となります。ただし、Slony-I はカスケードレプリケーションをサポートしているため、Subscriber が別の Replication set の Origin である可能性もあります\nslon daemons # Cluster 内の各 Node ではレプリケーションイベントを処理する slon プロセスが稼働している。C 言語で書かれており、処理する主な2つのイベントは次の通り\n Configuration events\nSlonik スクリプトが実行された場合に発生し、クラスタ構成の変更が送られる SYNC events\nレプリケーションされたテーブルへの変更が SYNC にまとめられて Subscriber に送られ、適用される  slonik configuration processor # Slonik コマンドは小さな言語となっているスクリプトを実行してクラスタの設定変更イベントを送る。このイベントには Node の追加や削除、通信 path の変更、Subscriber の追加、削除が含まれる\nCurrent Limitations # Slony-I は次の変更を自動でレプリケートしない\n Large objects (BLOBS) DDL Uses and Roles  Slony-I は trigger によって変更を捉えているため、これらの変更を捉えることができませんが、SLONIK EXECUTE SCRIPT を使うことで DDL を各 Node で実行することができます\nTutorial # Replicating Your First Database # pgbench を使ってレプリケーション設定を試します。PostgreSQL 9.3 から PostgreSQL 9.6 に同期させてみます。CentOS 7 のサーバー2台 (pg1, pg2) に PGDG リポジトリから PostgreSQL と Slony-I をインストールします。簡略化のために pg_hba.conf の認証設定は trust で。\npg1 # sudo yum -y install yum install https://download.postgresql.org/pub/repos/yum/9.3/redhat/rhel-7-x86_64/pgdg-centos93-9.3-3.noarch.rpm sudo yum -y install perl sudo yum -y install postgresql93 postgresql93-contrib slony1-93 sudo /usr/pgsql-9.3/bin/postgresql93-setup sudo /usr/pgsql-9.3/bin/postgresql93-setup initdb sudoedit /var/lib/pgsql/9.3/data/pg_hba.conf sudo systemctl enable postgresql-9.3 sudo systemctl start postgresql-9.3 pg2 # sudo yum -y install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpm sudo yum -y install perl sudo yum -y install postgresql96 postgresql96-contrib slony1-96 sudo /usr/pgsql-9.6/bin/postgresql96-setup initdb sudoedit /var/lib/pgsql/9.6/data/pg_hba.conf sudo systemctl enable postgresql-9.6 sudo systemctl start postgresql-9.6 今後の手順で使う環境変数を次のように設定します\nexport CLUSTERNAME=slony_example export MASTERDBNAME=pgbench export SLAVEDBNAME=pgbench export MASTERHOST=pg1 export SLAVEHOST=pg2 export REPLICATIONUSER=postgres export PGBENCHUSER=pgbench Creating the pgbench User # pgbench 用ユーザーの作成\nsudo -iu postgres createuser -SRD $PGBENCHUSER Preparing the Databases # pgbench 用のデータベースを作成し、1度 pgbench を実行してテーブルを作成します\nsudo -iu postgres createdb -O $PGBENCHUSER $MASTERDBNAME /usr/pgsql-9.3/bin/pgbench -i -s 1 -U $PGBENCHUSER $MASTERDBNAME pgbench=# \\d+ List of relations Schema | Name | Type | Owner | Size | Description --------+------------------+-------+---------+---------+------------- public | pgbench_accounts | table | pgbench | 13 MB | public | pgbench_branches | table | pgbench | 40 kB | public | pgbench_history | table | pgbench | 0 bytes | public | pgbench_tellers | table | pgbench | 40 kB | (4 rows) pgbench=# \\d pgbench_accounts Table \u0026quot;public.pgbench_accounts\u0026quot; Column | Type | Modifiers ----------+---------------+----------- aid | integer | not null bid | integer | abalance | integer | filler | character(84) | Indexes: \u0026quot;pgbench_accounts_pkey\u0026quot; PRIMARY KEY, btree (aid) pgbench=# \\d pgbench_branches Table \u0026quot;public.pgbench_branches\u0026quot; Column | Type | Modifiers ----------+---------------+----------- bid | integer | not null bbalance | integer | filler | character(88) | Indexes: \u0026quot;pgbench_branches_pkey\u0026quot; PRIMARY KEY, btree (bid) pgbench=# \\d pgbench_tellers Table \u0026quot;public.pgbench_tellers\u0026quot; Column | Type | Modifiers ----------+---------------+----------- tid | integer | not null bid | integer | tbalance | integer | filler | character(84) | Indexes: \u0026quot;pgbench_tellers_pkey\u0026quot; PRIMARY KEY, btree (tid) pgbench=# pgbench_history テーブルには PRIMARY KEY が存在しないが、Slony は PRIMARY KEY またはそれ相当の INDEX を必要とするため PRIMARY KEY を追加する。\npgbench=# \\d pgbench_history Table \u0026quot;public.pgbench_history\u0026quot; Column | Type | Modifiers --------+-----------------------------+----------- tid | integer | bid | integer | aid | integer | delta | integer | mtime | timestamp without time zone | filler | character(22) | pgbench=# psql -U $PGBENCHUSER -d $MASTERDBNAME -c \u0026#34;ALTER TABLE pgbench_history ADD COLUMN id serial\u0026#34; psql -U $PGBENCHUSER -d $MASTERDBNAME -c \u0026#34;ALTER TABLE pgbench_history ADD PRIMARY KEY(id)\u0026#34; Slony は PL/PGSQL を使うので createlang で作成する\nsudo -iu postgres createlang plpgsql $MASTERDBNAME pg2 に DB を作成する\nsudo -iu postgres createdb -O $PGBENCHUSER $SLAVEDBNAME pg1 から pg2 へスキーマをコピーする\npg_dump -s -U $REPLICATIONUSER -h $MASTERHOST $MASTERDBNAME \\  | psql -U $REPLICATIONUSER -h $SLAVEHOST $SLAVEDBNAME Configuring the Database For Replication # 設定テーブル、ストアド・プロシージャ、トリガーの作成と設定はすべて slonik コマンドをを使って行います\nUsing slonik Command Directly # slonik コマンドへの入力を自分で用意する方法です。簡略化およびミスを減らすために後述の別のツールを使う方法もあります\nsudo /usr/pgsql-9.3/bin/slonik \u0026lt;\u0026lt;_EOF_ #-- # レプリケーションシステムのネームスペースを定義する # この例では slony_example とする #-- cluster name = $CLUSTERNAME; #-- # admin conninfo で slonik が DB にログインするための情報を定義する # 構文は C-API の PQconnectdb に渡すもの # -- node 1 admin conninfo = \u0026#39;dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER\u0026#39;; node 2 admin conninfo = \u0026#39;dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER\u0026#39;; #-- # 最初のノードを初期化する # これは _$CLUSTERNAME スキーマを作成する #-- init cluster ( id=1, comment = \u0026#39;Master Node\u0026#39;); #-- # Slony-I はレプリケーションテーブルを set として定義する # 次のコマンドは 4 つの pgbench 用テーブルを set (id = 1) としている # master (origin) は node 1 #-- create set (id=1, origin=1, comment=\u0026#39;All pgbench tables\u0026#39;); set add table (set id=1, origin=1, id=1, fully qualified name = \u0026#39;public.pgbench_accounts\u0026#39;, comment=\u0026#39;accounts table\u0026#39;); set add table (set id=1, origin=1, id=2, fully qualified name = \u0026#39;public.pgbench_branches\u0026#39;, comment=\u0026#39;branches table\u0026#39;); set add table (set id=1, origin=1, id=3, fully qualified name = \u0026#39;public.pgbench_tellers\u0026#39;, comment=\u0026#39;tellers table\u0026#39;); set add table (set id=1, origin=1, id=4, fully qualified name = \u0026#39;public.pgbench_history\u0026#39;, comment=\u0026#39;history table\u0026#39;); #-- # 2 番目の node (id = 2) を定義して node 間で接続するための情報 (path) を定義する #-- store node (id=2, comment = \u0026#39;Slave node\u0026#39;, event node=1); store path (server = 1, client = 2, conninfo=\u0026#39;dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER\u0026#39;); store path (server = 2, client = 1, conninfo=\u0026#39;dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER\u0026#39;); _EOF_ これを実行すると pg1, pg2 の pgbench データベースの _slony_example スキーマに沢山の slony 用テーブルが作成され、レプリケーション対象テーブルに trigger が作成されています。\npgbench=# set search_path to public,_slony_example; SET pgbench=# \\d List of relations Schema | Name | Type | Owner ----------------+----------------------------+----------+---------- _slony_example | sl_action_seq | sequence | postgres _slony_example | sl_apply_stats | table | postgres _slony_example | sl_archive_counter | table | postgres _slony_example | sl_components | table | postgres _slony_example | sl_config_lock | table | postgres _slony_example | sl_confirm | table | postgres _slony_example | sl_event | table | postgres _slony_example | sl_event_lock | table | postgres _slony_example | sl_event_seq | sequence | postgres _slony_example | sl_failover_targets | view | postgres _slony_example | sl_listen | table | postgres _slony_example | sl_local_node_id | sequence | postgres _slony_example | sl_log_1 | table | postgres _slony_example | sl_log_2 | table | postgres _slony_example | sl_log_script | table | postgres _slony_example | sl_log_status | sequence | postgres _slony_example | sl_node | table | postgres _slony_example | sl_nodelock | table | postgres _slony_example | sl_nodelock_nl_conncnt_seq | sequence | postgres _slony_example | sl_path | table | postgres _slony_example | sl_registry | table | postgres _slony_example | sl_seqlastvalue | view | postgres _slony_example | sl_seqlog | table | postgres _slony_example | sl_sequence | table | postgres _slony_example | sl_set | table | postgres _slony_example | sl_setsync | table | postgres _slony_example | sl_status | view | postgres _slony_example | sl_subscribe | table | postgres _slony_example | sl_table | table | postgres public | pgbench_accounts | table | pgbench public | pgbench_branches | table | pgbench public | pgbench_history | table | pgbench public | pgbench_history_id_seq | sequence | pgbench public | pgbench_tellers | table | pgbench (34 rows) pgbench=# \\d pgbench_accounts Table \u0026quot;public.pgbench_accounts\u0026quot; Column | Type | Modifiers ----------+---------------+----------- aid | integer | not null bid | integer | abalance | integer | filler | character(84) | Indexes: \u0026quot;pgbench_accounts_pkey\u0026quot; PRIMARY KEY, btree (aid) Triggers: _slony_example_logtrigger AFTER INSERT OR DELETE OR UPDATE ON pgbench_accounts FOR EACH ROW EXECUTE PROCEDURE _slony_example.logtrigger('_slony_example', '1', 'k') _slony_example_truncatetrigger BEFORE TRUNCATE ON pgbench_accounts FOR EACH STATEMENT EXECUTE PROCEDURE _slony_example.log_truncate('1') Disabled triggers: _slony_example_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON pgbench_accounts FOR EACH ROW EXECUTE PROCEDURE _slony_example.denyaccess('_slony_example') _slony_example_truncatedeny BEFORE TRUNCATE ON pgbench_accounts FOR EACH STATEMENT EXECUTE PROCEDURE _slony_example.deny_truncate() pgbench=# \\d pgbench_branches Table \u0026quot;public.pgbench_branches\u0026quot; Column | Type | Modifiers ----------+---------------+----------- bid | integer | not null bbalance | integer | filler | character(88) | Indexes: \u0026quot;pgbench_branches_pkey\u0026quot; PRIMARY KEY, btree (bid) Triggers: _slony_example_logtrigger AFTER INSERT OR DELETE OR UPDATE ON pgbench_branches FOR EACH ROW EXECUTE PROCEDURE _slony_example.logtrigger('_slony_example', '2', 'k') _slony_example_truncatetrigger BEFORE TRUNCATE ON pgbench_branches FOR EACH STATEMENT EXECUTE PROCEDURE _slony_example.log_truncate('2') Disabled triggers: _slony_example_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON pgbench_branches FOR EACH ROW EXECUTE PROCEDURE _slony_example.denyaccess('_slony_example') _slony_example_truncatedeny BEFORE TRUNCATE ON pgbench_branches FOR EACH STATEMENT EXECUTE PROCEDURE _slony_example.deny_truncate() pgbench=# \\d pgbench_history Table \u0026quot;public.pgbench_history\u0026quot; Column | Type | Modifiers --------+-----------------------------+-------------------------------------------------------------- tid | integer | bid | integer | aid | integer | delta | integer | mtime | timestamp without time zone | filler | character(22) | id | integer | not null default nextval('pgbench_history_id_seq'::regclass) Indexes: \u0026quot;pgbench_history_pkey\u0026quot; PRIMARY KEY, btree (id) Triggers: _slony_example_logtrigger AFTER INSERT OR DELETE OR UPDATE ON pgbench_history FOR EACH ROW EXECUTE PROCEDURE _slony_example.logtrigger('_slony_example', '4', 'vvvvvvk') _slony_example_truncatetrigger BEFORE TRUNCATE ON pgbench_history FOR EACH STATEMENT EXECUTE PROCEDURE _slony_example.log_truncate('4') Disabled triggers: _slony_example_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON pgbench_history FOR EACH ROW EXECUTE PROCEDURE _slony_example.denyaccess('_slony_example') _slony_example_truncatedeny BEFORE TRUNCATE ON pgbench_history FOR EACH STATEMENT EXECUTE PROCEDURE _slony_example.deny_truncate() pgbench=# \\d pgbench_tellers Table \u0026quot;public.pgbench_tellers\u0026quot; Column | Type | Modifiers ----------+---------------+----------- tid | integer | not null bid | integer | tbalance | integer | filler | character(84) | Indexes: \u0026quot;pgbench_tellers_pkey\u0026quot; PRIMARY KEY, btree (tid) Triggers: _slony_example_logtrigger AFTER INSERT OR DELETE OR UPDATE ON pgbench_tellers FOR EACH ROW EXECUTE PROCEDURE _slony_example.logtrigger('_slony_example', '3', 'k') _slony_example_truncatetrigger BEFORE TRUNCATE ON pgbench_tellers FOR EACH STATEMENT EXECUTE PROCEDURE _slony_example.log_truncate('3') Disabled triggers: _slony_example_denyaccess BEFORE INSERT OR DELETE OR UPDATE ON pgbench_tellers FOR EACH ROW EXECUTE PROCEDURE _slony_example.denyaccess('_slony_example') _slony_example_truncatedeny BEFORE TRUNCATE ON pgbench_tellers FOR EACH STATEMENT EXECUTE PROCEDURE _slony_example.deny_truncate() pgbench=# pg1 (origin) で slon を起動\n/usr/pgsql-9.3/bin/slon $CLUSTERNAME \u0026#34;dbname=$MASTERDBNAMEuser=$REPLICATIONUSERhost=$MASTERHOST\u0026#34; pg2 (subscriber) で slon を起動\n/usr/pgsql-9.6/bin/slon $CLUSTERNAME \u0026#34;dbname=$SLAVEDBNAMEuser=$REPLICATIONUSERhost=$SLAVEHOST\u0026#34; pg2 で pg_stat_activity を見ると次のようなプロセスからの接続がありました\nslon.local_cleanup slon.local_listen slon.local_monitor slon.local_sync slon.node_2_listen slon.origin_2_provider_2 slon.remoteWorkerThread_1 slon プロセスがなにやら SYNC してそうな出力をしますが、ここまでではまだ同期が始まっていません\nここで再度 pgbench を実行してみます。今度は -T 300 で5分間実行されるようにしています。これの実行中に次の subscribe 設定を行うことで、更新中の同期開始を試みます\n/usr/pgsql-9.3/bin/pgbench -s 1 -c 5 -T 300 -U $PGBENCHUSER -h $MASTERHOST $MASTERDBNAME 次のようにして slonik で subscribe 指示を出します\nsudo /usr/pgsql-9.3/bin/slonik \u0026lt;\u0026lt;_EOF_ # ---- # This defines which namespace the replication system uses # ---- cluster name = $CLUSTERNAME; # ---- # 各 node への接続情報 # ---- node 1 admin conninfo = \u0026#39;dbname=$MASTERDBNAME host=$MASTERHOST user=$REPLICATIONUSER\u0026#39;; node 2 admin conninfo = \u0026#39;dbname=$SLAVEDBNAME host=$SLAVEHOST user=$REPLICATIONUSER\u0026#39;; # ---- # Node 2 subscribes set 1 # Subscriber が cascade や failover で provider になるのであれば forward は yes とします # ---- subscribe set ( id = 1, provider = 1, receiver = 2, forward = no); _EOF_ これによって origin テーブルのレコードをコピーし、完了後、コピー開始時からの変更を反映していきます\nUsing the altperl Scripts # cluster name や admin conninfo など slonik のへの入力を毎度生成するのは大変なので /etc/slony1-93/slon_tools.conf の設定を元に生成してくれるツールがあります。 次のようにして pipe で slonik に渡すことで初期化や起動、Subscribe などの指示を出せます。\n# Initialize cluster: $ slonik_init_cluster | slonik # Start slon (here 1 and 2 are node numbers) $ slon_start 1 $ slon_start 2 # Create Sets (here 1 is a set number) $ slonik_create_set 1 | slonik # subscribe set to second node (1= set ID, 2= node ID) $ slonik_subscribe_set 1 2 | slonik テーブル / レコードの比較 # 正しく同期できているかどうか、次のスクリプトで確認することができます。order by つきでそれぞれのデータベースからレコードを取得して diff で比較しています\n#!/bin/sh echo -n \u0026#34;**** comparing sample1 ... \u0026#34; psql -U $REPLICATIONUSER -h $MASTERHOST $MASTERDBNAME \u0026gt;dump.tmp.1.$$ \u0026lt;\u0026lt;_EOF_ select \u0026#39;accounts:\u0026#39;::text, aid, bid, abalance, filler from pgbench_accounts order by aid; select \u0026#39;branches:\u0026#39;::text, bid, bbalance, filler from pgbench_branches order by bid; select \u0026#39;tellers:\u0026#39;::text, tid, bid, tbalance, filler from pgbench_tellers order by tid; select \u0026#39;history:\u0026#39;::text, tid, bid, aid, delta, mtime, filler, id from pgbench_history order by id; _EOF_ psql -U $REPLICATIONUSER -h $SLAVEHOST $SLAVEDBNAME \u0026gt;dump.tmp.2.$$ \u0026lt;\u0026lt;_EOF_ select \u0026#39;accounts:\u0026#39;::text, aid, bid, abalance, filler from pgbench_accounts order by aid; select \u0026#39;branches:\u0026#39;::text, bid, bbalance, filler from pgbench_branches order by bid; select \u0026#39;tellers:\u0026#39;::text, tid, bid, tbalance, filler from pgbench_tellers order by tid; select \u0026#39;history:\u0026#39;::text, tid, bid, aid, delta, mtime, filler, id from pgbench_history order by id; _EOF_ if diff dump.tmp.1.$$ dump.tmp.2.$$ \u0026gt;$CLUSTERNAME.diff ; then echo \u0026#34;success - databases are equal.\u0026#34; rm dump.tmp.?.$$ rm $CLUSTERNAME.diff else echo \u0026#34;FAILED - see $CLUSTERNAME.diff for database differences\u0026#34; fi Conclusion # リアルワールドでは pgbench の用な単純な構成ではないため、実際にこれを使ってデータベースのアップグレードを行うにはより詳しく調査する必要がありますが、なんとなく概要がわかりました\n","date":"2018年12月30日","permalink":"/2018/12/what-is-slony-i/","section":"Posts","summary":"Slony-I 2.2.7 のドキュメントからの情報です 概要の確認とチュートリアルを試してみます。 Slony-I とは # Slony-I は PostgreSQL のレプリケーションシステムで、複数レプリカの作成、カ","title":"Slony-I の調査"},{"content":"IDCF Cloud が個人向けサービスを終了する # IDCF Cloud の個人向けサービスが終了することとなりました。これまで IDCF Cloud の500円/月サーバーで WordPress を運用していたため、他のクラウドサービスなどへ移す必要がでてきました。まあ、みんな500円インスタンスだけつかって儲からないとか、脆弱性放置で乗っ取られるとかありそうだからね。\n引越し先選定 # 時代はコンテナだし、Kubernetes の勉強がてら出たばかりの DigitalOcean の Managed Kubernetes を使って構築しようかとも思ったのだけれど、最小インスタンス1つだけで $10/月 かかり、Load Balancer でさらに $10/月、加えて Block storage も必要になります。これはちょっと高すぎだなあということで、AWS Lightsail の $3.5/月インスタンスを使うことにしました。CloudFront は以前から導入済みでした。DigitalOcean の Kubernetes は日本から一番近いシンガポールリージョンではまだ提供されていない（まだ Beta だしね）という理由もあります。\n Lightsail には他の VPC サービスの様な（あ、EC2 にもありますね）アプリインストール済みイメージも選択可能なのでここから WordPress を選択します。Version が 4.9.8 となっていますが、起動後に更新可能です。\n Lightsail の WordPress は Bitnami のパッケージが使われています。(Get Started With Bitnami Applications Using Amazon Lightsail)\nデータ移行 # 移行元の WordPress で Tools → Export で All content を選択して Export し、新しい WordPress で Tools → Import で WordPress Importer をインストールして取り込むだけです。画像も直接旧サイトから取り込んでくれます。 ということなのでインポートをする時点では DNS や CloudFront の Origin サイトを切り替えたりしてはいけませんん。\nBitnami の WordPress を使うにあたっていじったところのメモ # mod_expires 設定 # /opt/bitnami/apache2/conf/expires.conf を次の内容で作成。\n\u0026lt;IfModule mod_expires.c\u0026gt; ExpiresActive On ExpiresByType image/jpg \u0026#34;access 1 year\u0026#34; ExpiresByType image/jpeg \u0026#34;access 1 year\u0026#34; ExpiresByType image/gif \u0026#34;access 1 year\u0026#34; ExpiresByType image/png \u0026#34;access 1 year\u0026#34; ExpiresByType text/css \u0026#34;access 1 month\u0026#34; ExpiresByType application/pdf \u0026#34;access 1 month\u0026#34; ExpiresByType text/x-javascript \u0026#34;access 1 month\u0026#34; ExpiresByType application/x-shockwave-flash \u0026#34;access 1 month\u0026#34; ExpiresByType image/x-icon \u0026#34;access 1 year\u0026#34; ExpiresDefault \u0026#34;access 2 days\u0026#34; \u0026lt;/IfModule\u0026gt; /opt/bitnami/apache2/conf/httpd.conf でコメントアウトされている行をアンコメントします。\n#LoadModule expires_module modules/mod_expires.so ↓ LoadModule expires_module modules/mod_expires.so 先程作成した expires.conf を Include する。\nInclude conf/expires.conf を /opt/bitnami/apache2/conf/httpd.conf の末尾に追加。 Apache の再起動\nsudo /opt/bitnami/ctlscript.sh restart apache mod_pagespeed の無効化 # mod_expires 設定を行っても mod_pagespeed が\nCache-Control: max-age=0, no-cache, s-maxage=10 をセットしてしまうので無効にする。 /opt/bitnami/apache2/conf/httpd.conf の当該行をコメントアウト。\nInclude conf/pagespeed.conf Include conf/pagespeed_libraries.conf ↓ #Include conf/pagespeed.conf #Include conf/pagespeed_libraries.conf Apache の再起動\nsudo /opt/bitnami/ctlscript.sh restart apache php.ini の変更 # /opt/bitnami/php/etc/php.ini にあるので変更の必要があればここを書き換えて wordpress の再起動\n/opt/bitnami/ctlscript.sh restart php-fpm Response Header に PHP の Version を入れたくない場合は expose_php を Off にする。\nexpose_php = Off https 対応 # /opt/bitnami/apps/wordpress/htdocs/wp-config.php の WP_SITEURL, WP_HOME を https に変更\ndefine(\u0026#39;WP_SITEURL\u0026#39;, \u0026#39;http://\u0026#39; . $_SERVER[\u0026#39;HTTP_HOST\u0026#39;] . \u0026#39;/\u0026#39;); define(\u0026#39;WP_HOME\u0026#39;, \u0026#39;http://\u0026#39; . $_SERVER[\u0026#39;HTTP_HOST\u0026#39;] . \u0026#39;/\u0026#39;); ↓ define(\u0026#39;WP_SITEURL\u0026#39;, \u0026#39;https://\u0026#39; . $_SERVER[\u0026#39;HTTP_HOST\u0026#39;] . \u0026#39;/\u0026#39;); define(\u0026#39;WP_HOME\u0026#39;, \u0026#39;https://\u0026#39; . $_SERVER[\u0026#39;HTTP_HOST\u0026#39;] . \u0026#39;/\u0026#39;); この行の下に CloudFront 版の X-Forwarded-Proto 対応設定を挿入\nif (strpos($_SERVER[\u0026#39;HTTP_CLOUDFRONT_FORWARDED_PROTO\u0026#39;], \u0026#39;https\u0026#39;) !== false) $_SERVER[\u0026#39;HTTPS\u0026#39;]=\u0026#39;on\u0026#39;; WordPress の再起動\n/opt/bitnami/ctlscript.sh restart php-fpm CloudFront を使って困ること # WordPress の preview は通常の URL に QUERY STRING がついているだけなので、投稿のプレビューができない or キャッシュされて機能しないという状況に陥ります。 URL の path にプレビュー用の prefix を入れるプラグインとかないかな？\nhttps://gist.github.com/wokamoto/ecfd3a7ea9ef80ea1628 というのは見つけたけど。\n現状は投稿する PC では hosts でサーバーに直接アクセスするようにして回避しています。\nサーバー直でも https でアクセスできるようにする # サーバーに直接アクセスするために Apache にも TLS の証明書をセットする必要があります。/opt/bitnami/apache2/conf/bitnami/bitnami.conf に\nSSLCertificateFile \u0026#34;/opt/bitnami/apache2/conf/server.crt\u0026#34; SSLCertificateKeyFile \u0026#34;/opt/bitnami/apache2/conf/server.key\u0026#34; とあるのでここに Let\u0026rsquo;s Encrypt で取得した証明書と鍵をコピペして apache を再起動します。\nsudo /opt/bitnami/ctlscript.sh restart apache DNS は Route53 を使っているため Let\u0026rsquo;s Encrypt の証明書は lego で次のようにして取得することがます。\nlego \\  --domains blog.1q77.com \\  --accept-tos \\  --path . \\  --email 自分のメールアドレス \\  --dns route53 \\  --pem \\  run データのバックアップ # Lightsail インスタンスはスナップショットを作成することができます。 https://github.com/amazon-archives/lightsail-auto-snapshots という SAM を使った Lambda Function で自動化する方法があるようです\n追記\n「AWS Lightsail の snapshot 取得を自動化する」に上記の Lambda を使って自動化する手順を書きました。\n","date":"2018年12月29日","permalink":"/2018/12/building-wrodpress-site-on-lightsail-with-cloudfront/","section":"Posts","summary":"IDCF Cloud が個人向けサービスを終了する # IDCF Cloud の個人向けサービスが終了することとなりました。これまで IDCF Cloud の500円/月サーバーで WordPress を運用していたため","title":"WrodPress を Lightsail + CloudFront で構築する"},{"content":"GitHub とか文字としても残るので意味を知って使ってみよう\n   emoji 記号 / 意味      :bowtie: / 蝶ネクタイ   😄 😄 / 笑顔     🙂 :simple_smile: / シンプルな笑顔     😆 😆 / 心のこもった笑い     😊 😊 / 赤面     😃 😃 / 笑顔       ☺️ / くつろぎ     😏 😏 / ニヤニヤ     😍 😍 / 大好き、目がハート     😘 😘 / キス     😚 😚 / 目を閉じてキス     😳 😳 / 紅潮、興奮     😌 😌 / 安堵       😆 / 満足 (😆 と同じじゃない？)      😁\n😁\nにっこり\n  😉\n😉\nウインク\n  😜\n😜\n舌を出してウインク\n  😝\n😝\n舌を出して目を閉じる\n  😃\n😀\nにやにや\n  😗\n😗\nキス\n  😙\n😙\n笑顔でキス\n  😛\n😛\nベー\n  😴\n😴\n寝てる\n  😟\n😟\n心配\n  ☹️\n😦\n眉をひそめる\n  😧\n😧\n苦悩\n  😮\n😮\n口を開ける\n  😬\n😬\nしかめっ面\n  😕\n😕\n混乱\n  😯\n😯\n静寂、シーン\n  😑\n😑\n無表情\n  😒\n😒\n不満、しらけ\n  😅\n😅\n苦笑い\n  😓\n😓\n汗、冷や汗\n  😞\n😥\n悲しいけど安堵\n  😩\n😩\n疲れた\n  😔\n😔\n悲しい\n  😞\n😞\n失望\n  😖\n😖\n混乱\n  😨\n😨\n恐怖\n  😰\n😰\n冷や汗、青い顔\n  😣\n😣\n無力（ガマンに見えるけど）\n  😢\n😢\n泣き顔、悲しい\n  😭\n😭\n大泣き\n  😂\n😂\n泣き笑い\n  😲\n😲\n驚き\n  😱\n😱\n悲鳴をあげる\n   :neckbeard:\nキモヲタ\n（こんなのあるの？）\n  😫\n😫\n疲れた\n  😠\n😠\n怒った顔\n  😡\n😡\n怒り\n  😤\n😤\n怒り、欲求不満\n  😪\n😪\n眠い\n  😋\n😋\n美味しい\n  😷\n😷\nマスク\n  😎\n😎\nサングラス\n  😵\n😵\nめまい\n  👿\n👿\n悪魔\n  😈\n😈\n笑顔の悪魔\n  😐\n😐\n無表情\n  😶\n😶\n口無し\n  😇\n😇\n天使\n🤗\n🤗\nハグ\n🤐\n🤐\nお口チャック\n🙄\n軽蔑、退屈\n🤑\n🤑\nお金への愛\n🤯\nショック\n🤭\n口に手を当てる\n","date":"2018年11月7日","permalink":"/2018/11/emoji/","section":"Posts","summary":"GitHub とか文字としても残るので意味を知って使ってみよう emoji 記号 / 意味 :bowtie: / 蝶ネクタイ 😄 😄 / 笑顔 🙂 :simple_smile: / シンプルな笑顔 😆 😆 / 心のこもった笑い 😊 😊 / 赤面 😃","title":"絵文字の顔の使い分け"},{"content":"https://kazeburo.hatenablog.com/entry/2018/10/09/174111\nという記事をみて、次のようなブコメを書いたのですが実際には試したことがなかったのでやってみることにしました。\nこれは1サーバーでも簡単にローリングアップデートできる Docker swarm が便利に使えるパターンなのではないだろうか / “普通のサーバでDocker ContainerをHot DeployしたかったのでProxy書い…” https://t.co/2KQhsm7Wfl\n\u0026mdash; yteraoka (@yteraoka) October 10, 2018  実行するアプリの準備 # 次のような構成のものを構築します\n構成  Global mode は kubernetes での DaemonSet のようなモードで、各ノードで起動されます。今回は1ノードしか用意しませんが、あとからノードを追加した場合にそのまま LB や DNS Round robin に追加できます。\nDocker swarm のセットアップ # セットアップと言ってもシングルノードなので docker がインストールされていれば次の1コマンドで完了です。node 追加の予定がないので listen-addr を 127.0.0.1 にしてあります。\ndocker swarm init --advertise-addr 127.0.0.1 --listen-addr 127.0.0.1:2377 docker info コマンドで swarm モードが有効になっているかどうかを確認できます\n# docker info | grep ^Swarm Swarm: active docker swarm init\ndocker-compse でテスト # Swarm の stack, service は docker-compose.yml からも作れるので、まず、docker-compose で動く状態にしてみます。コードは https://github.com/yteraoka/single-node-swarm-test に置いてあります。\ngit clone https://github.com/yteraoka/single-node-swarm-test.git cd single-node-swarm-test docker-compose up # docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d7f2b3a76c26 myapp-app:1.0.0 \u0026quot;./app\u0026quot; 2 minutes ago Up 2 minutes (healthy) single-node-swarm-test_app_1 ef520dbf910e myapp-web:1.0.0 \u0026quot;nginx -g 'daemon of…\u0026quot; 2 minutes ago Up 2 minutes (healthy) 0.0.0.0:80-\u0026gt;80/tcp single-node-swarm-test_web_1 起動したらブラウザで port 80 の / にアクセスすると\nVersion: 1.0.0 Hostname: d7f2b3a76c26 というのが返ってくるはずです。Hostname は app の Container ID です。/color にアクセスすると背景が緑色になります。あとでイメージの更新をする際にわかりやすいように色をつけてみました。\n How services work Deploy services to a swarm  動作確認できたら Ctrl-C で停止して docker-compose down で不要なコンテナを削除します\nStack の作成 # service では build 済みのイメージファイルが必要です。今回は先程 docker-compose up を実行した際に build されているのでそれが使えます。なにか書き換えた場合は docker-compose build を実行することでイメージが作成されます\ndocker stack deploy --compose-file docker-compose.yml myapp ▼ docker ps で起動コンテナを確認\n# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ea742c69bd61 myapp-web:1.0.0 \u0026quot;nginx -g 'daemon of…\u0026quot; 4 seconds ago Up 3 seconds (health: starting) 0.0.0.0:80-\u0026gt;80/tcp myapp_web.2jqq8twdy8x549tyl0i2yokst.kenbu7e7519nk54tkeld20s4u 7fa3f88bb36f myapp-app:1.0.0 \u0026quot;./app\u0026quot; 8 seconds ago Up 7 seconds (health: starting) myapp_app.2.6tmaj4j37srfn5lxqu836h0w1 4777a3a0d76d myapp-app:1.0.0 \u0026quot;./app\u0026quot; 8 seconds ago Up 7 seconds (health: starting) myapp_app.1.h08cosodbi870jjt5555bart7 ▼ stack の一覧確認\n# docker stack ls NAME SERVICES ORCHESTRATOR myapp 2 Swarm ▼ stack のコンテナを確認（nginx コンテナの HEALTHCHECK を見直す余地があるかな)\n# docker stack ps myapp ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 3thdglvcpbt5 myapp_web.2jqq8twdy8x549tyl0i2yokst myapp-web:1.0.0 swarm Running Starting 2 seconds ago rd3gxvhbvpuk \\_ myapp_web.2jqq8twdy8x549tyl0i2yokst myapp-web:1.0.0 swarm Shutdown Failed 7 seconds ago \u0026quot;task: non-zero exit (1)\u0026quot; yia37uy212na \\_ myapp_web.2jqq8twdy8x549tyl0i2yokst myapp-web:1.0.0 swarm Shutdown Failed 18 seconds ago \u0026quot;task: non-zero exit (1)\u0026quot; kenbu7e7519n \\_ myapp_web.2jqq8twdy8x549tyl0i2yokst myapp-web:1.0.0 swarm Shutdown Failed 29 seconds ago \u0026quot;task: non-zero exit (1)\u0026quot; h08cosodbi87 myapp_app.1 myapp-app:1.0.0 swarm Running Running 7 seconds ago 6tmaj4j37srf myapp_app.2 myapp-app:1.0.0 swarm Running Running 7 seconds ago ▼ stack 内の service の一覧確認\n# docker stack services myapp ID NAME MODE REPLICAS IMAGE PORTS oup9x86rr54q myapp_app replicated 2/2 myapp-app:1.0.0 qmk60m0sytxn myapp_web global 1/1 myapp-web:1.0.0 ▼ stack を限定しない service の一覧確認\n# docker service ls ID NAME MODE REPLICAS IMAGE PORTS oup9x86rr54q myapp_app replicated 2/2 myapp-app:1.0.0 qmk60m0sytxn myapp_web global 1/1 myapp-web:1.0.0 新しい docker image の作成 # docker-compose.yml を書き換えて新しいイメージファイルをビルドします。 app イメージのバージョンを 1.0.1 に書き換え、先程の背景色を緑から赤に変更します。\ndiff --git a/docker-compose.yml b/docker-compose.yml index bc8bba3..5199cfe 100644 --- a/docker-compose.yml +++ b/docker-compose.yml @@ -15,13 +15,13 @@ services:  protocol: tcp mode: host app: - image: myapp-app:1.0.0 + image: myapp-app:1.0.1  build: context: ./app dockerfile: Dockerfile args: - VERSION: 1.0.0 - COLOR: green + VERSION: 1.0.1 + COLOR: red  networks: - overlay deploy: ▼ 書き換えたら docker-compose build でイメージをビルドします。(build-arg で渡す変数を go の compile 時に指定する変数として使ってアプリにバージョンとか色を埋め込んでます)\n# docker-compose build ... Successfully built 9cb764797c20 Successfully tagged myapp-app:1.0.1 # docker image ls | grep myapp-app myapp-app 1.0.1 9cb764797c20 23 seconds ago 10.8MB myapp-app 1.0.0 ca6303e343e2 About an hour ago 10.8MB 余談：今回、テストアプリを Go で書いたので、イメージを小さくするために multistage-build を試してみました。scratch じゃなくて alpine をベースにしましたけど\nイメージの入れ替え # docker service update でイメージを入れ替えます\ndocker service update myapp_app --image myapp-app:1.0.1 WARNING が出ていますが、これは本来 swarm は複数 node で実行されるものなので、image は registry にアップしてねということですね (docker service create --name registry --publish published=5000,target=5000 registry:2 で swarm 内に registry サービスを作って、docker-compose push でプッシュすることも可能 docker-compose.yml はイジる必要があります)\n# docker service update myapp_app --image myapp-app:1.0.1 image myapp-app:1.0.1 could not be accessed on a registry to record its digest. Each node will access myapp-app:1.0.1 independently, possibly leading to different nodes running different versions of the image. myapp_app overall progress: 0 out of 2 tasks 1/2: starting 2/2: 順に入れ替えられていきます\nmyapp_app overall progress: 2 out of 2 tasks 1/2: running 2/2: running verify: Service converged ▼ docker service ps で更新などの履歴が確認できます\n# docker service ps myapp_app ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS iu6nmiijnxby myapp_app.1 myapp-app:1.0.1 swarm Running Running 2 minutes ago h08cosodbi87 \\_ myapp_app.1 myapp-app:1.0.0 swarm Shutdown Shutdown 2 minutes ago 896sinhjy4r4 myapp_app.2 myapp-app:1.0.1 swarm Running Running 2 minutes ago 6tmaj4j37srf \\_ myapp_app.2 myapp-app:1.0.0 swarm Shutdown Shutdown 3 minutes ago これでまた port 80 の /color にアクセスすると背景が赤色になっています。バージョンの部分も 1.0.1 になっています\nVersion: 1.0.1 Hostname: 0597b98c6ff4 しかし、Chrome でアクセスすると2つの app コンテナが起動しているのに毎回同じ Hostname (Container ID) しか表示されません。これは毎回 /favicon.ico へもアクセスしていて、2 アクセスずつしているからでした\ndocker service update には他にもたくさんオプションがあります\nstack を使っているので docker-compose.yml の image を書き換える、もしくは環境変数で指定することにして、再度 docker stack deploy を実行することでも image の更新が可能です。さらに、image だけじゃなくて replica 数だったり、メモリのリミットとか各種設定の更新にも使えます。deploy.update_config.order を start-first にしておくと replicas が 1 の場合にも downtime をなくせます。\nrollback # 新しいバージョンがそもそも起動しないとかであれば自動で rollback させたりもできるようですが、そうではないなんらかの不具合があって一つ前のバージョンに戻したいという場合は\ndocker service rollback myapp_app とするだけで戻せます。ただし、一つ前に戻すだけで、rollback 後の一つ前は戻す必要のあった問題のあるバージョンになるので2度続けて実行するのは危険です。イメージ入れ替えだけだから前のバージョンがわかっているなら (docker service ps myapp_app すればわかる) そのイメージを指定するのが良いかも\n実行コンテナ数を増減させる # service で実行するコンテナの数は docker service scale で変更することができます。2つだった app コンテナを 3 つにしてみます\ndocker service scale myapp_app=3 # docker service ps myapp_app ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS iu6nmiijnxby myapp_app.1 myapp-app:1.0.1 swarm Running Running 9 minutes ago h08cosodbi87 \\_ myapp_app.1 myapp-app:1.0.0 swarm Shutdown Shutdown 9 minutes ago 896sinhjy4r4 myapp_app.2 myapp-app:1.0.1 swarm Running Running 9 minutes ago 6tmaj4j37srf \\_ myapp_app.2 myapp-app:1.0.0 swarm Shutdown Shutdown 10 minutes ago bi3v0wo0m1fi myapp_app.3 myapp-app:1.0.1 swarm Running Running 13 seconds ago コンテナが3つになったので、Chrome でもアクセスの度に Hostname (Container ID) が変わることが確認できます\n更新は順番に行われますが、同時にいくつのコンテナを入れ替えるのか(--update-parallelism)、更新間隔(--update-delay)をどうするかなども調整可能です。 エラーが続く場合に自動でロールバックさせる機能もあります(--update-failure-actionなど)。\n Apply rolling updates to a service  イメージ更新時に downtime はある？ # while : ; do curl -so /dev/null -w \u0026quot;%{http_code} %{time_total}\\n\u0026quot; -m 5 http://localhost/ done を実行しながらイメージの更新をしてみてもダウンタイムはありませんでした。 が、nginx コンテナの HEALTHCHECK 実行時になんかレスポンスが悪くなるのが気になるな。 docker events で見てると、このログが出るときにレスポンスが1秒超えることがある。なんでだろ？\n2018-10-13T04:31:04.542378009Z container exec_create: /bin/sh -c curl -f http://127.0.0.1/healthcheck || exit 1 fc4105bc721c89956d358d078d8f689110e68cd0b02d3e48fd4074cb7d0f635f (com.docker.stack.namespace=myapp, com.docker.swarm.node.id=2jqq8twdy8x549tyl0i2yokst, com.docker.swarm.service.id=qmk60m0sytxnqlpgwz483hq58, com.docker.swarm.service.name=myapp_web, com.docker.swarm.task=, com.docker.swarm.task.id=3thdglvcpbt5i5bac38k8nrcq, com.docker.swarm.task.name=myapp_web.2jqq8twdy8x549tyl0i2yokst.3thdglvcpbt5i5bac38k8nrcq, execID=3387082ab0c7e6c1b8f9ff0f1b73003e975c4ccb83dfb3d99144e9951d88d65b, image=myapp-web:1.0.0, maintainer=NGINX Docker Maintainers , name=myapp_web.2jqq8twdy8x549tyl0i2yokst.3thdglvcpbt5i5bac38k8nrcq) 2018-10-13T04:31:04.542511350Z container exec_start: /bin/sh -c curl -f http://127.0.0.1/healthcheck || exit 1 fc4105bc721c89956d358d078d8f689110e68cd0b02d3e48fd4074cb7d0f635f (com.docker.stack.namespace=myapp, com.docker.swarm.node.id=2jqq8twdy8x549tyl0i2yokst, com.docker.swarm.service.id=qmk60m0sytxnqlpgwz483hq58, com.docker.swarm.service.name=myapp_web, com.docker.swarm.task=, com.docker.swarm.task.id=3thdglvcpbt5i5bac38k8nrcq, com.docker.swarm.task.name=myapp_web.2jqq8twdy8x549tyl0i2yokst.3thdglvcpbt5i5bac38k8nrcq, execID=3387082ab0c7e6c1b8f9ff0f1b73003e975c4ccb83dfb3d99144e9951d88d65b, image=myapp-web:1.0.0, maintainer=NGINX Docker Maintainers , name=myapp_web.2jqq8twdy8x549tyl0i2yokst.3thdglvcpbt5i5bac38k8nrcq) 2018-10-13T04:31:04.644207191Z container exec_die fc4105bc721c89956d358d078d8f689110e68cd0b02d3e48fd4074cb7d0f635f (com.docker.stack.namespace=myapp, com.docker.swarm.node.id=2jqq8twdy8x549tyl0i2yokst, com.docker.swarm.service.id=qmk60m0sytxnqlpgwz483hq58, com.docker.swarm.service.name=myapp_web, com.docker.swarm.task=, com.docker.swarm.task.id=3thdglvcpbt5i5bac38k8nrcq, com.docker.swarm.task.name=myapp_web.2jqq8twdy8x549tyl0i2yokst.3thdglvcpbt5i5bac38k8nrcq, execID=3387082ab0c7e6c1b8f9ff0f1b73003e975c4ccb83dfb3d99144e9951d88d65b, exitCode=0, image=myapp-web:1.0.0, maintainer=NGINX Docker Maintainers , name=myapp_web.2jqq8twdy8x549tyl0i2yokst.3thdglvcpbt5i5bac38k8nrcq) さらに # swarm config を使うと nginx の設定はイメージを変更したり、ホスト側のファイルをマウントしなくても docker service update で更新できそう Store configuration data using Docker Configs\nしばらく見ていなかったけど swarm も意外と使えるのかな？ Sidecar とかが使えないのを割り切れば。\n","date":"2018年10月13日","permalink":"/2018/10/rolling-update-on-single-node-docker-swarm/","section":"Posts","summary":"https://kazeburo.hatenablog.com/entry/2018/10/09/174111 という記事をみて、次のようなブコメを書いたのですが実際には試したことがなかったのでやってみることにしました。 これは1サーバーでも簡単にロー","title":"Single node docker swarm でお手軽 rolling update"},{"content":"","date":"2018年10月13日","permalink":"/tags/swarm/","section":"Tags","summary":"","title":"Swarm"},{"content":"以前にも書いたように我が家にはメッシュ WiFi システムの TP-Link Deco M5 3台セット を導入したわけですが、ふと Amazon のページを開いてみると「この商品には新しいモデルがあります」という表示が！\n 新しいモデル？？ Deco M9 ならそうだが、M5 に新しいモデルがあるのか？\nよく見ると末尾に V2.0 とある。そういえばハードウェアに V1, V2 っていうのが存在するというのは知っている。ファームウェアのダウンロードページに行くと V1, V2 ってのがあるから持ってるものに合わせてダウンロードしてねとなっている。そして、V1 と V2 のどちらなのかは地域によって異なるのだよと書かれている。接頭辞に V を使ったのが良くないと思うんだけど、これは新しさを表すものではなかった\nそれでは何がちがうのか？それぞれのデータシートをダウンロードして比較してみましょう\n Deco M5_V1_Datasheet.pdf Deco M5_V2_Datasheet.pdf  結果発表 # 違いは電源供給ポートが USB Type-C (V1) であるか、一般的なDCの丸いやつか (V2) という差でしかありませんでした。我が家のモデルは V1 でした。壁に書けてあるので USB の方が抜けにくそうだしこっちで良かった。\n新しいって書かれて、さらにそっちの方が安いので私が購入する際にもそれがあったら V2 を買っていただろうなあ、危ない危ない\nHow to find the hardware version on a TP-Link device? 我が家の危機の型番は Deco M5(3-pack)(JP) Ver:1.1\n","date":"2018年10月7日","permalink":"/2018/10/tp-link-deco-m5-difference-between-v1-and-v2/","section":"Posts","summary":"以前にも書いたように我が家にはメッシュ WiFi システムの TP-Link Deco M5 3台セット を導入したわけですが、ふと Amazon のページを開いてみると「この商品には新しいモデ","title":"TP-Link Deco M5 の V1 と V2 の違いって何だ？"},{"content":"設定方法は「Choose an SSL Option and Install Rancher」に書いてあるわけですが、どうやら最近の Chrome では HTST の設定されたサーバーは自己署名の証明書ではアクセスできないみたいなので Let\u0026rsquo;s Encrypt で取得して設定することにします。\nNET::ERR_CERT_AUTHORITY_INVALID  Rancher サーバーは docker container として実行するので、証明書取得も docker を使いましょう。Single ノードでの方法です\nrancher での証明書指定方法 # rancher は /etc/rancher/ssl/cert.pem, /etc/rancher/ssl/key.pem に置かれたものを使うので、docker container 起動時に -v /host/file/path:/etc/rancher/ssl/cert.pem:ro などどしてマウントしてやれば良い。 そして --no-cacerts オプションを追加して自前 CA を作成しないようにします。\nlego を使って証明書を取得 # docker run \\  --rm \\  -v /etc/rancher:/etc/rancher \\  -p 80:80 \\  xenolf/lego \\  --path /etc/rancher \\  --domains \u0026#34;証明書のドメイン名\u0026#34; \\  --email \u0026#34;自分のメールアドレス\u0026#34; \\  --filename server \\  --accept-tos run これで、/etc/rancher/certificates/server.crt, /etc/rancher/certificates/server.key が生成されます。（他のファイルもある） 上記のコマンドは http-01 ですが、lego は dns-01 や tls-alpn-01 にも対応しています。\nrancher 起動 # docker run -d --restart=unless-stopped -p 80:80 -p 443:443 \\  -v /etc/rancher/certificates/server.crt:/etc/rancher/ssl/cert.pem \\  -v /etc/rancher/certificates/server.key:/etc/rancher/ssl/key.pem \\  rancher/rancher:v2.0.8 --no-cacerts これで Chrome でもアクセスできる。\nプライベートなドメインの場合は CA 作って /etc/rancher/ssl/cacerts.pem も置いて、ブラウザにも登録しましょう。\n","date":"2018年9月16日","permalink":"/2018/09/rancher-2-0-bring-your-own-certificate/","section":"Posts","summary":"設定方法は「Choose an SSL Option and Install Rancher」に書いてあるわけですが、どうやら最近の Chrome では HTST の設定されたサーバーは自己署名の証明書ではア","title":"Rancher 2.0 サーバーへのサーバー証明書の持ち込み"},{"content":"🐵 去る 2018年9月7日、Tech-on MeetUp#02「マネージドサービスだけに頼らないコンテナ基盤」 に参加してオンプレ Kubernetes 他の話を聞いてきた。司会の方の登壇者いじりが浮いていた… というはどうでも良くって、時代は Self-hosted Kubernetes だということらしい\nCoreOS の blog では2016年8月時点ですでに self-hosted について述べられています Self-Hosted Kubernetes makes Kubernetes installs, scaleouts, upgrades easier | CoreOS Kubernetes の更新が簡単になり、High Availability 化も容易であると\n私も先日から kubeadm を使った Kubernetes 構築を試してみたりしているわけですが、kubeadm もゆくゆくは self-hosted となる予定のようです\n現在はまだ alpha 段階ですが kubeadm init で\n(--feature-gates=SelfHosting=true というオプションを指定することで self-hosted な control plane が構築できるようです\n If kubeadm init is invoked with the alpha self-hosting feature enabled, (--feature-gates=SelfHosting=true), the static Pod based control plane is transformed into a self-hosted control plane.\n  As of 1.8, you can experimentally create a self-hosted Kubernetes control plane. This means that key components such as the API server, controller manager, and scheduler run as DaemonSet pods configured via the Kubernetes API instead of static pods configured in the kubelet via static files.\n /etc/kubernetes/manifests/ 配下に Kubernetes Control plane の各コンポーネント用 manifest ファイルを配置する Static pods ではなく、etcd に情報をもつ DaemonSet pods として起動させます\nまだ alpha 版であるためいくつか重要な制限事項があるようです\n Self-hosting in 1.8 has some important limitations. In particular, a self-hosted cluster cannot recover from a reboot of the master node without manual intervention. This and other limitations are expected to be resolved before self-hosting graduates from alpha.\n マスターノードの再起動時には手動オペレー所のが必要\n By default, self-hosted control plane Pods rely on credentials loaded from hostPath volumes. Except for initial creation, these credentials are not managed by kubeadm. You can use --feature-gates=StoreCertsInSecrets=true to enable an experimental mode where control plane credentials are loaded from Secrets instead. This requires very careful control over the authentication and authorization configuration for your cluster, and may not be appropriate for your environment.\n デフォルトでは credential を hostPath ボリュームから読むようになっているが、これは初期構築時を覗いて kubeadm では管理されない。--feature-gates=StoreCertsInSecrets=true オプションを使うことで Secrets の中に置くことができるが、これを行う場合、クラスタの認証認可を厳重に管理する必要がでてくる\n In kubeadm 1.8, the self-hosted portion of the control plane does not include etcd, which still runs as a static Pod.\n kubeadm 1.8 においては etcd は static Pod のままである。1.8 よりも新しいものだと違うのかな？\n引用したドキュメントは Kubernetes 1.11 のものです。\n今度 --feature-gates=SelfHosting=true を試してみよう。\n","date":"2018年9月11日","permalink":"/2018/09/self-hosted-kubernetes/","section":"Posts","summary":"🐵 去る 2018年9月7日、Tech-on MeetUp#02「マネージドサービスだけに頼らないコンテナ基盤」 に参加してオンプレ Kubernetes 他の話を聞いて","title":"Self-hosted Kubernetes"},{"content":"Rancher 2.1 の話がぜんぜん聞こえてこないなーどうなってるのかな？と思って GitHub の releases ページを覗いてみたらいつの間にか 2.0.8 がリリースされていました。\nということで各バージョンでの変更点を見てみましょう。\nv2.0.1 (2018/5/23) # releases/tag/v2.0.1\n Workloads/Services/Namespaces を YAML で import/export できるようになった #12371 RoleTemplates に protected 状態が追加された #13418 CLI に複数の機能が追加された #12831, #13101, #13235, #13390, #13518  v2.0.2 (2018/5/24) # releases/tag/v2.0.2\n 2.0.1 で GitHub 認証が動作しなくなってしまったことの修正 #13665 アップグレードとロールバックをサポート  v2.0.3 (2018/6/23) # releases/tag/v2.0.3\n Oauth provider として AzureAD が追加された #12942 クラスタ作成時のオプション(RKE configuration options) 指定がフルサポートされた #13816, #13076, #13115 AKS の更新への対応 (multi K8S versions, DNS prefix, private network and subnet) #13395 CLI の強化 #13701 多くのバグフィックス 更新後に ingress が正しく機能しなくなる既知の問題がある、ワークアラウンドは ingress の再作成 #13611  v2.0.4 (2018/6/26) # releases/tag/v2.0.4\n カタログの更新が出来なかった問題の修正 #14186, #14183 更新後に ingress が正しく機能しなくなる既知の問題がある、ワークアラウンドは ingress の再作成 #13611  v2.0.5 (2018/7/8) # releases/tag/v2.0.5\n 新規クラスタ作成時のデフォルトネットワークプロバイダが canal から flannel に変更されたため、ネットワークポリシーを使いたい場合は canal を指定する必要がある 認証プロバイダとして OpenLDAP がサポートされた、ただし、匿名バインディングのみ #13814 認証プロバイダとして FreeIPA がサポートされた #13815 ノードの cordon/uncordon がサポートされた。cordon 状態では新たな Pod がスケジュールされない。drain は既存 Pod を停止して別のノードへ移動させるが cordon は既存 Pod には影響しない #13623 AKS 対応の強化。advanced networking options が使えるようになった #14164 CLI の強化 バグ修正  既知の問題\n Active Directory のグループ再帰検索がデフォルトで無効になった #14369, #14482 RacherOS, CoreOS, boot2docker のようなパーシステントディレクトリを持たない OS では更新時に証明書が見つからなくてコケる #14454 2.0.5 と 2.0.6 で Active Directory を使っている場合、サービスアカウントの ID でにドメインも指定しなければならなくなった #14708  v2.0.6 (2018/7/12) # releases/tag/v2.0.6\n Active Directory と OpenLDAP でネストされたグループの検索が遅い問題が解消された。AzureAD と FreeIPA にこの問題はなかった #14482 RancherOS, CoreOS, boot2docker などで更新にコケる問題が解消された #14454 OpenLDAP で匿名バインドしか使えなかった問題が解消された #12729, #14456 デフォルトのネットワークプロバイダが canal に戻された #14462  v2.0.7 (2018/8/13) # releases/tag/v2.0.7\n パフォーマンス改善 #14372, #14402, #14409 canal のネットワークポリシーがデフォルトで無効になった #14462 PING Federate (SAML?) が認証プロバイダに追加された #11169 ADFS が認証プロバイダに追加された #14609 ユーザーの初回ログイン時に割り当てられるデフォルトロールを設定できるようになった #12737 Rancher API の監査ログを取得可能になった #12733 System Project というプロジェクトが追加され、Rancher や Kubernetes 自身のネームスペースはここに入るようになった #12706 restricted と unrestricted という2つのデフォルト Pod security policy が追加された #12832 RKE のクラスタに Metric サーバーをデプロイ可能になった #13745 GKE, EKS のより良い管理のためにフィールドが追加された #14566, #13789 Kubernetes 1.11 がサポートされ #13837、1.8 での新規作成がサポートがサポートされなくなった #14517 Docker machine が v0.15.0 に更新され、cn-northwest-1 リージョンがサポートされた #11192 NGINX ingress controller が v1.16 に更新された #13294 Helm が v2.9.1 に更新された #14023 多数のバグフィックス  v2.0.8 (2018/8/25) # releases/tag/v2.0.8\n Kubernetes 1.11.1 には正しい Cipher Suite が含まれていなかったため 1.11.2 に更新しましょう #15120 Etcd の起動に10秒以上かかる場合に単一サーバーでのセットアップに失敗する可能性があった問題の修正 #15077 他バグ修正  まとめ？ # てな感じです。認証プロバイダの追加は運用面では大きいですかね。2.0.8 での認証プロバイダ選択画面は次のようになっています\n RKE クラスタ作成時のオプション選択はこんな感じ\n ドキュメントも揃ってきてますかね\n Rancher 2.0 RKE  ","date":"2018年9月10日","permalink":"/2018/09/after-the-rancher-2-0/","section":"Posts","summary":"Rancher 2.1 の話がぜんぜん聞こえてこないなーどうなってるのかな？と思って GitHub の releases ページを覗いてみたらいつの間にか 2.0.8 がリリースされていました。 ということ","title":"Rancher 2.0.8 までの変更点まとめ"},{"content":"CentOS 7 で kubeadm を使って Kubernetes をセットアップしていると docker の logging driver が journald になっています。 各 container は docker daemon 配下で動作するため、systemd の unit 名はどれも docker.service になってしまい、journalctl -u xxx ではコンテナ単位での確認ができません。 journalctl は -o json とすると、default の short では出力されないメタデータも出力されます。pipe で [jq](https://stedolan.github.io/jq/) コマンドに渡せば次のように出力されます。\n{ \u0026quot;__CURSOR\u0026quot;: \u0026quot;s=3e5c46fa81a54c369a8e1858c04b5204;i=3f72;b=ae5c5309f05e4a699808679fe6781eb8;m=1b352c11d;t=5756cf7db2231;x=66b27cca0ce000f1\u0026quot;, \u0026quot;__REALTIME_TIMESTAMP\u0026quot;: \u0026quot;1536485758804529\u0026quot;, \u0026quot;__MONOTONIC_TIMESTAMP\u0026quot;: \u0026quot;7303512349\u0026quot;, \u0026quot;_BOOT_ID\u0026quot;: \u0026quot;ae5c5309f05e4a699808679fe6781eb8\u0026quot;, \u0026quot;PRIORITY\u0026quot;: \u0026quot;6\u0026quot;, \u0026quot;CONTAINER_ID_FULL\u0026quot;: \u0026quot;7930a5a8469b5d68ac6048a0df508c22425999d7a8cbe60f8a4165ba16a51256\u0026quot;, \u0026quot;CONTAINER_NAME\u0026quot;: \u0026quot;k8s_calico-node_calico-node-7t275_kube-system_12fd8feb-b404-11e8-8de4-76af76a1afa6_0\u0026quot;, \u0026quot;CONTAINER_TAG\u0026quot;: \u0026quot;7930a5a8469b\u0026quot;, \u0026quot;CONTAINER_ID\u0026quot;: \u0026quot;7930a5a8469b\u0026quot;, \u0026quot;_TRANSPORT\u0026quot;: \u0026quot;journal\u0026quot;, \u0026quot;_PID\u0026quot;: \u0026quot;9950\u0026quot;, \u0026quot;_UID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;_GID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;_COMM\u0026quot;: \u0026quot;dockerd-current\u0026quot;, \u0026quot;_EXE\u0026quot;: \u0026quot;/usr/bin/dockerd-current\u0026quot;, \u0026quot;_CMDLINE\u0026quot;: \u0026quot;/usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --init-path=/usr/libexec/docker/docker-init-current --seccomp-profile=/etc/docker/seccomp.json --selinux-enabled --log-driver=journald --signature-verification=false --storage-driver overlay2\u0026quot;, \u0026quot;_CAP_EFFECTIVE\u0026quot;: \u0026quot;1fffffffff\u0026quot;, \u0026quot;_SYSTEMD_CGROUP\u0026quot;: \u0026quot;/system.slice/docker.service\u0026quot;, \u0026quot;_SYSTEMD_UNIT\u0026quot;: \u0026quot;docker.service\u0026quot;, \u0026quot;_SYSTEMD_SLICE\u0026quot;: \u0026quot;system.slice\u0026quot;, \u0026quot;_SELINUX_CONTEXT\u0026quot;: \u0026quot;system_u:system_r:container_runtime_t:s0\u0026quot;, \u0026quot;_MACHINE_ID\u0026quot;: \u0026quot;b671b9b84b830619f4d211595b94cb0d\u0026quot;, \u0026quot;_HOSTNAME\u0026quot;: \u0026quot;cp1\u0026quot;, \u0026quot;MESSAGE\u0026quot;: \u0026quot;2018-09-09 09:35:58.797 [INFO][62] health.go 150: Overall health summary=\u0026amp;health.HealthReport{Live:true, Ready:true}\u0026quot;, \u0026quot;_SOURCE_REALTIME_TIMESTAMP\u0026quot;: \u0026quot;1536485758803706\u0026quot; } jouranalctl (1) の man を見ると journalctl [OPTIONS...] [MATCHES...] と MATCHES... にフィルタリング条件を渡せるとあります。 別の項目に対して複数の条件を指定すると両方にマッチしたログが出力されます\njournalctl _SYSTEMD_UNIT=avahi-daemon.service _PID=28097 同じ項目に対して複数の条件をしていすると、どちらかにマッチしたログが出力されます\njournalctl _SYSTEMD_UNIT=avahi-daemon.service _SYSTEMD_UNIT=dbus.service + をセパレータとして使うと OR でつなげることになります。次の例では PID が 28097 でかつ systemd の unit が avahi-daemon のログと dbus (PID は関係ない) のログが出力されます。\njournalctl _SYSTEMD_UNIT=avahi-daemon.service _PID=28097 + _SYSTEMD_UNIT=dbus.service というわけで、上の docker のログの例では CONTAINER_ID や CONTAINER_NAME などでフィルタリングすると特定の docker のログが確認できます。\n","date":"2018年9月9日","permalink":"/2018/09/journalctl-log-filtering/","section":"Posts","summary":"CentOS 7 で kubeadm を使って Kubernetes をセットアップしていると docker の logging driver が journald になっています。 各 container は docker daemon 配下で動作するため、systemd の unit 名はどれも docker.service になってし","title":"journalctl で docker のログをフィルタリング"},{"content":"","date":"2018年9月9日","permalink":"/tags/journald/","section":"Tags","summary":"","title":"journald"},{"content":"","date":"2018年9月9日","permalink":"/tags/systemd/","section":"Tags","summary":"","title":"systemd"},{"content":"先月、我が家に TP-Link Deco M5 3台セット を導入したわけですが、ネットの紹介記事を見ていると次のような課題があるようでした（私はたまたまこれには困っていない）。\n VPN 接続の機能が無い WPS (Wi-Fi Protected Setup) に対応していない LAN 側の IP アドレスを 192.168.0.0/24 から変更できない  なんとこれが、最新の更新ですべて解決されました。\nファームウェアバージョン: 1.2.3 Build 20180819 Rel. 68100 Modifications and Bug Fixes:\n Added custom configuration options for LAN IP and DHCP server. Added WPS function. Added support for PPTP and L2TP internet connection types. Improved system performance.  VPN # IPv4 のインターネット接続タイプに「L2TP」、「PPTP」が追加されました   \nWPS # 設定メニューに WPS が追加されました\n各 Deco の右の丸い WPS アイコンをクリックすると 120 秒のカウントダウンが始まります。この間に端末側の WPS 機能で接続させます。\nLAN IPアドレス、DHCP サーバー設定 # 詳細設定に「LAN IP」と「DHCPサーバー」メニューが追加されました\n「LAN IP」設定で IP アドレスとサブネットマスクを設定できます\n「DHCPサーバー」設定でアドレス範囲と DNS サーバーを設定できます。DNS サーバーについては以前も IPv4 設定で指定ができました。ただ、9.9.9.9 を指定していたら DAZN が提供エリア外と判定されて視聴できなくなって戻した\u0026hellip;\n今回の更新で導入へのネックが解消されたら導入してみては？ まあ、いろんな意見がありますけどね。\n「もはや、もっといい選択肢はある！　鳴り物入りで日本上陸した「Google Wifi」のメッシュを試す【イニシャルB】 - INTERNET Watch」\n","date":"2018年9月1日","permalink":"/2018/09/deco-m5-update-1/","section":"Posts","summary":"先月、我が家に TP-Link Deco M5 3台セット を導入したわけですが、ネットの紹介記事を見ていると次のような課題があるようでした（私はたまたまこれには困ってい","title":"Deco M5 が VPN、WPS、LANアドレス設定に対応"},{"content":"Creating Highly Available Clusters with kubeadm の Stacked control plane nodes で etcd を control-plane に相乗りさせる kubernetes cluster の作成を試します。今回も DigitalOcean で CentOS 7 を使います。\nまた、面倒なので Ansible Playbook でセットアップするようにしました github.com/yteraoka/do-k8s-stacked-etcd\n","date":"2018年8月20日","permalink":"/2018/08/etcd-stacked-highly-available-kubernetes-cluster/","section":"Posts","summary":"Creating Highly Available Clusters with kubeadm の Stacked control plane nodes で etcd を control-plane に相乗りさせる kubernetes cluster の作成を試します。今回も DigitalOcean で CentOS 7 を使います。 また、面倒なので Ansible Playbook でセットアップするようにしま","title":"etcd stacked Highly Available Kubernetes cluster を Ansible でセットアップ"},{"content":"Google Home mini に任意の文章を読み上げさせるために https://github.com/ikasamah/homecast をいじって使ってましたが、これは https://translate.google.com/translate_tts?client=tw-ob\u0026amp;ie=UTF-8\u0026amp;q=テスト\u0026amp;tl=ja といった Google 翻訳の非公式(?) API にアクセスしていました。その後、Google は Cloud Text-to-Speech サービスを発表したので Go でこの API を使ってみます。\ncloud.google.com/go/texttospeech/apiv1 を使えば良さそうです。これを使った例もあったので参考にさせていただきました。\nGoogle Cloud Developer Console でプロジェクトを作成し、Cloud Text-to-Speech API を有効にし、サービスアカウントを作って JSON 形式の秘密鍵をダウンロードします。 GOOGLE_APPLICATION_CREDENTIALS という環境変数にこの JSON ファイルへの PATH を設定します。(アプリケーションに認証情報を提供する)\ngo get go build echo \u0026quot;テキストメッセージ\u0026quot; | ./tts このようにすることで speech.mp3 というファイル(出力先ファイルは -o オプションで指定可能)が作成されます。-v で voice name を指定できます、日本語は ja-JP-Wavenet-A or ja-JP-Standard-A の2つが指定可能で、Wavenet はより高品質らしいが料金も高い (料金体系)\n","date":"2018年8月19日","permalink":"/2018/08/google-text-to-speech-golang/","section":"Posts","summary":"Google Home mini に任意の文章を読み上げさせるために https://github.com/ikasamah/homecast をいじって使ってましたが、これは https://translate.google.com/tran","title":"golang で Google の Text-to-Speech を使う"},{"content":"","date":"2018年8月19日","permalink":"/tags/google/","section":"Tags","summary":"","title":"google"},{"content":"","date":"2018年8月18日","permalink":"/tags/digitalocean/","section":"Tags","summary":"","title":"DigitalOcean"},{"content":"ケチって都度々々削除するため手作業は辛いので Ansible Playbook にしてみた https://github.com/yteraoka/do-k8s-external-etcd\n","date":"2018年8月18日","permalink":"/2018/08/do-k8s-external-etcd/","section":"Posts","summary":"ケチって都度々々削除するため手作業は辛いので Ansible Playbook にしてみた https://github.com/yteraoka/do-k8s-external-etcd","title":"外部 etcd な Kubernetes を DigitalOcean に構築する Ansible Playbook"},{"content":"","date":"2018年8月16日","permalink":"/tags/calico/","section":"Tags","summary":"","title":"calico"},{"content":"etcd を外部にもつ HA な Kubernetes クラスタを kubeadm で構築します。Creating Highly Available Clusters with kubeadm をなぞります。\n各種バージョン情報 #  CentOS Linux release 7.5.1804 kubeadm v1.11.2 (GitCommit:\u0026ldquo;bb9ffb1654d4a729bb4cec18ff088eacc153c239\u0026rdquo;) docker 1.13.1 (docker-1.13.1-68.gitdded712.el7.centos.x86_64) kubelet v1.11.2 (Kubernetes v1.11.2) etcd 3.2.18  Set up the cluster # etcd クラスタは「kubeadm で HA な etcd をセットアップ」の手順でセットアップ済みとします。\nCopy required files to other control plane nodes # etcd クラスタ作成で生成した次のファイルを kubernetes のコントロールプレーンとなるホストにコピーします。コントロール用プレーンに kubeadm, docker, kubelet のインストールが必要です。\n /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/apiserver-etcd-client.key  for s in HOST0 HOST1 HOST2; do scp /etc/kubernetes/pki/etcd/ca.crt \\\\ /etc/kubernetes/pki/apiserver-etcd-client.crt \\\\ /etc/kubernetes/pki/apiserver-etcd-client.key $s: ssh $s \u0026quot;sudo mkdir -p /etc/kubernetes/pki/etcd; sudo cp -p ca.crt /etc/kubernetes/pki/etcd/; sudo cp apiserver-etcd-client.crt apiserver-etcd-client.key /etc/kubernetes/pki/\u0026quot; done Set up the first control plane node #   kubeadm-config.yaml テンプレートファイルを作成する\napiVersion: kubeadm.k8s.io/v1alpha2 kind: MasterConfiguration kubernetesVersion: v1.11.0 apiServerCertSANs: - \u0026quot;LOAD\\_BALANCER\\_DNS\u0026quot; api: controlPlaneEndpoint: \u0026quot;LOAD\\_BALANCER\\_DNS:LOAD\\_BALANCER\\_PORT\u0026quot; etcd: external: endpoints: - https://ETCD\\_0\\_IP:2379 - https://ETCD\\_1\\_IP:2379 - https://ETCD\\_2\\_IP:2379 caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key networking: # This CIDR is a calico default. Substitute or remove for your CNI provider. podSubnet: \u0026quot;192.168.0.0/16\u0026quot;   テンプレートの次の変数を適切な値に置換します\n LOAD_BALANCER_DNS LOAD_BALANCER_PORT ETCD_0_IP ETCD_1_IP ETCD_2_IP    kubeadm init --config kubeadm-config.yaml を実行します\nAPIサーバー container は 6443/tcp ポートを listen するのでロードバランサーからの転送先はこのポートを指定する必要があります\n  \\[root@ctrl1 ~\\]# kubeadm init --config kubeadm-config.yaml \\[endpoint\\] WARNING: port specified in api.controlPlaneEndpoint overrides api.bindPort in the controlplane address \\[init\\] using Kubernetes version: v1.11.0 \\[preflight\\] running pre-flight checks I0816 12:26:37.018590 14467 kernel\\_validator.go:81\\] Validating kernel version I0816 12:26:37.018733 14467 kernel\\_validator.go:96\\] Validating kernel config \\[WARNING Service-Kubelet\\]: kubelet service is not enabled, please run 'systemctl enable kubelet.service' \\[preflight/images\\] Pulling images required for setting up a Kubernetes cluster \\[preflight/images\\] This might take a minute or two, depending on the speed of your internet connection \\[preflight/images\\] You can also perform this action in beforehand using 'kubeadm config images pull' \\[kubelet\\] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; \\[kubelet\\] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; \\[preflight\\] Activating the kubelet service \\[certificates\\] Generated ca certificate and key. \\[certificates\\] Generated apiserver certificate and key. \\[certificates\\] apiserver serving cert is signed for DNS names \\[ctrl1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local k8s-api.example k8s-api.example.com\\] and IPs \\[10.96.0.1 203.0.113.123\\] \\[certificates\\] Generated apiserver-kubelet-client certificate and key. \\[certificates\\] Generated sa key and public key. \\[certificates\\] Generated front-proxy-ca certificate and key. \\[certificates\\] Generated front-proxy-client certificate and key. \\[certificates\\] valid certificates and keys now exist in \u0026quot;/etc/kubernetes/pki\u0026quot; \\[endpoint\\] WARNING: port specified in api.controlPlaneEndpoint overrides api.bindPort in the controlplane address \\[kubeconfig\\] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/admin.conf\u0026quot; \\[kubeconfig\\] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/kubelet.conf\u0026quot; \\[kubeconfig\\] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/controller-manager.conf\u0026quot; \\[kubeconfig\\] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/scheduler.conf\u0026quot; \\[controlplane\\] wrote Static Pod manifest for component kube-apiserver to \u0026quot;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026quot; \\[controlplane\\] wrote Static Pod manifest for component kube-controller-manager to \u0026quot;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026quot; \\[controlplane\\] wrote Static Pod manifest for component kube-scheduler to \u0026quot;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026quot; \\[init\\] waiting for the kubelet to boot up the control plane as Static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot; \\[init\\] this might take a minute or longer if the control plane images have to be pulled \\[apiclient\\] All control plane components are healthy after 71.020852 seconds \\[uploadconfig\\] storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace \\[kubelet\\] Creating a ConfigMap \u0026quot;kubelet-config-1.11\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster \\[markmaster\\] Marking the node ctrl1 as master by adding the label \u0026quot;node-role.kubernetes.io/master=''\u0026quot; \\[markmaster\\] Marking the node ctrl1 as master by adding the taints \\[node-role.kubernetes.io/master:NoSchedule\\] \\[patchnode\\] Uploading the CRI Socket information \u0026quot;/var/run/dockershim.sock\u0026quot; to the Node API object \u0026quot;ctrl1\u0026quot; as an annotation \\[bootstraptoken\\] using token: e69t47.k98pkcidzvgexwbz \\[bootstraptoken\\] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials \\[bootstraptoken\\] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token \\[bootstraptoken\\] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster \\[bootstraptoken\\] creating the \u0026quot;cluster-info\u0026quot; ConfigMap in the \u0026quot;kube-public\u0026quot; namespace \\[addons\\] Applied essential addon: CoreDNS \\[endpoint\\] WARNING: port specified in api.controlPlaneEndpoint overrides api.bindPort in the controlplane address \\[addons\\] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f \\[podnetwork\\].yaml\u0026quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join k8s-api.example.com:443 --token e69t47.k98pkcidzvgexwbz --discovery-token-ca-cert-hash sha256:b0e28afb25529ad1405d6adecd4a154ace51b6245ff59477f5ea465e221936de \\[root@ctrl1 ~\\]# ```1台目のコントロールプレーンサーバーが出来ました。ここで表示される `kubeadm join ...` コマンドは worker node を追加する際に使用しますが、忘れてしまっても `kubeadm token create` で作成可能です。忘れなくても 24 時間で有効期限が切れます。`kubeadm token list`, `kubeadm token delete` で確認や削除も可能です。 cert hash の方は次のようにして取得できます。``` openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.\\* //' \\# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9275bcc15515 1d3d7afd77d1 \u0026quot;/usr/local/bin/ku...\u0026quot; 54 minutes ago Up 54 minutes k8s\\_kube-proxy\\_kube-proxy-csbw4\\_kube-system\\_e2d559d3-a14f-11e8-afea-5e45cb53c031\\_0 a87d07e0927d k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 54 minutes ago Up 54 minutes k8s\\_POD\\_kube-proxy-csbw4\\_kube-system\\_e2d559d3-a14f-11e8-afea-5e45cb53c031\\_0 aa6575c2ebe6 214c48e87f58 \u0026quot;kube-apiserver --...\u0026quot; 55 minutes ago Up 55 minutes k8s\\_kube-apiserver\\_kube-apiserver-ctrl1\\_kube-system\\_7a1112ade7b8b6480eabec800c07c9ce\\_0 7a92eac6f28d 55b70b420785 \u0026quot;kube-controller-m...\u0026quot; 55 minutes ago Up 55 minutes k8s\\_kube-controller-manager\\_kube-controller-manager-ctrl1\\_kube-system\\_7954798510b3874bbd133bb6fceac113\\_0 997e4d0d65ff 0e4a34a3b0e6 \u0026quot;kube-scheduler --...\u0026quot; 55 minutes ago Up 55 minutes k8s\\_kube-scheduler\\_kube-scheduler-ctrl1\\_kube-system\\_31eabaff7d89a40d8f7e05dfc971cdbd\\_0 4eaff0f0743e k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 56 minutes ago Up 55 minutes k8s\\_POD\\_kube-apiserver-ctrl1\\_kube-system\\_7a1112ade7b8b6480eabec800c07c9ce\\_0 f344f457f0d2 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 56 minutes ago Up 55 minutes k8s\\_POD\\_kube-controller-manager-ctrl1\\_kube-system\\_7954798510b3874bbd133bb6fceac113\\_0 18a8824b734c k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 56 minutes ago Up 55 minutes k8s\\_POD\\_kube-scheduler-ctrl1\\_kube-system\\_31eabaff7d89a40d8f7e05dfc971cdbd\\_0 /etc/kubernetes/admin.conf を $HOME/.kube/config にコピーすると kubectl コマンドでアクセスできるようになります。(insall-kubectl)\n$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcdf6894-c444w 0/1 Pending 0 55m kube-system coredns-78fcdf6894-v95pf 0/1 Pending 0 55m kube-system kube-apiserver-ctrl1 1/1 Running 0 54m kube-system kube-controller-manager-ctrl1 1/1 Running 0 55m kube-system kube-proxy-csbw4 1/1 Running 0 55m kube-system kube-scheduler-ctrl1 1/1 Running 0 58m ```おや？coredns が Pending なのはなぜだ？？ `/etc/kubernetes/` 配下のファイルは次のようになっています # find /etc/kubernetes/ -type f /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/apiserver-etcd-client.key /etc/kubernetes/pki/ca.key /etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/sa.key /etc/kubernetes/pki/sa.pub /etc/kubernetes/pki/front-proxy-ca.key /etc/kubernetes/pki/front-proxy-ca.crt /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/manifests/kube-controller-manager.yaml /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf\n ### Copy required files to the correct locations `kubeadm init` で作成された次のファイルを他のコントロールプレーン用ホストにコピーします * `/etc/kubernetes/pki/ca.crt` * `/etc/kubernetes/pki/ca.key` * `/etc/kubernetes/pki/sa.key` * `/etc/kubernetes/pki/sa.pub` * `/etc/kubernetes/pki/front-proxy-ca.crt` * `/etc/kubernetes/pki/front-proxy-ca.key` for host in HOST1 HOST2; do scp /etc/kubernetes/pki/ca.crt \\ /etc/kubernetes/pki/ca.key \\ /etc/kubernetes/pki/sa.key \\ /etc/kubernetes/pki/sa.pub \\ /etc/kubernetes/pki/front-proxy-ca.crt \\ /etc/kubernetes/pki/front-proxy-ca.key $host: done\n ### Set up the other control plane nodes 1. コピーしたファイルが次の場所にあることを確認 * `/etc/kubernetes/pki/apiserver-etcd-client.crt` * `/etc/kubernetes/pki/apiserver-etcd-client.key` * `/etc/kubernetes/pki/ca.crt` * `/etc/kubernetes/pki/ca.key` * `/etc/kubernetes/pki/front-proxy-ca.crt` * `/etc/kubernetes/pki/front-proxy-ca.key` * `/etc/kubernetes/pki/sa.key` * `/etc/kubernetes/pki/sa.pub` * `/etc/kubernetes/pki/etcd/ca.crt` 2. `kubeadm init --config kubeadm-config.yaml`をそれぞれのホストで実行する（`kubeadm join` ではない） $ kubectl get pods \u0026ndash;all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcdf6894-c444w 0/1 Pending 0 1h kube-system coredns-78fcdf6894-v95pf 0/1 Pending 0 1h kube-system kube-apiserver-ctrl1 1/1 Running 0 1h kube-system kube-apiserver-ctrl2 1/1 Running 0 11m kube-system kube-apiserver-ctrl3 1/1 Running 0 4s kube-system kube-controller-manager-ctrl1 1/1 Running 0 1h kube-system kube-controller-manager-ctrl2 1/1 Running 0 11m kube-system kube-controller-manager-ctrl3 1/1 Running 0 4s kube-system kube-proxy-csbw4 1/1 Running 0 1h kube-system kube-proxy-vtq7h 1/1 Running 0 24s kube-system kube-proxy-xdl86 1/1 Running 0 11m kube-system kube-scheduler-ctrl1 1/1 Running 0 1h kube-system kube-scheduler-ctrl2 1/1 Running 0 11m kube-system kube-scheduler-ctrl3 1/1 Running 0 4s\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION ctrl1 NotReady master 1h v1.11.2 ctrl2 NotReady master 11m v1.11.2 ctrl3 NotReady master 10s v1.11.2\n### Installing a pod network add-on [Calico](https://www.projectcalico.org/) をインストール kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml\n $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-node-nq9mr 2/2 Running 0 31s calico-node-pt4kk 2/2 Running 0 31s calico-node-qf29t 2/2 Running 0 31s coredns-7d4db77c45-5vzf2 1/1 Running 0 34m coredns-7d4db77c45-jkgdj 1/1 Running 0 34m kube-apiserver-ctrl1 1/1 Running 1 2h kube-apiserver-ctrl2 1/1 Running 1 1h kube-apiserver-ctrl3 1/1 Running 1 54m kube-controller-manager-ctrl1 1/1 Running 1 2h kube-controller-manager-ctrl2 1/1 Running 1 1h kube-controller-manager-ctrl3 1/1 Running 2 54m kube-proxy-csbw4 1/1 Running 1 2h kube-proxy-vtq7h 1/1 Running 1 55m kube-proxy-xdl86 1/1 Running 1 1h kube-scheduler-ctrl1 1/1 Running 1 2h kube-scheduler-ctrl2 1/1 Running 1 1h kube-scheduler-ctrl3 1/1 Running 1 54m\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION ctrl1 Ready master 2h v1.11.2 ctrl2 Ready master 1h v1.11.2 ctrl3 Ready master 55m v1.11.2\nCalico をインストールする前に CoreDNS が起動しないな？と調べてて `allowPrivilegeEscalation: false` を `true` にするか SELinux を無効にする必要があるっていう issue を見つけて、先にやってしまったのだが、必要だったかどうかを今度確認する。不要でした。 [CoreDNS not started with k8s 1.11 and weave (CentOS 7)](https://github.com/kubernetes/kubeadm/issues/998) ### Worker node の追加 [kubeadm join](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/) で追加します。","date":"2018年8月16日","permalink":"/2018/08/creating-highly-available-clusters-with-kubeadm/","section":"Posts","summary":"etcd を外部にもつ HA な Kubernetes クラスタを kubeadm で構築します。Creating Highly Available Clusters with kubeadm をなぞります。 各種バージョン情報 # CentOS Linux release 7.5.1804 kubeadm v1.11.2 (GitCommit:\u0026ldquo;bb9ffb1654d4a729bb4cec18ff088eacc153c239\u0026rdquo;) docker 1.13.1 (docker-1.13.1-68.gitdded712.el7.centos.x86_64) kubelet v1.11.2 (Kubernetes v1.11.2) etcd 3.2.18 Set","title":"kubeadm で外部 etcd で HA な Kubernetes クラスタをセットアップする"},{"content":"This is an example page. It\u0026rsquo;s different from a blog post because it will stay in one place and will show up in your site navigation (in most themes). Most people start with an About page that introduces them to potential site visitors. It might say something like this:\n Hi there! I\u0026rsquo;m a bike messenger by day, aspiring actor by night, and this is my website. I live in Los Angeles, have a great dog named Jack, and I like piña coladas. (And gettin' caught in the rain.)\n \u0026hellip;or something like this:\n The XYZ Doohickey Company was founded in 1971, and has been providing quality doohickeys to the public ever since. Located in Gotham City, XYZ employs over 2,000 people and does all kinds of awesome things for the Gotham community.\n As a new WordPress user, you should go to your dashboard to delete this page and create new pages for your content. Have fun!\n","date":"2018年8月15日","permalink":"/2018/08/sample-page-2/","section":"Posts","summary":"This is an example page. It\u0026rsquo;s different from a blog post because it will stay in one place and will show up in your site navigation (in most themes). Most people start with an About page that introduces them to potential site visitors. It might say something like this:\n Hi there! I\u0026rsquo;m a bike messenger by day, aspiring actor by night, and this is my website. I live in Los Angeles, have a great dog named Jack, and I like piña coladas.","title":"Sample Page"},{"content":"","date":"2018年8月13日","permalink":"/tags/etcd/","section":"Tags","summary":"","title":"etcd"},{"content":"Set up a Highly Availabile etcd Cluster With kubeadm をなぞります。\n各種バージョン情報 #  CentOS Linux release 7.5.1804 kubeadm v1.11.2 (GitCommit:\u0026ldquo;bb9ffb1654d4a729bb4cec18ff088eacc153c239\u0026rdquo;) docker 1.13.1 (docker-1.13.1-68.gitdded712.el7.centos.x86_64) kubelet v1.11.2 (Kubernetes v1.11.2) etcd 3.2.18  Before you begin #  互いに 2379/tcp (client ↔ server), 2380/tcp (server ↔ server peering) ポートで通信可能な 3 台のホスト (ポート番号を変更する場合は kubeadm の設定を変更する必要がある) それぞれのホストには docker, kubelet, kubeadm がインストールされていること (Installing kubeadm) ssh や scp で互いにファイルをコピーできること  Setting up the cluster # ひとつのホストですべての証明書を作成し、他のホストへ配ります。\n  kubelet が etcd のサービスマネージャとなるように設定する\netcd は kubernetes よりもシンプルであり、kubeadm が生成する kubelet の unit ファイルを上書きする必要があります\nmkdir /var/lib/kubelet cat \u0026lt;\u0026lt; EOF \u0026gt; /var/lib/kubelet/config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: \u0026#34;systemd\u0026#34; address: \u0026#34;127.0.0.1\u0026#34; staticPodPath: \u0026#34;/etc/kubernetes/manifests\u0026#34; authentication: x509: clientCAFile: \u0026#34;/etc/kubernetes/pki/etcd/ca.crt\u0026#34; enabled: true webhook: enabled: false anonymous: enabled: false authorization: mode: \u0026#34;AlwaysAllow\u0026#34; EOF staticPodPath で指定したディレクトリに手順の後の方で出てくる kubeadm コマンドで etcd.yaml が出力されます。webhook での認証・認可環境がないので無効にして認証の x509 を有効にして認可を AlwaysAllow にしてあります。x509 ではなく anonymous を有効にするという方法もある\ncat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_CONFIG_ARGS --allow-privileged=true Restart=always EOF systemctl daemon-reload $KUBELET_CONFIG_ARGS は --config=/var/lib/kubelet/config.yaml となっている、--allow-privileged オプションは将来無くなるらしいが、そもそも etcd の実行に必要なんだろうか？\n  kubeadm の設定ファイルを作成する\n次のスクリプトを使って各ホストの kubeadm 用設定ファイルを生成します\n# Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts export HOST0=10.0.0.6 export HOST1=10.0.0.7 export HOST2=10.0.0.8 # Create temp directories to store files that will end up on other hosts. mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2}) NAMES=(\u0026#34;etcd1\u0026#34; \u0026#34;etcd2\u0026#34; \u0026#34;etcd3\u0026#34;) for i in \u0026#34;${!ETCDHOSTS[@]}\u0026#34;; do HOST=${ETCDHOSTS[$i]} NAME=${NAMES[$i]} cat \u0026lt;\u0026lt; EOF \u0026gt; /tmp/${HOST}/kubeadmcfg.yaml apiVersion: \u0026#34;kubeadm.k8s.io/v1alpha2\u0026#34; kind: MasterConfiguration etcd: local: serverCertSANs: - \u0026#34;${HOST}\u0026#34; peerCertSANs: - \u0026#34;${HOST}\u0026#34; extraArgs: initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380 initial-cluster-state: new name: ${NAME} listen-peer-urls: https://${HOST}:2380 listen-client-urls: https://${HOST}:2379 advertise-client-urls: https://${HOST}:2379 initial-advertise-peer-urls: https://${HOST}:2380 EOF done   認証局(CA)ファイルの生成\nすでに CA が存在すれが、その crt と key ファイルを /etc/kubernetes/pki/etcd/ca.crt, /etc/kubernetes/pki/etcd/ca.key にコピーするだけです。\nCA ファイルがまだない場合は次のコマンドを $HOST0 （kubeadm 用の設定ファイルを作成したホスト）上で実行します\nkubeadm alpha phase certs etcd-ca これで次の2ファイルが生成されます\n /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/ca.key    各ホスト用の証明書を作成する\nkubeadm alpha phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST2}/ # cleanup non-reusable certificates find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete kubeadm alpha phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST1}/ find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete kubeadm alpha phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm alpha phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm alpha phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm alpha phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml # No need to move the certs because they are for HOST0 # clean up certs that should not be copied off this host find /tmp/${HOST2} -name ca.key -type f -delete find /tmp/${HOST1} -name ca.key -type f -delete   証明書と kubeadm 用の設定ファイルをコピーする\nscp -r /tmp/${HOST1}/* ${HOST1}: ssh ${HOST1} \u0026#34;sudo chown -R root:root pki; sudo mv pki /etc/kubernetes/\u0026#34; scp -r /tmp/${HOST2}/\\* ${HOST2}: ssh ${HOST2} \u0026#34;sudo chown -R root:root pki; sudo mv pki /etc/kubernetes/\u0026#34;   必要なファイルが存在していることを確認\n$HOST0\n/tmp/${HOST0} └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── ca.key ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key $HOST1\n$HOME └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key $HOST2\n$HOME └── kubeadmcfg.yaml --- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key apiserver-etcd-client.crt Subject O=system:masters, CN=kube-apiserver-etcd-client server.crt Subject CN=etcd1 SAN DNS:etcd1, DNS:localhost, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1, IP Address:${HOSTn} healthcheck-client.crt Subject O=system:masters, CN=kube-etcd-healthcheck-client peer.crt Subject CN=etcd1 SAN DNS:etcd1, DNS:localhost, IP Address:${HOST0}, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1, IP Address:${HOSTn} etcd1 は kubeadm コマンドを実行した1台目のホスト名\n  静的Podマニフェストを作成\n証明書と設定ファイルのコピーが終わったらマニフェストを作成します。各ホストで kubeadm コマンドを使ってマニフェストを作成します。\nroot@HOST0 $ kubeadm alpha phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml root@HOST1 $ kubeadm alpha phase etcd local --config=kubeadmcfg.yaml root@HOST2 $ kubeadm alpha phase etcd local --config=kubeadmcfg.yaml   任意：クラスタの状態を確認する\ndocker run --rm -it \\ --net host \\ -v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:v3.2.18 etcdctl \\ --cert-file /etc/kubernetes/pki/etcd/peer.crt \\ --key-file /etc/kubernetes/pki/etcd/peer.key \\ --ca-file /etc/kubernetes/pki/etcd/ca.crt \\ --endpoints https://${HOST0}:2379 cluster-health 出力に cluster is healthy があれば etcd クラスタは正常に稼働しているはずです\n  ","date":"2018年8月13日","permalink":"/2018/08/setup-ha-etcd-with-kubeadm/","section":"Posts","summary":"Set up a Highly Availabile etcd Cluster With kubeadm をなぞります。 各種バージョン情報 # CentOS Linux release 7.5.1804 kubeadm v1.11.2 (GitCommit:\u0026ldquo;bb9ffb1654d4a729bb4cec18ff088eacc153c239\u0026rdquo;) docker 1.13.1 (docker-1.13.1-68.gitdded712.el7.centos.x86_64) kubelet v1.11.2 (Kubernetes v1.11.2) etcd 3.2.18 Before you begin # 互いに 2379/tcp (client ↔ server), 2380/tcp (server ↔ server peering) ポートで通信可能な 3 台の","title":"kubeadm で HA な etcd をセットアップ"},{"content":"我が家の無線LAN環境改善のために TP-Link Deco M5 3台セットを導入しました。\nこの記事公開後に新しいファームウエアが出てます「Deco M5 が VPN、WPS、LANアドレス設定に対応」\n導入前 # これまでの環境は次のようになっており、2つの無線LANアクセスポイントがそれぞれ別の SSID で設置していました。安いのでいいやって適当に2台目を買ったせいでシームレスな移動ができない状況でした。\ndeco M5 導入前  「無線AP(B)」というやつがブリッジモードにできず、同じ SSID ではうまくハンドオーバーできなかったので別の SSID にして部屋の移動時には手動で切り替えたりしていました。切り替えないで、遠い AP を使い続けると、他の近い端末まで遅くなってしまいます。\n導入後 # deco M5 導入後  Ethernet Backhaul を使うようにしました（これが使えるのもルーターモードだけでした）。でも、部屋の間をつないでいるマンション備え付けのHUBが 100Mbps なんですよね\u0026hellip; 今どきの無線LAN規格の方が速い。\nクローゼット内のマルチメディアボックス？に入ってる HUB も 1Gbps 対応のものに交換しようかな。Panasonic のやつは高いな。あまりケチると電源周りが心配になるけど。\n壁掛け # 無線APはできれば高い位置に設置することが望ましいとされています。deco M5 にはサードパーティ製品ですが、壁掛け、天井取付も可能なホルダーがあるので、これを購入して壁にかけました。壁にネジの穴を開けるのはちょっと気になるのでフックにかけました。\n   IPv6対応 # IPv6 はこれまで通り使えるようにしたいので設定アプリの「詳細設定」から「パススルー」を選択しました。「動的IP」では IPv6 アドレスが配れれなかった。パススルーの場合セキュリティ関連機能はどうなるのかな？\nセキュリティ、ペアレンタルコントロール # 端末ごとにグループ分け（アプリの UI 上は誰の端末かを指定する）して、アダルトサイトなどへのアクセス制限をしたり、1日何時間の通信を許可するか、通信を許可する時間帯を制限するかを設定できます。スマホなど、無線LAN以外の通信方法がある場合はそれで通信できちゃうから単にギガの無駄遣いに終わる可能性がありますね\u0026hellip;\nAlexa との連携 # Alexa スキル TP-Link ルーター\n 来客時にゲスト用のWi-Fiを開放する WPS機能を使用して、パスワード入力の手間を省いてWi-Fiに端末を接続する 夜にルーターのLEDを消す インターネット回線の速度を測る QoS設定をして特定のデバイスやアクティビティを優先させる  うーん、使う必要なさそう・・・ アレクサっていう名前すら出てこないことがあるのに、コマンドなんて覚えられないよね、天気予報とタイマー以外使ってない・・・\nIFTTT 連携 #  Trigger and Action list of IFTTT that Deco supports? Do more with TP-Link Router - IFTTT  これも、今すぐには使う予定はないな\ndeco M5 紹介サイト #  なにこのサクサクつながっていくWi-Fiルーター！TP-Link『Deco M5』が快適すぎる - 週刊アスキー GoogleWiFiと、TP-Link社のDECOの違いを比べてみた！オススメはどっち！？ Google Wifi vs. deco M5　メッシュ対応Wi-Fiルーター徹底比較 (1/6) - ITmedia PC USER Google Wifiより安いTP-LINK Deco m5で問題なし - 就職氷河期 最初の世代、残り20年のための… TP-Link Deco M5 による家庭内メッシュ Wi-Fi 環境 - Qiita  ","date":"2018年8月13日","permalink":"/2018/08/deco-m5/","section":"Posts","summary":"我が家の無線LAN環境改善のために TP-Link Deco M5 3台セットを導入しました。 この記事公開後に新しいファームウエアが出てます「Deco M5 が VPN、WPS","title":"Deco M5 を softbank 光環境に導入しました"},{"content":"\u0026ldquo;Installing kubeadm - Kubernetes\u0026rdquo; を参考に CentOS 7 に kubeadm をインストールします\n各種バージョン情報 #  CentOS Linux release 7.5.1804 kubeadm v1.11.2 (GitCommit:\u0026ldquo;bb9ffb1654d4a729bb4cec18ff088eacc153c239\u0026rdquo;) docker 1.13.1 (docker-1.13.1-68.gitdded712.el7.centos.x86_64) kubelet v1.11.2 (Kubernetes v1.11.2)  Before you begin #  サポートされている OS は沢山あるが今回は CentOS 7 を使います 2GB 以上のメモリ 2つ以上の CPU ホストは互いに通信可能なこと 一意な hostname, MAC address, product_uuid (`sudo cat /sys/class/dmi/id/product_uuid`) 必要なポートが開いていること(用途によって異なる) swap の無効化 (kubelet の動作のために必要)  Installing Docker # yum install -y docker systemctl enable docker \u0026amp;\u0026amp; systemctl start docker Installing kubeadm, kubelet and kubectl # 次のパッケージをインストールします\nkubeadm\nクラスタ構築用コマンド\nkubelet\nクラスタ内のすべてのホストで実行され、pod やコンテナの起動などを行う\nkubectl\nクラスタ操作用コマンド\nkubeadm は kubelet と kubectl のインストールや更新を行わないため、kubeadm でインストールされる kubernetes のコントロールプレーンバージョンと合わせる必要があります。kubelet のバージョンはコントロールプレーンのバージョンの一つ前のマイナーバージョンまではサポートされます。たとえば、1.7.0 の kubelet は 1.8.0 のコントロールプレーンとは互換性があります。\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kube* EOF setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=permissive/' /etc/selinux/config yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes mkdir /var/lib/kubelet cat \u0026lt;\u0026lt;EOF \u0026gt; /var/lib/kubelet/config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: \u0026quot;systemd\u0026quot; EOF systemctl enable kubelet SELinux を無効にするため setenforce 0 の実行が必要です。kubelet の SELinux 対応が改善されるまではこれが必要です。 永続化\nsed -i 's/^SELINUX=.*/SELINUX=permissive/' /etc/selinux/config iptables がバイパスされるために正しくルーティングされないという問題が報告されているため sysctl で net.bridge.bridge-nf-call-iptables を 1 にする必要があります。\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system Configure cgroup driver used by kubelet on Master Node # 「Docker を使う場合、kubeadm は自動で cgroup driver を検出し、/var/lib/kubelet/kubeadm-flags.env にセットします」。別の CRI を使う場合は /etc/default/kubelet で次のように cgroup-driver を指定する必要があります」と書いてあるけど CentOS 7 の docker では kubeadm-flags.env は生成されなかった。 ドキュメントには KUBELET_KUBEADM_EXTRA_ARGS とあるが、systemd の unit ファイルにこの変数は無いので KUBELET_EXTRA_ARGS だろうか。\nKUBELET_EXTRA_ARGS=--cgroup-driver=\u0026lt;value\u0026gt; 変更の反映には kubelet の次のようにして再起動する必要があります\nsystemctl daemon-reload systemctl restart kubelet しかし、1.10 以降、--cgroup-driver などの引数はすべて DEPRECATED となっており --config で指定するファイルに書けということになっている。\u0026quot;Set Kubelet parameters via a config file\u0026quot; 項目と値は types.go を見て探す。 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf に\nEnvironment=\u0026quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\u0026quot; と書かれているのでこのファイルを使うのが良いと思うが、ディレクトリは存在しないので作成する必要がある。設定内容はセットアップするものや環境に依存するようなので、それぞれのセットアップにて試してみる。 参考： Kubernetes 1.10のkubeletの起動オプションをKubelet ConfigファイルとPodSecurityPolicyで置き換える\n後に、コントロールプレーン用に kubeadm init を実行したらその処理の中で /var/lib/kubelet/kubeadm-flags.env と /var/lib/kubelet/config.yaml が作成されていた。\n[preflight/images] Pulling images required for setting up a Kubernetes cluster [preflight/images] This might take a minute or two, depending on the speed of your internet connection [preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [preflight] Activating the kubelet service Troubleshooting # なにか問題にぶつかったら \u0026ldquo;Troubleshooting kubeadm\u0026rdquo; を見てみましょう。\n","date":"2018年8月11日","permalink":"/2018/08/install-kubeadm-on-centos7/","section":"Posts","summary":"\u0026ldquo;Installing kubeadm - Kubernetes\u0026rdquo; を参考に CentOS 7 に kubeadm をインストールします 各種バージョン情報 # CentOS Linux release 7.5.1804 kubeadm v1.11.2 (GitCommit:\u0026ldquo;bb9ffb1654d4a729bb4cec18ff088eacc153c239\u0026rdquo;) docker 1.13.1 (docker-1.13.1-68.gitdded712.el7.centos.x86_64) kubelet v1.11.2 (Kubernetes v1.11.2) Before you begin # サポートされている OS は沢山あるが今回は CentOS","title":"CentOS 7 に kubeadm をインストール"},{"content":"","date":"2018年7月17日","permalink":"/tags/redash/","section":"Tags","summary":"","title":"Redash"},{"content":"Redash でクエリの実行は redis の queue を通して celery にて実行されているようです、この celery で実行している処理が不慮の事故で死んでしまうと Redash の画面上ではずっと IN PROGRESS 状態として残ってしまうという問題がありました。これを削除するためにやったことのメモです。 最初は Database (PostgreSQL) に入ってるのかなと思ったけど、それらしきレコードは見つかりませんでした``` List of relations Schema | Name | Type | Owner \u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026ndash; public | access_permissions | table | redash public | alembic_version | table | redash public | alert_subscriptions | table | redash public | alerts | table | redash public | api_keys | table | redash public | changes | table | redash public | dashboards | table | redash public | data_source_groups | table | redash public | data_sources | table | redash public | events | table | redash public | groups | table | redash public | notification_destinations | table | redash public | organizations | table | redash public | queries | table | redash public | query_results | table | redash public | query_snippets | table | redash public | users | table | redash public | visualizations | table | redash public | widgets | table | redash\nそれでは redis だろうと **\\`redis-cli keys \\\\\\*\\`** でキーの一覧を出してみる data_source:schema:1 data_source:schema:2 data_source:schema:3 data_source:schema:4 data_source:schema:5 data_source:schema:6 data_source:schema:7 data_source:schema:8 data_source:schema:9 data_source:schema:10 _kombu.binding.celery _kombu.binding.celeryev _kombu.binding.celery.pidbox _kombu.binding.queries _kombu.binding.scheduled_queries _kombu.binding.schemas new_version_available query_task_trackers:done redash:status schemas sq:executed_at celery-task-meta-UUID \u0026lt;\u0026ndash; いろんな UUID で沢山 query_task_tracker:UUID \u0026lt;\u0026ndash; いろんな UUID で沢山\nUUID はログから分かっていたので **keys \\*UUID\\*** を試してみたら **query\\_task\\_tracker:164b9632-40a5-4af4-8500-21022b06215d** というのがあった。 中身は JSON でこんなのが入ってました。 { \u0026ldquo;username\u0026rdquo;: \u0026ldquo;user@example.com\u0026rdquo;, \u0026ldquo;retries\u0026rdquo;: 0, \u0026ldquo;started_at\u0026rdquo;: 1527501588.652453, \u0026ldquo;task_id\u0026rdquo;: \u0026ldquo;164b9632-40a5-4af4-8500-21022b06215d\u0026rdquo;, \u0026ldquo;created_at\u0026rdquo;: 1527501564.920558, \u0026ldquo;updated_at\u0026rdquo;: 1527501588.652842, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;executing_query\u0026rdquo;, \u0026ldquo;query_id\u0026rdquo;: 91, \u0026ldquo;run_time\u0026rdquo;: null, \u0026ldquo;scheduled\u0026rdquo;: false, \u0026ldquo;scheduled_retries\u0026rdquo;: 0, \u0026ldquo;data_source_id\u0026rdquo;: 2, \u0026ldquo;query_hash\u0026rdquo;: \u0026ldquo;2b0cc6eac4188b1b0097b02b9d4f1f6e\u0026rdquo; }\nでも IN PROGRESS のリストにこの情報が出るってことはこの key=value だけじゃなくてどこかにこの key を持ったリストのようなデータがあるはず、ということで **redis-cli monitor** で redis に対してどんなリクエストが発行されてるかを眺めてみました。 上に並べた **keys \\*** のリストは IN PROGRESS のクエリがない状態なので存在しませんが、**monitor** コマンドで確認したら **query\\_task\\_trackers:in\\_progress** というキーがありました。名前からしてこれですね。 さて、**query\\_task\\_trackers:in\\_progress** の型は何だろうか？ そんな場合は [type](https://redis.io/commands/type) コマンドで確認できます。これは **zset** でした。 \u0026gt; type query_task_trackers:in_progress zset\n**zset** のリストを全部取り出すには次の様に [zrange](https://redis.io/commands/zrange) で 0 から -1 までと指定します \u0026gt; zrange query_task_trackers:in_progress 0 -1\nこの zset から当該の task 情報を削除すれば良さそうなので [zrem](https://redis.io/commands/zrem) で消します \u0026gt; zrem query_task_trackers:in_progress query_task_tracker:164b9632-40a5-4af4-8500-21022b06215d\n","date":"2018年7月17日","permalink":"/2018/07/cleanup-redash-in-progress-task/","section":"Posts","summary":"Redash でクエリの実行は redis の queue を通して celery にて実行されているようです、この celery で実行している処理が不慮の事故で死んでしまうと Redash の画面上ではずっと IN PROGRESS 状","title":"Redash でコケてしまったタスク情報の掃除"},{"content":"","date":"2018年7月17日","permalink":"/tags/redis/","section":"Tags","summary":"","title":"redis"},{"content":"play1 には status というサブコマンドがあります。 status.py, PlayStatusPlugin.java あたりのコード。 コマンドのドキュメント (cmd-status.txt)\n$ play status APPDIR port がデフォルトと違う場合は port を指定\n$ play status APPDIR --http.port=8080 環境ごとの設定を行っている場合は --%ENV で指定\n$ play status APPDIR --%prod JVM のメモリ情報、Thread の状態や、Request と Job を実行する thread pool の情報\nJava: ~~~~~ Version: 1.8.0_171 Home: /usr/java/jdk1.8.0_171/jre Max memory: 4294967296 Free memory: 2981053320 Total memory: 4294967296 Available processors: 4 Requests execution pool: ~~~~~~~~~~~~~~~~~~~~~~~~ Pool size: XX Active count: XX Scheduled task count: XXXXX Queue size: X URL にマップされているメソッドごとの実行回数(hit), 実行時間(avg, min, max)、 DBCP 設定、Job の情報などが確認できます。 status コマンドは内部で /@status に HTTP でアクセスしてそのレスポンスをそのまま返しています。 /@status.json という URL も用意されており、JSON でレスポンスが返されます。ただし、残念ながら情報量がずっと少ない、バグかな？ ただ、HTTP でアクセスしても 401 Unauthorized となります、認証を通すためには conf/application.conf で設定してある secret が必要です。 status コマンドは python で次のようになっているので\nhm = hmac.new(secret_key, \u0026#39;@status\u0026#39;, sha) authorization = hm.hexdigest() shell では openssl で次のようにして得ることができます\necho -n \u0026#39;@status\u0026#39; | openssl sha1 -hmac \u0026#39;secret_key\u0026#39; これを Authorization ヘッダーとして渡せばアクセスできます。\n","date":"2018年7月15日","permalink":"/2018/07/play1-status-endpoing/","section":"Posts","summary":"play1 には status というサブコマンドがあります。 status.py, PlayStatusPlugin.java あたりのコード。 コマンドのドキュメント (cmd-status.txt) $ play status APPDIR port がデフォルトと違う場合は port を指定 $ play status APPDIR --http.port=8080 環境ご","title":"play1 の @status endpoing"},{"content":"","date":"2018年7月15日","permalink":"/tags/playframework/","section":"Tags","summary":"","title":"PlayFramework"},{"content":"Rancher 2.0 を使うメリットとして GUI でポチポチすることで helm から簡単にデプロイできたり、ポチポチ Ingress Controller の設定を行ったりできることもありますが、そのためには誰がどのリソースに対して何を行えるのかを制御できる必要があります。「Understanding Authentication \u0026amp; Authorization In Rancher 2.0」に Rancher 2.0 の認証・認可についての説明がありました。Rancher 2.0 導入のメリットはココが大きいんじゃないかなと思っている。 マルチクラウド＆マルチクラスターで統一した認証・認可ができます\n Project # Rancher 2.0 には Kubernetes には無い Project というレイヤーがあります。Project は namespace の集合です。そしてこの Project に対して Policy を設定します。Policy は RBAC, Network acccess, Pod security, Quota を管理します。\nTemplate # roleTemplates, clusterRoleTemplateBindings, projectRoleTemplateBindings を使って同じ設定を簡単に使い回せます。管理者が作成したこれらのテンプレートを Cluster や Project のオーナーが使う（ユーザーに割り当てる）ことができます。\nOwner # ユーザーは自分のクラスタを作成し、その所有者となることができます。そして、他のユーザーやグループにクラスタの権限を設定できます。クラスタのメンバーとなったユーザーはプロジェクトを作成しそのオーナーになる。そして、また別のユーザーをプロジェクトのメンバーやオーナーとして招待することができます。プロジェクトメンバーはネームスペースを作成してワークロードをデプロイすることができます。\npodSecurityPoicy # どんなコンテナをデプロイ可能にするかを制御するために kubernetes には podSecurityPolicies がありますが、Rancher はこれの使いやすさを大きく改善しています。podSecurityPoicy テンプレートを管理でえきます。特権の使用を制限したり、使用可能な Capability を制限したり、使用可能な Volume Type を制限したり、ホストのファイルシステムへのアクセスを制限したりできます。 クラスタ所有者はプロジェクトごとに例外を管理することもできます。\n認証 # 現状では Active DIrectory と GitHub での認証だけですが、今後 2.1 などで増えていく計画のようです（Rancher 1.x では Active Directory, Azure AD, GitHub, OpenLDAP, Shibboleth に対応しています）\nマルチクラウド # GKE (Google), AKS (Azure), EKS (AWS) はたまたオンプレの自前クラスタのどれを使っていても Rancher 経由で統一的に管理できます。それぞれにユーザーや権限を個別に管理する必要がないのです。 まあ、私も使うかどうかわかりませんけどね。\n","date":"2018年5月29日","permalink":"/2018/05/understanding-authentication-authorization-in-rancher-2-0/","section":"Posts","summary":"Rancher 2.0 を使うメリットとして GUI でポチポチすることで helm から簡単にデプロイできたり、ポチポチ Ingress Controller の設定を行ったりできることもありますが、そのためには","title":"Understanding Authentication \u0026 Authorization In Rancher 2.0"},{"content":"","date":"2018年5月26日","permalink":"/tags/lego/","section":"Tags","summary":"","title":"Lego"},{"content":"","date":"2018年5月26日","permalink":"/tags/lets-encrypt/","section":"Tags","summary":"","title":"Let's Encrypt"},{"content":"前回、運悪く起動しないタイミングで試してしまった Rancher server の HA セットアップですが、その後、当該変更が Revert されていたので再度試せばうまくいきそうです。で、ただ同じことを繰り返しても面白くないので Terraform の DigitalOcean Provider を使って構築してみます。\nコードは GitHub に置いてあります。1コマンドで簡単に構築・削除できます。\nRancher 2.0 リリースパーティ (Rancher Meetup Tokyo #12) で @yamamoto-febc さんが RKE Provider を作ったと発表されていました。気になりますが今回は shell script で rke コマンドを使いました。\nPrerequisite #  DigitalOcean のアカウント 独自ドメインを所有しており、DigitalOcean の DNS サービスに zone 登録（サブドメインでも可） Terraform （tfenv 推奨、下に手順を書いてあります） jq コマンド  rancher.yourdomain.example.com という名前で DNS 登録と Let\u0026rsquo;s Encrypt での証明書取得も行うため、ドメインが必要です。lego というコマンドで dns-01 で証明書取得するため DigitalOcean の DNS サービスへの登録が必要です。（lego はもっと沢山の DNS サービスに対応していますし、もちろん http-01, tls-sni-01 にも対応しています）\ntfenv のインストール # Terraform は version によって結構差があるのでプロジェクトごとに簡単に切り替えられるように tfenv を使います\n$ git clone https://github.com/tfutils/tfenv.git ~/.tfenv $ echo 'PATH=~/.tfenv/bin:$PATH' \u0026gt;\u0026gt; ~/.bash_profile $ tfenv install latest [INFO] Installing Terraform v0.11.7 [INFO] Downloading release tarball from https://releases.hashicorp.com/terraform/0.11.7/terraform_0.11.7_windows_amd64.zip ######################################################################## 100.0% [INFO] Downloading SHA hash file from https://releases.hashicorp.com/terraform/0.11.7/terraform_0.11.7_SHA256SUMS tfenv: tfenv-install: [WARN] No keybase install found, skipping GPG signature verification Archive: tfenv_download.kdvWNs/terraform_0.11.7_windows_amd64.zip inflating: /c/Users/ytera/.tfenv/versions/0.11.7/terraform.exe [INFO] Installation of terraform v0.11.7 successful [INFO] Switching to v0.11.7 [INFO] Switching completed $ terraform version Terraform v0.11.7 $ echo 0.11.7 \u0026gt; .terraform-version git clone # $ git clone https://github.com/yteraoka/rancher-ha-tf-do.git $ cd rancher-ha-tf-do 環境変数設定 # $ export DIGITALOCEAN_TOKEN=*** $ export DOMAIN_SUFFIX=yourdomain.example.com $ export CERT_EMAIL=user@example.com DIGITALOCEAN_TOKEN\nDigitalOcean の Writable な API Token (API ページで取得できます）\nDOMAIN_SUFFIX\nDigitalOcean の DNS サービスに設定したドメイン名、この前に rancher. をつけた FQDN で rancher にアクセスすることになる\nCERT_EMAIL\nLet\u0026rsquo;s Encrypt での証明書取得時に指定するメールアドレス、期限切れ通知が送られてくる\n全部必須です\n実行 # あとは ./make.sh up と実行するだけです。\n$ ./make.sh up 中で実行されること\n SSH 用の key pair 作成（作成するサーバーへのログインに使います） terraform init terraform plan \u0026amp; apply  SSH の public key を登録 3 つの Ubuntu サーバー（droplet）を作成（docker 17.03 をインストール） 作成したサーバーのIPアドレスで DNS レコードを作成   lego コマンドのダウンロード lego で証明書の取得 rke コマンドのダウンロード rke コマンドの設定ファイルテンプレート 3-node-certificate-recognizedca.yml のダウンロード テンプレート内の必要な箇所を置換  サーバーのIPアドレスを terraform output から取得して置換 SSH のユーザー名（DigitalOcean は root） FQDN サーバー証明書、秘密鍵の Base64 文字列   rke up コマンドの実行  削除 # make.sh destroy で terraform destroy を実行することでサーバーとDNSレコードを削除します\n$ ./make.sh destroy make.sh cleanup で lego, rke バイナリや lego で取得した証明書なども削除します\n$ ./make.sh cleanup kubectl でのアクセス # rke コマンドによって kube_config_rke.yml が生成されているので、これを ~/.kube/config にコピーすることで kubectl コマンドでアクセスできます\nkubectl で複数のクラスタにアクセスする場合は kubectx が便利そう（Windows の git-bash で使えるかな？）\nPod 一覧の確認 # $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE cattle-system cattle-84f9cd8589-5k88b 1/1 Running 0 4m cattle-system cattle-cluster-agent-756f689478-t5dm5 1/1 Running 0 26s cattle-system cattle-node-agent-cdqxp 1/1 Running 0 26s cattle-system cattle-node-agent-cgfml 1/1 Running 0 26s cattle-system cattle-node-agent-wdnvn 1/1 Running 0 26s ingress-nginx default-http-backend-564b9b6c5b-phsvc 1/1 Running 0 4m ingress-nginx nginx-ingress-controller-pdmgj 1/1 Running 0 4m ingress-nginx nginx-ingress-controller-pxwdd 1/1 Running 0 4m ingress-nginx nginx-ingress-controller-qkltb 1/1 Running 0 4m kube-system canal-n68nf 3/3 Running 0 4m kube-system canal-twt2r 3/3 Running 0 4m kube-system canal-vh876 3/3 Running 0 4m kube-system kube-dns-5ccb66df65-kjz9w 3/3 Running 0 4m kube-system kube-dns-autoscaler-6c4b786f5-n9gbw 1/1 Running 0 4m kube-system rke-ingress-controller-deploy-job-ltnf5 0/1 Completed 0 4m kube-system rke-kubedns-addon-deploy-job-mpnzc 0/1 Completed 0 4m kube-system rke-network-plugin-deploy-job-vsmb9 0/1 Completed 0 4m kube-system rke-user-addon-deploy-job-rf6r9 0/1 Completed 0 4m cattle-system の daemonset # Rancher 関連は cattle-system というネームスペースに構築されています。各 node で実行される daemonset として cattle-node-agent が\n$ kubectl get daemonsets --namespace=cattle-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE cattle-node-agent 3 3 3 3 3 \u0026lt;none\u0026gt; 14m cattle-system の deployment # deployment は Rancher サーバーの cattle と、この Kubernetes を rancher で管理するための cattle-cluster-agent が\n$ kubectl get deployments --namespace=cattle-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE cattle 1 1 1 1 16m cattle-cluster-agent 1 1 1 1 12m cattle-system の service # service としては Rancher サーバーの cattle 用の cattle-service があります\n$ kubectl get services --namespace=cattle-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cattle-service ClusterIP 10.43.52.123 80/TCP,443/TCP 16m RKE の ingress-controller # RKE で構築した Kubernetes には各 node で daemonset として nginx-ingress-controller が実行されています。この nginx-ingress-controller は Host ヘッダーの値によって Proxy 先の Service を切り替えてくれます。Rancher も指定した rancher.${DOMAIN_SUFFIX} でアクセスすると cattle-service に forward してくれます。https でも SNI のホスト名で振り分けてくれます。\n$ kubectl get daemonsets --namespace=ingress-nginx NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE nginx-ingress-controller 3 3 3 3 3 \u0026lt;none\u0026gt; 23m HA な Rancher サーバーとは？ # Rancher 2.0 のコンテナを Kubernetes 内で起動し、Kubernetes の使っている etcd をデータストアとして使います。今回の RKE で構築するものでは起動後にその Kubernetes を Rancher にインポートするようになっているのでログインするとすぐにその環境にコンテナをデプロイしたりできます。\n   まとめ # HA な Rancher 2.0 環境および Kubernetes クラスタを簡単に作ったり削除したりできるようになりました。\n","date":"2018年5月26日","permalink":"/2018/05/rancher-2-0-ha-install-using-terraform-and-rke/","section":"Posts","summary":"前回、運悪く起動しないタイミングで試してしまった Rancher server の HA セットアップですが、その後、当該変更が Revert されていたので再度試せばうまくいきそうです。","title":"続 Rancher 2.0 の HA 構成を試す"},{"content":"Beta を試すシリーズ（その1、その2）を書いていましたがついに 5月1日に Rancher 2.0 が GA になりました Announcing Rancher 2.0 GA! (Docker Hub にある image の tag はまだ preview でした)\n今回は High Availability 構成をどうやって構築するかを確認してみます\nHigh Availability Installation\nにドキュメントがあります。\nRancher 1.x では Rancher server + MySQL という構成で、MySQL をなんらかの方法で冗長構成として Rancher server を単純に複数用意すれば良かった（DigitalOcean にて Rancher を試す – その2 (HA構成)）のですが、Rancher 2.0 では Kubernetes に Rancher server を deploy するということになっています。Kubernetes を管理する Rancher を別の Kubernetes に入れないと行けないというのは gcc を gcc でコンパイルする的な感じで？？\nさて、Rancher を使えば簡単に構築できる Kubernetes をどうやって構築するのか？先日試した kubeadm か？ いいえ、RKE (Rancher Kubernetes Engine) というものがあるようです\nAn Introduction To Rancher Kubernetes Engine (RKE)\n次のような構成を作るみたいです。Rancher server は1個だけで kubernetes による healthcheck と再起動頼みってことでしょうか。内蔵 etcd のデータはどうなるんだろう？\n   それでは手順にそって進めてみましょう。今回も DigitalOcean で試します。サブドメイン切って DigitalOcean でレコード管理することにしてみました。これは無料らしい。 クライアント環境は Windows ですが、コマンドは go 製ばかりだし git-bash で作業してるのでほぼ Linux でも mac でも同じかな。\nドメインを決めて SSL/TLS 署名書を用意する # Let\u0026rsquo;s Encrypt で証明書を取得します DigitalOcean で管理しているドメインで、サーバー3台にインストールするので ACME の DNS-01 が簡単に使える lego を使います。（lego についての過去の投稿）\n$ curl -LO https://github.com/xenolf/lego/releases/download/v0.4.1/lego_windows_amd64.zip $ unzip lego_windows_amd64.zip $ mv lego_windows_amd64.exe ~/bin/lego DigitalOcean の DNS レコードをいじるために DO_AUTH_TOKEN 環境変数に API Token をセットする必要があります\n$ export DO_AUTH_TOKEN=0123456789abcdef0123456789abcdef012346789abcdef012346789abcdef01 $ lego --domains rancher.mydomain.example.com --email メールアドレス --accept-tos --dns digitalocean run これで .lego/certificates/ ディレクトリに DOMAIN.crt, DOMAIN.issuer.crt, DOMAIN.json, DOMAIN.key が作成されます\nDocker 入りの Ubuntu 16.04 を 3 台用意する # Kubernetes でサポートされている Docker をインストールするスクリプトを user-data として渡します\n$ cat \u0026gt; userdata.txt \u0026lt;\u0026lt;EOD #!/bin/bash curl https://releases.rancher.com/install-docker/17.03.sh | sh EOD $ for i in 1 2 3; do doctl compute droplet create rke-node${i} \\ --image ubuntu-16-04-x64 \\ --region sgp1 \\ --size s-1vcpu-2gb \\ --ssh-keys 16797382 \\ --user-data-file userdata.txt \\ --enable-monitoring done $ doctl compute droplet list ID Name Public IPv4 Private IPv4 Public IPv6 Memory VCPUs Disk Region Image Status Tags Features Volumes 93518929 rke-node1 188.166.xxx.111 2048 1 50 sgp1 Ubuntu 16.04.4 x64 active monitoring 93518934 rke-node2 188.166.xxx.222 2048 1 50 sgp1 Ubuntu 16.04.4 x64 active monitoring 93518938 rke-node3 188.166.xxx.333 2048 1 50 sgp1 Ubuntu 16.04.4 x64 active monitoring DNS 登録 # 3台の node の IP アドレスを rancher.mydomain.example.com として登録します\n$ doctl compute domain records create mydomain.example.com --record-name rancher --record-type A --record-data 188.166.xxx.111 --record-ttl 300 $ doctl compute domain records create mydomain.example.com --record-name rancher --record-type A --record-data 188.166.xxx.222 --record-ttl 300 $ doctl compute domain records create mydomain.example.com --record-name rancher --record-type A --record-data 188.166.xxx.333 --record-ttl 300 list コマンドで確認します\n$ doctl compute domain records list mydomain.example.com rke バイナリのダウンロード # github.com/rancher/rke の releases ページからバイナリをダウンロードします\ncurl -Lo ~/bin/rke https://github.com/rancher/rke/releases/download/v0.1.7-rc2/rke_windows-amd64.exe $ rke -version rke version v0.1.7-rc2 Template のダウンロード # rke 用のテンプレートをダウンロードします。自己署名の証明書を使うか、公の証明機関の証明書を使うかによって2種類あります。今回は Let\u0026rsquo;s Encrypt を使うので 3-node-certificate-recognizedca.yml の方を使います\n Template for using Self Signed Certificate (3-node-certificate.yml) Template for using Certificate Signed By A Recognized Certificate Authority (3-node-certificate-recognizedca.yml)  テンプレートなので必要な箇所を置換する必要があります\nNodes セクションの置換 # nodes にある \u0026lt;IP\u0026gt; を各サーバーの IP アドレスに、\u0026lt;USER\u0026gt; を ssh でログインするユーザー名（DigitalOcean ではデフォルトが root）、\u0026lt;SSHKEY_FILE\u0026gt; を ssh に使う private key のファイル（一般的には ~/.ssh/id_rsa かな）に置換します\n証明書の置換 # $ cat .lego/certificates/rancher.mydomain.example.com.crt .lego/certificates/rancher.mydomain.example.com.issuer.crt \u0026gt; crt.pem $ base64 -w 0 crt.pem で \u0026lt;BASE64_CRT\u0026gt; を\n$ base64 -w 0 .lego/certificates/rancher.mydomain.example.com.key で \u0026lt;BASE64_KEY\u0026gt; を置換します\nFQDN の置換 # \u0026lt;FQDN\u0026gt; を rancher.mydomain.example.com に置換します。2箇所あります。\nrke コマンドの実行 # $ rke up --config 3-node-certificate-recognizedca.yml $ rke up --config 3-node-certificate-recognizedca.yml time=\u0026quot;2018-05-13T22:46:38+09:00\u0026quot; level=info msg=\u0026quot;Building Kubernetes cluster\u0026quot; time=\u0026quot;2018-05-13T22:46:38+09:00\u0026quot; level=info msg=\u0026quot;[dialer] Setup tunnel for host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:46:39+09:00\u0026quot; level=info msg=\u0026quot;[dialer] Setup tunnel for host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:46:41+09:00\u0026quot; level=info msg=\u0026quot;[dialer] Setup tunnel for host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:46:42+09:00\u0026quot; level=info msg=\u0026quot;[network] Deploying port listener containers\u0026quot; time=\u0026quot;2018-05-13T22:46:42+09:00\u0026quot; level=info msg=\u0026quot;[network] Pulling image [rancher/rke-tools:v0.1.6] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:46:42+09:00\u0026quot; level=info msg=\u0026quot;[network] Pulling image [rancher/rke-tools:v0.1.6] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:46:42+09:00\u0026quot; level=info msg=\u0026quot;[network] Pulling image [rancher/rke-tools:v0.1.6] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:01+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully pulled image [rancher/rke-tools:v0.1.6] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:01+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully pulled image [rancher/rke-tools:v0.1.6] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:01+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully pulled image [rancher/rke-tools:v0.1.6] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:01+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-etcd-port-listener] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:02+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-etcd-port-listener] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:02+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-etcd-port-listener] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:03+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-cp-port-listener] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:03+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-cp-port-listener] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:03+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-cp-port-listener] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:03+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-worker-port-listener] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:03+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-worker-port-listener] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:04+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-worker-port-listener] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:04+09:00\u0026quot; level=info msg=\u0026quot;[network] Port listener containers deployed successfully\u0026quot; time=\u0026quot;2018-05-13T22:47:04+09:00\u0026quot; level=info msg=\u0026quot;[network] Running etcd \u0026lt;-\u0026gt; etcd port checks\u0026quot; time=\u0026quot;2018-05-13T22:47:04+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:04+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:05+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:05+09:00\u0026quot; level=info msg=\u0026quot;[network] Running control plane -\u0026gt; etcd port checks\u0026quot; time=\u0026quot;2018-05-13T22:47:05+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:06+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:06+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:06+09:00\u0026quot; level=info msg=\u0026quot;[network] Running control plane -\u0026gt; worker port checks\u0026quot; time=\u0026quot;2018-05-13T22:47:07+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:07+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:07+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:07+09:00\u0026quot; level=info msg=\u0026quot;[network] Running workers -\u0026gt; control plane port checks\u0026quot; time=\u0026quot;2018-05-13T22:47:08+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:08+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:08+09:00\u0026quot; level=info msg=\u0026quot;[network] Successfully started [rke-port-checker] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:08+09:00\u0026quot; level=info msg=\u0026quot;[network] Checking KubeAPI port Control Plane hosts\u0026quot; time=\u0026quot;2018-05-13T22:47:08+09:00\u0026quot; level=info msg=\u0026quot;[network] Removing port listener containers\u0026quot; time=\u0026quot;2018-05-13T22:47:09+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-etcd-port-listener] Successfully removed container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:09+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-etcd-port-listener] Successfully removed container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:09+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-etcd-port-listener] Successfully removed container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:09+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-cp-port-listener] Successfully removed container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:09+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-cp-port-listener] Successfully removed container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:09+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-cp-port-listener] Successfully removed container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:10+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-worker-port-listener] Successfully removed container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:10+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-worker-port-listener] Successfully removed container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:10+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-worker-port-listener] Successfully removed container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:10+09:00\u0026quot; level=info msg=\u0026quot;[network] Port listener containers removed successfully\u0026quot; time=\u0026quot;2018-05-13T22:47:10+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Attempting to recover certificates from backup on [etcd] hosts\u0026quot; time=\u0026quot;2018-05-13T22:47:11+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Successfully started [cert-fetcher] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:47:11+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Successfully started [cert-fetcher] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:13+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Successfully started [cert-fetcher] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:13+09:00\u0026quot; level=info msg=\u0026quot;[certificates] No Certificate backup found on [etcd] hosts\u0026quot; time=\u0026quot;2018-05-13T22:47:13+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Generating CA kubernetes certificates\u0026quot; time=\u0026quot;2018-05-13T22:47:13+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Generating Kubernetes API server certificates\u0026quot; time=\u0026quot;2018-05-13T22:47:14+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Generating Kube Controller certificates\u0026quot; time=\u0026quot;2018-05-13T22:47:14+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Generating Kube Scheduler certificates\u0026quot; time=\u0026quot;2018-05-13T22:47:15+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Generating Kube Proxy certificates\u0026quot; time=\u0026quot;2018-05-13T22:47:16+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Generating Node certificate\u0026quot; time=\u0026quot;2018-05-13T22:47:16+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Generating admin certificates and kubeconfig\u0026quot; time=\u0026quot;2018-05-13T22:47:17+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Generating etcd-188.166.xxx.111 certificate and key\u0026quot; time=\u0026quot;2018-05-13T22:47:18+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Generating etcd-188.166.xxx.222 certificate and key\u0026quot; time=\u0026quot;2018-05-13T22:47:18+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Generating etcd-188.166.xxx.333 certificate and key\u0026quot; time=\u0026quot;2018-05-13T22:47:18+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Temporarily saving certs to [etcd] hosts\u0026quot; time=\u0026quot;2018-05-13T22:47:25+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Saved certs to [etcd] hosts\u0026quot; time=\u0026quot;2018-05-13T22:47:25+09:00\u0026quot; level=info msg=\u0026quot;[reconcile] Reconciling cluster state\u0026quot; time=\u0026quot;2018-05-13T22:47:25+09:00\u0026quot; level=info msg=\u0026quot;[reconcile] This is newly generated cluster\u0026quot; time=\u0026quot;2018-05-13T22:47:25+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Deploying kubernetes certificates to Cluster nodes\u0026quot; time=\u0026quot;2018-05-13T22:47:32+09:00\u0026quot; level=info msg=\u0026quot;Successfully Deployed local admin kubeconfig at [./kube_config_3-node-certificate-recognizedca.yml]\u0026quot; time=\u0026quot;2018-05-13T22:47:32+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Successfully deployed kubernetes certificates to Cluster nodes\u0026quot; time=\u0026quot;2018-05-13T22:47:32+09:00\u0026quot; level=info msg=\u0026quot;Pre-pulling kubernetes images\u0026quot; time=\u0026quot;2018-05-13T22:47:32+09:00\u0026quot; level=info msg=\u0026quot;[pre-deploy] Pulling image [rancher/hyperkube:v1.10.1-rancher2] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:47:32+09:00\u0026quot; level=info msg=\u0026quot;[pre-deploy] Pulling image [rancher/hyperkube:v1.10.1-rancher2] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:47:32+09:00\u0026quot; level=info msg=\u0026quot;[pre-deploy] Pulling image [rancher/hyperkube:v1.10.1-rancher2] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:48:21+09:00\u0026quot; level=info msg=\u0026quot;[pre-deploy] Successfully pulled image [rancher/hyperkube:v1.10.1-rancher2] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:48:24+09:00\u0026quot; level=info msg=\u0026quot;[pre-deploy] Successfully pulled image [rancher/hyperkube:v1.10.1-rancher2] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:48:34+09:00\u0026quot; level=info msg=\u0026quot;[pre-deploy] Successfully pulled image [rancher/hyperkube:v1.10.1-rancher2] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:48:34+09:00\u0026quot; level=info msg=\u0026quot;Kubernetes images pulled successfully\u0026quot; time=\u0026quot;2018-05-13T22:48:34+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Building up etcd plane..\u0026quot; time=\u0026quot;2018-05-13T22:48:34+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Pulling image [rancher/coreos-etcd:v3.1.12] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:48:40+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Successfully pulled image [rancher/coreos-etcd:v3.1.12] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:48:40+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Successfully started [etcd] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:48:41+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Successfully started [rke-log-linker] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:48:41+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:48:42+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Pulling image [rancher/coreos-etcd:v3.1.12] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:48:48+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Successfully pulled image [rancher/coreos-etcd:v3.1.12] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:48:48+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Successfully started [etcd] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:48:49+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Successfully started [rke-log-linker] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:48:49+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:48:50+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Pulling image [rancher/coreos-etcd:v3.1.12] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:48:56+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Successfully pulled image [rancher/coreos-etcd:v3.1.12] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:48:56+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Successfully started [etcd] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:48:57+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Successfully started [rke-log-linker] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:48:57+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:48:57+09:00\u0026quot; level=info msg=\u0026quot;[etcd] Successfully started etcd plane..\u0026quot; time=\u0026quot;2018-05-13T22:48:57+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Building up Controller Plane..\u0026quot; time=\u0026quot;2018-05-13T22:48:58+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [kube-apiserver] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:48:58+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-apiserver] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:48:58+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [kube-apiserver] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:48:58+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-apiserver] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:48:58+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [kube-apiserver] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:48:58+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-apiserver] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:15+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-apiserver] on host [188.166.xxx.111] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:15+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-apiserver] on host [188.166.xxx.222] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:16+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [rke-log-linker] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:16+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:16+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [rke-log-linker] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:16+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [kube-controller-manager] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:16+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-controller-manager] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:17+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:17+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [kube-controller-manager] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:17+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-controller-manager] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:17+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-controller-manager] on host [188.166.xxx.111] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:18+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-apiserver] on host [188.166.xxx.333] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:18+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-controller-manager] on host [188.166.xxx.222] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:18+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [rke-log-linker] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:18+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [rke-log-linker] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:19+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:19+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:19+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [rke-log-linker] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:19+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [kube-scheduler] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:19+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-scheduler] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:19+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [kube-controller-manager] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:19+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-controller-manager] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:19+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:20+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [kube-scheduler] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:20+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-scheduler] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:20+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-scheduler] on host [188.166.xxx.111] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:20+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-controller-manager] on host [188.166.xxx.333] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:21+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-scheduler] on host [188.166.xxx.222] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:21+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [rke-log-linker] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:21+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [rke-log-linker] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:22+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:22+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:22+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [rke-log-linker] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:22+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:22+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [kube-scheduler] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:22+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-scheduler] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:23+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-scheduler] on host [188.166.xxx.333] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:24+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started [rke-log-linker] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:24+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:24+09:00\u0026quot; level=info msg=\u0026quot;[controlplane] Successfully started Controller Plane..\u0026quot; time=\u0026quot;2018-05-13T22:49:24+09:00\u0026quot; level=info msg=\u0026quot;[authz] Creating rke-job-deployer ServiceAccount\u0026quot; time=\u0026quot;2018-05-13T22:49:25+09:00\u0026quot; level=info msg=\u0026quot;[authz] rke-job-deployer ServiceAccount created successfully\u0026quot; time=\u0026quot;2018-05-13T22:49:25+09:00\u0026quot; level=info msg=\u0026quot;[authz] Creating system:node ClusterRoleBinding\u0026quot; time=\u0026quot;2018-05-13T22:49:25+09:00\u0026quot; level=info msg=\u0026quot;[authz] system:node ClusterRoleBinding created successfully\u0026quot; time=\u0026quot;2018-05-13T22:49:25+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Save kubernetes certificates as secrets\u0026quot; time=\u0026quot;2018-05-13T22:49:26+09:00\u0026quot; level=info msg=\u0026quot;[certificates] Successfully saved certificates as kubernetes secret [k8s-certs]\u0026quot; time=\u0026quot;2018-05-13T22:49:26+09:00\u0026quot; level=info msg=\u0026quot;[state] Saving cluster state to Kubernetes\u0026quot; time=\u0026quot;2018-05-13T22:49:26+09:00\u0026quot; level=info msg=\u0026quot;[state] Successfully Saved cluster state to Kubernetes ConfigMap: cluster-state\u0026quot; time=\u0026quot;2018-05-13T22:49:26+09:00\u0026quot; level=info msg=\u0026quot;[worker] Building up Worker Plane..\u0026quot; time=\u0026quot;2018-05-13T22:49:27+09:00\u0026quot; level=info msg=\u0026quot;[sidekick] Sidekick container already created on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:27+09:00\u0026quot; level=info msg=\u0026quot;[sidekick] Sidekick container already created on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:27+09:00\u0026quot; level=info msg=\u0026quot;[sidekick] Sidekick container already created on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:27+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [kubelet] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:27+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kubelet] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:27+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [kubelet] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:27+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kubelet] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:27+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [kubelet] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:27+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kubelet] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:34+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kubelet] on host [188.166.xxx.111] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:34+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kubelet] on host [188.166.xxx.222] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:34+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kubelet] on host [188.166.xxx.333] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:35+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [rke-log-linker] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:35+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [rke-log-linker] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:35+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [rke-log-linker] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:36+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:36+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:36+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:36+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [kube-proxy] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:36+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-proxy] on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:36+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [kube-proxy] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:36+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-proxy] on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:36+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [kube-proxy] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:36+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] Start Healthcheck on service [kube-proxy] on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:37+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-proxy] on host [188.166.xxx.111] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:37+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-proxy] on host [188.166.xxx.222] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:38+09:00\u0026quot; level=info msg=\u0026quot;[healthcheck] service [kube-proxy] on host [188.166.xxx.333] is healthy\u0026quot; time=\u0026quot;2018-05-13T22:49:38+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [rke-log-linker] container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:38+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [rke-log-linker] container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:38+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.222]\u0026quot; time=\u0026quot;2018-05-13T22:49:38+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.111]\u0026quot; time=\u0026quot;2018-05-13T22:49:38+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started [rke-log-linker] container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:39+09:00\u0026quot; level=info msg=\u0026quot;[remove/rke-log-linker] Successfully removed container on host [188.166.xxx.333]\u0026quot; time=\u0026quot;2018-05-13T22:49:39+09:00\u0026quot; level=info msg=\u0026quot;[worker] Successfully started Worker Plane..\u0026quot; time=\u0026quot;2018-05-13T22:49:39+09:00\u0026quot; level=info msg=\u0026quot;[sync] Syncing nodes Labels and Taints\u0026quot; time=\u0026quot;2018-05-13T22:49:41+09:00\u0026quot; level=info msg=\u0026quot;[sync] Successfully synced nodes Labels and Taints\u0026quot; time=\u0026quot;2018-05-13T22:49:41+09:00\u0026quot; level=info msg=\u0026quot;[network] Setting up network plugin: canal\u0026quot; time=\u0026quot;2018-05-13T22:49:41+09:00\u0026quot; level=info msg=\u0026quot;[addons] Saving addon ConfigMap to Kubernetes\u0026quot; time=\u0026quot;2018-05-13T22:49:41+09:00\u0026quot; level=info msg=\u0026quot;[addons] Successfully Saved addon to Kubernetes ConfigMap: rke-network-plugin\u0026quot; time=\u0026quot;2018-05-13T22:49:41+09:00\u0026quot; level=info msg=\u0026quot;[addons] Executing deploy job..\u0026quot; time=\u0026quot;2018-05-13T22:49:51+09:00\u0026quot; level=info msg=\u0026quot;[addons] Setting up KubeDNS\u0026quot; time=\u0026quot;2018-05-13T22:49:51+09:00\u0026quot; level=info msg=\u0026quot;[addons] Saving addon ConfigMap to Kubernetes\u0026quot; time=\u0026quot;2018-05-13T22:49:52+09:00\u0026quot; level=info msg=\u0026quot;[addons] Successfully Saved addon to Kubernetes ConfigMap: rke-kubedns-addon\u0026quot; time=\u0026quot;2018-05-13T22:49:52+09:00\u0026quot; level=info msg=\u0026quot;[addons] Executing deploy job..\u0026quot; time=\u0026quot;2018-05-13T22:49:57+09:00\u0026quot; level=info msg=\u0026quot;[addons] KubeDNS deployed successfully..\u0026quot; time=\u0026quot;2018-05-13T22:49:57+09:00\u0026quot; level=info msg=\u0026quot;[ingress] Setting up nginx ingress controller\u0026quot; time=\u0026quot;2018-05-13T22:49:57+09:00\u0026quot; level=info msg=\u0026quot;[addons] Saving addon ConfigMap to Kubernetes\u0026quot; time=\u0026quot;2018-05-13T22:49:57+09:00\u0026quot; level=info msg=\u0026quot;[addons] Successfully Saved addon to Kubernetes ConfigMap: rke-ingress-controller\u0026quot; time=\u0026quot;2018-05-13T22:49:57+09:00\u0026quot; level=info msg=\u0026quot;[addons] Executing deploy job..\u0026quot; time=\u0026quot;2018-05-13T22:50:08+09:00\u0026quot; level=info msg=\u0026quot;[ingress] ingress controller nginx is successfully deployed\u0026quot; time=\u0026quot;2018-05-13T22:50:08+09:00\u0026quot; level=info msg=\u0026quot;[addons] Setting up user addons\u0026quot; time=\u0026quot;2018-05-13T22:50:08+09:00\u0026quot; level=info msg=\u0026quot;[addons] Saving addon ConfigMap to Kubernetes\u0026quot; time=\u0026quot;2018-05-13T22:50:08+09:00\u0026quot; level=info msg=\u0026quot;[addons] Successfully Saved addon to Kubernetes ConfigMap: rke-user-addon\u0026quot; time=\u0026quot;2018-05-13T22:50:08+09:00\u0026quot; level=info msg=\u0026quot;[addons] Executing deploy job..\u0026quot; time=\u0026quot;2018-05-13T22:50:13+09:00\u0026quot; level=info msg=\u0026quot;[addons] User addons deployed successfully\u0026quot; time=\u0026quot;2018-05-13T22:50:13+09:00\u0026quot; level=info msg=\u0026quot;Finished building Kubernetes cluster successfully\u0026quot; ブラウザで Rancher にアクセス # Finished building Kubernetes cluster successfully なので Rancher の Web UI にアクセスできるはずですが\u0026hellip;\n\u0026quot;503 Service Temporarily Unavailable\u0026quot; ありゃ？？\nサーバーにログインして docker logs してみると\nIncorrect Usage. flag provided but not defined: -advertise-address NAME: rancher - A new cli application USAGE: rancher [global options] command [command options] [arguments...] VERSION: v2.0.0 COMMANDS: help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --kubeconfig value Kube config for accessing k8s cluster [$KUBECONFIG] --debug Enable debug logs --add-local value Add local cluster (true, false, auto) (default: \u0026quot;auto\u0026quot;) --http-listen-port value HTTP listen port (default: 8080) --https-listen-port value HTTPS listen port (default: 8443) --k8s-mode value Mode to run or access k8s API server for management API (embedded, external, auto) (default: \u0026quot;auto\u0026quot;) --log-format value Log formatter used (json, text, simple) (default: \u0026quot;simple\u0026quot;) --acme-domain value Domain to register with LetsEncrypt --help, -h show help --version, -v print the version なんでじゃ？rancher コマンドに --advertise-address オプションなんてないはずなのに\u0026hellip;\n3-node-certificate-recognizedca.yml\nkind:DeploymentapiVersion:extensions/v1beta1metadata:namespace:cattle-systemname:cattlespec:replicas:1template:metadata:labels:app:cattlespec:serviceAccountName:cattle-admincontainers:- image:rancher/rancher:latestimagePullPolicy:Alwaysname:cattle-serverports:- containerPort:80protocol:TCP- containerPort:443protocol:TCPargs:- --advertise-address=$(POD_IP)env:- name:POD_IPvalueFrom:fieldRef:fieldPath:status.podIPうむむ、、つい21時間前に master に変更が入っている\nhttps://github.com/rancher/rancher/commit/29bd33528aa810001b89580a654dddb1c53b2d6a#diff-7ddfb3e035b42cd70649cc33393fe32c https://github.com/rancher/rancher/issues/13292\n2.0 に取り込まれたらまた試してみよう\nmerge の3日後に revert されているので 5/17 以降は使えるようになっています。\nkubectl で RKE にアクセスしてみる # rke コマンドで作った Kubernetes にアクセスするための kubectl 用ファイルが kube_config_3-node-certificate-recognizedca.yml としてカレントディレクトリに作成されていましたので ~/.kube/config にコピーします\n$ cp kube_config_3-node-certificate-recognizedca.yml ~/.kube/config $ kubectl get nodes NAME STATUS ROLES AGE VERSION 188.166.xxx.111 Ready controlplane,etcd,worker 26m v1.10.1 188.166.xxx.222 Ready controlplane,etcd,worker 26m v1.10.1 188.166.xxx.333 Ready controlplane,etcd,worker 26m v1.10.1 $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE cattle-system cattle-95fc8b5fc-xbtm4 0/1 CrashLoopBackOff 11 34m ingress-nginx default-http-backend-564b9b6c5b-lpd75 1/1 Running 0 34m ingress-nginx nginx-ingress-controller-2hbtq 1/1 Running 0 34m ingress-nginx nginx-ingress-controller-kjs7q 1/1 Running 0 34m ingress-nginx nginx-ingress-controller-qc5r6 1/1 Running 0 34m kube-system canal-gszp9 3/3 Running 0 34m kube-system canal-ktg7w 3/3 Running 0 34m kube-system canal-ltspx 3/3 Running 0 34m kube-system kube-dns-5ccb66df65-q7wj9 3/3 Running 0 34m kube-system kube-dns-autoscaler-6c4b786f5-gsskl 1/1 Running 0 34m kube-system rke-ingress-controller-deploy-job-qjd9h 0/1 Completed 0 34m kube-system rke-kubedns-addon-deploy-job-rq4w5 0/1 Completed 0 34m kube-system rke-network-plugin-deploy-job-wmclj 0/1 Completed 0 34m kube-system rke-user-addon-deploy-job-959n6 0/1 Completed 0 34m 以上、HA 構成のセットアップ手順でした。。。\n非 HA 構成（docker run するだけ）だと Rancher がデータを保持する etcd は内蔵されていて /var/lib/rancher/etcd 配下にデータファイルが置かれているのだけれど HA 構成の場合にどうなるのか確認したかったのだけどまた後日確認だな\n「続 Rancher 2.0 の HA 構成を試す」に続きがあります\n","date":"2018年5月13日","permalink":"/2018/05/rancher-2-0-ha-install/","section":"Posts","summary":"Beta を試すシリーズ（その1、その2）を書いていましたがついに 5月1日に Rancher 2.0 が GA になりました Announcing Rancher 2.0 GA! (Docker Hub にある image の tag はまだ preview でした) 今回は High Availability 構","title":"Rancher 2.0 の HA 構成を試す"},{"content":"","date":"2018年5月9日","permalink":"/tags/cilium/","section":"Tags","summary":"","title":"Cilium"},{"content":"Cilium 1.0: Bringing the BPF Revolution to Kubernetes Networking and Security という記事を見かけた Cilium っていうのが Version 1.0 になったそうだ。はて？これは何をもったらすものだろう？ コンテナ化、マイクロサービス化が進みさまざまなサービス同士が通信するようになりますが、守るべき API があるし、どことどこの通信を許可するかというのを iptables による L3, L4 での制御ではスケールしない、ルールが万を超えたり、非常に頻繁に更新する必要もあるため BPF というものでもっとシンプルにしようというもののようです Introduction to Cilium では機能として次のようなものが挙げられています\n Protect and secure APIs transparently Secure service to service communication based on identities Secure access to and from external services Simple Networking Load balancing Monitoring and Troubleshooting  github.com/cilium/cilium\nHTTP, gRPC, and Kafka Aware Security and Networking for Containers with BPF and XDP Cilium Getting Started Guides ここでは Getting Started Using Minikube をなぞってみます\nLinux (ubuntu) # 今回は Windows で試すんだけどメモ\nkubectl のインストール # Install and Set Up kubectl``` apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat \u0026laquo;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubectl\n #### minikube のインストール [Releases · kubernetes/minikube](https://github.com/kubernetes/minikube/releases) からバイナリを取ってくるだけ``` curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.26.1/minikube-linux-amd64 \\\\ \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ Windows # kubectl のインストール # curl -Lo ~/bin/kubectl.exe \\\\ https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/windows/amd64/kubectl.exe minikube のインストール # curl -Lo ~/bin/minikube.exe \\\\ https://github.com/kubernetes/minikube/releases/download/v0.26.1/minikube-windows-amd64 minikube で kubernetes 環境をセットアップ # メモリを4GBにし RBAC を有効にして構築します``` minikube start \\ \u0026ndash;vm-driver=virtualbox \\ \u0026ndash;network-plugin=cni \\ \u0026ndash;bootstrapper=localkube \\ \u0026ndash;memory=4096 \\ \u0026ndash;extra-config=apiserver.Authorization.Mode=RBAC\nStarting local Kubernetes v1.10.0 cluster... Starting VM... Downloading Minikube ISO 150.53 MB / 150.53 MB 100.00% 0sss Getting VM IP address... WARNING: The localkube bootstrapper is now deprecated and support for it will be removed in a future release. Please consider switching to the kubeadm bootstrapper, which is intended to replace the localkube bootstrapper. To disable this message, run \\[minikube config set ShowBootstrapperDeprecationNotification false\\] Moving files into cluster... Downloading localkube binary 173.54 MB / 173.54 MB 100.00% 0sss 65 B / 65 B 100.00% 0s Setting up certs... Connecting to cluster... Setting up kubeconfig... Starting cluster components... Kubectl is now configured to use the cluster. Loading cached images from config file. ```(localkube bootstrapper は deprecated で kubeadm bootstrapper 使おうねって WARNING が出てるな) `kubectl get cs` で componentstatuses を確認``` $ kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026quot;health\u0026quot;: \u0026quot;true\u0026quot;} ```全部 Healthy なので構築できてるようです `RBAC` の有効な環境で `kube-dns` サービスを有効にできるよう Kubernetes のシステムアカウントを `cluster-admin` ロールに紐づけます``` $ kubectl create clusterrolebinding kube-system-default-binding-cluster-admin \\\\ --clusterrole=cluster-admin \\\\ --serviceaccount=kube-system:default clusterrolebinding.rbac.authorization.k8s.io \u0026quot;kube-system-default-binding-cluster-admin\u0026quot; created ```[standalone-etcd.yaml](https://github.com/cilium/cilium/blob/doc-1.0/examples/kubernetes/addons/etcd/standalone-etcd.yaml) で Cilium 用の etcd サービスを作成します``` $ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/doc-1.0/examples/kubernetes/addons/etcd/standalone-etcd.yaml service \u0026quot;etcd-cilium\u0026quot; created statefulset.apps \u0026quot;etcd-cilium\u0026quot; created $ kubectl get pods \u0026ndash;all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default etcd-cilium-0 1/1 Running 0 54s kube-system kube-addon-manager-minikube 1/1 Running 0 3m kube-system kube-dns-6dcb57bcc8-g58kh 3/3 Running 0 2m kube-system kubernetes-dashboard-5498ccf677-9xgdz 1/1 Running 3 2m kube-system storage-provisioner 1/1 Running 0 2m\n[cilium.yaml](https://github.com/cilium/cilium/blob/doc-1.0/examples/kubernetes/1.10/cilium.yaml) で cilium サービスとそれに必要な ConfigMap や Secret、ServiceAccount、ClusterRole などを作成します $ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/doc-1.0/examples/kubernetes/1.10/cilium.yaml configmap \u0026ldquo;cilium-config\u0026rdquo; created secret \u0026ldquo;cilium-etcd-secrets\u0026rdquo; created daemonset.apps \u0026ldquo;cilium\u0026rdquo; created clusterrolebinding.rbac.authorization.k8s.io \u0026ldquo;cilium\u0026rdquo; created clusterrole.rbac.authorization.k8s.io \u0026ldquo;cilium\u0026rdquo; created serviceaccount \u0026ldquo;cilium\u0026rdquo; created\n````ContainerCreatingなのでRunning` になるまでしばらく待つ``` $ kubectl get pods \u0026ndash;namespace kube-system NAME READY STATUS RESTARTS AGE cilium-rg6dc 0/1 ContainerCreating 0 47s kube-addon-manager-minikube 1/1 Running 0 6m kube-dns-6dcb57bcc8-g58kh 3/3 Running 0 5m kubernetes-dashboard-5498ccf677-9xgdz 1/1 Running 3 5m storage-provisioner 1/1 Running 0 5m\n起動しました $ kubectl get pods \u0026ndash;namespace kube-system NAME READY STATUS RESTARTS AGE cilium-rg6dc 0/1 Running 0 1m kube-addon-manager-minikube 1/1 Running 0 6m kube-dns-6dcb57bcc8-g58kh 3/3 Running 0 6m kubernetes-dashboard-5498ccf677-9xgdz 1/1 Running 3 6m storage-provisioner 1/1 Running 0 6m\n次に [http-sw-app.yaml](https://github.com/cilium/cilium/blob/doc-1.0/examples/minikube/http-sw-app.yaml) を使って [cilium/starwars](https://hub.docker.com/r/cilium/starwars/) イメージを使った deathstar サービス (コンテナ2つ) と [tgraf/netperf](https://hub.docker.com/r/tgraf/netperf/) イメージを使った tiefighter と xwing というコンテナ (Deployment) を作成します $ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/doc-1.0/examples/minikube/http-sw-app.yaml service \u0026ldquo;deathstar\u0026rdquo; created deployment.apps \u0026ldquo;deathstar\u0026rdquo; created deployment.apps \u0026ldquo;tiefighter\u0026rdquo; created deployment.apps \u0026ldquo;xwing\u0026rdquo; created\n````Running` になるまでしばらく待つ``` $ kubectl get pods,svc NAME READY STATUS RESTARTS AGE deathstar-765fd545f9-grr5g 0/1 ContainerCreating 0 46s deathstar-765fd545f9-pk9h8 0/1 ContainerCreating 0 46s etcd-cilium-0 1/1 Running 0 5m tiefighter-787b4ff698-kpmgl 0/1 ContainerCreating 0 46s xwing-d56b5c5-sdw7q 0/1 ContainerCreating 0 45s\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE deathstar ClusterIP 10.105.66.9  80/TCP 46s etcd-cilium NodePort 10.109.105.129  32379:31079/TCP,32380:31080/TCP 5m kubernetes ClusterIP 10.96.0.1  443/TCP 8m\n````Running` に変わった``` $ kubectl get pods,svc NAME READY STATUS RESTARTS AGE deathstar-765fd545f9-grr5g 1/1 Running 0 3m deathstar-765fd545f9-pk9h8 1/1 Running 0 3m etcd-cilium-0 1/1 Running 0 8m tiefighter-787b4ff698-kpmgl 1/1 Running 0 3m xwing-d56b5c5-sdw7q 1/1 Running 0 3m\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE deathstar ClusterIP 10.105.66.9  80/TCP 3m etcd-cilium NodePort 10.109.105.129  32379:31079/TCP,32380:31080/TCP 8m kubernetes ClusterIP 10.96.0.1  443/TCP 11m\nCilium の Pod 名確認 $ kubectl -n kube-system get pods -l k8s-app=cilium NAME READY STATUS RESTARTS AGE cilium-rg6dc 1/1 Running 0 6m\n````kubectl execで Cilium Pod 内でcilium endpoint list` コマンドを実行するとエンドポイントの一覧が表示されます。deathstar が2つ、tiefighter、xwing が1つずつ``` $ kubectl -n kube-system exec cilium-rg6dc cilium endpoint list ENDPOINT POLICY (ingress) POLICY (egress) IDENTITY LABELS (source:key[=value]) IPv6 IPv4 STATUS ENFORCEMENT ENFORCEMENT 173 Disabled Disabled 64509 k8s:class=deathstar f00d::a0f:0:0:ad 10.15.42.252 ready k8s:io.kubernetes.pod.namespace=default\nk8s:org=empire 13949 Disabled Disabled 44856 k8s:class=tiefighter f00d::a0f:0:0:367d 10.15.193.100 ready k8s:io.kubernetes.pod.namespace=default\nk8s:org=empire 29381 Disabled Disabled 61850 k8s:class=xwing f00d::a0f:0:0:72c5 10.15.13.37 ready k8s:io.kubernetes.pod.namespace=default\nk8s:org=alliance 29898 Disabled Disabled 51604 reserved:health f00d::a0f:0:0:74ca 10.15.242.54 ready 48896 Disabled Disabled 64509 k8s:class=deathstar f00d::a0f:0:0:bf00 10.15.167.158 ready k8s:io.kubernetes.pod.namespace=default\nk8s:org=empire\nxwing から deathstar に curl でアクセスしてみる $ kubectl exec xwing-d56b5c5-sdw7q \u0026ndash; curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed\ntiefighter から deathstar に curl でアクセスしてみる $ kubectl exec tiefighter-787b4ff698-kpmgl \u0026ndash; curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed\nどちらもアクセス可能でした [sw\\_l3\\_l4\\_policy.yaml](https://github.com/cilium/cilium/blob/doc-1.0/examples/minikube/sw_l3_l4_policy.yaml) を使って Cilium のルールを作成します `description: \u0026quot;L3-L4 policy to restrict deathstar access to empire ships only\u0026quot;` にあるように deathstar の ingress に制限をいれます。label で org=empire となっている接続元からのみ port 80 へのアクセスを許可します $ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/doc-1.0/examples/minikube/sw_l3_l4_policy.yaml ciliumnetworkpolicy.cilium.io \u0026ldquo;rule1\u0026rdquo; created\nxwing は org=alliance であるため deathstar にアクセスできなくなりました $ kubectl exec xwing-d56b5c5-sdw7q \u0026ndash; curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\nつながらない 再度 endpoint list を確認します deathstar の `POLICY (ingress) ENFORCEMENT` が `Enabled` になってます $ kubectl -n kube-system exec cilium-rg6dc cilium endpoint list ENDPOINT POLICY (ingress) POLICY (egress) IDENTITY LABELS (source:key[=value]) IPv6 IPv4 STATUS ENFORCEMENT ENFORCEMENT 173 Enabled Disabled 64509 k8s:class=deathstar f00d::a0f:0:0:ad 10.15.42.252 ready k8s:io.kubernetes.pod.namespace=default\nk8s:org=empire 13949 Disabled Disabled 44856 k8s:class=tiefighter f00d::a0f:0:0:367d 10.15.193.100 ready k8s:io.kubernetes.pod.namespace=default\nk8s:org=empire 29381 Disabled Disabled 61850 k8s:class=xwing f00d::a0f:0:0:72c5 10.15.13.37 ready k8s:io.kubernetes.pod.namespace=default\nk8s:org=alliance 29898 Disabled Disabled 51604 reserved:health f00d::a0f:0:0:74ca 10.15.242.54 ready 48896 Enabled Disabled 64509 k8s:class=deathstar f00d::a0f:0:0:bf00 10.15.167.158 ready k8s:io.kubernetes.pod.namespace=default\nk8s:org=empire\ncnp (CiliumNetworkPolicy) を確認 $ kubectl get cnp NAME AGE rule1 4m\nDescribe で詳細確認 $ kubectl describe cnp rule1 Name: rule1 Namespace: default Labels:  Annotations:  API Version: cilium.io/v2 Kind: CiliumNetworkPolicy Metadata: Cluster Name: Creation Timestamp: 2018-05-08T16:09:44Z Generation: 1 Resource Version: 1177 Self Link: /apis/cilium.io/v2/namespaces/default/ciliumnetworkpolicies/rule1 UID: 391b40cd-52da-11e8-8ef9-080027a11e00 Spec: Endpoint Selector: Match Labels: Any : Class: deathstar Any : Org: empire Ingress: From Endpoints: Match Labels: Any : Org: empire To Ports: Ports: Port: 80 Protocol: TCP Status: Nodes: Minikube: Enforcing: true Last Updated: 2018-05-08T16:09:52.34442593Z Local Policy Revision: 7 Ok: true Events: \n続いて L7 での制限です tiefighter (org=empire) からでも許可したくないエンドポイントが deathstar にあるかもしれません、たとえば次の `/v1/exhaust-port` とか $ kubectl exec tiefighter-787b4ff698-kpmgl \u0026ndash; curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-port Panic: deathstar exploded\ngoroutine 1 [running]: main.HandleGarbage(0x2080c3f50, 0x2, 0x4, 0x425c0, 0x5, 0xa) /code/src/github.com/empire/deathstar/ temp/main.go:9 +0x64 main.main() /code/src/github.com/empire/deathstar/ temp/main.go:5 +0x85\n[sw\\_l3\\_l4\\_l7\\_policy.yaml](https://github.com/cilium/cilium/blob/doc-1.0/examples/minikube/sw_l3_l4_l7_policy.yaml) で L7 で制御してみます org=empire でかつ `/v1/request-landing` への `POST` だけを許可するという Rule です $ kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/doc-1.0/examples/minikube/sw_l3_l4_l7_policy.yaml Warning: kubectl apply should be used on resource created by either kubectl create \u0026ndash;save-config or kubectl apply ciliumnetworkpolicy.cilium.io \u0026ldquo;rule1\u0026rdquo; configured\ntiefighter から `/v1/request-landing` へはアクセスできました $ kubectl exec tiefighter-787b4ff698-kpmgl \u0026ndash; curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed\nが、`/v1/exhaust-port` では Access denied となりました $ kubectl exec tiefighter-787b4ff698-kpmgl \u0026ndash; curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-port Access denied\nまた cnp (CiliumNetworkPolicy) を確認しておきます $ kubectl get ciliumnetworkpolicies NAME AGE rule1 9m\n$ kubectl describe ciliumnetworkpolicies rule1 Name: rule1 Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026quot;apiVersion\u0026quot;:\u0026quot;cilium.io/v2\u0026quot;,\u0026quot;description\u0026quot;:\u0026quot;L7 policy to restrict access to specific HTTP call\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;CiliumNetworkPolicy\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:... API Version: cilium.io/v2 Kind: CiliumNetworkPolicy Metadata: Cluster Name: Creation Timestamp: 2018-05-08T16:09:44Z Generation: 1 Resource Version: 1709 Self Link: /apis/cilium.io/v2/namespaces/default/ciliumnetworkpolicies/rule1 UID: 391b40cd-52da-11e8-8ef9-080027a11e00 Spec: Endpoint Selector: Match Labels: Any : Class: deathstar Any : Org: empire Ingress: From Endpoints: Match Labels: Any : Org: empire To Ports: Ports: Port: 80 Protocol: TCP Rules: Http: Method: POST Path: /v1/request-landing Status: Nodes: Minikube: Enforcing: true Last Updated: 2018-05-08T16:17:44.124075147Z Local Policy Revision: 11 Ok: true Events: \u0026lt;none\u0026gt; ```Cilium Pod 内で `cilium policy get` として確認することもできるようです``` $ kubectl -n kube-system exec cilium-rg6dc cilium policy get \\[ { \u0026quot;endpointSelector\u0026quot;: { \u0026quot;matchLabels\u0026quot;: { \u0026quot;any:class\u0026quot;: \u0026quot;deathstar\u0026quot;, \u0026quot;any:org\u0026quot;: \u0026quot;empire\u0026quot;, \u0026quot;k8s:io.kubernetes.pod.namespace\u0026quot;: \u0026quot;default\u0026quot; } }, \u0026quot;ingress\u0026quot;: \\[ { \u0026quot;fromEndpoints\u0026quot;: \\[ { \u0026quot;matchLabels\u0026quot;: { \u0026quot;any:org\u0026quot;: \u0026quot;empire\u0026quot;, \u0026quot;k8s:io.kubernetes.pod.namespace\u0026quot;: \u0026quot;default\u0026quot; } } \\], \u0026quot;toPorts\u0026quot;: \\[ { \u0026quot;ports\u0026quot;: \\[ { \u0026quot;port\u0026quot;: \u0026quot;80\u0026quot;, \u0026quot;protocol\u0026quot;: \u0026quot;TCP\u0026quot; } \\], \u0026quot;rules\u0026quot;: { \u0026quot;http\u0026quot;: \\[ { \u0026quot;path\u0026quot;: \u0026quot;/v1/request-landing\u0026quot;, \u0026quot;method\u0026quot;: \u0026quot;POST\u0026quot; } \\] } } \\] } \\], \u0026quot;labels\u0026quot;: \\[ { \u0026quot;key\u0026quot;: \u0026quot;io.cilium.k8s.policy.name\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;rule1\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;unspec\u0026quot; }, { \u0026quot;key\u0026quot;: \u0026quot;io.cilium.k8s.policy.namespace\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;unspec\u0026quot; } \\] } \\] Revision: 11 ``` #### Prometheus 連携 [prometheus.yaml](https://github.com/cilium/cilium/blob/doc-1.0/examples/kubernetes/prometheus.yaml) (document と path が変わってた) で Prometheus サービスをセットアップします。kubernetes service discovery で cilium コンテナを動的に見つけて cilium-agent から polling するようです``` $ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/doc-1.0/examples/kubernetes/prometheus.yaml namespace \u0026quot;prometheus\u0026quot; created service \u0026quot;prometheus\u0026quot; created deployment.extensions \u0026quot;prometheus-core\u0026quot; created configmap \u0026quot;prometheus-core\u0026quot; created clusterrolebinding.rbac.authorization.k8s.io \u0026quot;prometheus\u0026quot; created clusterrole.rbac.authorization.k8s.io \u0026quot;prometheus\u0026quot; created serviceaccount \u0026quot;prometheus-k8s\u0026quot; created configmap \u0026quot;cilium-metrics-config\u0026quot; created ```Cilium の再起動``` $ kubectl replace --force -f https://raw.githubusercontent.com/cilium/cilium/doc-1.0/examples/kubernetes/1.10/cilium.yaml configmap \u0026quot;cilium-config\u0026quot; deleted secret \u0026quot;cilium-etcd-secrets\u0026quot; deleted daemonset.apps \u0026quot;cilium\u0026quot; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026quot;cilium\u0026quot; deleted clusterrole.rbac.authorization.k8s.io \u0026quot;cilium\u0026quot; deleted serviceaccount \u0026quot;cilium\u0026quot; deleted configmap \u0026quot;cilium-config\u0026quot; replaced secret \u0026quot;cilium-etcd-secrets\u0026quot; replaced daemonset.apps \u0026quot;cilium\u0026quot; replaced clusterrolebinding.rbac.authorization.k8s.io \u0026quot;cilium\u0026quot; replaced clusterrole.rbac.authorization.k8s.io \u0026quot;cilium\u0026quot; replaced serviceaccount \u0026quot;cilium\u0026quot; replaced ``` #### 後片付け ``` $ minikube delete Deleting local Kubernetes cluster... Machine deleted. ``` ### まとめ 詳しい仕組みは確認してないけど Label を使って送信元、送信先を指定でき、L4, L7 でアクセス制御できる便利ツールだった この方のブロクを見ると BPF について理解が深まるだろうか [LinuxのBPF : (1) パケットフィルタ - 睡分不足 - mm\\_i - はてなブログ](http://mmi.hatenablog.com/entry/2016/08/01/031233)","date":"2018年5月9日","permalink":"/2018/05/cilium/","section":"Posts","summary":"Cilium 1.0: Bringing the BPF Revolution to Kubernetes Networking and Security という記事を見かけた Cilium っていうのが Version 1.0 になったそうだ。はて？これは何をもったらすものだろう？ コンテナ化、マイクロサービ","title":"Cilium ってなんだ？"},{"content":"DigitalOcean の Community サイトにある Tutorials に「How To Create a Kubernetes 1.10 Cluster Using Kubeadm on Ubuntu 16.04」というのがあったので試してみる。 Tutorial 書いて提供するとお金がもらえる（Write for DOnations）ということでなんかすごいペースで増えてる気がする\nGoal # master 1台、worker 2台という構成の Kubernetes を構築します\nPrerequisites # 1GB以上のメモリを積んだ Ubuntu 16.04 のサーバー3台を用意します (2GB Memory, 1vCPU, 50GB SSD $10/mo ($0.015/hr) のサーバーを3台用意しました)\nStep 1 - Setting Up the Workspace Directory and Ansible Inventory File # kubeadm を使えるようにするまでのセットアップを Ansible で行うため Inventory ファイルを作成\nStep 2 - Creating a Non-Root User on All Remote Servers # DigitalOcean はサーバー作成直後は root でログインする仕様なので non-root ユーザーを Ansible で作成します ubuntu という名前のユーザーで、SSH でログインでき、sudo で root になれるユーザーを作成します\nStep 3 - Installing Kubernetetes' Dependencies #  Docker のインストール Kubernetes の apt リポジトリ登録 kubelet のインストール (apt) kubeadm のインストール (apt) kubectl のインストール (apt) (master のみ)  Step 4 - Setting Up the Master Node #  kubeadm init --pod-network-cidr=10.244.0.0/16 の実行 kubeadm init で作成された /etc/kubernetes/admin.conf を /home/ubuntu/.kube/config にコピー kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml の実行  kubeadm init の実行時に crictl が無いよと言われるけど WARNING だからなくても大丈夫なのかな``` [WARNING FileExisting-crictl]: crictl not found in system path Suggestion: go get github.com/kubernetes-incubator/cri-tools/cmd/crictl\nmaster 1台だけの kubernetes ができたっぽい ubuntu@master1:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 24m v1.10.2\n次のような Pod が起動している ubuntu@master1:~$ kubectl get pods \u0026ndash;all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-master1 1/1 Running 0 23m kube-system kube-apiserver-master1 1/1 Running 0 23m kube-system kube-controller-manager-master1 1/1 Running 0 24m kube-system kube-dns-86f4d74b45-g4wvf 3/3 Running 0 24m kube-system kube-flannel-ds-rjnww 1/1 Running 0 45s kube-system kube-proxy-p9p7p 1/1 Running 0 24m kube-system kube-scheduler-master1 1/1 Running 0 24m\n ### Step 5 - Setting Up the Worker Nodes Master で `kubeadm token create --print-join-command` を実行して出力されるコマンドを Worker の2台で実行します ### Step 6 - Verifying the Cluster ubuntu@master1:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 1h v1.10.2 worker1 NotReady 10s v1.10.2 worker2 NotReady 7s v1.10.2 ubuntu@master1:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready master 1h v1.10.2 worker1 Ready 40s v1.10.2 worker2 Ready 37s v1.10.2 ubuntu@master1:~$ 2台の worker サーバーが kubernetes Cluster に追加されました ubuntu@master1:~$ kubectl get pods \u0026ndash;all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-master1 1/1 Running 0 1h kube-system kube-apiserver-master1 1/1 Running 0 1h kube-system kube-controller-manager-master1 1/1 Running 0 1h kube-system kube-dns-86f4d74b45-g4wvf 3/3 Running 0 1h kube-system kube-flannel-ds-2rqp5 1/1 Running 0 3m kube-system kube-flannel-ds-rjnww 1/1 Running 0 52m kube-system kube-flannel-ds-src6q 1/1 Running 1 3m kube-system kube-proxy-g5ptc 1/1 Running 0 3m kube-system kube-proxy-p9p7p 1/1 Running 0 1h kube-system kube-proxy-vwvgk 1/1 Running 0 3m kube-system kube-scheduler-master1 1/1 Running 0 1h ubuntu@master1:~$\n### Step 7 - Running An Application on the Cluster `kubectl run` で nginx コンテナを実行してみます``` ubuntu@master1:~$ kubectl run nginx --image=nginx --port 80 deployment.apps \u0026quot;nginx\u0026quot; created ubuntu@master1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-768979984b-sb72q 0/1 ContainerCreating 0 14s ubuntu@master1:~$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-768979984b-sb72q 1/1 Running 0 50s ubuntu@master1:~$ ```Pod の確認``` ubuntu@master1:~$ kubectl describe pods Name: nginx-768979984b-sb72q Namespace: default Node: worker2/206.189.xxx.yyy Start Time: Mon, 30 Apr 2018 12:59:47 +0000 Labels: pod-template-hash=3245355406 run=nginx Annotations: Status: Running IP: 10.244.2.2 Controlled By: ReplicaSet/nginx-768979984b Containers: nginx: Container ID: docker://8edc1fb94d6e3a43fba0074b4af14d6f1ee3617e4568036dccfe30990a22c305 Image: nginx Image ID: docker-pullable://nginx@sha256:80e2f223b2a53cfcf3fd491521e5fb9b4004d42dfc391c76011bcdd9565643df Port: 80/TCP Host Port: 0/TCP State: Running Started: Mon, 30 Apr 2018 13:00:01 +0000 Ready: True Restart Count: 0 Environment: Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-ttfqw (ro) Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-ttfqw: Type: Secret (a volume populated by a Secret) SecretName: default-token-ttfqw Optional: false QoS Class: BestEffort Node-Selectors: Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m default-scheduler Successfully assigned nginx-768979984b-sb72q to worker2 Normal SuccessfulMountVolume 2m kubelet, worker2 MountVolume.SetUp succeeded for volume \u0026quot;default-token-ttfqw\u0026quot; Normal Pulling 2m kubelet, worker2 pulling image \u0026quot;nginx\u0026quot; Normal Pulled 2m kubelet, worker2 Successfully pulled image \u0026quot;nginx\u0026quot; Normal Created 2m kubelet, worker2 Created container Normal Started 2m kubelet, worker2 Started container ubuntu@master1:~$ ```Deployment の確認``` ubuntu@master1:~$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 1 1 1 1 5m ubuntu@master1:~$ ubuntu@master1:~$ kubectl describe deployments Name: nginx Namespace: default CreationTimestamp: Mon, 30 Apr 2018 12:59:47 +0000 Labels: run=nginx Annotations: deployment.kubernetes.io/revision=1 Selector: run=nginx Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 1 max unavailable, 1 max surge Pod Template: Labels: run=nginx Containers: nginx: Image: nginx Port: 80/TCP Host Port: 0/TCP Environment: Mounts: Volumes: Conditions: Type Status Reason\n Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: NewReplicaSet: nginx-768979984b (1/1 replicas created) Events: Type Reason Age From Message\n Normal ScalingReplicaSet 6m deployment-controller Scaled up replica set nginx-768979984b to 1 ubuntu@master1:~$ このままでは外からアクセスできないため service を作成します 作成前 ubuntu@master1:~$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 2h ubuntu@master1:~$ ````kubectl exposeでNodePort` を指定``` ubuntu@master1:~$ kubectl expose deploy nginx \u0026ndash;port 80 \u0026ndash;target-port 80 \u0026ndash;type NodePort service \u0026ldquo;nginx\u0026rdquo; exposed ubuntu@master1:~$\nnginx サービスが作られました ubuntu@master1:~$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 2h nginx NodePort 10.108.40.119 80:30622/TCP 2s ubuntu@master1:~$ master1, worker1, worker2 3台にて kube-proxy が 30622 を listen しており、どの node の 30622 ポートにアクセスしても nginx へ proxy されるようになっています ubuntu@master1:~$ sudo ss -nltp | grep 30622 LISTEN 0 128 :::30622 :::* users:((\u0026ldquo;kube-proxy\u0026rdquo;,pid=7067,fd=8)) ubuntu@master1:~$\nubuntu@worker1:~$ sudo ss -nltp | grep 30622 LISTEN 0 128 :::30622 :::\\* users:((\u0026quot;kube-proxy\u0026quot;,pid=10000,fd=8)) ubuntu@worker1:~$ ubuntu@worker2:~$ sudo ss -nltp | grep 30622 LISTEN 0 128 :::30622 :::* users:((\u0026ldquo;kube-proxy\u0026rdquo;,pid=9769,fd=8)) ubuntu@worker2:~$\nService の削除 ubuntu@master1:~$ kubectl delete service nginx service \u0026ldquo;nginx\u0026rdquo; deleted ubuntu@master1:~$\n消えました。kube-proxy プロセスはいますが 30622 ポートはもう開かれていません。 ubuntu@master1:~$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 3h ubuntu@master1:~$ Deployment の削除 ubuntu@master1:~$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 1 1 1 1 35m ubuntu@master1:~$\n消えました ubuntu@master1:~$ kubectl delete deployment nginx deployment.extensions \u0026ldquo;nginx\u0026rdquo; deleted ubuntu@master1:~$ kubectl get deployments No resources found. ubuntu@master1:~$\n","date":"2018年4月30日","permalink":"/2018/04/create-kubernetes-using-kubeadm/","section":"Posts","summary":"DigitalOcean の Community サイトにある Tutorials に「How To Create a Kubernetes 1.10 Cluster Using Kubeadm on Ubuntu 16.04」というのがあったので試してみる。 Tutorial 書いて提供するとお金がもらえる（Write for","title":"kubeadm で kubernetes を構築"},{"content":"前回の続きです。 beta2 から beta3 に更新されて Amazon EKS も管理できるようになっていました。\n今回は Rancher の Web UI からポチポチやってコンテナをデプロイしてみます。\nDigitalOcean に3台の node で Kubernetes 環境を作った状態から始めます。\nWorkloads ページから Deploy ボタンをクリックします\n nginx コンテナをデプロイします。必須項目だけ埋めます。 Name: nginx Docker Image: nginx:1.12 Namespace: myapp1 (Default を使いたかったけど選択肢に出てこない) Port Mapping は Internal cluster IP で Container port を 80 とします\n 最初は namespace が active になるのにちょっと時間がかかりました\n scale: 1 の右にある「-」、「+」ボタンでコンテナの数を増減させられます\n ポチポチとクリックしてコンテナ数が3になりました\n 3つのコンテナのサービスができたところで Load Balancer を設定して外部からアクセスできるようにします。「Add Ingress」ボタンで作成画面に入ります。\n 今回はサービス1個なので「Set this rule as default backend」にチェックを入れて node の port 80 を全部先程作成した nginx のサービスに送ります。Host ヘッダーや Path で振り分け可能です。\n コンテナの右にある三点アイコンから「View Logs」を選択すると次のキャプチャの様にログを確認できます。「Execute Shell」を選択すれば docker exec / kubectl exec でコンテナ内に入れます\n こんな感じでログが流れていくのが確認できます。前回 embeded elasticsearch を有効化しましたが今回はまだ有効にしていません。\n Workload 単位の表示で三点アイコンをクリックすると次のようなメニューが表示されます。「Edit」からコンテナの更新ができます。今回はやらないけど Sidecar の追加も簡単にできそうです\n nginx を nginx:1.13.12-alpine に更新してみます、Docker Image を指定してアップグレードします\n アップグレード中のバージョンが混ざった状態をキャプチャし忘れたけど更新されました\n 本日はここまで。次は複数コンテナとかYAMLでインポートとか heml でセットアップしてみよう。\n","date":"2018年4月17日","permalink":"/2018/04/rancher-2-0-beta-part2/","section":"Posts","summary":"前回の続きです。 beta2 から beta3 に更新されて Amazon EKS も管理できるようになっていました。 今回は Rancher の Web UI からポチポチやってコンテナをデプロイしてみます。 DigitalOcean に","title":"Rancher 2.0 beta を触ってみる - その2"},{"content":"Envoy Proxy が気になっていて、docker-compose で手軽に試せる Sandbox というものがあったので試してみます\nDocs » Getting Started » Sandboxes » Front Proxy\nhttps://github.com/envoyproxy/envoy/tree/master/examples/front-proxy を使います 次のような構成を docker-compose で作ります\n$ git clone https://github.com/envoyproxy/envoy.git $ cd examples/front-proxy $ docker-compose up --build -d $ docker-compose ps Name Command State Ports --------------------------------------------------------------------------------------------------------------------------- frontproxy_front-envoy_1 /usr/bin/dumb-init -- /bin ... Up 10000/tcp, 0.0.0.0:8000-\u0026gt;80/tcp, 0.0.0.0:8001-\u0026gt;8001/tcp frontproxy_service1_1 /bin/sh -c /usr/local/bin/ ... Up 10000/tcp, 80/tcp frontproxy_service2_1 /bin/sh -c /usr/local/bin/ ... Up 10000/tcp, 80/tcp curl で /service/1 にアクセスすると service1 コンテナに proxy されます。また、/service/2 は service2 に proxy されます。\n$ curl -sv $(docker-machine ip default):8000/service/1 Hello from behind Envoy (service 1)! hostname: e145f697d053 resolvedhostname: 172.18.0.3 * Trying 192.168.99.100... * TCP_NODELAY set * Connected to 192.168.99.100 (192.168.99.100) port 8000 (#0) \u0026gt; GET /service/1 HTTP/1.1 \u0026gt; Host: 192.168.99.100:8000 \u0026gt; User-Agent: curl/7.57.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-type: text/html; charset=utf-8 \u0026lt; content-length: 89 \u0026lt; server: envoy \u0026lt; date: Tue, 10 Apr 2018 12:58:39 GMT \u0026lt; x-envoy-upstream-service-time: 8 \u0026lt; { [89 bytes data] * Connection #0 to host 192.168.99.100 left intact コンテナ内を確認してみます front-proxy コンテナのプロセスは次のようになっています\n$ winpty docker-compose exec front-envoy bash root@ad056b51de9b:/# ps auxwwf USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 13 0.1 0.3 18236 3400 pts/0 Ss 13:03 0:00 bash root 26 0.0 0.2 34420 2936 pts/0 R+ 13:04 0:00 \\_ ps auxwwf root 1 0.0 0.0 208 4 ? Ss 12:46 0:00 /usr/bin/dumb-init -- /bin/sh -c /u sr/local/bin/envoy -c /etc/front-envoy.yaml --service-cluster front-proxy root 5 0.0 0.0 4500 744 ? Ss 12:46 0:00 /bin/sh -c /usr/local/bin/envoy -c /etc/front-envoy.yaml --service-cluster front-proxy root 6 0.6 2.1 90740 22164 ? Sl 12:46 0:07 \\_ /usr/local/bin/envoy -c /etc/fr ont-envoy.yaml --service-cluster front-proxy root@ad056b51de9b:/# サービス側は次のようになっています\n$ winpty docker-compose exec service1 bash bash-4.4# ps auxwwf PID USER TIME COMMAND 1 root 0:00 /bin/sh -c /usr/local/bin/start_service.sh 5 root 0:00 bash /usr/local/bin/start_service.sh 6 root 0:00 python3 /code/service.py 7 root 0:07 envoy -c /etc/service-envoy.yaml --service-cluster servic 16 root 0:36 /usr/bin/python3 /code/service.py 21 root 0:00 bash 25 root 0:00 ps auxwwf bash-4.4# Proxy Server の /etc/front-envoy.yaml の内容は次のようになっています\nstatic_resources:listeners:- address:# 0.0.0.0:80 を listensocket_address:address:0.0.0.0port_value:80filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:backenddomains:# Host ヘッダー条件- \u0026#34;*\u0026#34;routes:- match:# /service/1 は下の clusters で定義されてる service1 に proxyprefix:\u0026#34;/service/1\u0026#34;route:cluster:service1- match:# /service/2 は下の clusters で定義されてる service2 に proxyprefix:\u0026#34;/service/2\u0026#34;route:cluster:service2http_filters:- name:envoy.routerconfig:{}clusters:- name:service1connect_timeout:0.25stype:strict_dnslb_policy:round_robin# 振り分けアルゴリズムhttp2_protocol_options:{}hosts:# proxy 先サーバーリスト- socket_address:address:service1port_value:80- name:service2connect_timeout:0.25stype:strict_dnslb_policy:round_robin# 振り分けアルゴリズムhttp2_protocol_options:{}hosts:# proxy 先サーバーリスト- socket_address:address:service2port_value:80admin:access_log_path:\u0026#34;/dev/null\u0026#34;address:socket_address:address:0.0.0.0port_value:8001:8001 で Admin UI にアクセスできます\n/help にアクセスすると各 path の説明が返ってきます。prometheus 用の stats endpoint もあるし、オンラインで logging やランタイム設定を変更できるようです。\nadmin commands are: /: Admin home page /certs: print certs on machine /clusters: upstream cluster status /config_dump: dump current Envoy configs /cpuprofiler: enable/disable the CPU profiler /healthcheck/fail: cause the server to fail health checks /healthcheck/ok: cause the server to pass health checks /help: print out list of admin commands /hot_restart_version: print the hot restart compatability version /listeners: print listener addresses /logging: query/change logging levels /quitquitquit: exit the server /reset_counters: reset all counters to zero /runtime: print runtime values /runtime_modify: modify runtime values /server_info: print server version/status information /stats: print server stats /stats/prometheus: print server stats in prometheus format Service Server 側の /etc/service-envoy.yaml の内容は次のようになっています。Kubernetes だと side car で動かすやつですね。 内容は front-proxy とほぼ同じでした。\nstatic_resources:listeners:- address:# 0.0.0.0:80 を listensocket_address:address:0.0.0.0port_value:80filter_chains:- filters:- name:envoy.http_connection_managerconfig:codec_type:autostat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:servicedomains:- \u0026#34;*\u0026#34;routes:- match:prefix:\u0026#34;/service\u0026#34;route:cluster:local_servicehttp_filters:- name:envoy.routerconfig:{}clusters:- name:local_serviceconnect_timeout:0.25stype:strict_dnslb_policy:round_robinhosts:- socket_address:address:127.0.0.1port_value:8080admin:access_log_path:\u0026#34;/dev/null\u0026#34;address:socket_address:address:0.0.0.0port_value:8081次に service1 のコンテナを3つに増やしてみます\n$ docker-compose scale service1=3 The scale command is deprecated. Use the up command with the --scale flag instead. Starting frontproxy_service1_1 ... done Creating frontproxy_service1_2 ... done Creating frontproxy_service1_3 ... done curl -sv $(docker-machine ip default):8000/service/1 を実行すると service1 の3つのコンテナに順にアクセスが振り分けられました。front-envoy.yaml が書き換わる感じはしなかったけどと、front-envoy.yaml を確認しましたがやはり変わっていません。単に DNS RoundRobin でした。\ncluster の type が strict_dns となってるために DNS で複数のIPアドレスが返ってくるとそれぞれのアドレスを別々のホストとして lb_policy にしたがって振り分けてくれるようです。\n# dig +short @127.0.0.11 service1 172.18.0.6 172.18.0.5 172.18.0.3 service discovery や healthcheck まわりをもうちょっと調べてみよう\n","date":"2018年4月10日","permalink":"/2018/04/envoy-front-proxy-sandbox/","section":"Posts","summary":"Envoy Proxy が気になっていて、docker-compose で手軽に試せる Sandbox というものがあったので試してみます Docs » Getting Started » Sandboxes » Front Proxy https://github.com/envoyproxy/envoy/tree/master/examples/front-proxy を使います 次のような","title":"Envoy を Front Proxy Sandbox で動作確認"},{"content":"Rancher 2.0 が beta になったそうなので試してみます Rancher 2.0 Now Feature Complete, Enters Final Beta Phase\n 2.0 のゴールはここに書かれています Release Goals\n準備 # Quick Start Guide の HOST AND NODE REQUIREMENTS に\nOperating System: Ubuntu 16.04 (64-bit) Memory: 4GB Ports: 80, 443 Software: Docker Supported Versions: 1.12.6 1.13.1 17.03.2 とあるので、DigitalOcean の One-click apps で Docker 17.12.0~ce on 16.04 を使って Docker のインストールされたメモリ 4GB の Ubuntu 16.04 を用意します。\ndoctl compute droplet create rancher20 \\  --image docker-16-04 \\  --region sgp1 \\  --size s-2vcpu-4gb \\  --ssh-keys 16797382 \\  --enable-private-networking \\  --enable-monitoring root@rancher20:~# docker version Client: Version: 17.12.0-ce API version: 1.35 Go version: go1.9.2 Git commit: c97c6d6 Built: Wed Dec 27 20:11:19 2017 OS/Arch: linux/amd64 Server: Engine: Version: 17.12.0-ce API version: 1.35 (minimum version 1.12) Go version: go1.9.2 Git commit: c97c6d6 Built: Wed Dec 27 20:09:53 2017 OS/Arch: linux/amd64 Experimental: false Docker の version がちょっと新しすぎるのかな？でもまあこのまま勧めてみよう。 (Kubernetes v1.10 Release Notes の External Dependencies には \u0026ldquo;The validated docker versions are the same as for v1.9: 1.11.2 to 1.13.1 and 17.03.x (ref)\u0026rdquo; とある) 後で出てきますが Kubernetes の node で使う docker はこれではなく、ちゃんと対応したバージョンを Rancher がインストールしてくれます。\nところで、いつの間にか Container Distributions っていう Image 種別ができてて RancherOS まで揃ってますね。\nDigital Ocean の Image 選択画面  Rancher 2.0 のインストール # QuickStart に INSTALL って書いてあるけど docker run するだけです\ndocker run -d --name rancher --restart=unless-stopped -p 80:80 -p 443:443 rancher/server:preview root@rancher20:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ed3e26b3b4b4 rancher/server:preview \u0026quot;rancher --http-list…\u0026quot; 6 seconds ago Up 4 seconds 0.0.0.0:80-\u0026gt;80/tcp, 0.0.0.0:443-\u0026gt;443/tcp rancher Rancher 2.0 にアクセス # サーバーの 443 ポートにアクセスすると、まず admin ユーザーのパスワードを設定せよ言われます\nadmin ユーザーのパスワード設定  次に URL の指定です。DNS を設定したりしてたらそれを指定します。この後作る各 node からアクセスできる必要があります。\nRancher Server の URL 設定  ここからクラスタを追加します。\nクラスタの追加  Node Drivers ページ。Amazon EC2, Azure, DigitalOcean, vSphere はデフォルトで有効になっています。これらを使うのであれば Rancher が API でアクセスして node をセットアップしてくれます。今回は DigitalOcean を使います、最初から有効になっているので特にすることは無し、Token 設定はクラスタ作成時の Node Template で行います\nNode Driver リスト  ユーザーの作成画面です。権限を細かく指定できます。次の画面から別途 Role を作成することも可能です\nUser の作成  Security -\u0026gt; Roles で Role の確認ができます。このキャプチャに載っているのは一部のみ。Add Role から追加できます\nRole 一覧  Security -\u0026gt; Authentication 画面がこちら、認証プロバイダとして Active Directory と GitHub が使えるようです。LDAP は使えないのかな？ Active Directory でなんとかなるのかな？\n認証設定  Pod の Security Policy を作成できるらしい、まだ良くわかってない\nSecurity Policy 設定画面  カタログは Helm 用の機能になったっぽい？\nカタログ  それでは DigitalOcean でクラスタの追加を行います。Kubernetes version は v1.8.10-rancher1-1, v1.9.5-rancher1-1, v1.10.0-rancher1-1 から選択できます。最新の v1.10.0 にしてみます。Network Provider は Flanel, Calico, Canal から選べます。デフォルトが Canal になってるのでこれを使ってみます。Docker version on nodes で \u0026ldquo;Require a supported Docker version\u0026rdquo; と \u0026ldquo;Allow unsupported versions\u0026rdquo; を選択できますが、Rancher Server 用に 17.12.0-ce 使ったので \u0026ldquo;Allow unsupported versios\u0026rdquo; を選んだのですが、node 用の docker はちゃんと Rancher が適したバージョンのものを入れてくれるのでそんな必要はありませんでした。\nクラスタの追加  Add Node Template をクリックすると DigitalOcean の Token を入力画面が出ます\nNode Template の追加  次にどこのリージョンにどんなスペックでホストを作るかを指定します\n これでいよいよ作成できますが、etcd, Control, Worker をどの template でセットアップするか指定することもできるのでそれぞれを適したサイズのインスタンスとすることができます\n 作成をはじめました\n 作成中\n 作成中\n セットアップ完了\n Node 情報はこんな感じ\nNode 情報  ログの送り先もいろいろ選択肢があります、Embedded Elasticsearch を選択してみました\n Namespace 一覧です、cattle-system ってところに Embedded Elasticsearch 環境が作られてそうだけど pod の確認方法がまだわからん\u0026hellip;\n Rancher 1.x での Kubernetes 管理は Kubernetes の Dashboard をそのまま使うことになっていましたが、2.0 では Rancher の画面で全部行うっぽいです。Pod の一覧とか見る方法がまだわからんのだが kubectl で見れば良いのかな\u0026hellip; クラスタ画面に Launch kubectl というボタンがあり、そこから kubectl コマンドがブラウザ上で実行できます。\n\u0026gt; kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE cattle-logging elasticsearch-5b4dbd9c6f-qjvvx 1/1 Running 0 41m cattle-logging fluentd-4vzxw 2/2 Running 0 41m cattle-logging fluentd-6rq4x 2/2 Running 0 41m cattle-logging fluentd-kppdh 2/2 Running 0 41m cattle-logging kibana-866d475695-z6t4b 1/1 Running 0 41m cattle-system cattle-cluster-agent-5697cbf779-hwmxq 1/1 Running 0 48m cattle-system cattle-node-agent-44g8r 1/1 Running 0 48m cattle-system cattle-node-agent-59tpp 1/1 Running 0 48m cattle-system cattle-node-agent-xp2p7 1/1 Running 0 48m ingress-nginx default-http-backend-564b9b6c5b-vjfc9 1/1 Running 0 48m ingress-nginx nginx-ingress-controller-7jgbr 1/1 Running 0 48m ingress-nginx nginx-ingress-controller-tpqmp 1/1 Running 0 48m ingress-nginx nginx-ingress-controller-w627f 1/1 Running 0 48m kube-system canal-2tpw4 3/3 Running 0 48m kube-system canal-6gkch 3/3 Running 0 48m kube-system canal-wm7hj 3/3 Running 0 48m kube-system kube-dns-7dfdc4897f-7skjn 3/3 Running 0 48m kube-system kube-dns-autoscaler-6c4b786f5-2qwvm 1/1 Running 0 48m Web UI では見えてなかった cattle-logging という Namespace が存在しますね、うーむ 今日はここまで。\n","date":"2018年4月9日","permalink":"/2018/04/rancher-2-0-beta2-part1/","section":"Posts","summary":"Rancher 2.0 が beta になったそうなので試してみます Rancher 2.0 Now Feature Complete, Enters Final Beta Phase 2.0 のゴールはここに書かれています Release Goals 準備 # Quick Start Guide の HOST AND NODE REQUIREMENTS に Operating System: Ubuntu 16.04 (64-bit) Memory: 4GB Ports: 80, 443 Software: Docker","title":"Rancher 2.0 beta を触ってみる - その1"},{"content":"今回は docker secret を使ってみます。 Manage sensitive data with Docker secrets に使い方が書かれています。Kubernetes の Secrets とかなり似てます。 docker secret コマンドの syntax は次のようになってます。\n$ docker secret --help Usage: docker secret COMMAND Manage Docker secrets Options: --help Print usage Commands: create Create a secret from a file or STDIN as content inspect Display detailed information on one or more secrets ls List secrets rm Remove one or more secrets Run 'docker secret COMMAND --help' for more information on a command. Secret の登録 # docker secret create で作成します。ファイルの中身を値として登録するか、ファイルの path として \u0026ldquo;-\u0026rdquo; を指定することで標準入力から値を読み取って登録します。\n$ docker secret create --help Usage: docker secret create [OPTIONS] SECRET [file|-] Create a secret from a file or STDIN as content Options: -d, --driver string Secret driver --help Print usage -l, --label list Secret labels 登録時に表示される文字列は作成した secret を指す ID です。\n$ echo password | docker secret create mypass - mhop3cx9kk5xuz4b95nokvzov $ docker secret ls ID NAME DRIVER CREATED UPDATED mhop3cx9kk5xuz4b95nokvzov mypass 5 seconds ago 5 seconds ago docker secret inspect で secret のメタ情報が確認ができます\n$ docker secret inspect mypass [ { \u0026quot;ID\u0026quot;: \u0026quot;mhop3cx9kk5xuz4b95nokvzov\u0026quot;, \u0026quot;Version\u0026quot;: { \u0026quot;Index\u0026quot;: 16 }, \u0026quot;CreatedAt\u0026quot;: \u0026quot;2018-03-24T14:41:50.123712806Z\u0026quot;, \u0026quot;UpdatedAt\u0026quot;: \u0026quot;2018-03-24T14:41:50.123712806Z\u0026quot;, \u0026quot;Spec\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;mypass\u0026quot;, \u0026quot;Labels\u0026quot;: {} } } ] ID でも名前でも確認できます。\n$ docker secret inspect mhop3cx9kk5xuz4b95nokvzov [ { \u0026quot;ID\u0026quot;: \u0026quot;mhop3cx9kk5xuz4b95nokvzov\u0026quot;, \u0026quot;Version\u0026quot;: { \u0026quot;Index\u0026quot;: 16 }, \u0026quot;CreatedAt\u0026quot;: \u0026quot;2018-03-24T14:41:50.123712806Z\u0026quot;, \u0026quot;UpdatedAt\u0026quot;: \u0026quot;2018-03-24T14:41:50.123712806Z\u0026quot;, \u0026quot;Spec\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;mypass\u0026quot;, \u0026quot;Labels\u0026quot;: {} } } ] 同名の secret を複数作成することはできません\n$ echo hogehoge | docker secret create mypass - Error response from daemon: rpc error: code = AlreadyExists desc = secret mypass already exists では値を更新したい場合はどうするかというと、secret は service で使うわけですが、どの secret をどんな名前で参照させるかを指定できるようになっているため、まず app_passwd_v1 を passwd として参照するようにしておき、docker service update で app_passwd_v2 を passwd で参照するように service を更新します。\nService から secret を参照する # secret は docker run で起動するコンテナでは使えず、service を使う必要があります。\n$ docker service create --detach --name nginx --secret mypass nginx:latest wxmhxc0po5fxzbfh2i84ug2nf $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 622efaa68880 nginx:latest \u0026quot;nginx -g 'daemon ...\u0026quot; 56 seconds ago Up 56 seconds 80/tcp nginx.1.zfcqa0ofjygrxe30jm1ftjb89 /run/secrets/ に secret 名のファイルができます\n$ docker exec 622efaa68880 cat //run/secrets/mypass password (Windows の git-bash を使ってるので無駄な \u0026ldquo;/\u0026rdquo; があります)\nService が参照する secret を入れ替える # 先程説明した方法で mypass を更新してみます 新しく mypass2 という名前の secret を作成する\n$ echo hogehoge | docker secret create mypass2 - 6kuu6o995snvokzudp1m5y7f1 先程は --secret で secret 名を指定しただけでした。これは secret 名をそのまま /run/secrets/ 下のファイル名として参照させることになりますが、source と target を別に指定することで secret 名と参照名を別にできます。\n$ docker service update --secret-add source=mypass2,target=mypass --secret-rm mypass nginx nginx service が更新され、コンテナが生まれ変わりました\n$ docker service ps nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS rmj99vdjpipe nginx.1 nginx:latest myvm1 Running Running 42 seconds ago zfcqa0ofjygr \\_ nginx.1 nginx:latest myvm1 Shutdown Shutdown 43 seconds ago docker ps で container id を確認して\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7198b32c47a0 nginx:latest \u0026quot;nginx -g 'daemon ...\u0026quot; About a minute ago Up About a minute 80/tcp nginx.1.rmj99vdjpipevrhpo2walrmm2 mypass を確認してみます。更新されてますね。\n$ docker exec 7198b32c47a0 cat //run/secrets/mypass hogehoge docker service inspect で確認してみます\n$ docker service inspect nginx | jq .[].Spec.TaskTemplate.ContainerSpec.Secrets [ { \u0026quot;File\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;mypass\u0026quot;, \u0026quot;UID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;GID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Mode\u0026quot;: 292 }, \u0026quot;SecretID\u0026quot;: \u0026quot;6kuu6o995snvokzudp1m5y7f1\u0026quot;, \u0026quot;SecretName\u0026quot;: \u0026quot;mypass2\u0026quot; } ] service は PreviousSpec に一つ前の情報も持っているのですね\n$ docker service inspect nginx | jq .[].PreviousSpec.TaskTemplate.ContainerSpec.Secrets [ { \u0026quot;File\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;mypass\u0026quot;, \u0026quot;UID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;GID\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;Mode\u0026quot;: 292 }, \u0026quot;SecretID\u0026quot;: \u0026quot;mhop3cx9kk5xuz4b95nokvzov\u0026quot;, \u0026quot;SecretName\u0026quot;: \u0026quot;mypass\u0026quot; } ] Stack で secret を使う # 次の内容で docker-compose.yml を用意します\nversion: '3.1' services: db: image: mysql:latest volumes: - db_data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD_FILE: /run/secrets/db_password secrets: - db_root_password - db_password wordpress: depends_on: - db image: wordpress:latest ports: - \u0026quot;8000:80\u0026quot; environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD_FILE: /run/secrets/db_password secrets: - db_password secrets: db_password: file: db_password.txt db_root_password: file: db_root_password.txt volumes: db_data: db_password.txt, db_root_password.txt というパスワードの書かれたファイルを用意して docker stack deploy で deploy します\n$ docker stack deploy --compose-file docker-compose.yml secrets-test Creating network secrets-test_default Creating service secrets-test_wordpress Creating service secrets-test_db secret が作成されています stack 名が secret 名の prefix として付加されていますね\n$ docker secret ls ID NAME DRIVER CREATED UPDATED mhop3cx9kk5xuz4b95nokvzov mypass About an hour ago About an hour ago 6kuu6o995snvokzudp1m5y7f1 mypass2 16 minutes ago 16 minutes ago jl70yk1b4ydo5rhc5mol7pybp secrets-test_db_password 20 seconds ago 20 seconds ago in18f83ord099qiu2q8r4ctkc secrets-test_db_root_password 20 seconds ago 20 seconds ago docker ps で container id を確認\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c9ae594886b9 mysql:latest \u0026quot;docker-entrypoint...\u0026quot; 3 minutes ago Up 3 minutes 3306/tcp secrets-test_db.1.35sznd2qiyucfstwepz85o22h 7198b32c47a0 nginx:latest \u0026quot;nginx -g 'daemon ...\u0026quot; 18 minutes ago Up 18 minutes 80/tcp nginx.1.rmj99vdjpipevrhpo2walrmm2 ファイルがありますね\n$ docker exec c9ae594886b9 ls //run/secrets/ db_password db_root_password 中身も確認できました\n$ docker exec c9ae594886b9 cat //run/secrets/db_password DbAppPass $ docker exec c9ae594886b9 cat //run/secrets/db_root_password DbRootPass パスワードファイルを書き換えて deploy すると secret のパスワードは更新されるでしょうか？\n$ cat db_password.txt DbAppPass2 $ docker stack deploy --compose-file docker-compose.yml secrets-test failed to update secret secrets-test_db_password: Error response from daemon: rpc error: code = InvalidArgument desc = only updates to Labels are allowed ダメでした\u0026hellip; services の secrets には LONG SYNTAX があり、source, target, uid, gid, mode が指定可能でした。そこで、docker-compose.yml を次のように書き換えて deploy すると更新できました。\nversion: '3.1' services: db: image: mysql:latest volumes: - db_data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD_FILE: /run/secrets/db_password secrets: - db_root_password - source: db_password2 target: db_password wordpress: depends_on: - db image: wordpress:latest ports: - \u0026quot;8000:80\u0026quot; environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD_FILE: /run/secrets/db_password secrets: - source: db_password2 target: db_password secrets: db_password2: file: db_password2.txt db_root_password: file: db_root_password.txt volumes: db_data: 更新のため、再 deploy\n$ docker stack deploy --compose-file docker-compose.yml secrets-test Updating service secrets-test_db (id: thimv598po993rnbetm85z7kt) Updating service secrets-test_wordpress (id: jkje0u6n7z690zm0y34p9j2go) secret が増えました\n$ docker secret ls ID NAME DRIVER CREATED UPDATED mhop3cx9kk5xuz4b95nokvzov mypass About an hour ago About an hour ago 6kuu6o995snvokzudp1m5y7f1 mypass2 31 minutes ago 31 minutes ago jl70yk1b4ydo5rhc5mol7pybp secrets-test_db_password 15 minutes ago 15 minutes ago m8drh3ldgnt0vstft2xewte78 secrets-test_db_password2 39 seconds ago 39 seconds ago in18f83ord099qiu2q8r4ctkc secrets-test_db_root_password 15 minutes ago 39 seconds ago コンテナが更新されて\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 722657b03d5f mysql:latest \u0026quot;docker-entrypoint...\u0026quot; 42 seconds ago Up 36 seconds 3306/tcp secrets-test_db.1.mw3wxtfgagoei3bxd8c3yfsh5 7198b32c47a0 nginx:latest \u0026quot;nginx -g 'daemon ...\u0026quot; 29 minutes ago Up 29 minutes 80/tcp nginx.1.rmj99vdjpipevrhpo2walrmm2 パスワードも更新されました。まあ、コンテナ起動時にこの変更が DB に反映されるのかというのは別問題だけどね\n$ docker exec 722657b03d5f cat //run/secrets/db_password DbAppPass2 以上\n","date":"2018年3月24日","permalink":"/2018/03/docker-swarm-mode-4/","section":"Posts","summary":"今回は docker secret を使ってみます。 Manage sensitive data with Docker secrets に使い方が書かれています。Kubernetes の Secrets とかなり似てます。 docker secret コマンドの syntax は次のようになって","title":"Docker Swarm mode を知る (secret)"},{"content":"インストール # CentOS 7 で Installing AWX にある Docker を使ったインストールで試します。 必要なパッケージのインストール\n$ sudo yum -y install git ansible docker python-docker-py docker daemon の起動\n$ sudo systemctl start docker $ sudo systemctl enable docker AWX リポジトリを clone して、installer ディレクトリにある install.yml を ansible-playbook で実行する なにもいじらないで実行すれば docker でのインストールになる\n$ git clone https://github.com/ansible/awx.git $ cd awx/installer $ sudo ansible-playbook -i inventory install.yml playbook が完走すると awx, memcached, RabbitMQ, PostgreSQL のコンテナが起動しています\n$ sudo docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES eca1ee848e82 ansible/awx_task:latest \u0026quot;/tini -- /bin/sh ...\u0026quot; About a minute ago Up About a minute 8052/tcp awx_task e57ea0f64383 ansible/awx_web:latest \u0026quot;/tini -- /bin/sh ...\u0026quot; 2 minutes ago Up 2 minutes 0.0.0.0:80-\u0026gt;8052/tcp awx_web aeadac6dc4fe memcached:alpine \u0026quot;docker-entrypoint...\u0026quot; 5 minutes ago Up 5 minutes 11211/tcp memcached b3777fcbf06a rabbitmq:3 \u0026quot;docker-entrypoint...\u0026quot; 5 minutes ago Up 5 minutes 4369/tcp, 5671-5672/tcp, 25672/tcp rabbitmq 5da460e26ea0 postgres:9.6 \u0026quot;docker-entrypoint...\u0026quot; 6 minutes ago Up 6 minutes 5432/tcp postgres これで port 80 にアクセスすると DB の migration が始まって、完了後に使えるようになります。\n ログイン画面が表示されたら ID: admin Password: password でアクセスできます。 ダッシュボード画面\nAWX DASHBOARD  使ってみる # ちょっと最初はわかりにくい 上から順にいくと、PROJECTS で playbook のリポジトリを設定します。Git や Mercurial、Subversion の repository を指定したり、ローカルファイルシステムのディレクトリを指定します。\n次に CREDENTIALS で ansible でセットアップするサーバーにアクセスするための SSH の秘密鍵やユーザー名を登録する。\nINVENTORIES で ansible-playbook コマンドの -i で指定する inventory ファイル相当のホスト情報を設定する。プロジェクトのリポジトリにインベントリファイルが含まれる場合はそれを指定することも可能です。\nここまでできたら TEMPLATES でこれらのオブジェクトを組み合わせてどのホストに対してどの Playbook を実行するのかを定義する 設定例として入ってる Demo Project の設定内容 (https://github.com/ansible/ansible-tower-samples が登録されています)\nAWX Demo Project  INVENTORIESに「My Inventory」という名前で3台のホスト(node-1, node-2, node-3)を設定してみたところ\nAWX Inventory Hosts  Demo Project を先の3台のホスト(Inventory)に対して実行するための Template 設定。この環境は Vagrant で構築してるので Vagrant という名前で CREDENTIALS に SSH の秘密鍵を登録してあります。Demo Project で指定されたリポジトリ内の hello_world.yml という Playbook を実行するようになっています\nAWX Test Template  Template の一覧画面、ロケットアイコンをクリックすると実行されます\nAWX Templates  実行結果画面\nAWX Jobs  lightbulb の playbook を実行してみる # https://github.com/ansible/lightbulb に Ansible の学習用コンテンツがあります。ここの examples 下にある playbook を AWX で実行してみます。\nまずは apache-simple-playboook を使ってみます。\nlightbulb プロジェクトを作成します\nAWX Project lightbulb  Get latest SCM revsion という雲に下矢印のアイコンをクリックするとリポジトリのファイルが取得され、Template 作成時に playbook の YAML をリストから選択できるようになります\nAWX Projects  apache-simple-playbook/site.yml を実行するテンプレートを作成します\nAWX Simple Apache Template  apache-simple-playbook は web グループのホストに対して実行するようになっているため Inventory 設定で web グループに参加させます\nAWX Inventory web group  実行された Job の情報\nAWX Simple Apache Job  ansible-playbook の出力はテキストでダウンロードも可能です\nIdentity added: /tmp/awx_17_8I3LZB/credential_2 (/tmp/awx_17_8I3LZB/credential_2) PLAY [Ensure apache is installed and started] ********************************** TASK [Gathering Facts] ********************************************************* ok: [node-3] ok: [node-1] ok: [node-2] TASK [Ensure httpd package is present] ***************************************** changed: [node-1] changed: [node-3] changed: [node-2] TASK [Ensure latest index.html file is present] ******************************** changed: [node-3] changed: [node-1] changed: [node-2] TASK [Ensure httpd is started] ************************************************* changed: [node-1] changed: [node-3] changed: [node-2] PLAY RECAP ********************************************************************* node-1 : ok=4 changed=3 unreachable=0 failed=0 node-2 : ok=4 changed=3 unreachable=0 failed=0 node-3 : ok=4 changed=3 unreachable=0 failed=0 その他、より高度な使い方 # AWX へのログイン認証には Azure AD, GitHub, GitHub Org, GitHub Team, Google OAuth2, LDAP, RADIUS, SAML, TACACS+ が使えるようになっています。\nユーザーごとの権限設定も可能で、どの playbook を誰が実行できるようにするかを制御できます。サーバーへ SSH したり sudo する権限が無い人でもサーバーへのログイン情報は AWX で管理されているため playbook を使った設定変更などは行えるようになります。\nNOTIFICATION 設定では Email, Slack, Twilio, Pagerduty, HipChat, Webhook, Mattermost, IRC を使った通知設定が可能になっています。\nCREDENTIALS ではさまざまなタイプの認証情報が登録でき、その中に Vault もあり、TEMPLATE には複数の Credentials が設定できるため Ansible Vault も認証情報を追加すれば問題なく使えるようです。\nINVENTORIES では SOURCE という設定があり Sourced from a Project, Amazon EC2, Google Compute Engine, Microsoft Azure Resource Manager, VMware vCenter, Red Hat Satellite 6, Red Hat CloudForms, OpenStack, Red Hat Virtualization, Ansible Tower, Custom Script があり、これらからホスト情報を簡単に取得して Inventory として使えるようになっているようです。 実行時に都度変数を指定するために Ansible では vars_prompt でプロンプトを表示して入力させることができますが、AWX (Ansible Tower) ではこれが使えず、代わりに Template で Survey を設定します。\nAnsible Tower って日本語ドキュメントもそろってるんですね、さすが Red Hat さん http://docs.ansible.com/ansible-tower/ 昔の自分が同じタイトルの記事を書いてたことを見つけてしまった\n Ansible AWX を試す その1 #ansible Ansible AWX を試す その2 #ansible  Ansible Tower はもともと AnsibleWorks AWX という名前だったのですね。\nansibleworks.com ドメインが保育士求人サイトによって買われてるのか\u0026hellip;\n","date":"2018年3月23日","permalink":"/2018/03/awx/","section":"Posts","summary":"インストール # CentOS 7 で Installing AWX にある Docker を使ったインストールで試します。 必要なパッケージのインストール $ sudo yum -y install git ansible docker python-docker-py docker daemon の起動 $ sudo systemctl start docker $ sudo systemctl enable","title":"AWX (OSS 版 Ansible Tower) を試す"},{"content":"Docker Swarm を Windows 上に構築してみます。 https://docs.docker.com/get-started/part4/ にドキュメントがありました。私の家の Laptop は Windows Pro じゃないから VirtualBox 使うやつです。\nDocker Machine を2台作成 # docker-machine create --driver virtualbox myvm1 docker-machine create --driver virtualbox myvm2 $ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS default - virtualbox Stopped Unknown myvm1 * virtualbox Running tcp://192.168.99.100:2376 v17.12.1-ce myvm2 - virtualbox Running tcp://192.168.99.101:2376 v17.12.1-ce Swarm の初期化と node の追加 # myvm1 で swarm init\n$ docker-machine ssh myvm1 \u0026quot;docker swarm init --advertise-addr \\$(ip a s eth1 | grep 'inet ' | awk '{print \\$2}' | sed 's/\\/.*//')\u0026quot; Swarm initialized: current node (u6otahnn4p9q72p734vcmtskt) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-4pwif80w4dgtdrb7zo2j6bq1v22cun6cn4ftt1fj99tx5nlhmh-dl0ikqtubkk2ejkx5tvh99ul0 192.168.99.100:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. myvm2 を swarm に worker として参加させる\n$ docker-machine ssh myvm2 \u0026quot;docker swarm join --token SWMTKN-1-4pwif80w4dgtdrb7zo2j6bq1v22cun6cn4ftt1fj99tx5nlhmh-dl0ikqtubkk2ejkx5tvh99ul0 192.168.99.100:2377\u0026quot; This node joined a swarm as a worker. 完成！！\n$ docker-machine ssh myvm1 docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS u6otahnn4p9q72p734vcmtskt * myvm1 Ready Active Leader su7h7svkmocwvrp55u3xygjd4 myvm2 Ready Active SSH しなくても環境変数を設定すれば docker コマンドで確認できます（Windows だけど git-bash だから eval です）。\n$ eval $(docker-machine env myvm1) $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS u6otahnn4p9q72p734vcmtskt * myvm1 Ready Active Leader su7h7svkmocwvrp55u3xygjd4 myvm2 Ready Active demo アプリを stack で deploy する # 前回の stackdemo を stack で deploy してみます。\n$ docker stack deploy --compose-file docker-compose.yml stackdemo Ignoring unsupported options: build Creating network stackdemo_default Creating service stackdemo_web Creating service stackdemo_redis できましたね。\n$ docker stack ls NAME SERVICES stackdemo 2 $ docker stack ps stackdemo ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS rlh00omfhh6g stackdemo_redis.1 redis:alpine myvm1 Running Running 32 seconds ago 4e07tj4g36u9 stackdemo_web.1 yteraoka/stackdemo:0.0.4 myvm2 Running Starting 28 seconds ago どちらの node の 8000 番ポートにアクセスしても mesh routing によってアクセスできます。\n$ curl -s http://192.168.99.100:8000/ Hello World! I have been seen 1 times. version: 0.0.4 $ curl -s http://192.168.99.101:8000/ Hello World! I have been seen 2 times. version: 0.0.4 Portainer を stack で deploy する # 次に Portainer を stack で deploy してみます。https://github.com/portainer/portainer-compose にある docker-stack.yml を使いますが、port のところをちょっといじります。\n$ git diff diff --git a/docker-stack.yml b/docker-stack.yml index b54df0e..c18e063 100644 --- a/docker-stack.yml +++ b/docker-stack.yml @@ -4,7 +4,7 @@ services:  portainer: image: portainer/portainer ports: - - \u0026#34;9000\u0026#34; + - \u0026#34;9000:9000\u0026#34;  networks: - portainer-net volumes: $ git clone https://github.com/portainer/portainer-compose.git Cloning into 'portainer-compose'... remote: Counting objects: 71, done. remote: Total 71 (delta 0), reused 0 (delta 0), pack-reused 71 Unpacking objects: 100% (71/71), done. $ cd portainer-compose/ $ docker stack deploy --compose-file docker-stack.yml portainer Creating network portainer_portainer-net Creating service portainer_portainer http://192.168.99.100:9000 にアクセスすると Portainer にアクセスできます。\nDashboard はこんな感じ\nPortainer Dashboard  Swarm Visualizer なんてのもありました\nPortainer Swarm Visualizer  ","date":"2018年3月20日","permalink":"/2018/03/docker-swarm-mode-3/","section":"Posts","summary":"Docker Swarm を Windows 上に構築してみます。 https://docs.docker.com/get-started/part4/ にドキュメントがありました。私の家の Laptop は Windows Pro じゃないから VirtualBox 使うやつです。 Docker Machine を2台作成 # docker-machine create --driver virtualbox myvm1 docker-machine create --driver virtualbox myvm2 $","title":"Docker Swarm mode を知る (Swarm on Windows)"},{"content":"","date":"2018年3月20日","permalink":"/tags/portainer/","section":"Tags","summary":"","title":"Portainer"},{"content":"","date":"2018年3月20日","permalink":"/tags/windows/","section":"Tags","summary":"","title":"Windows"},{"content":"docker-compose で stack を deploy する # https://docs.docker.com/engine/swarm/stack-deploy/ にあるサンプルアプリ、設定を用いて実際に動かしてみます。使ったコードは https://github.com/yteraoka/stackdemo に、イメージは https://hub.docker.com/r/yteraoka/stackdemo/ に。 次の内容の docker-compose.yml を用意し``` version: \u0026lsquo;3\u0026rsquo;\nservices: web: image: yteraoka/stackdemo build: . ports: - \u0026ldquo;8000:8000\u0026rdquo; redis: image: redis:alpine\n````docker stack deployの\u0026ndash;compose-fileで指定して実行するとdocker-compose upのような出力がされ、stack が作成されます ところで、上記のように YAML を書いておくとdocker-compose build で image の build ができ、docker-compose push でその image を push できちゃうんですね。image` に tag まで指定しておけばその tag がつけられる。便利！``` root@swarm1:~/stackdemo# docker stack deploy \u0026ndash;compose-file docker-compose.yml stackdemo Ignoring unsupported options: build\nCreating network stackdemo_default Creating service stackdemo_web Creating service stackdemo_redis root@swarm1:~/stackdemo#\n````docker stack ls` で stack に一覧が確認できます。各 stack にいくつの service が含まれるかも表示されます``` root@swarm1:~/stackdemo# docker stack ls NAME SERVICES stackdemo 2 root@swarm1:~/stackdemo#\nweb と redis のサービスが作成されています root@swarm1:~/stackdemo# docker service ls ID NAME MODE REPLICAS IMAGE PORTS tidm24xf7r2d stackdemo_redis replicated 1/1 redis:alpine iv6u57cs5wfo stackdemo_web replicated 1/1 yteraoka/stackdemo:latest *:8000-\u0026gt;8000/tcp root@swarm1:~/stackdemo#\n````docker stack ps` で指定 stack で実行されているコンテナ一覧が確認できます``` root@swarm1:~/stackdemo# docker stack ps stackdemo ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS o19qsdbdlkzq stackdemo_redis.1 redis:alpine swarm2 Running Running 2 minutes ago bua5sbi9t8di stackdemo_web.1 yteraoka/stackdemo:latest swarm1 Running Running 2 minutes ago root@swarm1:~/stackdemo#\n````docker service ps` でサービス単位でも確認できます``` root@swarm1:~/stackdemo# docker service ps stackdemo_web ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS bua5sbi9t8di stackdemo_web.1 yteraoka/stackdemo:latest swarm1 Running Running about a minute ago root@swarm1:~/stackdemo#\nroot@swarm1:~/stackdemo# docker service ps stackdemo\\_redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS o19qsdbdlkzq stackdemo\\_redis.1 redis:alpine swarm2 Running Running about a minute ago root@swarm1:~/stackdemo# ````docker service logs` でサービスのコンテナ全部のログをまとめて確認できます、レプリカ数が複数になるとどのコンテナにアクセスが来るかわからないのでこれができると便利``` root@swarm3:~# docker service logs -f stackdemo\\_web stackdemo\\_web.1.bua5sbi9t8di@swarm1 | \\* Running on http://0.0.0.0:8000/ (Press CTRL+C to quit) stackdemo\\_web.1.bua5sbi9t8di@swarm1 | \\* Restarting with stat stackdemo\\_web.1.bua5sbi9t8di@swarm1 | \\* Debugger is active! stackdemo\\_web.1.bua5sbi9t8di@swarm1 | \\* Debugger PIN: 298-511-468 stackdemo\\_web.1.bua5sbi9t8di@swarm1 | 10.255.0.2 - - \\[17/Mar/2018 10:07:01\\] \u0026quot;GET / HTTP/1.1\u0026quot; 200 - stackdemo\\_web.1.bua5sbi9t8di@swarm1 | 10.255.0.2 - - \\[17/Mar/2018 10:07:02\\] \u0026quot;GET /favicon.ico HTTP/1.1\u0026quot; 404 - stackdemo\\_web.1.bua5sbi9t8di@swarm1 | 10.255.0.2 - - \\[17/Mar/2018 10:07:51\\] \u0026quot;GET / HTTP/1.1\u0026quot; 200 - stackdemo\\_web.1.bua5sbi9t8di@swarm1 | 10.255.0.2 - - \\[17/Mar/2018 10:07:51\\] \u0026quot;GET / HTTP/1.1\u0026quot; 200 - ```ここまでのコマンドは manager node で実行する必要があります。worker node でも実行できる `docker ps` ではその node で動いているコンテナしか確認できません``` root@swarm1:~/stackdemo# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c760d3d1d55e yteraoka/stackdemo:latest \u0026quot;python app.py\u0026quot; 11 seconds ago Up 10 seconds stackdemo\\_web.1.bua5sbi9t8dimdiqnog0ukct8 root@swarm1:~/stackdemo# ``` ### Service の更新 `stack` を使わないで作った `service` は `docker service update` でイメージの入れ替えなどを行いますが、`stack` の場合は `docker-compose.yml` を書き換えて作成時と同じコマンド `docker stack deploy --compose-file docker-compose.yml` を実行することで変更のあったサービスだけ更新が行われます 今回、redis を使っていますが、redis の方をいじらなければコンテナの入れ替えなどは行われないため web のアプリ側が新しくなってもデータは残っています レプリカ数やローリングアップデートに関する設定は [deploy](https://docs.docker.com/compose/compose-file/#deploy) という設定で行います レプリカ数を 3 にしてみます``` root@swarm1:~/stackdemo# git diff docker-compose.yml diff --git a/docker-compose.yml b/docker-compose.yml index 5be7bcb..df3d5c8 100644 --- a/docker-compose.yml +++ b/docker-compose.yml @@ -6,5 +6,7 @@ services: build: . ports: - \u0026quot;8000:8000\u0026quot; + deploy: + replicas: 3 redis: image: redis:alpine root@swarm1:~/stackdemo# ```更新のために `docker stack deploy` を再度実行します``` root@swarm1:~/stackdemo# docker stack deploy --compose-file docker-compose.yml stackdemo Ignoring unsupported options: build Updating service stackdemo\\_web (id: iv6u57cs5wfo4tuuhle6dl7kv) Updating service stackdemo\\_redis (id: tidm24xf7r2dqexa82wfiwrfu) root@swarm1:~/stackdemo# ```**stackdemo\\_web** に2つのコンテナが追加されました``` root@swarm1:~/stackdemo# docker stack ps stackdemo ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS o19qsdbdlkzq stackdemo\\_redis.1 redis:alpine swarm2 Running Running 2 hours ago bua5sbi9t8di stackdemo\\_web.1 yteraoka/stackdemo:latest swarm1 Running Running 2 hours ago umzedgj5vhew stackdemo\\_web.2 yteraoka/stackdemo:latest swarm3 Running Preparing 8 seconds ago v4cbfc1kpnud stackdemo\\_web.3 yteraoka/stackdemo:latest swarm2 Running Running less than a second ago root@swarm1:~/stackdemo# ```なかなか便利そうですね。stack の削除は `docker stack rm STACKNAME` です。","date":"2018年3月17日","permalink":"/2018/03/docker-swarm-mode-2/","section":"Posts","summary":"docker-compose で stack を deploy する # https://docs.docker.com/engine/swarm/stack-deploy/ にあるサンプルアプリ、設定を用いて実際に動かしてみます。使ったコードは https://github.com/yteraoka/stackdemo に、イメージは https://hub.docker.com/r/yteraoka/stackdemo/ に。 次の内容の docker-compose.yml を用意し``` version: \u0026lsquo;3\u0026rsquo;","title":"Docker Swarm mode を知る (stack)"},{"content":"時代は Kubernetes ですが、Docker Swarm mode を再調査していみます。 Swarm mode については 1.12 での登場時に調査した（Docker 1.12 の衝撃 [slideshare]）際にまだちょっと使うには早いなということで見送ってそれ以降ほとんど調査していませんでした。\nサーバーの準備 # DigitalOcean で Docker インストール済みの Ubuntu を3台用意します。``` for i in $(seq 3); do doctl compute droplet create swarm${i} \\ \u0026ndash;image docker-16-04 \\ \u0026ndash;region sgp1 \\ \u0026ndash;size s-2vcpu-4gb \\ \u0026ndash;ssh-keys 16797382 \\ \u0026ndash;enable-private-networking \\ \u0026ndash;enable-monitoring \\ \u0026ndash;user-data-file userdata.sh done\nroot@swarm1:~# ufw status verbose Status: active Logging: on (low) Default: deny (incoming), allow (outgoing), allow (routed) New profiles: skip To Action From -- ------ ---- 22 LIMIT IN Anywhere 2375/tcp ALLOW IN Anywhere 2376/tcp ALLOW IN Anywhere 22 (v6) LIMIT IN Anywhere (v6) 2375/tcp (v6) ALLOW IN Anywhere (v6) 2376/tcp (v6) ALLOW IN Anywhere (v6) ```クラスタ管理用の通信で 2377/tcp を、ノード間通信で 7946/tcp, 7946/udp を、オーバーレイ・ネットワークで 4789/udp が使われるため ufw で firewall 設定を行います。 今回使ったイメージでは 2375/tcp, 2376/tcp が全開になっている（でも TCP の docker socket は使わないから開いてても大丈夫かな）のでこれを閉じて、上記のポートを eth1 でだけ開けます(DigitalOcean の eth1 は Private Network ですが、自分のアカウントに閉じられたネットワークではないため注意が必要）。``` ufw delete allow 2375/tcp ufw delete allow 2376/tcp ufw allow in on eth1 from any port 2375 proto tcp ufw allow in on eth1 from any port 2376 proto tcp ufw allow in on eth1 from any port 2377 proto tcp ufw allow in on eth1 from any port 7946 proto tcp ufw allow in on eth1 from any port 7946 proto udp ufw allow in on eth1 from any port 4789 proto udp ```overlay network 作成時に `--opt encrypte` をつけて通信の暗号化を有効にした場合は追加で ESP プロトコルも通るようにする必要があります。``` ufw allow from any proto esp ```（`doctl compute ssh NAME` で便利に ssh できるはずなんだけど Windows では winpty が必要でこれが ANSI エスケープシーケンスの扱いが良くなくて使いものにならないのが残念だ） ### Swarm クラスタの作成 まず、`docker swarm init` コマンドで初期化します eth1 だけを listen するように次のようにしました``` docker swarm init \\\\ --advertise-addr $(ip a s eth1 | grep 'inet ' | awk '{print $2}' | sed 's/\\\\/.\\*//') \\\\ --listen-addr $(ip a s eth1 | grep 'inet ' | awk '{print $2}' | sed 's/\\\\/.\\*//') root@swarm1:~# docker swarm init \\\n \u0026ndash;advertise-addr $(ip a s eth1 | grep \u0026lsquo;inet ' | awk \u0026lsquo;{print $2}\u0026rsquo; | sed \u0026rsquo;s/\\/.*//') \\ \u0026ndash;listen-addr $(ip a s eth1 | grep \u0026lsquo;inet ' | awk \u0026lsquo;{print $2}\u0026rsquo; | sed \u0026rsquo;s/\\/.*//') Swarm initialized: current node (asqtfpeef1ur58dijnnykuwuk) is now a manager.\n To add a worker to this swarm, run the following command:\ndocker swarm join --token SWMTKN-1-49x7ir3ihge066dretzivcu2gmdbwtys7v6h1yvybx784e5g5v-734ruykh451rsguxdjfhonuxn 10.130.27.207:2377  To add a manager to this swarm, run \u0026lsquo;docker swarm join-token manager\u0026rsquo; and follow the instructions.\nroot@swarm1:~#\nswarm init のオプション一覧は次のようになっています Usage: docker swarm init [OPTIONS]\nInitialize a swarm\nOptions: \u0026ndash;advertise-addr string Advertised address (format: [:port]) \u0026ndash;autolock Enable manager autolocking (requiring an unlock key to start a stopped manager) \u0026ndash;availability string Availability of the node (\u0026ldquo;active\u0026rdquo;|\u0026ldquo;pause\u0026rdquo;|\u0026ldquo;drain\u0026rdquo;) (default \u0026ldquo;active\u0026rdquo;) \u0026ndash;cert-expiry duration Validity period for node certificates (ns|us|ms|s|m|h) (default 2160h0m0s) \u0026ndash;data-path-addr string Address or interface to use for data path traffic (format: ) \u0026ndash;dispatcher-heartbeat duration Dispatcher heartbeat period (ns|us|ms|s|m|h) (default 5s) \u0026ndash;external-ca external-ca Specifications of one or more certificate signing endpoints \u0026ndash;force-new-cluster Force create a new cluster from current state \u0026ndash;listen-addr node-addr Listen address (format: [:port]) (default 0.0.0.0:2377) \u0026ndash;max-snapshots uint Number of additional Raft snapshots to retain \u0026ndash;snapshot-interval uint Number of log entries between Raft snapshots (default 10000) \u0026ndash;task-history-limit int Task history retention limit (default 5) ````\u0026ndash;cert-expiry` のデフォルト 2160h0m0s は90日\nworker node の join # docker swarm init で出力されたコマンドを使って worker node として swarm クラスタに参加させられます。init の時と同じように --advertise-addr, `\u0026ndash;listen-addr```` root@swarm2:~# docker swarm join \\\n \u0026ndash;token SWMTKN-1-4h3mm7ekejoqq9vg4axaba8rdu3cisgck84feon7mqmh3kmf2a-3rus2x8h6p3rvti4r27by5wbt \\ \u0026ndash;advertise-addr $(ip a s eth1 | grep \u0026lsquo;inet ' | awk \u0026lsquo;{print $2}\u0026rsquo; | sed \u0026rsquo;s/\\/.*//') \\ \u0026ndash;listen-addr $(ip a s eth1 | grep \u0026lsquo;inet ' | awk \u0026lsquo;{print $2}\u0026rsquo; | sed \u0026rsquo;s/\\/.*//') \\ 10.130.71.24:2377 This node joined a swarm as a worker. root@swarm2:~#\n root@swarm1:~# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS bj8muio9ov1wkkxg7ts1lqozc \\* swarm1 Ready Active Leader vywjboq8e8pk1buntrulod9fu swarm2 Ready Active r55jic8bg1giuljuv9901lic1 swarm3 Ready Active root@swarm1:~# root@swarm1:~# docker info | grep -A 5 ^Swarm Swarm: active NodeID: bj8muio9ov1wkkxg7ts1lqozc Is Manager: true ClusterID: udczyrb1xk8pvmehncli8y5o7 Managers: 1 Nodes: 3 root@swarm1:~#\nmanager ノードを増やすには `docker node promote NODE-ANME` とします root@swarm1:~# docker node promote swarm2 swarm3 Node swarm2 promoted to a manager in the swarm. Node swarm3 promoted to a manager in the swarm. root@swarm1:~#\nroot@swarm1:~# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS bj8muio9ov1wkkxg7ts1lqozc \\* swarm1 Ready Active Leader vywjboq8e8pk1buntrulod9fu swarm2 Ready Active Reachable r55jic8bg1giuljuv9901lic1 swarm3 Ready Active Reachable root@swarm1:~# ```MANAGER STATUS が Reachable に変わりました``` root@swarm1:~# docker info | grep -A 5 ^Swarm Swarm: active NodeID: bj8muio9ov1wkkxg7ts1lqozc Is Manager: true ClusterID: udczyrb1xk8pvmehncli8y5o7 Managers: 3 Nodes: 3 root@swarm1:~# ```Managers が 3 に増えました node を増やすための token は忘れても `docker swarm join-token {worker|manager}` コマンドで確認することができます ### Service の作成 ``` Usage: docker service create \\[OPTIONS\\] IMAGE \\[COMMAND\\] \\[ARG...\\] Create a new service Options: --config config Specify configurations to expose to the service --constraint list Placement constraints --container-label list Container labels --credential-spec credential-spec Credential spec for managed service account (Windows only) -d, --detach Exit immediately instead of waiting for the service to converge --dns list Set custom DNS servers --dns-option list Set DNS options --dns-search list Set custom DNS search domains --endpoint-mode string Endpoint mode (vip or dnsrr) (default \u0026quot;vip\u0026quot;) --entrypoint command Overwrite the default ENTRYPOINT of the image -e, --env list Set environment variables --env-file list Read in a file of environment variables --generic-resource list User defined resources --group list Set one or more supplementary user groups for the container --health-cmd string Command to run to check health --health-interval duration Time between running the check (ms|s|m|h) --health-retries int Consecutive failures needed to report unhealthy --health-start-period duration Start period for the container to initialize before counting retries towards unstable (ms|s|m|h) --health-timeout duration Maximum time to allow one check to run (ms|s|m|h) --host list Set one or more custom host-to-IP mappings (host:ip) --hostname string Container hostname --isolation string Service container isolation mode -l, --label list Service labels --limit-cpu decimal Limit CPUs --limit-memory bytes Limit Memory --log-driver string Logging driver for service --log-opt list Logging driver options --mode string Service mode (replicated or global) (default \u0026quot;replicated\u0026quot;) --mount mount Attach a filesystem mount to the service --name string Service name --network network Network attachments --no-healthcheck Disable any container-specified HEALTHCHECK --no-resolve-image Do not query the registry to resolve image digest and supported platforms --placement-pref pref Add a placement preference -p, --publish port Publish a port as a node port -q, --quiet Suppress progress output --read-only Mount the container's root filesystem as read only --replicas uint Number of tasks --reserve-cpu decimal Reserve CPUs --reserve-memory bytes Reserve Memory --restart-condition string Restart when condition is met (\u0026quot;none\u0026quot;|\u0026quot;on-failure\u0026quot;|\u0026quot;any\u0026quot;) (default \u0026quot;any\u0026quot;) --restart-delay duration Delay between restart attempts (ns|us|ms|s|m|h) (default 5s) --restart-max-attempts uint Maximum number of restarts before giving up --restart-window duration Window used to evaluate the restart policy (ns|us|ms|s|m|h) --rollback-delay duration Delay between task rollbacks (ns|us|ms|s|m|h) (default 0s) --rollback-failure-action string Action on rollback failure (\u0026quot;pause\u0026quot;|\u0026quot;continue\u0026quot;) (default \u0026quot;pause\u0026quot;) --rollback-max-failure-ratio float Failure rate to tolerate during a rollback (default 0) --rollback-monitor duration Duration after each task rollback to monitor for failure (ns|us|ms|s|m|h) (default 5s) --rollback-order string Rollback order (\u0026quot;start-first\u0026quot;|\u0026quot;stop-first\u0026quot;) (default \u0026quot;stop-first\u0026quot;) --rollback-parallelism uint Maximum number of tasks rolled back simultaneously (0 to roll back all at once) (default 1) --secret secret Specify secrets to expose to the service --stop-grace-period duration Time to wait before force killing a container (ns|us|ms|s|m|h) (default 10s) --stop-signal string Signal to stop the container -t, --tty Allocate a pseudo-TTY --update-delay duration Delay between updates (ns|us|ms|s|m|h) (default 0s) --update-failure-action string Action on update failure (\u0026quot;pause\u0026quot;|\u0026quot;continue\u0026quot;|\u0026quot;rollback\u0026quot;) (default \u0026quot;pause\u0026quot;) --update-max-failure-ratio float Failure rate to tolerate during an update (default 0) --update-monitor duration Duration after each task update to monitor for failure (ns|us|ms|s|m|h) (default 5s) --update-order string Update order (\u0026quot;start-first\u0026quot;|\u0026quot;stop-first\u0026quot;) (default \u0026quot;stop-first\u0026quot;) --update-parallelism uint Maximum number of tasks updated simultaneously (0 to update all at once) (default 1) -u, --user string Username or UID (format: \\[:\\]) --with-registry-auth Send registry authentication details to swarm agents -w, --workdir string Working directory inside the container ```alpine イメージで ping を実行する helloworld というサービスを作ります。実行するコンテナ数は1個。``` root@swarm1:~# docker service create --replicas 1 --name helloworld alpine ping docker.com nti2sn57ise7179iewt15bvkz overall progress: 1 out of 1 tasks 1/1: running verify: Service converged root@swarm1:~# ```サービスの確認``` root@swarm1:~# docker service ls ID NAME MODE REPLICAS IMAGE PORTS nti2sn57ise7 helloworld replicated 1/1 alpine:latest root@swarm1:~# ```サービスのコンテナ（タスクと呼ぶらしい）を確認``` root@swarm1:~# docker service ps helloworld ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS u2vgsm3uuu2i helloworld.1 alpine:latest swarm1 Running Running about a minute ago root@swarm1:~# ````docker ps` ではその node で実行されているコンテナだけが確認できます``` root@swarm1:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 02ad80ae8ec8 alpine:latest \u0026quot;ping docker.com\u0026quot; 2 minutes ago Up 2 minutes helloworld.1.u2vgsm3uuu2i1f4f70y3f3yhe root@swarm1:~# ```service inspect は `--pretty` をつけると見やすい！！``` root@swarm1:~# docker service inspect --pretty helloworld ID: nti2sn57ise7179iewt15bvkz Name: helloworld Service Mode: Replicated Replicas: 1 Placement: UpdateConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: alpine:latest@sha256:7b848083f93822dd21b0a2f14a110bd99f6efb4b838d499df6d04a49d0debf8b Args: ping docker.com Resources: Endpoint Mode: vip root@swarm1:~# ``` ### Scale の変更 サービスで実行するコンテナ数を増減させられます``` root@swarm1:~# docker service scale helloworld=5 helloworld scaled to 5 overall progress: 5 out of 5 tasks 1/5: running 2/5: running 3/5: running 4/5: running 5/5: running verify: Service converged root@swarm1:~# ```5つのコンテナに増えました``` root@swarm1:~# docker service ps helloworld ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS u2vgsm3uuu2i helloworld.1 alpine:latest swarm1 Running Running 8 minutes ago udthnceo672p helloworld.2 alpine:latest swarm3 Running Running about a minute ago to5h2akq19tl helloworld.3 alpine:latest swarm1 Running Running 59 seconds ago jtnr25puybdb helloworld.4 alpine:latest swarm2 Running Running about a minute ago y8561et8m5e1 helloworld.5 alpine:latest swarm3 Running Running 59 seconds ago root@swarm1:~# ````docker ps` ではその node で実行されているコンテナだけが確認できます``` root@swarm1:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7f828e94f492 alpine:latest \u0026quot;ping docker.com\u0026quot; About a minute ago Up About a minute helloworld.3.to5h2akq19tliyshxwe7enzyc 02ad80ae8ec8 alpine:latest \u0026quot;ping docker.com\u0026quot; 9 minutes ago Up 9 minutes helloworld.1.u2vgsm3uuu2i1f4f70y3f3yhe root@swarm1:~# ``` ### Service の削除 ``` root@swarm1:~# docker service rm helloworld helloworld root@swarm1:~# ```service はすぐに削除して見えなくなるが container は cleanup されるまで数秒残るため docker ps ではしばらく見えます ### Rolling update Rolling update の確認用に 3 コンテナの redis サービスを作成します。`--update-delay` で更新と更新の間の待ち時間を指定します。``` root@swarm1:~# docker service create \\\\ \u0026gt; --replicas 3 \\\\ \u0026gt; --name redis \\\\ \u0026gt; --update-delay 10s \\\\ \u0026gt; redis:3.0.6 d97voxnq0jt74a7hlb1749ytt overall progress: 3 out of 3 tasks 1/3: running 2/3: running 3/3: running verify: Service converged root@swarm1:~# root@swarm1:~# docker service ls ID NAME MODE REPLICAS IMAGE PORTS d97voxnq0jt7 redis replicated 3/3 redis:3.0.6 root@swarm1:~# docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS po89kjashmcm redis.1 redis:3.0.6 swarm2 Running Running 29 seconds ago dg8cu0lz71t7 redis.2 redis:3.0.6 swarm3 Running Running 29 seconds ago sbs7yrrr3ct2 redis.3 redis:3.0.6 swarm1 Running Running 26 seconds ago root@swarm1:~#\nデフォルトでは1コンテナずつ更新するが `--update-parallelism` でいくつ同時に更新するかを指定可能。 root@swarm1:~# docker service inspect \u0026ndash;pretty redis\nID: d97voxnq0jt74a7hlb1749ytt Name: redis Service Mode: Replicated Replicas: 3 Placement: UpdateConfig: Parallelism: 1 Delay: 10s On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: redis:3.0.6@sha256:6a692a76c2081888b589e26e6ec835743119fe453d67ecf03df7de5b73d69842 Resources: Endpoint Mode: vip root@swarm1:~#\n**On failure** が **pause** なので更新に失敗すると更新を中断(pause)します。 **Update order**, **Rollback order** が **stop-first** なのでまず、現在のコンテナを停止して新しいものを起動させます。 中断してしまった更新は `docker service update CONTAINER-ID` で再開できます。上記の redis サービスでは `docker service update redis` とします。 root@swarm1:~# docker service update \u0026ndash;image redis:3.0.7 redis redis overall progress: 3 out of 3 tasks 1/3: running 2/3: running 3/3: running verify: Service converged root@swarm1:~#\nroot@swarm1:~# docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS tak46phep0nr redis.1 redis:3.0.7 swarm2 Running Running 45 seconds ago po89kjashmcm \\\\\\_ redis.1 redis:3.0.6 swarm2 Shutdown Shutdown 52 seconds ago i510owz0oag8 redis.2 redis:3.0.7 swarm3 Running Running about a minute ago dg8cu0lz71t7 \\\\\\_ redis.2 redis:3.0.6 swarm3 Shutdown Shutdown about a minute ago trnsqcuxgrt9 redis.3 redis:3.0.7 swarm1 Running Running 24 seconds ago sbs7yrrr3ct2 \\\\\\_ redis.3 redis:3.0.6 swarm1 Shutdown Shutdown 33 seconds ago root@swarm1:~# ``` ### Drain node node をクラスタから外す際には `docker node update` で `--availability drain` とします。サービスで実行中のコンテナ（タスク）は他の node に移動してくれます。``` Usage: docker node update \\[OPTIONS\\] NODE Update a node Options: --availability string Availability of the node (\u0026quot;active\u0026quot;|\u0026quot;pause\u0026quot;|\u0026quot;drain\u0026quot;) --label-add list Add or update a node label (key=value) --label-rm list Remove a node label if exists --role string Role of the node (\u0026quot;worker\u0026quot;|\u0026quot;manager\u0026quot;) root@swarm1:~# docker node update \u0026ndash;availability drain swarm2 swarm2 root@swarm1:~# docker node inspect \u0026ndash;pretty swarm2 | head ID: 7h2d9guflr988w7r6eanuml1l Hostname: swarm2 Joined at: 2018-03-12 14:11:26.07479248 +0000 utc Status: State: Ready Availability: Drain Address: 10.130.39.87 Platform: Operating System: linux Architecture: x86_64 root@swarm1:~#\n```再度 docker node update で --availability active にすれば再度タスクが割り当てられるようになります。が、既に他の node で実行中のコンテナが移動されるわけではなく、scale コマンドや service update による更新、他の node を drain にしたり他の node 障害が発生した場合にタスクが移ってきます。\nRouting mesh # サービス作成時に --publish (-p) オプションを指定すると、全 node の published port へアクセスするとコンテナにルーティングしてくれるようになります。``` $ docker service create \\ \u0026ndash;name my-web \\ \u0026ndash;publish published=8080,target=80 \\ \u0026ndash;replicas 2 \\ nginx\n````\u0026ndash;publish published=8080,target=80は-p 8080:80でも ok 上記のコマンドではどの node の port 8080 にアクセスしても my-web コンテナの port 80 に転送してくれます。複数のコンテナに順に振り分けてくれます（ロードバランス）。アクセスした node 上で当該コンテナが起動してるかどうかに関係なく他の node であっても転送されます。 udp の場合は\u0026ndash;publish published=53,target=53,protocol=udp (-p 53:53/udp) のように指定します。 \u0026ndash;publish (-p) は複数指定可能なので複数ポート使うことができます。 mesh routing が不要な場合は \u0026ndash;publish published=53,target=53,protocol=udp,mode=hostのようにmode=hostを追加します。ただし、この場合はその node でポートがかぶらないように調整が必要となるため注意が必要です。このため通常は\u0026ndash;mode global` で各 node でひとつずつ実行するタイプのサービスで使用することになると思われます。\n次回 # 次はもっと実践的なサービスを構築してみようかと思います。\n","date":"2018年3月12日","permalink":"/2018/03/docker-swarm-mode-1/","section":"Posts","summary":"時代は Kubernetes ですが、Docker Swarm mode を再調査していみます。 Swarm mode については 1.12 での登場時に調査した（Docker 1.12 の衝撃 [slideshare]）際","title":"Docker Swarm mode を知る (setup)"},{"content":"kano っていう子供向け Rasberry Pi キットをその昔買っていました。\nスクラッチもマインクラフトもできるし息子が遊んでくれるかなって。\nkano package  しかしながら、数回起動させただけでずっと眠っていました。だってくっそ遅いんですもの。。。 （今のモデルはもっと快適だと思いますが） 息子はスクラッチもマインクラフトも別の Windows PC でやってます。\nで、Google Home で遊ぶためのツールやサーバーを動かしておくためにこれを使おうと DietPi でセットアップしてみました（「ラズパイZeroでもサクサク動く、軽量Linux「DietPi」」で紹介されてました）。イメージファイルをダウンロードして Win32 Disk Imager などで SD カードに入れて起動するだけです。後はほぼ Wizard でセットアップできます。\nIMG形式のイメージファイルをUSBメモリやSD/CFカードへ書き込める「Win32 Disk Imager」\nkano のキーボードは中央が膨らんでしまってました。バッテリーが膨らんだ？？？\n   kano OS はフリーらしいので試してみたい方はどうぞ\n 【Raspberry Pi】Kano OSをインストールしてみた（前編） 【Raspberry Pi】Kano OSをインストールしてみた（後編）  ","date":"2018年3月10日","permalink":"/2018/03/kano-dietpi/","section":"Posts","summary":"kano っていう子供向け Rasberry Pi キットをその昔買っていました。 スクラッチもマインクラフトもできるし息子が遊んでくれるかなって。 kano package しかしながら、数回起","title":"kano を DietPi でサーバーにした"},{"content":"PostgreSQL の psql コマンドではクエリの結果表示に pager（more とか less とか）が使われますが PAGER 環境変数での設定以外に変更する方法があったのでメモ。 わざわざ psql を終了して PAGER をセットし直して再度 psql を実行することとはオサラバです。\n\\setenv PAGER less で pager を less にできます。 pager 使用の有効・無効を切り替えるには \\pset を使います。PAGER を cat にしたりしなくても良いです。\n \\pset [NAME [VALUE]] set table output option (NAME := {border|columns|expanded|fieldsep|fieldsep_zero| footer|format|linestyle|null|numericlocale|pager| pager_min_lines|recordsep|recordsep_zero|tableattr|title| tuples_only|unicode_border_linestyle| unicode_column_linestyle|unicode_header_linestyle}) \\pset pager で pager の toggle となります。実行するたびに有効と無効に順に切り替わります。 \\pset pager on, \\pset pager off と on / off を明示すれば指定した方になります。\n","date":"2018年3月8日","permalink":"/2018/03/psql-pager/","section":"Posts","summary":"PostgreSQL の psql コマンドではクエリの結果表示に pager（more とか less とか）が使われますが PAGER 環境変数での設定以外に変更する方法があったのでメモ。 わざ","title":"psql での pager 設定"},{"content":"","date":"2018年1月7日","permalink":"/tags/xperia/","section":"Tags","summary":"","title":"Xperia"},{"content":"機種選定 # 2015年の4月頃に Xperia Z3 に乗り換えたようなので2年半ちょいでしたが、もはやソフトウェアの更新もされないし、バッテーリーのヘタリがひどいのでそろそろ買い替え時だなと、次の機種はどれにしようかなと、32GB の内蔵ストレージでは辛くなってるので 64GB は欲しい。しばらく悩んだ末に G8232 に決めました。\nHUAWEI Mate 10 Pro はいいなと思ったけれども高い、HUAWEI P10 Plus も高い、HUAWEI P10 lite はちょっと物足りない。Zenfone 4、Moto Z2 Play は店頭で触ってみてなんだかしっくりこなかった。Galaxy S8、Galaxy Note8 も高い\nAndroid はソフトウェアのサポート期間が短いのが辛い感じなのであまり高いのは避けたい。致命的な脆弱性を放置されるのは問題だと思いつまでもサポートするわけにはいかないのは分かるが脆弱性があるよっていう表示くらいは必要じゃないか？買い替えてもらうためにも。\nZ3 は新古を3万円弱で買ったのだけれども、今はこの価格帯で欲しいものがなかった。調べているうちに Xperia の海外モデルがなかなかリーズナブルだと気づいて EXPANSYS にたどり着くと2万円台から4万円台でなかなか悪くなさそうなのが購入できそうでした。\n Sony Xperia XZs Dual G8232 Snapdragon 820 / 4GB / 64GB (spec) Sony Xperia XA1 Ultra Dual G3226 Mediatek P20 / 4GB / 64GB (spec) Sony Xperia X Dual F5122 Snapdragon 650 / 3GB / 64GB (spec) Sony Xperia XZ Dual F8332 Snapdragon 820 / 3GB / 64GB (spec)  Ultra の画面サイズにはちょっと惹かれるものの、2cm 長くなるのはちょっと長すぎる気がするし、本を呼んだりするのには横幅の方が欲しい、長く使うなら CPU もやっぱり高性能が良いなということで結局 XZs (G8232) にしました。\n実際の購入は ETOREN で US$415 + 送料 US$18 でした。\n Z3 の使いみち # Z3 の詳細スペック （実際には docomo の SO-01G だからちょっと違うかも）\nSO-01G はフルセグ視聴が可能なのでポータブルTV端末としてしばらくたまに使うと思う。\n使ってみて # 正直、見た目や持った感じはほぼ同じなのでワクワク感は全然ない。\nAndroid が 6 から 7.1 に更新されたことで通知まわりが大きく変わっていたことに驚いた程度。\nAndroid 8 Oreo はいつになったら降ってくるのかな？？\nカメラ比較（静止画） # Z3  XZS  Z3  XZS  XZs はちょっとシャープネスフィルタがかかったような感じでかつ、ちょっと歪みがあるようにみえる。歪みの方はソフトウェア・アップデートで改善されるとか。\n新しい機能「先読み撮影」はシャッターを切るちょっと前のデータも持っていることから動く被写体については最大4枚保存される\nシャッタースピード調整 は良さそう。\n歪みの確認 # Z3\nXZs\n  動画 スーパースロー #  Xperia XZs / XZ Premiumから搭載された「スーパースロー」は、通常スピード（1秒30コマ）の映像に約6秒間のスローモーション映像（1秒最大960コマ）を組み合わせることで、緩急のある映像が簡単に作成できます。\n通常のビデオ撮影中、スローモーションで残したい瞬間にスローボタンをタップすると、約0.2秒の一瞬を約6秒のスローモーション映像として記録。\n http://www.sonymobile.co.jp/myxperia/howtoxperia/camera/function29.html\nこの機能はちょっとおもしろいかなと思うけど、0.2秒のタイミングを合わせるのが至難の業かなと。ずっと回ってるハンドスピナーやずっと流れてる噴水を取ってみた。ボールを投げたり、ハトが飛び立つところを取ってみたかったけど簡単ではなさそうだったので諦めた。\n120fps であれば普通に撮影した後で任意の箇所をスローにすることができるのでこっちの方が使いやすいかも\nバッテリー # 2年半使った Z3 はもう全然持たなくなってたので劇的改善を期待していたのだが、これは期待外れだった。毎日充電は必須だし心配ならモバイルバッテリー持ち歩けってことなんだろうな。\nアクセサリ # ケースやフィルムなど買ってみたものを紹介\nフィルム # フィルムは「Timkyo Xperia XZsフィルム 3D曲面 全面保護 硬度9H ソニー エクスペリア XZ 強化ガラスフィルム 品質保証」で、超薄型0.26mmって書いてあるけどこれまで使ってきたフィルムとは違ってだいぶ厚い。これってフィルムって呼ぶの？って感じ。3Dラウンドエッジでなんとなく Galaxy っぽい感じになる。触り心地は良いですが、端はほんの少し浮き気味でタッチしづらいことはある。\nフィルム装着-上部  フィルム装着 - 下部  縁のところにホコリが入っちゃうのがマイナスポイント\nもう一度買うかと言われると買わない\nベッドから落とした際にパコって外れちゃったので捨てました。フィルム(?)なんて要らんかったんや！！\nケース # ケースは「Willnorn Xperia XZs ケース シリコン TPU クリア バンパー 耐衝撃 落下防止 アクリル板背面 擦り傷防止 透明 XZs XZ 適用 スマホカバー (クリア)」で透明のしっかりしたシリコンケースで安心感があります。電源ボタンが指紋センサーを兼ねているため、ここの穴があまり大きくないのが影響してちょっと指紋認証しづらいところがマイナスポイント。意図せず、分厚いフィルムといい感じにマッチしてる。一体感がある。\nXZs ケース装着 - 下部  XZs ケース装着 - 側部  Type-C ケーブル # 所持端末初の USB 端子が Type-C なのでケーブルも購入しました。（本体付属で1本はついてきます）\n「Anker PowerLine+ USB-C \u0026amp; USB-A 3.0 ケーブル (0.9m レッド) Galaxy S8 / S8+、MacBook、Xperia XZ対応」\nAnker が安心感があるかなと。\n0.9m で 899円なのにオサレなスウェードケースまで付いててびっくりしました。ケースの USB 端子部の穴が小さいのでちょっと挿しづらいけど使えないわけではない。\nAnker ケーブルケース  Anker ケーブル Red  Apple が電池交換値下げしたし、型落ち iPhone 狙って更新していくのがよい気がする\n はぁぁぁ、待ちに待ちましたよ。年末に購入して3月7日にやっと Android Oreo が降ってきましたよ。静止画歪み補正機能がやっと手に入った。\n ","date":"2018年1月7日","permalink":"/2018/01/xperia-xzs-g8232/","section":"Posts","summary":"機種選定 # 2015年の4月頃に Xperia Z3 に乗り換えたようなので2年半ちょいでしたが、もはやソフトウェアの更新もされないし、バッテーリーのヘタリがひ","title":"Xperia Z3 から G8232 XZs Dual に乗り換えた"},{"content":"","date":"2018年1月7日","permalink":"/tags/%E3%82%B9%E3%83%9E%E3%83%9B/","section":"Tags","summary":"","title":"スマホ"},{"content":"","date":"2018年1月6日","permalink":"/tags/color/","section":"Tags","summary":"","title":"color"},{"content":"Docker Toolbox for Windows のインストールで一緒に入った Git for Windows (mintty 2.0.3) を使ってて、git-bash 色の変更が Foreground, Background, Cursor しかできなくて残念だなあと思ってたところ、別の環境に Git for Windows の最新版 (Git-2.15.1.2) を単体で入れてみたら次のように Option 設定の Colours 欄に Theme っていう選択肢があるじゃないですか！！ (mintty 2.8.1)\nmintty options  ということで Git for Windows を更新しました。 「Color Scheme Designer」というボタンをクリックすると 4bit Terminal Color Scheme Designer\n4bit Terminal Color Scheme Designer  ここでグリグリ調整して右上の「Get Sceheme」ボタンから「mintty」を選択すると ~/.minttyrc に書くための設定が取得できます（Firefox だけで機能し、Chrome, IE, Edge では機能しませんでした）\nexport scheme to the configuration file  次のようなテキストが取得できるので ~/.minttyrc に書くことで色を指定できます。\nBackgroundColour=13,25,38 ForegroundColour=217,230,242 CursorColour=217,230,242 Black=0,0,0 BoldBlack=38,38,38 Red=184,122,122 BoldRed=219,189,189 Green=122,184,122 BoldGreen=189,219,189 Yellow=184,184,122 BoldYellow=219,219,189 Blue=122,122,184 BoldBlue=189,189,219 Magenta=184,122,184 BoldMagenta=219,189,219 Cyan=122,184,184 BoldCyan=189,219,219 White=217,217,217 BoldWhite=255,255,255 しかし、これでは選択式になってる Theme の意味がありません。いくつか用意して切り替えたい場合は ~/.mintty/theme/ フォルダに任意のファイル名で保存すればファイル名が theme 名として表示され選択可能になります。\n https://github.com/oumu/mintty-color-schemes https://github.com/PhilipDaniels/mintty/tree/master/themes https://github.com/goreliu/wsl-terminal/tree/master/src/etc/themes  solarized は https://github.com/karlin/mintty-colors-solarized/ にあったのでありがたく使わせていただきました。 てなわけで私の設定ファイルは .minttyrc\n$ cat .minttyrc BoldAsFont=yes Font=Consolas FontHeight=11 Term=xterm-256color BoldAsColour=no Transparency=low BellFlash=yes ScrollMod=off CopyAsRTF=no Columns=100 Rows=30 CursorType=block PgUpDnScroll=yes BellType=0 ThemeFile=solarized-dark で、.mintty/themes/solarized-dark が\nBlack=7,54,66 Red=220,50,47 Green=133,153,0 Yellow=181,137,0 Blue=38,139,210 Magenta=211,54,130 Cyan=42,161,152 White=238,232,213 BoldBlack=0,43,54 BoldRed=203,75,22 BoldGreen=88,110,117 BoldYellow=101,123,131 BoldBlue=131,148,150 BoldMagenta=108,113,196 BoldCyan=147,161,161 BoldWhite=253,246,227 ForegroundColour=238,232,213 BackgroundColour=0,43,54 CursorColour=133,153,0 となりました。 https://github.com/mintty/mintty/wiki/Tips ちなみに PuTTY は https://github.com/AlexAkulov/putty-color-themes/ に沢山の theme が公開されてます\n","date":"2018年1月6日","permalink":"/2018/01/git-bash-color-theme/","section":"Posts","summary":"Docker Toolbox for Windows のインストールで一緒に入った Git for Windows (mintty 2.0.3) を使ってて、git-bash 色の変更が Foreground, Background, Cursor しかできなくて残念だなあと思ってたところ、別の環境","title":"git-bash (mintty) の color theme"},{"content":"","date":"2018年1月6日","permalink":"/tags/mintty/","section":"Tags","summary":"","title":"mintty"},{"content":"DigitalOcean にはサーバーの作成、削除などの操作をコマンドラインで行うツール https://github.com/digitalocean/doctl があります。 Linux での使い方は過去にも書いてきました。近頃 Windows 使いとなっているので Windows で git-bash から使おうと思いました。 doctl は go で書かれており、マルチプラットフォームのバイナリが用意されているので Windows 用をダウンロードして展開すればすぐに使えます。 私の環境では $PATH に /c/Users/ytera/bin が入っていたので doctl.exe をそこに置きました。 そして doctl auth init を実行すれば token を聞かれて $HOME/.config/doctl/config.yaml が作成されるはずでしたが git-bash では期待の動作となりませんでした\u0026hellip;\n$ doctl auth init initialize configuration Usage: doctl auth init [flags] Flags: -h, --help help for init Global Flags: -t, --access-token string API V2 Access Token -u, --api-url string Override default API V2 endpoint -c, --config string config file (default is $HOME/.config/doctl/config.yaml) -o, --output string output format [text|json] (default \u0026quot;text\u0026quot;) --trace trace api access -v, --verbose verbose output Error: unable to read DigitalOcean access token: unknown terminal そこで cmd.exe から実行してみました\nC:\\Users\\ytera\\bin\u0026gt;doctl auth init DigitalOcean access token: bc522054e5045088a089321ca4e900de33ae383b692bbe1f3507bf50a4747513 Validating token... OK (もちろん上記の token はダミーです dd if=/dev/urandom count=1 bs=512 | sha256sum てな感じで生成) さて、config.yaml はいずこに？ C:\\Users\\ytera\\.config とか C:\\Users\\ytera\\_config といったフォルダは存在しません。 答えは LOCALAPPDATA=C:\\Users\\ytera\\AppData\\Local でした。 C:\\Users\\ytera\\AppData\\Local\\doctl\\config\\config.yaml ここまでできたら以降は git-bash からでも操作できました。 実は git-bash でも winpty を使えば良いのでした。\n$ winpty doctl auth init DigitalOcean access token: bc522054e5045088a089321ca4e900de33ae383b692bbe1f3507bf50a4747513 Validating token... OK イメージの確認 # Docker のサーバーを立ち上げたいのでインストール済みのイメージ（アプリケーション）を使います\n$ doctl compute image list-application --public ID Name Type Distribution Slug Public Min Disk 24232319 Ruby-on-Rails on 16.04 snapshot Ubuntu ruby-on-rails-16-04 true 20 24976861 Dokku 0.9.4 on 16.04 snapshot Ubuntu dokku-16-04 true 20 25399919 Discourse 2.0.20170531 on 16.04 snapshot Ubuntu discourse-16-04 true 40 27223373 MongoDB 3.4.7 on 16.04 snapshot Ubuntu mongodb-16-04 true 20 27419663 Machine Learning and AI snapshot Ubuntu ml-16-04 true 20 27768998 MySQL on 16.04 snapshot Ubuntu mysql-16-04 true 20 28008792 GitLab 10.0.0-ce.0 on 16.04 snapshot Ubuntu gitlab-16-04 true 60 29160863 NodeJS 6.11.5 on 16.04 snapshot Ubuntu node-16-04 true 20 29160891 Ghost on 16.04 snapshot Ubuntu ghost-16-04 true 30 29160892 LAMP on 16.04 snapshot Ubuntu lamp-16-04 true 20 29160902 Django 1.8.7 on 16.04 snapshot Ubuntu django-16-04 true 20 29160903 Docker 17.09.0-ce on 16.04 snapshot Ubuntu docker-16-04 true 20 29160904 PhpMyAdmin on 16.04 snapshot Ubuntu phpmyadmin-16-04 true 20 29160905 WordPress 4.8.3 on 16.04 snapshot Ubuntu wordpress-16-04 true 30 29160933 MEAN on 16.04 snapshot Ubuntu mean-16-04 true 30 29161212 LEMP on 16.04 snapshot Ubuntu lemp-16-04 true 20 作成時にイメージ(--image)として docker-16-04 を指定すれば Docker インストール済みの Ubuntu 16.04 が起動します\n公開鍵の登録 # $ doctl compute ssh-key import vaio-win --public-key-file ~/.ssh/id_rsa.pub ID Name FingerPrint 12345678 vaio-win 91:ad:57:b2:f6:23:e3:41:0d:3d:a8:e6:fb:a8:a3:33 Droplet の作成 # doctl compute droplet create testsv01 \\ --image docker-16-04 \\ --region sgp1 \\ --size 2gb \\ --ssh-keys 12345678 --wait をつけると起動してくるまで待たされます doctl compute droplet list で droplet のリストが確認できます。Status が new から active になれば ssh でログインできます region は doctl compute region list で、size は doctl compute size list で確認できます\nSSH でログイン # Linux であれば doctl compute ssh testsv01 で普通に使えるのですが、git-bash からは winpty をかます必要がありました。 ががが、ローカルエコーが効かない(?)ので諦めて doctl compute droplet list --format Name,PublicIPv4 で IP アドレスを確認して普通に ssh します。 DigitalOcean では作成直後は root ユーザーで ssh します。``` ssh root@xxx.xxx.xxx.xxx\n ### サーバーの削除 停止しててもお金がかかるので不要になったら削除する $ doctl compute droplet delete testsv01 Warning: Are you sure you want to delete droplet(s) (y/N) ? y\n ### 参考資料 [How To Use Doctl, the Official DigitalOcean Command-Line Client](https://www.digitalocean.com/community/tutorials/how-to-use-doctl-the-official-digitalocean-command-line-client) ","date":"2017年12月20日","permalink":"/2017/12/doctl-on-windows/","section":"Posts","summary":"DigitalOcean にはサーバーの作成、削除などの操作をコマンドラインで行うツール https://github.com/digitalocean/doctl があります。 Linux での使い方は過去にも書いてきました。近頃 Windows 使いとなっているの","title":"Windows で doctl を使う"},{"content":"","date":"2017年11月29日","permalink":"/tags/vagrant/","section":"Tags","summary":"","title":"vagrant"},{"content":"","date":"2017年11月29日","permalink":"/tags/virtualbox/","section":"Tags","summary":"","title":"VirtualBox"},{"content":"https://www.vagrantup.com/downloads.html からインストラーをダウンロードしてインストールすれば普通に使える。が、今の環境では Docker Toolbox のインストールで一緒に入ってた。\nホスト側フォルダのマウント (vbguest) # ホスト(Windows)側のフォルダを /vargant としてマウントさせるには vagrant-vbguest plugin を入れる必要がある。標準では vagrant up に rsync でコピーされるだけで稼働中はホスト側の変更が反映されない。\n$ vagrant plugin install vagrant-vbguest Installing the 'vagrant-vbguest' plugin. This can take a few minutes... Fetching: vagrant-share-1.1.9.gem (100%) Fetching: micromachine-2.0.0.gem (100%) Fetching: vagrant-vbguest-0.15.0.gem (100%) Installed the plugin 'vagrant-vbguest (0.15.0)'! $ vagrant plugin list vagrant-share (1.1.9, system) vagrant-vbguest (0.15.0) このプラグインを入れると vagrant up 時に VirtualBox Guest Additions のインストールが走り、必要なパッケージのインストールや kernel module のコンパイルが行われて遅くなるので不要であれば入れないほうが良いかも。既にインストール済みの場合でも \u0026quot;C:\\Program Files\\Oracle\\VirtualBox\\VBoxGuestAdditions.iso\u0026quot; にあるバージョンと異なる場合は再インストールされます。各バージョンのファイルは http://download.virtualbox.org/virtualbox/ からダウンロード可能です。\nvagrant vbguest --status で GuestAdditions のインストール状況が確認できる\nインストールしただけではマウントはされず、次の行を Vagrantfile に追加する必要があります。\nconfig.vm.synced_folder \u0026quot;.\u0026quot;, \u0026quot;/vagrant\u0026quot;, type: \u0026quot;virtualbox\u0026quot; SSH # Git for Windows についてくる git-bash.exe を Terminal として使うと vagrant ssh で接続できるけどプロンプトが表示されないという問題がある。しかし、ssh コマンドでは問題ない。これは大変不便なので ~/.bashrc\nこの vagrant ssh の問題はいつのまにか改善していました (2018/9/15 追記)\nvssh() { if [ \u0026#34;$1\u0026#34; = \u0026#34;-r\u0026#34; ] ; then rm -f ssh_config shift fi test -f ssh_config || vagrant ssh-config | sed \u0026#39;s,C:,/c,\u0026#39; \u0026gt; ssh_config ssh -F ssh_config \u0026#34;$@\u0026#34; } と書いてみた。vagrant コマンドの実行はいちいち遅いので ssh_config の更新が不要な場合はすぐに ssh できて早いという利点もある。\n最近の OpenSSH は .ssh/config ファイルを分割できるらしい ~/.ssh/configについて\nvagrant ssh が機能するようになったため上記の vssh() は不要になったわけですが、IdentityFile の path として C:/Users/... と書かれていると git-bash についてくる OpenSSH ではアクセスできないため、/c/Users/... に置換するようにしました。C:\\Windows\\System32\\OpenSSH\\ssh.exe の方であれば C:/Users/... でもアクセスできました。ただし、こちらの ssh は winpty を使う必要がある。\nHostonly Network # 標準では NAT ネットワークで、ホスト側からアクセスするためにはいちいち port forwarding 設定が必要です。 Vagrantfile にある次の行をアンコメントするとホストオンリーネットワークアダプタが作成され\n# config.vm.network \u0026quot;private_network\u0026quot;, ip: \u0026quot;192.168.33.10\u0026quot; NAT 設定不要で ip: で指定した IP アドレスにホスト側から直接アクセスできるようになります。\nBridge Network # セキュリティ的に問題ないとわかっている場合は Bridge Network を使えるようにすることで、他のホストからもアクセスできるようにできます。Vagrantfile の次の行をアンコメントすると使えます。\n# config.vm.network \u0026quot;public_network\u0026quot; こちらは IP アドレスの指定が不要ですが、きっとホストPCのあるネットワークには DHCP サーバーがありますよね\nBox の更新 # ==\u0026gt; default: Checking if box 'centos/7' is up to date... ==\u0026gt; default: A newer version of the box 'centos/7' for provider 'virtualbox' is ==\u0026gt; default: available! You currently have version '1804.02'. The latest is version ==\u0026gt; default: '1809.01'. Run `vagrant box update` to update. $ vagrant box update ==\u0026gt; default: Checking for updates to 'centos/7' default: Latest installed version: 1804.02 default: Version constraints: default: Provider: virtualbox ==\u0026gt; default: Updating 'centos/7' with provider 'virtualbox' from version ==\u0026gt; default: '1804.02' to '1809.01'... ==\u0026gt; default: Loading metadata for box 'https://vagrantcloud.com/centos/7' ==\u0026gt; default: Adding box 'centos/7' (v1809.01) for provider: virtualbox default: Downloading: https://vagrantcloud.com/centos/boxes/7/versions/1809.01/providers/virtualbox.box default: Download redirected to host: cloud.centos.org default: ==\u0026gt; default: Successfully added box 'centos/7' (v1809.01) for 'virtualbox'! $ vagrant box list centos/7 (virtualbox, 1804.02) centos/7 (virtualbox, 1809.01) ubuntu/xenial64 (virtualbox, 20180224.0.0) 古いものを消すには\n$ vagrant box remove centos/7 --box-version 1804.02 などとする\n","date":"2017年11月29日","permalink":"/2017/11/vagrant-on-windows/","section":"Posts","summary":"https://www.vagrantup.com/downloads.html からインストラーをダウンロードしてインストールすれば普通に使える。が、今の環境では Docker Toolbox のインストールで一緒に入ってた。 ホスト側フォルダのマ","title":"Windows で Vagrant"},{"content":"普段使いの家の端末を Windows にしてみたので、使い勝手改善中。 Docker Toolbox for Windows についてのメモ。 インストールについては公式ドキュメントを参照 Install Docker Toolbox on Windows VirtualBox がインストールされ、ゲスト OS としての Linux を docker machine として使います。 デスクトップに「Docker Quickstart Terminal」というショートカットが作成されており、これを開くと\n\u0026quot;C:\\\\Program Files\\\\Git\\\\bin\\\\bash.exe\u0026quot; --login -i \u0026quot;C:\\\\Program Files\\\\Docker Toolbox\\\\start.sh\u0026quot; というコマンドが実行されます。 この bash 環境では / が\nC:\\\\Program Files\\\\Git になっており、mingw64/、bin/、usr/bin/ に各種 unix コマンドが入ってます。 start.sh は docker-machine が存在しなければ作成し、起動していなければ起動して docker-machine env で得られる環境変数をセットした状態にしてくれます。 ですからこの bash 内ではすぐに docker コマンドでコンテナの起動などができます。 docker コンテナは boot2docker イメージから作られた仮想サーバー内で実行されるため、コンテナの使用するメモリなどはこの仮想サーバーの上限の影響を受けます。 次のように docker-machine create を行うことでメモリやストレージのサイズを指定可能です。もちろん、作成後に直接 VirtualBox 上の設定を変更することも可能。\n$ docker-machine create --driver virtualbox --virtualbox-memory 2048 --virtualbox-disk-size 40000 {machine-name} 「Docker Quickstart Terminal」を使わずともコマンドプロンプトや PowerShell からも操作可能です。もちろん Cmder でも。docker コマンドを使うには環境変数の設定が必要ですが、docker-machine env {machine-name} で出力されるものを指定します。 Windows ではコメントとして次のような出力もされます。\nREM Run this command to configure your shell: REM @FOR /f \u0026quot;tokens=*\u0026quot; %i IN ('docker-machine env') DO @%i 2行目の @FOR /f \u0026quot;tokens=*\u0026quot; %i IN ('docker-machine env') DO @%i をコピペするだけで一括で設定されます。\ndocker run -it -d -P --rm nginx とすれば nginx コンテナが起動され docker ps で port がわかる（docker port CONTAINER_NAME でも可）\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f8942efa52e8 nginx \u0026quot;nginx -g 'daemon ...\u0026quot; 7 seconds ago Up 3 seconds 0.0.0.0:32768-\u0026gt;80/tcp modest_volhard docker-machine ip で docker ホストのIPアドレスがわかるため、これを組み合わせることで http://192.168.99.100:3276/ で nginx にアクセスできることがわかる。\n Docker Machine command-line reference Docker Machine driver reference (Oracle VirtualBox)  ","date":"2017年11月26日","permalink":"/2017/11/docker-toolbox-for-windows/","section":"Posts","summary":"普段使いの家の端末を Windows にしてみたので、使い勝手改善中。 Docker Toolbox for Windows についてのメモ。 インストールについては公式ドキュメントを参照 Install Docker Toolbox on Windows VirtualBox がイン","title":"Docker Toolbox for Windows の使い方"},{"content":"USB 2.0 / 3.0 を使うためには Extention のインストールが必要 http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html Windows の場合、ダウンロードしたファイル (Oracle_VM_VirtualBox_Extension_Pack-5.2.0-118431.vbox-extpack) のダブルクリックでインストール可能 メニューからの場合は「ファイル」→「環境設定」→「機能拡張」\n","date":"2017年11月26日","permalink":"/2017/11/using-usb-device-from-virtualbox-guest/","section":"Posts","summary":"USB 2.0 / 3.0 を使うためには Extention のインストールが必要 http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html Windows の場合、ダウンロードしたファイル (Oracle_VM_VirtualBox_Extension_Pack-5.2.0-118431.vbox-extpack) のダブルクリックでインストール可能 メニューからの場合は「フ","title":"VirtualBox のゲスト OS でホスト側の USB デバイスにアクセスする"},{"content":"「DNS resolver 9.9.9.9 will check requests against IBM threat database」という記事を見かけたので使ってみることにしました。 DNS サーバーとして 9.9.9.9 を使うことで、ブラウザで誤って怪しいリンクをクリックしてしまうとか、フィッシングに気づかずクリックしてしまった場合でも Quad9 の持っているリストにマッチすればIPアドレスを返さないことでアクセスをブロックできることになります。 もちろんすべての怪しいサイトがブロックされるわけではないですし、IPアドレスでリンクがはられているものには効果がないですが、https 対応していればIPアドレスでのリンクということもないでしょうし、ある程度の効果は見込めそうです。 家族もスマホやタブレットで無線LANを使ってインターネットへアクセスするので DHCP サーバー側（ルーター）でこのDNSサーバーを使うようにしました。 IBM のプレスリリースはこちら 「IBM、Packet Clearing House、Global Cyber Alliance、インターネットの脅威から企業と消費者を保護するために協業（無数の悪意あるWebサイトからユーザーを保護する Quad9 DNSプライバシー＆セキュリティー・サービス）」 Quad9 のサイトは https://www.quad9.net/ Google も Public DNS サービス (8.8.8.8, 8.8.4.4) を提供してくれておりキャッシュポイズニングやサーバー保護系の対策が取られています。(Security Benefits) Google には DNS-over-HTTPS なんてもあります。\nλ curl -s https://dns.google.com/resolve?name=www.google.com | jq . { \u0026quot;Status\u0026quot;: 0, \u0026quot;TC\u0026quot;: false, \u0026quot;RD\u0026quot;: true, \u0026quot;RA\u0026quot;: true, \u0026quot;AD\u0026quot;: false, \u0026quot;CD\u0026quot;: false, \u0026quot;Question\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;www.google.com.\u0026quot;, \u0026quot;type\u0026quot;: 1 } ], \u0026quot;Answer\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;www.google.com.\u0026quot;, \u0026quot;type\u0026quot;: 1, \u0026quot;TTL\u0026quot;: 299, \u0026quot;data\u0026quot;: \u0026quot;172.217.25.100\u0026quot; } ], \u0026quot;Comment\u0026quot;: \u0026quot;Response from 216.239.36.10.\u0026quot; } ちなみに、上のコマンド (curl や jq) は Windows でやってます。\n前回書いた Cmder で。便利。\nブラウザでアクセスするならこっち https://dns.google.com/query?name=www.google.com\u0026amp;type=A\u0026amp;dnssec=true\n","date":"2017年11月23日","permalink":"/2017/11/quad9/","section":"Posts","summary":"「DNS resolver 9.9.9.9 will check requests against IBM threat database」という記事を見かけたので使ってみることにしました。 DNS サーバーとして 9.9.9.9 を使うことで、ブラウザで誤って","title":"Quad9 (9.9.9.9) でセキュリティ強化"},{"content":"ずっと Linux mint や Ubuntu で使ってたノート PC の OS を Ubuntu 17.10 にしたらフリーズして強制電源 Off 後に Operating System Not Found となる現象が短期間に2度も発生したので気分転換に Windows 10 にしてみた。\nせっかくなので快適に使いたいなと cmd.exe に代わるものがないかとググってみたら Cmder (github) が良いらしいということで入れてみました。\nインストールは Chocolatey で行いました。\n\u0026quot;C:\\tools\\cmder\\Cmder.exe\u0026quot; にインストールされました。 C:\\tools\\cmder\\vendor\\clink にインストールされる clink のおかけで bash に近い感じで使えます(Ctrl-A, Ctrl-E, Ctrl-U, Ctrl-N, Ctrl-P, Ctrl-R,\u0026hellip;)。\n快適！！タブ機能もある。 さらに、C:\\tools\\cmder\\vendor\\git-for-windows に coreutils とか findutils に入ってる各種コマンドが入ってて Linux 的に使えるんです（しかし、今の環境では別途インストールしている Docker Tools for Windows によって C:\\Program Files\\Git にも Git for Windows が入っており、こちらが使われてるっぽい）。\nPerl まで入ってる。 bash も入ってるけど私の場合は Windows Subsystem for Linux を先に入れていたからか bash と打つとそのまま WSL の Ubuntu に入ってしまいます。直接 git-for-windows 内の bash を指定すればそっちの bash も起動しますが。 git-for-windows なのでもちろん git も普通に使えます。vim も gawk も find も xargs もある。\nipconfig /all | iconv -f sjis -t utf-8 | less なんてこともできる。 ssh も入ってるので vagrant を別途入れておけば vagrant ssh もできますし、普通に ssh でサーバーにログインできます。Putty 要らず。 素晴らしい。\nWindows Subsystem for Linux でどれほど快適になってるのかも試してみたかったわけですが、Cmder と Vagrant があればそれで良いかも。Docker も手軽に使いたいところだがこれはシームレスにはいかないようだ。Windows 10 Home Edition だから Docker toolbox なんだけど Docker for Windows だとた違うんだろうか？ ・・・\n使ってたらやっぱり不満は出てきた\u0026hellip;\nCtrl-V との組み合わせでコントロールコード入れたくてもクリップボードからのペーストが機能しちゃって困る そして、結局 Git for Windows についてくる git-bash.exe を使うことにした。\nそんなわけで Cmder とはお別れです。あんいんすとーーる\n","date":"2017年11月22日","permalink":"/2017/11/using-cmder/","section":"Posts","summary":"ずっと Linux mint や Ubuntu で使ってたノート PC の OS を Ubuntu 17.10 にしたらフリーズして強制電源 Off 後に Operating System Not Found となる現象が短期間に2度も発生したので気分転換に Windows 10 にし","title":"Cmder で Windows 生活を快適に？？"},{"content":"","date":"2017年11月9日","permalink":"/tags/container/","section":"Tags","summary":"","title":"container"},{"content":"","date":"2017年11月9日","permalink":"/tags/lxd/","section":"Tags","summary":"","title":"LXD"},{"content":"PC が \u0026ldquo;Operating System Not Found\u0026rdquo; と起動しなくなってしまったので Ubuntu 17.10 をクリーンインストールして「ソフトウェア」ってソフトを起動したらコレクションに LXD があったので試してみることにした。\n$ sudo snap install lxd でインストールできる。\n$ sudo lxd init Do you want to configure a new storage pool (yes/no) [default=yes]? Name of the new storage pool [default=default]: Name of the storage backend to use (dir, btrfs, ceph, lvm, zfs) [default=zfs]: dir Would you like LXD to be available over the network (yes/no) [default=no]? Would you like stale cached images to be updated automatically (yes/no) [default=yes]? Would you like to create a new network bridge (yes/no) [default=yes]? What should the new bridge be called [default=lxdbr0]? What IPv4 address should be used (CIDR subnet notation, “auto” or “none”) [default=auto]? What IPv6 address should be used (CIDR subnet notation, “auto” or “none”) [default=auto]? LXD has been successfully configured. CentOS 7 のコンテナを起動させてみる\n$ sudo lxc launch images:centos/7/amd64 srv01 と実行するとイメージのダウンロードが始まります、2回目以降など、既にダウンロード済みならすぐに作成されます。\n$ sudo lxc launch images:centos/7/amd64 srv01 Creating srv01 Retrieving image: rootfs: 63% (1.65MB/s) $ sudo lxc launch images:centos/7/amd64 srv01 Creating srv01 Starting srv01 $ sudo lxc list +-------+---------+---------------------+----------------------------------------------+------------+-----------+ | NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS | +-------+---------+---------------------+----------------------------------------------+------------+-----------+ | srv01 | RUNNING | 10.20.22.236 (eth0) | fd42:ea65:7ecf:78e8:216:3eff:fe60:5e4 (eth0) | PERSISTENT | 0 | +-------+---------+---------------------+----------------------------------------------+------------+-----------+ $ sudo lxc info srv01 Name: srv01 Remote: unix:// Architecture: x86_64 Created: 2017/11/09 14:42 UTC Status: Running Type: persistent Profiles: default Pid: 8883 Ips: eth0:\tinet\t10.20.22.236\tvethHJ05J0 eth0:\tinet6\tfd42:ea65:7ecf:78e8:216:3eff:fe60:5e4\tvethHJ05J0 eth0:\tinet6\tfe80::216:3eff:fe60:5e4\tvethHJ05J0 lo:\tinet\t127.0.0.1 lo:\tinet6\t::1 Resources: Processes: 11 CPU usage: CPU usage (in seconds): 1 Memory usage: Memory (current): 37.02MB Memory (peak): 40.30MB Network usage: eth0: Bytes received: 33.09kB Bytes sent: 1.77kB Packets received: 331 Packets sent: 17 lo: Bytes received: 0B Bytes sent: 0B Packets received: 0 Packets sent: 0 コンテナ内に入るには lxc exec を使います\n$ sudo lxc exec srv01 -- bash [root@srv01 ~]# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 14:42 ? 00:00:00 /sbin/init root 33 1 0 14:42 ? 00:00:00 /usr/lib/systemd/systemd-journald root 37 1 0 14:42 ? 00:00:00 /usr/lib/systemd/systemd-udevd root 55 1 0 14:42 ? 00:00:00 /usr/sbin/rsyslogd -n dbus 57 1 0 14:42 ? 00:00:00 /bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation root 64 1 0 14:42 ? 00:00:00 /usr/lib/systemd/systemd-logind root 69 1 0 14:42 ? 00:00:00 /usr/sbin/crond -n root 70 1 0 14:42 console 00:00:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 linux root 242 1 0 14:42 ? 00:00:00 /sbin/dhclient -1 -q -lf /var/lib/dhclient/dhclient--eth0.lease -pf /var/run/dhclient-et root 303 0 0 14:45 ? 00:00:00 bash root 312 303 0 14:46 ? 00:00:00 ps -ef https://linuxcontainers.org/ja/lxd/getting-started-cli/ を見ると基本的なコマンドの使い方はわかります 停止は lxc stop srv01 削除は lxc delete srv01 イメージのリストは lxc image list イメージの削除は lxc image delete XXXXX (XXXXX は list で表示される FINGERPRINT)\n$ sudo lxc image list +-------+--------------+--------+---------------------------------+--------+---------+-----------------------------+ | ALIAS | FINGERPRINT | PUBLIC | DESCRIPTION | ARCH | SIZE | UPLOAD DATE | +-------+--------------+--------+---------------------------------+--------+---------+-----------------------------+ | | bd115f8374ba | no | Centos 7 amd64 (20171109_02:28) | x86_64 | 82.26MB | Nov 9, 2017 at 2:42pm (UTC) | +-------+--------------+--------+---------------------------------+--------+---------+-----------------------------+ lxd グループに自分を追加しておけば lxc コマンドを sudo なしで使えるようになる\n","date":"2017年11月9日","permalink":"/2017/11/lxd-on-ubuntu-17-10/","section":"Posts","summary":"PC が \u0026ldquo;Operating System Not Found\u0026rdquo; と起動しなくなってしまったので Ubuntu 17.10 をクリーンインストールして「ソフトウェア」ってソフトを起動したらコレクションに LXD があったので試","title":"Ubuntu 17.10 で LXD を試してみる"},{"content":"","date":"2017年10月31日","permalink":"/tags/kvm/","section":"Tags","summary":"","title":"KVM"},{"content":"以前、「Ubuntu 17.04 に KVM をインストール」というのと書いていた。 Ubuntu 17.10 を入れたのでここでも KVM を使えるようにする。\n$ lsb_release -a No LSB modules are available. Distributor ID:\tUbuntu Description:\tUbuntu 17.10 Release:\t17.10 Codename:\tartful $ uname -r 4.13.0-16-lowlatency $ sudo apt install qemu-kvm libvirt-clients libvirt-bin bridge-utils virt-install を使うためには\n$ sudo apt install virtinst ","date":"2017年10月31日","permalink":"/2017/10/ubuntu-17-10-install-kvm/","section":"Posts","summary":"以前、「Ubuntu 17.04 に KVM をインストール」というのと書いていた。 Ubuntu 17.10 を入れたのでここでも KVM を使えるようにする。 $ lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 17.10 Release:","title":"Ubuntu 17.10 に KVM をインストール"},{"content":"Linux からのリモートデスクトップ接続には rdesktop を使ってきたが接続先環境の変更によって\nERROR: CredSSP: Initialize failed, do you have correct kerberos tgt initialized ? Failed to connect, CredSSP required by server. というエラーで接続できなくなってしまったので FreeRDP に切り替えた。\nUbuntu 17.04 では apt install freerdp2-x11 でインストールした。\nrdesktop -k ja -x 0x80 -g 1350x700 -z -a 16 -u {USERNAME} -d {DOMAIN} {SERVER}:{PORT} を\nxfreerdp /size:1350x700 /u:{USERNAME} /d:{DOMAIN} /v:{SERVER}:{PORT} \\ /bpp:16 /gdi:hw /compression-level:2 +fonts -wallpaper てな感じにしてみた。 関連：rdesktop コマンドで font smoothing を有効にする Ctrl と CapsLock を入れ替えているのが RDP 先に適用されないのが辛い\n","date":"2017年9月16日","permalink":"/2017/09/freerdp/","section":"Posts","summary":"Linux からのリモートデスクトップ接続には rdesktop を使ってきたが接続先環境の変更によって ERROR: CredSSP: Initialize failed, do you have correct kerberos tgt initialized ? Failed to connect, CredSSP required by server. というエラーで接続できなく","title":"FreeRDP でリモートデスクトップ"},{"content":"","date":"2017年6月15日","permalink":"/tags/apache/","section":"Tags","summary":"","title":"Apache"},{"content":"","date":"2017年6月15日","permalink":"/tags/haproxy/","section":"Tags","summary":"","title":"HAProxy"},{"content":"Apache, nginx, HAProxy の ReverseProxy において、proxy 先の障害をどう回避するかを調べてみます。\nApache # Apache 2.4.x の mod_proxy_balancer, mod_proxy_http を調査対象とします (より正確には CentOS 7 の httpd-2.4.6-45.el7.centos.4.x86_64 です)。 mod_proxy_hcheck については触れません。 次のような mod_proxy_balancer を使わない Proxy 設定では retry は行われません\nProxyPass / http://backend:8080/ ProxyPass / balancer://backend/ \u0026lt;Proxy balancer://backend\u0026gt; BalancerMember http://backend1:8080 BalancerMember http://backend2:8080 BalancerMember http://backend3:8080 \u0026lt;/Proxy\u0026gt; BalancerMember が複数設定されている場合は、接続できない限りは順に次のサーバーで retry されます、デフォルトでは Member の数だけ試行されます。BalancerMember が1つの場合は ProxyPass がデフォルトのままでは retry されません。接続できなかった Member はエラー状態とされ、BalancerMember オプションの retry (秒) で指定されて時間はリクエストが割り振られなくなります。retry のデフォルトは 60 (秒) です。\nBalancerMember http://backend1:8080 retry=10 全ての Member がエラー状態ではリクエストを捌けなくなるので retry の時間を待たずして全て復活します。ProxyPass のオプションで forcerecovery=Off と指定すれば全滅の場合も retry を待ちます (forcerecovery の default は On です)。 接続のタイムアウトは BalancerMember に connectiontimeout で指定します(デフォルトの単位は秒ですが ms をつけることでミリ秒指定が可能)。接続後のタイムアウトは timeout (秒) です。接続後の timeout ではリトライされません。504 Proxy Error がクライアントに返されます。デフォルトではこのタイムアウトでは Error 状態にならないので次からのリクエストもその Member へ振り分けられます。この場合も Error にするには ProxyPass 設定で failontimeout を On にします。サーバーが 500 や 503 を返した場合もデフォルトでは Error 状態になりませんが、failonstatus=500,503 などとカンマ区切りでステータスを並べることで Error 状態にでき、retry 秒間リクエストが割り振られません。 ProxyPass の maxattempts でリトライの回数を指定できます。これを指定すれば BalancerMember が1つでも接続のリトライが可能です。デフォルトでは BalancerMember を全部試すように調整されます。Member の数を超える数を指定すると往復するようなかんじでリトライされます。\nProxyPass / balancer://backend/ maxattempts=10 \u0026lt;Proxy balancer://backend\u0026gt; BalancerMember http://backend1:8080 retry=10 disablereuse=On connectiontimeout=2 timeout=2 BalancerMember http://backend2:8080 retry=10 disablereuse=On connectiontimeout=2 timeout=2 BalancerMember http://backend3:8080 retry=10 disablereuse=On connectiontimeout=2 timeout=2 \u0026lt;/Proxy\u0026gt; こんな感じで、どれにも接続できないとすると 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 3 -\u0026gt; 2 -\u0026gt; 1 -\u0026gt; 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 3 -\u0026gt; \u0026hellip; という順でリトライされました。\nping 設定で各リクエストを送る前に proxy 先が生きているかどうか、あるいはレスポンスが遅くないかを確認するとあります (http://httpd.apache.org/docs/2.4/mod/mod_proxy.html#proxypass) が試してみてもどうもそんな動作をしてないなと思って mod_proxy_http.c を確認したら、POST などリクエストに BODY が含まれる場合だけになっていました。\n do_100_continue = (worker-\u0026gt;s-\u0026gt;ping_timeout_set \u0026amp;\u0026amp; ap_request_has_body(r) \u0026amp;\u0026amp; (PROXYREQ_REVERSE == r-\u0026gt;proxyreq) \u0026amp;\u0026amp; !(apr_table_get(r-\u0026gt;subprocess_env, \u0026quot;force-proxy-request-1.0\u0026quot;))); POST で試してみると Expect: 100-Continue というヘッダーが追加されていました。この場合、100 Continue が返ってこないと継続データを送りません。全てが1度の write buffer に収まってる場合は追加で送るデータはありませんが、100 Continue が期待の時間内に返ってこなければ切断してしまうので処理がされません。\n[Proxy =\u0026gt; Backend] POST / HTTP/1.1 Host: 127.0.0.1:8080 User-Agent: curl/7.29.0 Accept: */* Content-Type: application/x-www-form-urlencoded Expect: 100-Continue X-Forwarded-For: 127.0.0.1 X-Forwarded-Host: 127.0.0.1 X-Forwarded-Server: ::1 Connection: Keep-Alive Content-Length: 3 a=b [Proxy \u0026lt;= Backend] HTTP/1.1 100 Continue [Proxy =\u0026gt; Backend] POST データの残り(あれば) [Proxy \u0026lt;= Backend] HTTP/1.1 200 OK Date: Sat, 01 Jul 2017 10:29:25 GMT Server: Apache/2.4.26 (Unix) OpenSSL/1.0.1e-fips Last-Modified: Mon, 11 Jun 2007 18:53:14 GMT ETag: \u0026quot;2d-432a5e4a73a80\u0026quot; Accept-Ranges: bytes Content-Length: 45 Keep-Alive: timeout=62, max=100 Connection: Keep-Alive Content-Type: text/html Apache のまとめ # 接続できてしまったらリトライされません。遅いサーバーはとっとと諦めて次に移るということができない。エラー状態になればダウンしているサーバーへのアクセスを試みることがなくなり、無駄なタイムアウト待ちを減らせますが、retry の指定秒を経過すると実際のサーバーの状態にかかわらず(まだダウンしているかもしれないのに)再度リクエストを割り振られてしまいます。\nnginx # nginx mainline repository の RPM package を CentOS 7 で実行して試しています (nginx-1.13.1-1.el7.ngx.x86_64) nginx 1.9.13 から POST, LOCK, PATCH メソッドの場合、デフォルトではリトライされないようになっています。 nginx で Load Balance \u0026amp; retry を行うには upstream module を使う必要があります。これを使わないで\nproxy_pass http://backend.example.com; とした場合は接続に失敗しても retry が行われません。\nupstream backend { server backend1:8080; server backend2:8080; server backend3:8080; } ... server { location / { proxy_pass http://backend; } } と、upstream module を使うと接続に失敗したり、timeout すると順に次のサーバーに対して retry が試みられます。 proxy 先が1つしか無い場合にも接続に失敗したら retry して欲しい場合にはどうしたら良いか？\nupstream backend { server backend1:8080; server backend1:8080; server backend1:8080; } 同じサーバーを並べることで retry が可能になります (Apache の場合は同一の proxy 先は1つにまとめられてしまいます)。 タイムアウト時の retry が続く場合はどこかで諦めないと10台とかに2秒ずつとか待っていられないのでどこかで早めに打ち切らせるべきです。試行回数を制限したい場合は proxy_next_upstream_tries で回数を指定します。デフォルトでは upstream に指定したサーバーの数となります。Apache と違い失敗した後の試行回数ではなく最初の試行からカウントされます。合計の待ち時間を制限したい場合は proxy_next_upstream_timeout で時間を指定します。retry を繰り返す間にこの時間に到達すると 504 Gateway Timeout がクライアントに返されます。retries と timeout を両方していすることもできます。どちらか先に達した方でエラーが返されます。\n接続の失敗やタイムアウトしたサーバーは一時的に無効状態にされ、しばらくリクエストを割り振られなくなります。このしばらくという時間は server のパラーメーター fail_timeout で指定します。デフォルトは10秒です。何回失敗したら無効にするかというと、fail_timeout の間に max_fails 回となっています。デフォルトは fail_timeout=10, max_fails=1 となっているので1度の失敗で10秒間無効にされます。\n失敗、失敗と書きましたが何を持って失敗とするかは proxy_next_upstream の値が関係してきます。デフォルトでは接続の失敗とタイムアウトとサーバーから異常なレスポンスがあった場合です。これらは proxy_next_upstream で指定せずとも失敗としてカウントされます。http_500, http_502, http_503, http_504, http_429 は proxy_next_upstream で指定した場合のみカウントされます。http_403 と http_404 は proxy_next_upstream で指定したとしてもカウントされません。proxy_next_upstream はリトライするかどうかの指定でもあり、その意味では設定したもののみがリトライ対象となります。デフォルト値から変更し connect や timeout を指定しなければ、失敗としてカウントされはするもののリトライはされません。\nHTTP CODE の 500 や 503 でも失敗としたい場合は proxy_next_upstream http_500 http_503 と指定します。error, timeout, invalid_header は指定しなくても常に失敗として扱われます。http_403, http_404 は指定しても失敗扱いになりません。(POST, LOCK, PATCH でもリトライさせたい場合は non_idempotent を指定します) サーバーのパラメーター指定は次のようにします。\nupstream backend { server backend1:8080 max_fail=1 fail_timeout=15; server backend2:8080 max_fail=1 fail_timeout=15; server backend3:8080 max_fail=1 fail_timeout=15; } パラメーターはこれ以外にも weight, max_conns, backup, down が使えます。 失敗が続き、retry 可能な別のサーバーが無くなった場合は最後のレスポンスを返します。すでに全てのサーバーが無効状態であれば 502 Bad Gateway が返されます。\n   Name Type Default Description     weight number 1 サーバー振り分けの重み付け   mx_conns number 0 同時接続数の上限を指定する。共有メモリを使うように zone が指定されていなければ workker プロセス単位で制限となる。非商用版で使えるのは 1.11.5 以降のみ   max_fails number 1 fail_timeout 単位時間にここで指定した回数の失敗が発生すると fail_timeout の間サーバーを無効にする (リクエストを割り振らない)   fail_timeout time 10秒 max_fails の説明を参照   backup - - backup でないサーバーが全て無効になった場合にのみ有効になる   down - - ずっと無効にしておく    有償版の nginx ではさらに resolve, route, service, slow_start という便利そうなパラメーターが使えるようです。\nnginx のまとめ # 失敗が続く限りは upstream に指定したサーバー全てに順にリトライする。失敗の定義は proxy_next_upstream で行い、リトライの回数、時間の制限は proxy_next_upstream_tries, proxy_next_upstream_timeout で制限する。サーバーの数を超えるリトライ回数は意味をなさない。単位時間(fail_timeout)内の失敗の回数(max_fails)によってサーバーが一時的(fail_timeout)に無効になる。 Proxy 先が1台でも retry したい場合は同じサーバーを複数定義する。 retry したくない場合は proxy_next_upstream off とする (proxy_next_upstream_tries 1 でも良さそうな気がする)。\nupstream backend { server backend1:8080 max_fail=1 fail_timeout=20 weight=1; server backend2:8080 max_fail=1 fail_timeout=20 weight=2; server backend3:8080 max_fail=1 fail_timeout=20 weight=3; server backend4:8080 backup; } ... server { location / { proxy_connect_timeout 2s; proxy_read_timeout 2s; proxy_next_upstream_timeout 5s; proxy_next_upstream_tries 3; proxy_pass http://backend; } } HAProxy # HAProxy 1.5.18 で確認しました (CentOS 6 の haproxy-1.5.18-1.el6.x86_64 で試したが CentOS 7 でも yum で入るのは haproxy-1.5.18-3.el7_3.1.x86_64 なので同じでしょう) HAProxy には active check があります、任意の間隔で proxy 先のチェックを行えます。(Apache の mod_proxy_hcheck はまだイマイチで nginx は有償版だけで使える)\nActive Check を行わない場合 (リトライしない) # backend test-be balance roundrobin server test1 backend1:8080 server test2 backend2:8080 server test3 backend3:8080 単純に順番に server で指定したサーバーに proxy します。接続できない場合は別のサーバーにトライせずに 503 Service Unavailable を返します。\nActive Check を行わない場合 (リトライする) # redispatch を有効にすると接続できない場合は次のサーバー、それでもダメならさらに次のサーバーへとリトライしてくれます。\nbackend test-be balance roundrobin option redispatch server test1 backend1:8080 server test2 backend2:8080 server test3 backend3:8080 Active Check をする場合 (リトライしない) # server に check パラメータをつけるとそのサーバーに接続できるかどうかをチェックします。default-server でチェック間隔を指定します。inter で通常時の間隔を、downinter で DOWN 状態の間隔、fastinter で DOWN から UP に変わる途中の間隔を。 途中というのは下の例では rise 3 としているので DOWN 状態になると 3 回チェックに成功しないと UP 状態にならなず、1 回成功した後 UP になるまでの間です。rise のデフォルトは 2 で、fall のデフォルトは 3 で、3 回エラーになると DOWN 状態になります。下の例では 1 回の失敗で DOWN になります。 定期的に監視して DOWN 状態になれば DOWN 中はそのサーバーにリクエストを振り分けなくなりますが、それまでは振り分けてしまってエラーを返してしまいます。\nbackend test-be balance roundrobin default-server inter 5000 downinter 10000 fastinter 3000 rise 3 fall 1 server test1 backend1:8080 **check** server test2 backend2:8080 **check** server test3 backend3:8080 **check** option httpchk 設定で TCP の接続確認だけでなく HTTP でのチェックができます。レスポンスが 2xx か 3xx であれば成功、それ以外が失敗です。\nbackend test-be balance roundrobin option httpchk GET /healthcheck default-server inter 5000 downinter 10000 fastinter 3000 rise 3 fall 1 server test1 backend1:8080 check server test2 backend2:8080 check server test3 backend3:8080 check Active Check をする場合 (リトライする) # option redispatch を設定すれば、接続できなかったりしても次のサーバーにリトライしてくれるのでクライアントにエラーを返さないですみます。\nbackend test-be balance roundrobin option redispatch default-server inter 5000 downinter 10000 fastinter 3000 rise 3 fall 1 server test1 backend1:8080 check server test2 backend2:8080 check server test3 backend3:8080 check 通常のリクエストもエラーカウントの対象にする # check に加えてて observe layer4 を設定することで通常のリクエストの処理でエラーになったものも fall のカウントに使われます。短い間隔で check を実行するよりもこの機能を使う方が監視のアクセスによる負荷を減らせます。沢山の HAProxy サーバーから頻繁な監視アクセスがあるとその処理内容によっては負荷が気になるかもしれないので。layer7 も使えます。この場合 100 から 499 と 501, 505 が成功として扱われます。404 Not Found で DOWN になったりしないように。\nbackend test-be balance roundrobin option httpchk GET /healthcheck default-server inter 5000 downinter 10000 fastinter 3000 rise 3 fall 1 server test1 backend1:8080 check observe layer4 server test2 backend2:8080 check observe layer4 server test3 backend3:8080 check observe layer4 まだ書きかけ\n","date":"2017年6月15日","permalink":"/2017/06/retry-function-of-reverseproxy/","section":"Posts","summary":"Apache, nginx, HAProxy の ReverseProxy において、proxy 先の障害をどう回避するかを調べてみます。 Apache # Apache 2.4.x の mod_proxy_balancer, mod_proxy_http を調査対象とします (より正確には CentOS 7 の httpd-2.4.6-45.el7.centos.4.x86_64 です)。 mod_proxy_hcheck につい","title":"ReverseProxyのretry機能を調査"},{"content":"Rancher の冗長構成には MySQL の冗長化が必要になります。そこで Replication より単純かなということでまずは DRBD + Pacemaker + Corosync 構成を試してみます。\n構成 # db1 [192.168.122.11], db2 [192.16.122.12] という2台のホストで VIP [192.168.122.10] をもたせます。 KVM に CentOS 7 をインストールして試しました。/dev/cl/drbd0 という LogicalVolume を DRBD に使用した。\nホスト名設定 # DRBD の設定でのサーバー指定は hostname コマンドの出力と一致する必要があるので db1, db2 としておきます。\n[db1]# hostnamectl set-hostname db1 [db2]# hostnamectl set-hostname db2 IPv6 を無効にする # [ALL]# echo \u0026quot;net.ipv6.conf.all.disable_ipv6 = 1\u0026quot; \u0026gt; /etc/sysctl.d/disable-ipv6.conf [ALL]# echo \u0026quot;net.ipv6.conf.default.disable_ipv6 = 1\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.d/disable-ipv6.conf [ALL]# sysctl -p firewalld 設定 # [ALL]# systemctl start firewalld [ALL]# systemctl enable firewalld [ALL]# firewall-cmd --add-service high-availability [ALL]# firewall-cmd --add-service high-availability --permanent pcs, pacemaker, corosync のインストール # [ALL]# yum install -y pcs pcs の依存で pacemaker, corosync, resource-agents などもインストールされます pcs で使われる hacluster ユーザーのパスワード設定\n[ALL]# echo passwd | passwd hacluster --stdin pcsd の起動\n[ALL]# systemctl start pcsd [ALL]# systemctl enable pcsd 1方のサーバーでクラスタ間の認証を通す\n[db1]# pcs cluster auth db1 db2 -u hacluster -p passwd /var/lib/pcsd/tokens に token が保存される\nクラスタの作成 # mysql_cluster という名前のクラスタをセットアップします\n[db1]# pcs cluster setup --name mysql_cluster db1 db2 [root@db1 ~]# pcs cluster setup --name mysql_cluster db1 db2 Destroying cluster on nodes: db1, db2... db1: Stopping Cluster (pacemaker)... db2: Stopping Cluster (pacemaker)... db1: Successfully destroyed cluster db2: Successfully destroyed cluster Sending cluster config files to the nodes... db1: Succeeded db2: Succeeded Synchronizing pcsd certificates on nodes db1, db2... db1: Success db2: Success Restarting pcsd on the nodes in order to reload the certificates... db1: Success db2: Success /etc/corosync/corosync.conf が作成されています\n[root@db1 ~]# cat /etc/corosync/corosync.conf totem { version: 2 secauth: off cluster_name: mysql_cluster transport: udpu } nodelist { node { ring0_addr: db1 nodeid: 1 } node { ring0_addr: db2 nodeid: 2 } } quorum { provider: corosync_votequorum two_node: 1 } logging { to_logfile: yes logfile: /var/log/cluster/corosync.log to_syslog: yes } クラスタの起動 # [db1]# pcs cluster start --all [root@db1 ~]# pcs cluster start --all db1: Starting Cluster... db2: Starting Cluster... corosync の状態確認\n[root@db1 ~]# pcs status corosync Membership information ---------------------- Nodeid Votes Name 1 1 db1 (local) 2 1 db2 [root@db1 ~]# pcs status Cluster name: mysql_cluster WARNING: no stonith devices and stonith-enabled is not false Stack: corosync Current DC: db2 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum Last updated: Sun Jun 11 15:29:22 2017\tLast change: Sun Jun 11 15:25:58 2017 by hacluster via crmd on db2 2 nodes and 0 resources configured Online: [ db1 db2 ] No resources Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled DRBD のインストール # [ALL]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org [ALL]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm [ALL]# yum install -y kmod-drbd84 drbd84-utils DRBD は SELinux が enforcing な環境では問題があるらしく次のようにして回避する\n[ALL]# yum install -y policycoreutils-python [ALL]# semanage permissive -a drbd_t DRBD 通信を許可する # 互いに 7789/tcp ポートへアクセスできるようにする\n[db1]# firewall-cmd --add-rich-rule='rule family=ipv4 source address=192.168.122.12 port port=7789 protocol=tcp accept' [db1]# firewall-cmd --add-rich-rule='rule family=ipv4 source address=192.168.122.12 port port=7789 protocol=tcp accept' --permanent [db2]# firewall-cmd --add-rich-rule='rule family=ipv4 source address=192.168.122.11 port port=7789 protocol=tcp accept' [db2]# firewall-cmd --add-rich-rule='rule family=ipv4 source address=192.168.122.11 port port=7789 protocol=tcp accept' --permanent DRBD のリソースを作成する # [ALL]# cat \u0026lt; /etc/drbd.d/mysql.res resource mysql { protocol C; meta-disk internal; device /dev/drbd0; disk /dev/cl/drbd0; handlers { split-brain \u0026quot;/usr/lib/drbd/notify-split-brain.sh root\u0026quot;; } # スプリットブレインからの自動復旧ポリシー net { allow-two-primaries no; # スプリットブレインが検出されたときに両ノードともセカンダリロールの場合に適用されるポリシーの定義 after-sb-0pri discard-zero-changes; # 変更がなかったホストでは、他方に加えられたすべての変更内容を適用して続行 # スプリットブレインが検出されたときにどちらか1つのノードがプライマリロールである場合に適用されるポリシーの定義 after-sb-1pri discard-secondary; # クラスタからノードを強制的に削除 # スプリットブレインが検出されたときに両ノードともプライマリロールである場合に適用されるポリシーの定義 after-sb-2pri disconnect; # split-brain ハンドラスクリプト(構成されている場合)を呼び出し、コネクションを切断して切断モードで続行 rr-conflict disconnect; } disk { on-io-error detach; } syncer { verify-alg sha1; } on db1 { address 192.168.122.11:7789; } on db2 { address 192.168.122.12:7789; } } EOF https://blog.3ware.co.jp/drbd-users-guide-8.3/s-configure-split-brain-behavior.html\n[ALL]# drbdadm create-md mysql [root@db1 ~]# drbdadm create-md mysql [ 9270.237905] Request for unknown module key 'The ELRepo Project (http://elrepo.org): ELRepo.org Secure Boot Key: f365ad3481a7b20e3427b61b2a26635b83fe427b' err -11 [ 9270.240696] drbd: loading out-of-tree module taints kernel. [ 9270.241865] drbd: module verification failed: signature and/or required key missing - tainting kernel [ 9270.256491] drbd: initialized. Version: 8.4.9-1 (api:1/proto:86-101) [ 9270.257797] drbd: GIT-hash: 9976da086367a2476503ef7f6b13d4567327a280 build by akemi@Build64R7, 2016-12-04 01:08:48 [ 9270.260611] drbd: registered as block device major 147 initializing activity log NOT initializing bitmap Writing meta data... New drbd meta data block successfully created. success [ALL]# drbdadm up mysql db1 側を primary と指定することで同期が始まる\n[db1]# drbdadm primary --force mysql drbd-overview で同期の進捗が確認できる\n[root@db1 ~]# drbd-overview 0:mysql/0 SyncSource Primary/Secondary UpToDate/Inconsistent [\u0026gt;....................] sync'ed: 2.8% (1022252/1048508)K [root@db1 ~]# drbd-overview 0:mysql/0 SyncSource Primary/Secondary UpToDate/Inconsistent [=\u0026gt;..................] sync'ed: 11.8% (929188/1048508)K [root@db1 ~]# drbd-overview 0:mysql/0 Connected Primary/Secondary UpToDate/UpToDate 同期完了\nDRBD デバイスをフォーマット # DRBD によってできたデバイスを XFS でフォーマットする\n[db1]# mkfs.xfs /dev/drbd0 MariaDB のインストール # MySQL でも良いのだけれど CentOS 7 の標準 repository は MariaDB に変わっているのでこれを使います。そういえば最近 MariaDB の話聞かないね。\n[ALL]# yum install -y mariadb-server mariadb DRBD デバイスを仮に /mnt にマウント\n[db1]# mount /dev/drbd0 /mnt /mnt に MySQL(?) をセットアップ\n[db1]# mysql_install_db --datadir=/mnt --user=mysql /var/lib/mysql と同じように SELinux の context を設定\n[db1]# semanage fcontext -a -t mysqld_db_t \u0026quot;/mnt(/.*)?\u0026quot; [db1]# restorecon -Rv /mnt 準備ができたので umount\n[db1]# umount /mnt my.cnf の設定\n[ALL]# cat \u0026lt;\u0026lt; EOL \u0026gt; /etc/my.cnf [mysqld] symbolic-links=0 bind_address = 0.0.0.0 datadir = /var/lib/mysql pid_file = /var/run/mariadb/mysqld.pid socket = /var/run/mariadb/mysqld.sock [mysqld_safe] bind_address = 0.0.0.0 datadir = /var/lib/mysql pid_file = /var/run/mariadb/mysqld.pid socket = /var/run/mariadb/mysqld.sock !includedir /etc/my.cnf.d EOL Pacemaker 設定 # 第12章 PACEMAKER クラスターのプロパティ\n[db1]# pcs cluster cib clust_cfg stonith の無効化（使えるなら使ったほうが良い）\n[db1]# pcs -f clust_cfg property set stonith-enabled=false quorum の無効化（2台構成では過半数はとれない）\n[db1]# pcs -f clust_cfg property set no-quorum-policy=ignore failback を抑制する\n[db1]# pcs -f clust\\_cfg resource defaults resource-stickiness=200 DRBD の resource として mysql (/etc/drbd.d/mysql.res で設定したやつ) を指定\n[db1]# pcs -f clust_cfg resource create mysql_data ocf:linbit:drbd \\ drbd_resource=mysql \\ op monitor interval=30s [db1]# pcs -f clust_cfg resource master MySQLClone mysql_data \\ master-max=1 master-node-max=1 \\ clone-max=2 clone-node-max=1 \\ notify=true master-max=1\nマスターに昇格させることができるリソースのコピー数\nmaster-node-max=1\n1つのノード上でマスターに昇格させることができるリソースのコピー数\nclone-max=2\nいくつのリソースコピーを開始するか。デフォルトはクラスタ内のノード数\nclone-node-max=1\n1つのノードで開始状態にできるリソースのコピー数\nnotify=true\nクローンのコピーを開始、停止する前後に他の全てのコピーに伝える\nmysql_fs という名前で /dev/drbd0 を /var/lib/mysql にマウントする resource を定義\n[db1]# pcs -f clust_cfg resource create mysql_fs Filesystem \\ device=\u0026quot;/dev/drbd0\u0026quot; \\ directory=\u0026quot;/var/lib/mysql\u0026quot; \\ fstype=\u0026quot;xfs\u0026quot; MySQLClone には mysql_fs が必須\n[db1]# pcs -f clust_cfg constraint colocation add mysql_fs with MySQLClone \\ INFINITY with-rsc-role=Master MySQLClone を master に昇格させるときに mysql_fs を開始する\n[db1]# pcs -f clust_cfg constraint order promote MySQLClone then start mysql_fs mysql_service resource の作成、MariaDB (MySQL) の起動設定 (ocf💓mysql のファイルは /usr/lib/ocf/resource.d/heartbeat/mysql にあります) monitor interval はもっと短い方が良いかな\n[db1]# pcs -f clust_cfg resource create mysql_service ocf💓mysql \\ binary=\u0026quot;/usr/bin/mysqld_safe\u0026quot; \\ config=\u0026quot;/etc/my.cnf\u0026quot; \\ datadir=\u0026quot;/var/lib/mysql\u0026quot; \\ pid=\u0026quot;/var/lib/mysql/mysql.pid\u0026quot; \\ socket=\u0026quot;/var/lib/mysql/mysql.sock\u0026quot; \\ additional_parameters=\u0026quot;--bind-address=0.0.0.0\u0026quot; \\ op start timeout=60s \\ op stop timeout=60s \\ op monitor interval=20s timeout=30s mysql_fs resource 起動しているノードで mysql_service を起動する (7.3. リソースのコロケーション (constraint colocation))\n[db1]# pcs -f clust_cfg constraint colocation add mysql_service with mysql_fs INFINITY mysql_fs の後に mysql_service を開始 (7.2. 順序の制約 (constraint order))\n[db1]# pcs -f clust_cfg constraint order mysql_fs then mysql_service VIP resource を定義 (IPaddr2 は Linux 向けの VIP 設定、場所は /usr/lib/ocf/resource.d/heartbeat/IPaddr2)\n[db1]# pcs -f clust_cfg resource create mysql_VIP ocf💓IPaddr2 \\ ip=192.168.122.10 cidr_netmask=32 \\ op monitor interval=30s IPaddr\nmanages virtual IPv4 addresses (portable version)\nIPaddr2\nmanages virtual IPv4 addresses (Linux specific version).\nmysql_VIP は mysql_service の実行ノードで実行\n[db1]# pcs -f clust_cfg constraint colocation add mysql_VIP with mysql_service INFINITY mysql_service の後に mysql_VIP を開始\n[db1]# pcs -f clust_cfg constraint order mysql_service then mysql_VIP 制約確認\n[db1]# pcs -f clust_cfg constraint 実行例\n[root@db1 ~]# pcs -f clust_cfg constraint Location Constraints: Ordering Constraints: promote MySQLClone then start mysql_fs (kind:Mandatory) start mysql_fs then start mysql_service (kind:Mandatory) start mysql_service then start mysql_VIP (kind:Mandatory) Colocation Constraints: mysql_fs with MySQLClone (score:INFINITY) (with-rsc-role:Master) mysql_service with mysql_fs (score:INFINITY) mysql_VIP with mysql_service (score:INFINITY) Ticket Constraints: MySQLClone という Master / Slave な clone があり、これを Master に昇格させると mysql_fs, mysql_service, mysql_VIP の順に起動させる\n[db1]# pcs -f clust_cfg resource show [root@db1 ~]# pcs -f clust_cfg resource show Master/Slave Set: MySQLClone [mysql_data] Stopped: [ db1 db2 ] mysql_fs\t(ocf:💓Filesystem):\tStopped mysql_service\t(ocf:💓mysql):\tStopped mysql_VIP\t(ocf:💓IPaddr2):\tStopped ここまで、cluster_cfg というファイルに設定を入れていたが、ここで cib に push することで /var/lib/pacemaker/cib/cib.xml に保存されます\n[db1]# pcs cluster cib-push clust_cfg Pacemaker の状態確認\n[db1]# pcs status [root@db1 ~]# pcs status Cluster name: mysql_cluster Stack: corosync Current DC: db2 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum Last updated: Tue Jun 13 23:44:58 2017\tLast change: Tue Jun 13 20:22:08 2017 by root via crm_attribute on db2 2 nodes and 5 resources configured Online: [ db1 db2 ] Full list of resources: Master/Slave Set: MySQLClone [mysql_data] Masters: [ db1 ] Slaves: [ db2 ] mysql_fs\t(ocf:💓Filesystem):\tStarted db1 mysql_service\t(ocf:💓mysql):\tStarted db1 mysql_VIP\t(ocf:💓IPaddr2):\tStarted db1 Daemon Status: corosync: active/enabled pacemaker: active/enabled pcsd: active/enabled 参考ドキュメント #  Active/Passive MySQL High Availability Pacemaker Cluster with DRBD on CentOS 7 | Lisenet.com :: Linux | Security | Networking Capitolo 7. Replicate Storage Using DRBD スプリットブレイン時の動作の設定 DRBD 8.4 の設定、切り替え手順（マルチPrimaryバージョン） - Qiita ELRepo : HomePage  ","date":"2017年6月13日","permalink":"/2017/06/mariadb-on-centos7-ha-using-drbd/","section":"Posts","summary":"Rancher の冗長構成には MySQL の冗長化が必要になります。そこで Replication より単純かなということでまずは DRBD + Pacemaker + Corosync 構成を試してみます。 構成 # db1 [192.168.122.11], db2 [192.16.122.12] という2台のホ","title":"CentOS7のMariaDB(MySQL)をDRBDでHA化"},{"content":"","date":"2017年6月13日","permalink":"/tags/corosync/","section":"Tags","summary":"","title":"Corosync"},{"content":"","date":"2017年6月13日","permalink":"/tags/drbd/","section":"Tags","summary":"","title":"DRBD"},{"content":"","date":"2017年6月13日","permalink":"/tags/mariadb/","section":"Tags","summary":"","title":"MariaDB"},{"content":"","date":"2017年6月13日","permalink":"/tags/pacemaker/","section":"Tags","summary":"","title":"Pacemaker"},{"content":"","date":"2017年6月10日","permalink":"/tags/glusterfs/","section":"Tags","summary":"","title":"GlusterFS"},{"content":"今回は NFS の ACL についてです。通常の NFS サーバーであれば /etc/exports などでどのホストにどのディレクトリを公開するか、Read-Only か Read-Write かなどを設定できます。 Ganesha NFS ではどのようにすれば良いでしょうか。 /usr/libexec/ganesha/create-export-ganesha.sh では次のような export.vol1.conf ファイルが生成されます。``` # WARNING : Using Gluster CLI will overwrite manual\nchanges made to this file. To avoid it, edit the # file and run ganesha-ha.sh \u0026ndash;refresh-config. # EXPORT{ Export_Id = 2; Path = \u0026ldquo;/vol1\u0026rdquo;; FSAL { name = GLUSTER; hostname=\u0026ldquo;localhost\u0026rdquo;; volume=\u0026ldquo;vol1\u0026rdquo;; } Access_type = RW; Disable_ACL = true; Squash=\u0026ldquo;No_root_squash\u0026rdquo;; Pseudo=\u0026quot;/vol1\u0026quot;; Protocols = \u0026ldquo;3\u0026rdquo;, \u0026ldquo;4\u0026rdquo; ; Transports = \u0026ldquo;UDP\u0026rdquo;,\u0026ldquo;TCP\u0026rdquo;; SecType = \u0026ldquo;sys\u0026rdquo;; }\n````Access_typeをRWからROにすれば Read-Only での公開になりますが、クライアントホストの限定はできません。EXPORTブロック内には次のようなclientブロックを入れることができ、ここでクライアント毎のaccess_type` を上書き可能です。``` client { clients = \u0026ldquo;10.xx.xx.xx\u0026rdquo;; # IP of the client. allow_root_access = true; access_type = \u0026ldquo;RO\u0026rdquo;; # Read-only permissions Protocols = \u0026ldquo;3\u0026rdquo;; # Allow only NFSv3 protocol. anonymous_uid = 1440; anonymous_gid = 72; }\n````EXPORT内のaccess_typeをnone にしておいて、client内でROやRWと指定すればそのclients で指定した接続元IPアドレス毎のアクセス権設定が可能になります。client` ブロックは次のように複数定義できます。``` EXPORT { \u0026hellip; access_type = none; \u0026hellip; client { clients = 192.168.122.71; access_type = RW; } client { clients = 192.168.122.0/24; access_type = RO; } }\nこれで 192.168.122.71 からは書き込み可、192.168.122.0/24 内の他のホストからは読み込み専用、その他のホストからはマウント不可となります。 このファイルは変更後に次のコマンドで反映させる必要があります。 # /usr/libexec/ganesha/ganesha-ha.sh \u0026ndash;refresh-config /var/run/gluster/shared_storage/nfs-ganesha vol1 Refresh-config completed on gluster2. Success: refresh-config completed.\n","date":"2017年6月10日","permalink":"/2017/06/building-ha-nfs-server-part5/","section":"Posts","summary":"今回は NFS の ACL についてです。通常の NFS サーバーであれば /etc/exports などでどのホストにどのディレクトリを公開するか、Read-Only か Read-Write かなどを設定できま","title":"GlusterFS + NFS-Ganesha で HA な NFS サーバーを構築する (5)"},{"content":"パート1 で GlusterFS Volume をセットアップしたところから始めようと思ったが DigitalOcean でも Vagrant (VirtualBox) でもうまくいかないので KVM で試してみた（Network Interface が複数あるとうまくいかないのだろうか？）。\nKVM に kickstart でセットアップ # 次のような Kickstart 用ファイルを gluster1, gluster2 分作成する\n#version=DEVEL # System authorization information auth --enableshadow --passalgo=sha512 # Use network installation url --url=\u0026quot;http://ftp.iij.ad.jp/pub/linux/centos/7/os/x86_64/\u0026quot; # Use text mode install text # Run the Setup Agent on first boot firstboot --enable ignoredisk --only-use=vda # Keyboard layouts keyboard --vckeymap=jp --xlayouts='jp','us' # System language lang en_US.UTF-8 # Network information network --bootproto=static --device=eth0 --gateway=192.168.122.1 --ip=192.168.122.51 --nameserver=192.168.122.1 --netmask=255.255.255.0 --ipv6=auto --activate network --hostname=gluster1.example.com # Root password rootpw --iscrypted $6$SnOAnBBQRh0lVpFR$fI.QeH4QU4fjrBvQVjNaXngHLWIHj5MeVSMX.37ws9qUiHJ9FkJqiofgNlW8xJky2O4QelVLSEvW63ckjv2a60 # System services services --enabled=\u0026quot;chronyd\u0026quot; # System timezone timezone Asia/Tokyo --isUtc user --name=centos --password=$6$vm6CEJebCNPiAW9h$ZUTwIRMJbZCI5OqLSXFhT4i3W/nbXawEx3hPHSrJr7/N25anniULRSAGzBsbnN87LXIP5d3SDVS8y0j5sOaUk. --iscrypted # System bootloader configuration bootloader --append=\u0026quot; crashkernel=auto\u0026quot; --location=mbr --boot-drive=vda # Partition clearing information clearpart --none --initlabel # Disk partitioning information part /boot --fstype=\u0026quot;xfs\u0026quot; --ondisk=vda --size=512 part pv.183 --fstype=\u0026quot;lvmpv\u0026quot; --ondisk=vda --size=1 --grow part biosboot --fstype=\u0026quot;biosboot\u0026quot; --ondisk=vda --size=2 volgroup cl --pesize=4096 pv.183 logvol swap --fstype=\u0026quot;swap\u0026quot; --size=512 --name=swap --vgname=cl logvol none --fstype=\u0026quot;None\u0026quot; --size=1 --grow --thinpool --name=pool00 --vgname=cl logvol / --fstype=\u0026quot;xfs\u0026quot; --size=4096 --thin --poolname=pool00 --name=root --vgname=cl logvol /gluster/vol1/brick1 --fstype=\u0026quot;xfs\u0026quot; --size=1024 --thin --poolname=pool00 --name=gluster_vol1_brick1 --vgname=cl %packages @^minimal @core chrony kexec-tools %end %addon com_redhat_kdump --enable --reserve-mb='auto' %end %anaconda pwpolicy root --minlen=6 --minquality=50 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=50 --notstrict --nochanges --notempty pwpolicy luks --minlen=6 --minquality=50 --notstrict --nochanges --notempty %end %post --log=/root/ks-post.log yum -y update yum -y install centos-release-gluster310 yum -y install install glusterfs glusterfs-server glusterfs-ganesha cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/hosts 192.168.122.51 gluster1 192.168.122.52 gluster2 EOF echo redhat | sudo passwd --stdin hacluster systemctl start glusterd systemctl enable glusterd systemctl disable NetworkManager.service systemctl disable NetworkManager-wait-online.service systemctl enable pcsd systemctl start pcsd cat \u0026lt;\u0026lt;EOF \u0026gt; /var/lib/glusterd/nfs/secret.pem -----BEGIN EC PRIVATE KEY----- MHcCAQEEIDWZLwNkk5Za1PTAIOjEDrafAeA+MA5tSL7t2XAPnn+uoAoGCCqGSM49 AwEHoUQDQgAEpMGlqMXYci0EOceoT+kRmRnaHcT5F7AXvez0tu5ujm9cHYXT5k14 hDCRoqBR6NTnpYMnER6uE6AG43gX+HPACg== -----END EC PRIVATE KEY----- EOF chmod 600 /var/lib/glusterd/nfs/secret.pem install -o root -g root -m 0700 -d /root/.ssh echo \u0026quot;ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBKTBpajF2HItBDnHqE/pEZkZ2h3E+RewF73s9Lbubo5vXB2F0+ZNeIQwkaKgUejU56WDJxEerhOgBuN4F/hzwAo= root@gluster\u0026quot; \u0026gt;\u0026gt; /root/.ssh/authorized_keys cat \u0026lt;\u0026lt;EOF \u0026gt; /tmp/000-firewalld-config.sh #!/bin/bash systemctl start firewalld systemctl enable firewalld firewall-cmd --add-service glusterfs firewall-cmd --add-service glusterfs --permanent firewall-cmd --add-service high-availability firewall-cmd --add-service high-availability --permanent firewall-cmd --add-service nfs firewall-cmd --add-service nfs --permanent EOF chmod 755 /tmp/000-firewalld-config.sh cat \u0026lt;\u0026lt;EOF \u0026gt; /tmp/001-setup-gluster-volume.sh #!/bin/bash gluster peer probe gluster2 gluster volume create vol1 replica 2 transport tcp gluster1:/gluster/vol1/brick1/brick gluster2:/gluster/vol1/brick1/brick gluster volume start vol1 EOF chmod 755 /tmp/001-setup-gluster-volume.sh cat \u0026lt;\u0026lt;EOF \u0026gt; /tmp/002-mount-shared-storage.sh #!/bin/bash gluster volume set all cluster.enable-shared-storage enable EOF chmod 755 /tmp/002-mount-shared-storage.sh cat \u0026lt;\u0026lt;EOF \u0026gt; /tmp/003-configure-ganesha.sh #!/bin/bash mkdir /var/run/gluster/shared_storage/nfs-ganesha touch /var/run/gluster/shared_storage/nfs-ganesha/ganesha.conf cat \u0026lt;\u0026lt;_EOF_ \u0026gt; /var/run/gluster/shared_storage/nfs-ganesha/ganesha-ha.conf HA_NAME=\u0026quot;ganesha\u0026quot; HA_CLUSTER_NODES=\u0026quot;gluster1,gluster2\u0026quot; VIP_gluster1=192.168.122.61 VIP_gluster2=192.168.122.62 _EOF_ pcs cluster auth gluster1 gluster2 -u hacluster -p redhat /usr/libexec/ganesha/ganesha-ha.sh setup /var/run/gluster/shared_storage/nfs-ganesha EOF chmod 755 /tmp/003-configure-ganesha.sh cat \u0026lt;\u0026lt;EOF \u0026gt; /tmp/004-enable-services.sh #!/bin/bash systemctl enable pacemaker systemctl enable corosync systemctl enable nfs-ganesha systemctl start nfs-ganesha EOF chmod 755 /tmp/004-enable-services.sh %end 次のように virt-install で仮想サーバーをセットアップする（インストール時に No space left on device で / に書き込めないというエラーが出てハマったが 1GB メモリでは足りないということだったので 2GB に増やした）\nvirt-install \\ --name gluster1 \\ --vcpus 1 \\ --memory 2048 \\ --os-variant rhel7 \\ --location http://ftp.iij.ad.jp/pub/linux/centos/7/os/x86_64/ \\ --network network=default,model=virtio \\ --disk pool=ytera,size=10,format=qcow2,bus=virtio \\ --graphics none \\ --initrd-inject gluster1.ks \\ --extra-args=\u0026quot;console=tty0 console=ttyS0,115200n8 ks=file:/gluster1.ks\u0026quot; これでセットアップすると %post で glusterfs-ganesha パッケージのインストールや hosts への追記、お互いに公開鍵によって root で ssh できるようにする設定などが行われています。glusterfs-ganesha では pcs, pacemaker, corosync といった HA 構築用パッケージもインストールされています。 また /tmp に次に実行するファイルができています\n/tmp/000-firewalld-config.sh /tmp/001-setup-gluster-volume.sh /tmp/002-mount-shared-storage.sh /tmp/003-configure-ganesha.sh /tmp/004-enable-services.sh /tmp/000-firewalld-config.sh # firewalld で必要なポートを開けます。これは gluster1, gluster2 の両方で実行します\nsystemctl start firewalld systemctl enable firewalld firewall-cmd --add-service glusterfs firewall-cmd --add-service glusterfs --permanent firewall-cmd --add-service high-availability firewall-cmd --add-service high-availability --permanent firewall-cmd --add-service nfs firewall-cmd --add-service nfs --permanent それぞれの service がどの port を開けているのかは /usr/lib/firewalld/services/ にあるファイルを見るとわかります\n$ cat /usr/lib/firewalld/services/high-availability.xml \u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;utf-8\u0026quot;?\u0026gt; \u0026lt;service\u0026gt; \u0026lt;short\u0026gt;Red Hat High Availability\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;This allows you to use the Red Hat High Availability (previously named Red Hat Cluster Suite). Ports are opened for corosync, pcsd, pacemaker_remote, dlm and corosync-qnetd.\u0026lt;/description\u0026gt; \u0026lt;port protocol=\u0026quot;tcp\u0026quot; port=\u0026quot;2224\u0026quot;/\u0026gt; \u0026lt;port protocol=\u0026quot;tcp\u0026quot; port=\u0026quot;3121\u0026quot;/\u0026gt; \u0026lt;port protocol=\u0026quot;tcp\u0026quot; port=\u0026quot;5403\u0026quot;/\u0026gt; \u0026lt;port protocol=\u0026quot;udp\u0026quot; port=\u0026quot;5404\u0026quot;/\u0026gt; \u0026lt;port protocol=\u0026quot;udp\u0026quot; port=\u0026quot;5405\u0026quot;/\u0026gt; \u0026lt;port protocol=\u0026quot;tcp\u0026quot; port=\u0026quot;21064\u0026quot;/\u0026gt; /tmp/001-setup-gluster-volume.sh # glusterfs の peer 設定と volume の作成を行います。gluster1 でのみ実行します。\n/tmp/002-mount-shared-storage.sh # gluster volume set all cluster.enable-shared-storage enable を実行します。gluster1 でのみ実行します。 これにより /run/gluster/shared_storage に glusterfs の共有 volume がマウントされます。 設定の共有や NFS の lock などに使われます。/etc/fstab への書き込みまでしてくれます。\n/tmp/003-configure-ganesha.sh # /var/run/gluster/shared_storage/nfs-ganesha/ganesha-ha.conf に HA セットアップ用のファイルを作成します。\nHA_NAME=\u0026quot;ganesha\u0026quot; HA_CLUSTER_NODES=\u0026quot;gluster1,gluster2\u0026quot; VIP_gluster1=192.168.122.61 VIP_gluster2=192.168.122.62 HA_NAME には任意の名前を設定、HA_CLUSTER_NODES にクラスタを組むサーバーのリストを列挙します。VIP_{hostname} にそれぞれのサーバー用の VIP (ip address) を指定します。 ドメインを入れても大丈夫です。\nHA_NAME=\u0026quot;ganesha\u0026quot; HA_CLUSTER_NODES=\u0026quot;gluster1.examplc.com,gluster2.examplc.com\u0026quot; VIP_gluster1.examplc.com=192.168.122.61 VIP_gluster2.examplc.com=192.168.122.62 この後 pcs cluster auth gluster1 gluster2 -u hacluster -p redhat で pcs コマンドで設定ができるように認証を通します。-u でユーザー名、-p でパスワードを指定しています。 hacluster ユーザーは pcs パッケージのインストールで作成され、kickstart の %post でパスワードを設定しています。 /usr/libexec/ganesha/ganesha-ha.sh setup /var/run/gluster/shared_storage/nfs-ganesha にて HA 設定が行われます。\n# bash -x 003-configure-ganesha.sh + mkdir /var/run/gluster/shared_storage/nfs-ganesha + touch /var/run/gluster/shared_storage/nfs-ganesha/ganesha.conf + cat + pcs cluster auth gluster1 gluster2 Username: hacluster Password: gluster2: Authorized gluster1: Authorized + /usr/libexec/ganesha/ganesha-ha.sh setup /var/run/gluster/shared_storage/nfs-ganesha gluster2: Already authorized gluster1: Already authorized Destroying cluster on nodes: gluster1, gluster2... gluster1: Stopping Cluster (pacemaker)... gluster2: Stopping Cluster (pacemaker)... gluster1: Successfully destroyed cluster gluster2: Successfully destroyed cluster Sending cluster config files to the nodes... gluster1: Succeeded gluster2: Succeeded Synchronizing pcsd certificates on nodes gluster1, gluster2... gluster2: Success gluster1: Success Restarting pcsd on the nodes in order to reload the certificates... gluster2: Success gluster1: Success gluster1: Starting Cluster... gluster2: Starting Cluster... Adding nfs-grace-clone gluster1-cluster_ip-1 (kind: Mandatory) (Options: first-action=start then-action=start) Adding nfs-grace-clone gluster2-cluster_ip-1 (kind: Mandatory) (Options: first-action=start then-action=start) CIB updated これで、次のような状況になります\n# pcs status Cluster name: ganesha Stack: corosync Current DC: gluster1 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum Last updated: Sun Jun 4 19:46:33 2017\tLast change: Sun Jun 4 19:46:23 2017 by root via cibadmin on gluster1 2 nodes and 12 resources configured Online: [ gluster1 gluster2 ] Full list of resources: Clone Set: nfs_setup-clone [nfs_setup] Started: [ gluster1 gluster2 ] Clone Set: nfs-mon-clone [nfs-mon] Started: [ gluster1 gluster2 ] Clone Set: nfs-grace-clone [nfs-grace] Stopped: [ gluster1 gluster2 ] Resource Group: gluster1-group gluster1-nfs_block\t(ocf:💓portblock):\tStopped gluster1-cluster_ip-1\t(ocf:💓IPaddr):\tStopped gluster1-nfs_unblock\t(ocf:💓portblock):\tStopped Resource Group: gluster2-group gluster2-nfs_block\t(ocf:💓portblock):\tStopped gluster2-cluster_ip-1\t(ocf:💓IPaddr):\tStopped gluster2-nfs_unblock\t(ocf:💓portblock):\tStopped Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled # pcs cluster corosync totem { version: 2 secauth: off cluster_name: ganesha transport: udpu } nodelist { node { ring0_addr: gluster1 nodeid: 1 } node { ring0_addr: gluster2 nodeid: 2 } } quorum { provider: corosync_votequorum two_node: 1 } logging { to_logfile: yes logfile: /var/log/cluster/corosync.log to_syslog: yes } /tmp/004-enable-services.sh # この中では自動起動を有効にしています、gluster1, gluster2 の両方で実行します\nsystemctl enable pacemaker systemctl enable corosync systemctl enable nfs-ganesha systemctl start nfs-ganesha nfs-ganesha が起動されることで HA 構成が完了です\n# pcs status Cluster name: ganesha Stack: corosync Current DC: gluster1 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum Last updated: Sun Jun 4 19:48:00 2017\tLast change: Sun Jun 4 19:47:49 2017 by root via crm_attribute on gluster2 2 nodes and 12 resources configured Online: [ gluster1 gluster2 ] Full list of resources: Clone Set: nfs_setup-clone [nfs_setup] Started: [ gluster1 gluster2 ] Clone Set: nfs-mon-clone [nfs-mon] Started: [ gluster1 gluster2 ] Clone Set: nfs-grace-clone [nfs-grace] Started: [ gluster1 gluster2 ] Resource Group: gluster1-group gluster1-nfs_block\t(ocf:💓portblock):\tStarted gluster1 gluster1-cluster_ip-1\t(ocf:💓IPaddr):\tStarted gluster1 gluster1-nfs_unblock\t(ocf:💓portblock):\tStarted gluster1 Resource Group: gluster2-group gluster2-nfs_block\t(ocf:💓portblock):\tStarted gluster2 gluster2-cluster_ip-1\t(ocf:💓IPaddr):\tStarted gluster2 gluster2-nfs_unblock\t(ocf:💓portblock):\tStarted gluster2 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled export する # NFS で export するための設定を作成するための /usr/libexec/ganesha/create-export-ganesha.sh があります\n# /usr/libexec/ganesha/create-export-ganesha.sh /var/run/gluster/shared_storage/nfs-ganesha on vol1 これによって /var/run/gluster/shared_storage/nfs-ganesha/exports/export.vol1.conf というファイルが作成されます\n# cat /var/run/gluster/shared_storage/nfs-ganesha/exports/export.vol1.conf # WARNING : Using Gluster CLI will overwrite manual # changes made to this file. To avoid it, edit the # file and run ganesha-ha.sh --refresh-config. EXPORT{ Export_Id = 2; Path = \u0026quot;/vol1\u0026quot;; FSAL { name = GLUSTER; hostname=\u0026quot;localhost\u0026quot;; volume=\u0026quot;vol1\u0026quot;; } Access_type = RW; Disable_ACL = true; Squash=\u0026quot;No_root_squash\u0026quot;; Pseudo=\u0026quot;/vol1\u0026quot;; Protocols = \u0026quot;3\u0026quot;, \u0026quot;4\u0026quot; ; Transports = \u0026quot;UDP\u0026quot;,\u0026quot;TCP\u0026quot;; SecType = \u0026quot;sys\u0026quot;; } そして、/var/run/gluster/shared_storage/nfs-ganesha/ganesha.con ファイルに次の include 文が追記されます\n%include \u0026quot;/var/run/gluster/shared_storage/nfs-ganesha/exports/export.vol1.conf\u0026quot; この変更を反映させるためには ganesha-ha.sh --refresh-config が必要です\n# /usr/libexec/ganesha/ganesha-ha.sh --refresh-config /var/run/gluster/shared_storage/nfs-ganesha vol1 Refresh-config completed on gluster2. Success: refresh-config completed. NFS Mount # [root@gluster1 ~]# ip address show eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 52:54:00:b0:fd:64 brd ff:ff:ff:ff:ff:ff inet 192.168.122.51/24 brd 192.168.122.255 scope global eth0 valid_lft forever preferred_lft forever inet 192.168.122.61/32 brd 192.168.122.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::5054:ff:feb0:fd64/64 scope link valid_lft forever preferred_lft forever 192.168.122.61 が VIP なのでこのアドレスを使ってマウントします。こうすることで gluster1 がダウンしても VIP が gluster2 に引き継がれることによってアクセスが継続できます。\n[root@client ~]# mount -t nfs 192.168.122.61:/vol1 /mnt [79243.819185] FS-Cache: Loaded [79243.856830] FS-Cache: Netfs 'nfs' registered for caching [79243.871002] Key type dns_resolver registered [79243.901417] NFS: Registering the id_resolver key type [79243.903629] Key type id_resolver registered [79243.905705] Key type id_legacy registered [root@client ~]# df /mnt Filesystem 1K-blocks Used Available Use% Mounted on 192.168.122.61:/vol1 1045504 33792 1011712 4% /mnt [root@client ~]# echo test \u0026gt; /mnt/test.txt [root@client ~]# cat /mnt/test.txt test [root@client ~]# サーバー側でファイルを見てみます\n[root@gluster1 tmp]# ls /gluster/vol1/brick1/brick/ test.txt [root@gluster1 tmp]# cat /gluster/vol1/brick1/brick/test.txt test [root@gluster1 tmp]# Replicated Volume なので gluster2 にもあります\n[root@gluster2 ~]# ls /gluster/vol1/brick1/brick/ test.txt [root@gluster2 ~]# cat /gluster/vol1/brick1/brick/test.txt test [root@gluster2 ~]# gluster1 を停止してみる # $ virsh destroy gluster1 ドメイン gluster1 は強制停止されました すぐに検知されて VIP が gluster2 に移動しました\n[root@gluster2 ~]# ip a s eth0 2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 52:54:00:98:a3:bc brd ff:ff:ff:ff:ff:ff inet 192.168.122.52/24 brd 192.168.122.255 scope global eth0 valid_lft forever preferred_lft forever inet 192.168.122.62/32 brd 192.168.122.255 scope global eth0 valid_lft forever preferred_lft forever inet 192.168.122.61/32 brd 192.168.122.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::5054:ff:fe98:a3bc/64 scope link valid_lft forever preferred_lft forever しかし、gluster1-nfs_unblock が Stopped となっており、しばらくは gluster1 についていた VIP の NFS ポート宛のパケットは iptables でブロックされます\n[root@gluster2 ~]# pcs status Cluster name: ganesha Stack: corosync Current DC: gluster2 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum Last updated: Mon Jun 5 22:45:20 2017\tLast change: Sun Jun 4 19:47:49 2017 by root via crm_attribute on gluster2 2 nodes and 12 resources configured Online: [ gluster2 ] OFFLINE: [ gluster1 ] Full list of resources: Clone Set: nfs_setup-clone [nfs_setup] Started: [ gluster2 ] Stopped: [ gluster1 ] Clone Set: nfs-mon-clone [nfs-mon] Started: [ gluster2 ] Stopped: [ gluster1 ] Clone Set: nfs-grace-clone [nfs-grace] Started: [ gluster2 ] Stopped: [ gluster1 ] Resource Group: gluster1-group gluster1-nfs_block\t(ocf:💓portblock):\tStarted gluster2 gluster1-cluster_ip-1\t(ocf:💓IPaddr):\tStarted gluster2 gluster1-nfs_unblock\t(ocf:💓portblock):\tStopped Resource Group: gluster2-group gluster2-nfs_block\t(ocf:💓portblock):\tStarted gluster2 gluster2-cluster_ip-1\t(ocf:💓IPaddr):\tStarted gluster2 gluster2-nfs_unblock\t(ocf:💓portblock):\tStarted gluster2 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled 1分ほどでブロックは解除されて NFS アクセスが復活します\n[root@gluster2 ~]# pcs status Cluster name: ganesha Stack: corosync Current DC: gluster2 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum Last updated: Mon Jun 5 22:46:30 2017\tLast change: Sun Jun 4 19:47:49 2017 by root via crm_attribute on gluster2 2 nodes and 12 resources configured Online: [ gluster2 ] OFFLINE: [ gluster1 ] Full list of resources: Clone Set: nfs_setup-clone [nfs_setup] Started: [ gluster2 ] Stopped: [ gluster1 ] Clone Set: nfs-mon-clone [nfs-mon] Started: [ gluster2 ] Stopped: [ gluster1 ] Clone Set: nfs-grace-clone [nfs-grace] Started: [ gluster2 ] Stopped: [ gluster1 ] Resource Group: gluster1-group gluster1-nfs_block\t(ocf:💓portblock):\tStarted gluster2 gluster1-cluster_ip-1\t(ocf:💓IPaddr):\tStarted gluster2 gluster1-nfs_unblock\t(ocf:💓portblock):\tStarted gluster2 Resource Group: gluster2-group gluster2-nfs_block\t(ocf:💓portblock):\tStarted gluster2 gluster2-cluster_ip-1\t(ocf:💓IPaddr):\tStarted gluster2 gluster2-nfs_unblock\t(ocf:💓portblock):\tStarted gluster2 Daemon Status: corosync: active/disabled pacemaker: active/disabled pcsd: active/enabled gluster1 を再起動させると元に戻ります\n[root@gluster1 ~]# pcs status Cluster name: ganesha Stack: corosync Current DC: gluster2 (version 1.1.15-11.el7_3.4-e174ec8) - partition with quorum Last updated: Mon Jun 5 23:04:58 2017\tLast change: Sun Jun 4 19:47:49 2017 by root via crm_attribute on gluster2 2 nodes and 12 resources configured Online: [ gluster1 gluster2 ] Full list of resources: Clone Set: nfs_setup-clone [nfs_setup] Started: [ gluster1 gluster2 ] Clone Set: nfs-mon-clone [nfs-mon] Started: [ gluster1 gluster2 ] Clone Set: nfs-grace-clone [nfs-grace] Started: [ gluster1 gluster2 ] Resource Group: gluster1-group gluster1-nfs_block\t(ocf:💓portblock):\tStarted gluster1 gluster1-cluster_ip-1\t(ocf:💓IPaddr):\tStarted gluster1 gluster1-nfs_unblock\t(ocf:💓portblock):\tStarted gluster1 Resource Group: gluster2-group gluster2-nfs_block\t(ocf:💓portblock):\tStarted gluster2 gluster2-cluster_ip-1\t(ocf:💓IPaddr):\tStarted gluster2 gluster2-nfs_unblock\t(ocf:💓portblock):\tStarted gluster2 Daemon Status: corosync: active/enabled pacemaker: active/enabled pcsd: active/enabled 停止中に作成したファイルも同期されています\n[root@gluster1 ~]# ls /gluster/vol1/brick1/brick/ test2.txt test.txt [root@gluster1 ~]# cat /gluster/vol1/brick1/brick/test2.txt test2 [root@gluster1 ~]# 次はアクセスコントロールを。\n","date":"2017年6月5日","permalink":"/2017/06/building-ha-nfs-server-part4/","section":"Posts","summary":"パート1 で GlusterFS Volume をセットアップしたところから始めようと思ったが DigitalOcean でも Vagrant (VirtualBox) でもうまくいかないので KVM で試してみた（Network Interface が複数あるとうま","title":"GlusterFS + NFS-Ganesha で HA な NFS サーバーを構築する (4)"},{"content":"","date":"2017年6月5日","permalink":"/tags/nfs/","section":"Tags","summary":"","title":"NFS"},{"content":"boxcutter の CentOS 7 で確認 vboxadd.service vboxadd-service.service という service が入っている。 vboxadd.service では kernel module を build するために gcc, make, kernel-devl パッケージのインストールが必要``` $ sudo mount -t vboxsf -o uid=1000,gid=1000 vagrant /vagrant\n","date":"2017年5月25日","permalink":"/2017/05/centos-7-%E3%81%AE-virtualbox-module/","section":"Posts","summary":"boxcutter の CentOS 7 で確認 vboxadd.service vboxadd-service.service という service が入っている。 vboxadd.service では kernel module を build するために gcc, make, kernel-devl パッケージのインストールが必要``` $ sudo mount -t vboxsf -o uid=1000,gid=1000 vagrant /vagrant","title":"CentOS 7 の Virtualbox module"},{"content":"doctl 1.6.1 で shell completion 機能が追加されていました Implementing Bash completion #206``` $ doctl completion completion is used to output completion code for bash and zsh shells.\nBefore using completion features, you have to source completion code from your .profile or .bashrc/.zshrc file. This is done by adding following line to one of above files: source \u0026lt;(doctl completion SHELL)\nBash users can as well save it to the file and copy it to: /etc/bash_completion.d/\nCorrect arguments for SHELL are: \u0026ldquo;bash\u0026rdquo; and \u0026ldquo;zsh\u0026rdquo;.\nNotes:\n  zsh completions requires zsh 5.2 or newer.\n  macOS users have to install bash-completion framework to utilize completion features. This can be done using homebrew: brew install bash-completion\n  Once installed, you must load bash_completion by adding following line to your .profile or .bashrc/.zshrc: source $(brew \u0026ndash;prefix)/etc/bash_completion\nUsage: doctl completion [command]\nAvailable Commands: bash generate bash completion code zsh generate zsh completion code\nFlags: -h, \u0026ndash;help help for completion\nGlobal Flags: -t, \u0026ndash;access-token string API V2 Access Token -c, \u0026ndash;config string config file (default is $HOME/.config/doctl/config.yaml) -o, \u0026ndash;output string output format [text|json] (default \u0026ldquo;text\u0026rdquo;) \u0026ndash;trace trace api access -v, \u0026ndash;verbose verbose output\nUse \u0026ldquo;doctl completion [command] \u0026ndash;help\u0026rdquo; for more information about a command.\n次のようにすればそのセッションですぐに補完が機能するようになります $ source \u0026lt;(doctl completion bash)\nファイルに書き出したりして bash 起動時やログイン時に自動で読み込まれるようにすることもできますね。方法は環境依存なので(ry ちなみに Kubernetes の kubectl にも同様の機能があります。doctl はここからアイデアが来てるっぽい。 $ kubectl completion -h Output shell completion code for the specified shell (bash or zsh). The shell code must be evalutated to provide interactive completion of kubectl commands. This can be done by sourcing it from the .bash _profile.\nNote: this requires the bash-completion framework, which is not installed by default on Mac. This can be installed by using homebrew:\n$ brew install bash-completion\nOnce installed, bash completion must be evaluated. This can be done by adding the following line to the .bash profile\n$ source $(brew \u0026ndash;prefix)/etc/bash_completion\nNote for zsh users: [1] zsh completions are only supported in versions of zsh \u0026gt;= 5.2\nExamples:\nInstall bash completion on a Mac using homebrew # brew install bash-completion printf \u0026ldquo;\\n# Bash completion support\\nsource $(brew \u0026ndash;prefix)/etc/bash_completion\\n\u0026rdquo; \u0026raquo; $HOME/.bash_profile source $HOME/.bash_profile\nLoad the kubectl completion code for bash into the current shell # source \u0026lt;(kubectl completion bash)\nWrite bash completion code to a file and source if from .bash_profile # kubectl completion bash \u0026gt; ~/.kube/completion.bash.inc printf \u0026ldquo;\\n# Kubectl shell completion\\nsource \u0026lsquo;$HOME/.kube/completion.bash.inc\u0026rsquo;\\n\u0026rdquo; \u0026raquo; $HOME/.bash_profile source $HOME/.bash_profile\nLoad the kubectl completion code for zsh[1] into the current shell # source \u0026lt;(kubectl completion zsh)\nUsage: kubectl completion SHELL [options]\nUse \u0026ldquo;kubectl options\u0026rdquo; for a list of global command-line options (applies to all commands).\n","date":"2017年5月21日","permalink":"/2017/05/doctl-shell-completion/","section":"Posts","summary":"doctl 1.6.1 で shell completion 機能が追加されていました Implementing Bash completion #206``` $ doctl completion completion is used to output completion code for bash and zsh shells. Before using completion features, you have to source completion code from your .profile or .bashrc/.zshrc file. This is done by adding following line to one of above files: source \u0026lt;(doctl completion SHELL) Bash users","title":"doctl の Bash / Zsh 補完"},{"content":"前回からの続き、GlusterFS シリーズです。\n今回は snapshot 機能を試します。これのために初回に Thin Provisioned Volume で構築しています。\n[root@client ~]# ls /vol1 01 03 05 07 09 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 02 04 06 08 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 上記状態の vol1 で snapshot を作成します。\n[root@gluster1 ~]# gluster snapshot create snaptest01 vol1 snapshot create: success: Snap snaptest01_GMT-2017.05.20-14.56.39 created successfully snapshot はデフォルトで名前に日時が追加されます、便利\n[root@gluster1 ~]# gluster snapshot list vol1 snaptest01_GMT-2017.05.20-14.56.39 [root@client ~]# rm -f /vol1/* [root@client ~]# ls /vol1/ [root@client ~]# for s in $(seq -w 41 60); do touch /vol1/$s; done [root@client ~]# ls /vol1 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 [root@gluster1 ~]# gluster snapshot create snaptest02 vol1 snapshot create: success: Snap snaptest02_GMT-2017.05.20-14.59.23 created successfully [root@gluster1 ~]# gluster snapshot list vol1 snaptest01_GMT-2017.05.20-14.56.39 snaptest02_GMT-2017.05.20-14.59.23 Thin Provisioning LVM の snapshot なのでパフォーマンス低下を気にせず沢山作成できます。 snapshot01 のファイルを参照したい場合は snapshot activate コマンドで有効化すればクライアントからマウントできるようになります。\n[root@gluster1 ~]# gluster snapshot info snaptest01_GMT-2017.05.20-14.56.39 Snapshot : snaptest01_GMT-2017.05.20-14.56.39 Snap UUID : 15e8d464-32a9-44c4-a516-101b093cb2c0 Created : 2017-05-20 14:56:39 Snap Volumes: Snap Volume Name : ec6da9f837a14187b869f33249024721 Origin Volume name : vol1 Snaps taken for vol1 : 2 Snaps available for vol1 : 254 Status : Stopped [root@gluster1 ~]# gluster snapshot activate snaptest01_GMT-2017.05.20-14.56.39 Snapshot activate: snaptest01_GMT-2017.05.20-14.56.39: Snap activated successfully [root@gluster1 ~]# gluster snapshot info snaptest01_GMT-2017.05.20-14.56.39 Snapshot : snaptest01_GMT-2017.05.20-14.56.39 Snap UUID : 15e8d464-32a9-44c4-a516-101b093cb2c0 Created : 2017-05-20 14:56:39 Snap Volumes: Snap Volume Name : ec6da9f837a14187b869f33249024721 Origin Volume name : vol1 Snaps taken for vol1 : 2 Snaps available for vol1 : 254 Status : Started Status が Started になりました、この状態でクライアントからマウントします\n[root@client ~]# mount -t glusterfs gluster1:/snaps/snaptest01_GMT-2017.05.20-14.56.39/vol1 /vol1-snap [root@client ~]# ls /vol1-snap 01 03 05 07 09 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 02 04 06 08 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 用が終わったら umount して deactivate します。\n[root@gluster1 ~]# gluster snapshot deactivate snaptest01_GMT-2017.05.20-14.56.39 Deactivating snap will make its data inaccessible. Do you want to continue? (y/n) y Snapshot deactivate: snaptest01_GMT-2017.05.20-14.56.39: Snap deactivated successfully volume の中身を特定の snapshot のものにごっそり入れ替えてしまいたい場合は snapshot restore コマンドで入れ替え可能です。restore するためにはまず volume を stop する必要があります。クライアントがマウントしたままの状態でも stop は可能です。もちろん stop 中は volume にアクセスできませんが。\n[root@gluster1 ~]# gluster volume stop vol1 Stopping volume will make its data inaccessible. Do you want to continue? (y/n) y volume stop: vol1: success [root@gluster1 ~]# gluster snapshot restore snaptest01_GMT-2017.05.20-14.56.39 Restore operation will replace the original volume with the snapshotted volume. Do you still want to continue? (y/n) y Snapshot restore: snaptest01_GMT-2017.05.20-14.56.39: Snap restored successfully [root@gluster1 ~]# gluster volume start vol1 volume start: vol1: success これで入れ替わりました。クライアント側で確認してみます。\n[root@client ~]# ls /vol1 01 03 05 07 09 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 02 04 06 08 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 restore するとその snapshot は消えますが、それより前の snapshot も後の snapshot も残っており使用可能です。 snapshot は保存可能な数に上限があります。snapshot config で確認できます。上限を超えたものの自動削除を有効にすることもできます。\n[root@gluster1 ~]# gluster snapshot config Snapshot System Configuration: snap-max-hard-limit : 256 snap-max-soft-limit : 90% auto-delete : disable activate-on-create : disable Snapshot Volume Configuration: Volume : vol1 snap-max-hard-limit : 256 Effective snap-max-hard-limit : 256 Effective snap-max-soft-limit : 230 (90%) Volume : vol2 snap-max-hard-limit : 256 Effective snap-max-hard-limit : 256 Effective snap-max-soft-limit : 230 (90%) 次はいよいよ NFS-Ganesha に進みます。\n","date":"2017年5月20日","permalink":"/2017/05/building-ha-nfs-server-part3/","section":"Posts","summary":"前回からの続き、GlusterFS シリーズです。 今回は snapshot 機能を試します。これのために初回に Thin Provisioned Volume で構築しています。 [root@client ~]# ls /vol1 01 03 05 07 09 11 13 15 17","title":"GlusterFS + NFS-Ganesha で HA な NFS サーバーを構築する (3)"},{"content":"前回 GlusterFS の volume を作成してクライアントとなる Linux からマウントするところまでをやってみました。今回は一部のサーバーが停止してしまったらどうなるのかを試してみます。\n[root@client ~]# df -h /vol1 /vol2 Filesystem Size Used Avail Use% Mounted on gluster1:/vol1 1021M 33M 988M 4% /vol1 gluster1:/vol2 2.0G 66M 2.0G 4% /vol2 /vol1 は Replicated Volume (1G + 1G = 1G) で /vol2 は Distributed Volume (1G + 1G = 2G) です。 置いたファイルがサーバー側でどう見えるかを確認してみます\n[root@client ~]# for s in $(seq -w 1 20); do touch /vol1/$s /vol2/$s; done [root@client ~]# ls /vol1 /vol2 /vol1: 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 /vol2: 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 [root@client ~]# [root@gluster1 ~]# ls /gluster/vol1/brick1/brick/ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 [root@gluster1 ~]# ls /gluster/vol2/brick1/brick/ 03 04 05 11 13 18 19 20 [root@gluster1 ~]# [root@gluster2 ~]# ls /gluster/vol1/brick1/brick/ 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 [root@gluster2 ~]# ls /gluster/vol2/brick1/brick/ 01 02 06 07 08 09 10 12 14 15 16 17 [root@gluster2 ~]# こんな感じで vol1 は gluster1,2 の両方に全部が、vol2 はファイルごとに gluster1,2 のどちらかにだけ存在します。 これで、gluster1 を down させると vol1 はそれでも全てのファイルにアクセス可能で、vo2 は gluster2 にあるファイルだけにアクセスできるというのが期待される動作です。試してみます。 gluster1 停止後は次のようになりました。\n[root@client ~]# ls /vol1 /vol2 /vol1: 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 /vol2: 01 02 06 07 08 09 10 12 14 15 16 17 [root@client ~]# 停止時間がどうだったかというと53秒ほど ls の結果が待たされました。\n2017-05-20 13:03:23.715827382 2017-05-20 13:04:17.859390739 クライアント側の syslog には次のログが出ていました。\nMay 20 13:04:17 client vol1[9362]: [2017-05-20 13:04:17.335765] C [rpc-clnt-ping.c:160:rpc_clnt_ping_timer_expired] 0-vol1-client-0: server 10.130.49.27:49152 has not responded in the last 42 seconds, disconnecting. 42秒というのは GlusterFS の volume 毎に設定する network.ping-timeout の値だということなのでこれを短くすることで固まる時間を短くできるのではないかということで試してみます。\n$ sudo gluster volume get vol1 network.ping-timeout Option Value ------ ----- network.ping-timeout 42 $ sudo gluster volume set vol1 network.ping-timeout 5 volume set: success $ sudo gluster volume get vol1 network.ping-timeout Option Value ------ ----- network.ping-timeout 5 ls /vol1 だけにして試してみたところ停止時間は8秒程度になりました。 次に gluster1 が停止している状態で vol1, vol2 それぞれにファイルを追加するとどうなるかを確認します。 gluster1 は停止しているので Disconnected 状態です。\n[root@gluster2 ~]# gluster peer status Number of Peers: 1 Hostname: gluster1 Uuid: 6d9fa83a-dd3b-4a36-956e-4a069e74745e State: Peer in Cluster (Disconnected) vol1, vol2 それぞれに 21 から 40 というファイルを置いてみます。\n[root@client ~]# for s in $(seq -w 21 40); do touch /vol1/$s /vol2/$s; done touch: cannot touch ‘/vol2/23’: Transport endpoint is not connected touch: cannot touch ‘/vol2/24’: Transport endpoint is not connected touch: cannot touch ‘/vol2/26’: Transport endpoint is not connected touch: cannot touch ‘/vol2/27’: Transport endpoint is not connected touch: cannot touch ‘/vol2/28’: Transport endpoint is not connected touch: cannot touch ‘/vol2/32’: Transport endpoint is not connected touch: cannot touch ‘/vol2/33’: Transport endpoint is not connected touch: cannot touch ‘/vol2/35’: Transport endpoint is not connected touch: cannot touch ‘/vol2/38’: Transport endpoint is not connected [root@client ~]# ls /vol1 /vol2 /vol1: 01 03 05 07 09 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 02 04 06 08 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 /vol2: 01 06 08 10 14 16 21 25 30 34 37 40 02 07 09 12 15 17 22 29 31 36 39 Distributed Volume な vol2 は計算によって gluster1 に置かれるべきファイルはサーバーが存在しないということでエラーになりました。 もう gluster1 は起動してこないので諦めようという場合は当該 brick を切り離すことで書き込めるようになります。 volume status では見えない brick がどれだったかわからないので\n[root@gluster2 ~]# gluster volume status vol2 Status of volume: vol2 Gluster process TCP Port RDMA Port Online Pid ------------------------------------------------------------------------------ Brick gluster2:/gluster/vol2/brick1/brick 49153 0 Y 28864 Task Status of Volume vol2 ------------------------------------------------------------------------------ There are no active volume tasks volume info で確認します\n[root@gluster2 ~]# gluster volume info vol2 Volume Name: vol2 Type: Distribute Volume ID: 35be21b5-c624-4cdb-a20f-96cdb6efefbd Status: Started Snapshot Count: 0 Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: gluster1:/gluster/vol2/brick1/brick Brick2: gluster2:/gluster/vol2/brick1/brick Options Reconfigured: network.ping-timeout: 5 transport.address-family: inet nfs.disable: on volume remove-brick で brick を削除します。アクセスできないので force を指定してますし、データロスするよと警告が出ていますが、アクセスできる状態では start, commit でデータを他の brick に移して安全に削除することができます。\n[root@gluster2 ~]# gluster volume remove-brick vol2 gluster1:/gluster/vol2/brick1/brick force Removing brick(s) can result in data loss. Do you want to Continue? (y/n) y volume remove-brick commit force: success 削除できたのでどんなファイルでも書き込めるようになりました。\n[root@client ~]# for s in $(seq -w 1 40); do touch /vol2/$s; done [root@client ~]# ls /vol2 01 03 05 07 09 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 02 04 06 08 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 gluster1 を起動させると Replicated Volume の vol1 brick には停止中に書き込んだファイルが自動で同期されています。\n[root@gluster1 ~]# ls /gluster/vol1/brick1/brick/ 01 03 05 07 09 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 02 04 06 08 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 gluster1 の /gluster/vol2/brick1/brick を vol2 に再度追加してみます\n[root@gluster1 ~]# gluster volume add-brick vol2 gluster1:/gluster/vol2/brick1/brick volume add-brick: failed: /gluster/vol2/brick1/brick is already part of a volume 前のデータが残っているのでそのままでは追加できないため、消してから再登録します。\n[root@gluster1 ~]# rm -fr /gluster/vol2/brick1/brick [root@gluster1 ~]# gluster volume add-brick vol2 gluster1:/gluster/vol2/brick1/brick volume add-brick: success 追加されましたが、自動で再配置されるわけではありません。volume rebalance コマンドで再配置させられます。\n[root@gluster1 ~]# gluster volume rebalance vol2 start volume rebalance: vol2: success: Rebalance on vol2 has been started successfully. Use rebalance status command to check status of the rebalance process. ID: 4f1bca89-8929-4b84-8e82-cbdf3528a640 volume rebalance の status コマンドでリバランス処理の進み具合を確認できます。今回はファイルが少ししかないので一瞬で終わってます。\n[root@gluster1 ~]# gluster volume rebalance vol2 status Node Rebalanced-files size scanned failures skipped status run time in hⓜ️s --------- ----------- ----------- ----------- ----------- ----------- ------------ -------------- localhost 0 0Bytes 0 0 0 completed 0:00:00 gluster2 17 0Bytes 40 0 0 completed 0:00:00 volume rebalance: vol2: success brick のディレクトリを確認するとファイルが移動されています。\n[root@gluster1 ~]# ls /gluster/vol2/brick1/brick/ 03 04 05 11 13 18 19 20 23 24 26 27 28 32 33 35 38 [root@gluster2 ~]# ls /gluster/vol2/brick1/brick/ 01 06 08 10 14 16 21 25 30 34 37 40 02 07 09 12 15 17 22 29 31 36 39 snapshot はまた次回\n","date":"2017年5月20日","permalink":"/2017/05/building-ha-nfs-server-part2/","section":"Posts","summary":"前回 GlusterFS の volume を作成してクライアントとなる Linux からマウントするところまでをやってみました。今回は一部のサーバーが停止してしまったらどうなるのかを試","title":"GlusterFS + NFS-Ganesha で HA な NFS サーバーを構築する (2)"},{"content":"GlusterFS と NFS-Ganesha で High-Availability NFS サーバーを構築してみます。 GlusterFS をそのままマウントさせられるクライアントばかりであればわざわざ NFS サーバーにする必要はないです。 OS は CentOS Linux release 7.3.1611 (Core) です。 gluster1, gluster2 という2台で試します。 DigitalOcean で試すので名前解決は hosts に書くことにします。\npackage のインストール # The CentOS Storage Special Interest Group にて package が提供されていますので yum で簡単にインストールできます。現在（2017年5月）提供されている最新のバージョンは 3.10 です。これをインストールします。 HowTos/GlusterFSonCentOS - CentOS Wiki もありますし、Red Hat Gluster Storage の INSTALLING RED HAT GLUSTER STORAGE 3.1 も参考になります。\n$ sudo yum -y install centos-release-gluster310 これで 3.10 の yum repository が登録されます。\n$ sudo yum -y install glusterfs glusterfs-server Firewall 設定 # glusterfs-server パッケージに /usr/lib/firewalld/services/glusterfs.xml が含まれるので firewall-cmd で glusterfs サービスを使います。\n$ sudo systemctl start firewalld $ sudo systemctl enable firewalld $ sudo firewall-cmd --add-rich-rule=\u0026quot;rule family=ipv4 source address=10.130.0.0/16 service name=glusterfs accept\u0026quot; --permanent $ sudo firewall-cmd --add-rich-rule=\u0026quot;rule family=ipv4 source address=10.130.0.0/16 service name=glusterfs accept\u0026quot; $ sudo firewall-cmd --list-all public target: default icmp-block-inversion: no interfaces: sources: services: dhcpv6-client ssh ports: protocols: masquerade: no forward-ports: sourceports: icmp-blocks: rich rules: rule family=\u0026quot;ipv4\u0026quot; source address=\u0026quot;10.130.0.0/16\u0026quot; service name=\u0026quot;glusterfs\u0026quot; accept 10.130.0.0/16 は DigitalOcean シンガポールリージョンの Provate Network アドレスですが他の契約者のサーバーと共用なので注意が必要です。実際には信頼できるIPアドレスだけに公開しましょう。\nクラスタリング # まずは全てのサーバーで glusterd を起動します\n$ sudo systemctl start glusterd $ sudo systemctl enable glusterd いずれか1台のサーバーから他のサーバーを peer に加えます\n$ sudo gluster peer probe gluster2 $ sudo gluster peer status Number of Peers: 1 Hostname: gluster2 Uuid: 0699a6ac-1352-4996-a7e8-53d30531b2ee State: Peer in Cluster (Connected) Volume の作成 # GlusterFS の snapshot 機能を使うためにシンプロビジョニングされた LVM の Logical Volume を使います。 ここでは /dev/sda をデータボリュームように使うことにします（DigitalOcean の Block Storage サービスを使うとこの path になったので）。\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 5G 0 disk vda 253:0 0 30G 0 disk ├─vda1 253:1 0 30G 0 part / └─vda15 253:15 0 1M 0 part Volume Group は通常通りに作成します\n$ sudo pvcreate /dev/sda Physical volume \u0026quot;/dev/sda\u0026quot; successfully created. $ sudo vgcreate data /dev/sda Volume group \u0026quot;data\u0026quot; successfully created Logial Volume として pool を作成します\n$ sudo lvcreate --thin -l 100%FREE data/thinpool Using default stripesize 64.00 KiB. Logical volume \u0026quot;thinpool\u0026quot; created. $ sudo lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert thinpool data twi-a-tz-- 4.98g 0.00 0.63 pool から volume を作成します vol1, vol2 をそれぞれ 1GB で作成しました。5GB を割り当てた pool から 1GB ずつなので普通に見えますね。でも実際にはまだ1GBは割り当てられていなくて必要になった分だけそときに割り当てられます。\n$ sudo lvcreate --thin --virtualsize 1G -n vol1 data/thinpool Using default stripesize 64.00 KiB. Logical volume \u0026quot;vol1\u0026quot; created. $ sudo lvcreate -T -V 1G -n vol2 data/thinpool Using default stripesize 64.00 KiB. Logical volume \u0026quot;vol2\u0026quot; created. では 10GB の volume も作ってみます。\n$ sudo lvcreate -T -V 10G -n vol3 data/thinpool Using default stripesize 64.00 KiB. WARNING: Sum of all thin volume sizes (12.00 GiB) exceeds the size of thin pool data/thinpool and the size of whole volume group (5.00 GiB)! For thin pool auto extension activation/thin_pool_autoextend_threshold should be below 100. Logical volume \u0026quot;vol3\u0026quot; created. 5GB の Volume Group 内に 10GB の Logical Volume が作成できました。これが Thin Provisioning というやつですね。必要になったときに Volume Group にディスクを追加するなどして対応できます。\n$ sudo lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert thinpool data twi-aotz-- 4.98g 0.00 0.78 vol1 data Vwi-a-tz-- 1.00g thinpool 0.00 vol2 data Vwi-a-tz-- 1.00g thinpool 0.00 vol3 data Vwi-a-tz-- 10.00g thinpool 0.00 こんな風に _tmeta と _tdata というのが別に見えます。\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 5G 0 disk ├─data-thinpool_tmeta 252:0 0 8M 0 lvm │ └─data-thinpool-tpool 252:2 0 5G 0 lvm │ ├─data-thinpool 252:3 0 5G 0 lvm │ ├─data-vol1 252:4 0 1G 0 lvm │ ├─data-vol2 252:5 0 1G 0 lvm │ └─data-vol3 252:6 0 10G 0 lvm └─data-thinpool_tdata 252:1 0 5G 0 lvm └─data-thinpool-tpool 252:2 0 5G 0 lvm ├─data-thinpool 252:3 0 5G 0 lvm ├─data-vol1 252:4 0 1G 0 lvm ├─data-vol2 252:5 0 1G 0 lvm └─data-vol3 252:6 0 10G 0 lvm vda 253:0 0 30G 0 disk ├─vda1 253:1 0 30G 0 part / └─vda15 253:15 0 1M 0 part それでは volume をフォーマットします。GLusterFS では XFS が推奨されているようです。inode の拡張属性として meta データを保持するようです。そのため inode サイズを 512 にするようにとなっています（CentOS 7 では default size が 512 なのであえて指定しなくても大丈夫ですが）。\n$ sudo mkfs.xfs -i size=512 /dev/data/vol1 $ sudo mkfs.xfs -i size=512 /dev/data/vol2 マウントします。\n$ sudo mkdir -p /gluster/vol1/brick1 $ sudo mkdir -p /gluster/vol2/brick1 $ echo \u0026quot;/dev/data/vol1 /gluster/vol1/brick1 xfs rw,noatime,nouuid 0 0\u0026quot; | sudo tee -a /etc/fstab $ echo \u0026quot;/dev/data/vol2 /gluster/vol2/brick1 xfs rw,noatime,nouuid 0 0\u0026quot; | sudo tee -a /etc/fstab $ sudo mount -a GlusterFS Volume の作成 # 次に GlusterFS のボリュームを作成します。GlusterFS では brick という実態はサーバーの1ディレクトリの組み合わせで volume を作成します。同じデータを複数の brick に保存する Replicated Volume や、ファイルによって保存する brick を振り分ける Distributed Volume、この2つを組み合わせた Distributed Replicated Volume あたりが基本的な Volume でしょうか。 他にもファイルを指定のサイズで分割して複数の brick に保存する方法や RAID5, RAID6 的な Dispersed Volume というものもあります。 Setting Up Volumes - Gluster Docs に各 Volume タイプの説明があります Volume の作成コマンドはどれか1台のサーバーで実行します Replicated Volume ミラーボリュームですね、3台以上でさらに冗長性を上げることもできます。\n$ sudo gluster volume create vol1 \\ replica 2 \\ transport tcp \\ gluster1:/gluster/vol1/brick1/brick \\ gluster2:/gluster/vol1/brick1/brick volume create: vol1: success: please start the volume to access data $ sudo gluster volume info vol1 Volume Name: vol1 Type: Replicate Volume ID: 0086e6e4-9d7c-40ec-badf-33f88b58f472 Status: Created Snapshot Count: 0 Number of Bricks: 1 x 2 = 2 Transport-type: tcp Bricks: Brick1: gluster1:/gluster/vol1/brick1/brick Brick2: gluster2:/gluster/vol1/brick1/brick Options Reconfigured: transport.address-family: inet nfs.disable: on Distributed Volume ファイル単位の RAID0 的なボリュームです。ファイルによってどこかの brick に保存されます。\n$ sudo gluster volume create vol2 \\ transport tcp \\ gluster1:/gluster/vol2/brick1/brick \\ gluster2:/gluster/vol2/brick1/brick volume create: vol2: success: please start the volume to access data $ sudo gluster volume info vol2 Volume Name: vol2 Type: Distribute Volume ID: 35be21b5-c624-4cdb-a20f-96cdb6efefbd Status: Created Snapshot Count: 0 Number of Bricks: 2 Transport-type: tcp Bricks: Brick1: gluster1:/gluster/vol2/brick1/brick Brick2: gluster2:/gluster/vol2/brick1/brick Options Reconfigured: transport.address-family: inet nfs.disable: on クライアントからマウントできるようにするためには start させる必要があります\n$ sudo gluster volume status vol1 Volume vol1 is not started $ sudo gluster volume status vol2 Volume vol2 is not started $ sudo gluster volume start vol1 volume start: vol1: success $ sudo gluster volume start vol2 volume start: vol2: success gluster volume status {volume_name} で状態が確認できます。Pid や TCP Port があることからわかるように brick ごとにプロセスがいますし、それぞれが別の TCP Port を Listen します。クライアントは volume を構成するそれぞれの brick の port に直接アクセスします。\n$ sudo gluster volume status vol1 Status of volume: vol1 Gluster process TCP Port RDMA Port Online Pid ------------------------------------------------------------------------------ Brick gluster1:/gluster/vol1/brick1/brick 49152 0 Y 30857 Brick gluster2:/gluster/vol1/brick1/brick 49152 0 Y 11657 Self-heal Daemon on localhost N/A N/A Y 30877 Self-heal Daemon on gluster2 N/A N/A Y 16882 Task Status of Volume vol1 ------------------------------------------------------------------------------ There are no active volume tasks $ sudo gluster volume status vol2 Status of volume: vol2 Gluster process TCP Port RDMA Port Online Pid # Brick gluster1:/gluster/vol2/brick1/brick 49153 0 Y 30908 Brick gluster2:/gluster/vol2/brick1/brick 49153 0 Y 28864\nTask Status of Volume vol2 # There are no active volume tasks\n ### クライアントからマウントする Linux クライアントからは FUSE でマウントします。 $ sudo yum -y install centos-release-gluster310\n $ sudo yum -y install glusterfs-fuse\n $ sudo mkdir /vol1 $ sudo mount -t glusterfs gluster1:/vol1 /vol1 $ mount | grep /vol1 gluster1:/vol1 on /vol1 type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)\n `/etc/fstab` には次のように `backup-volfile-servers` オプションで別サーバーも指定しておけば mount 時に gluster1 が down していてもマウントが可能となります gluster1:/vol1 /vol1 glusterfs backup-volfile-servers=gluster2 0 0\n [次回](/2017/05/building-ha-nfs-server-part2/) 一部のサーバーが down した時の動作を確認します。NFS-Ganesha はまだまだ先だな。 ### doctl 今回の環境は DigitalOcean で作ってます。doctl コマンドで volume 付きのサーバーを立てるのはこんな感じ $ doctl compute volume create gluster-data1 \u0026ndash;region sgp1 \u0026ndash;size 5GiB \u0026ndash;desc \u0026ldquo;GlusterFS Data Volume 1\u0026rdquo; ID Name Size Region Droplet IDs a60e32be-3e20-11e7-892a-0242ac113804 gluster-data1 5 GiB sgp1\n$ doctl compute volume create gluster-data2 \u0026ndash;region sgp1 \u0026ndash;size 5GiB \u0026ndash;desc \u0026ldquo;GlusterFS Data Volume 2\u0026rdquo; ID Name Size Region Droplet IDs ac9ead25-3e20-11e7-97d4-0242ac111505 gluster-data2 5 GiB sgp1\n$ doctl compute droplet create gluster1 \u0026ndash;image centos-7-x64 \u0026ndash;region sgp1 \u0026ndash;size 1gb \u0026ndash;volumes a60e32be-3e20-11e7-892a-0242ac113804 \u0026ndash;enable-private-networking \u0026ndash;enable-monitoring \u0026ndash;ssh-keys 76364 ID Name Public IPv4 Private IPv4 Public IPv6 Memory VCPUs Disk Region Image Status Tags 49480401 gluster1 1024 1 30 sgp1 CentOS 7.3.1611 x64 new\n$ doctl compute droplet create gluster2 \u0026ndash;image centos-7-x64 \u0026ndash;region sgp1 \u0026ndash;size 1gb \u0026ndash;volumes ac9ead25-3e20-11e7-97d4-0242ac111505 \u0026ndash;enable-private-networking \u0026ndash;enable-monitoring \u0026ndash;ssh-keys 76364 ID Name Public IPv4 Private IPv4 Public IPv6 Memory VCPUs Disk Region Image Status Tags 49480523 gluster2 1024 1 30 sgp1 CentOS 7.3.1611 x64 new\n$ doctl compute droplet create client \u0026ndash;image centos-7-x64 \u0026ndash;region sgp1 \u0026ndash;size 1gb \u0026ndash;enable-private-networking \u0026ndash;enable-monitoring \u0026ndash;ssh-keys 76364 ID Name Public IPv4 Private IPv4 Public IPv6 Memory VCPUs Disk Region Image Status Tags 49480546 client 1024 1 30 sgp1 CentOS 7.3.1611 x64 new\n","date":"2017年5月20日","permalink":"/2017/05/building-ha-nfs-server-part1/","section":"Posts","summary":"GlusterFS と NFS-Ganesha で High-Availability NFS サーバーを構築してみます。 GlusterFS をそのままマウントさせられるクライアントばかりであればわざわざ NFS サーバーにする必要はないです。 OS は CentOS","title":"GlusterFS + NFS-Ganesha で HA な NFS サーバーを構築する (1)"},{"content":"CD Boot # virt-install で ISO ファイルから仮想ゲストを作成＆起動します\n$ virt-install \\ --name rancher \\ --vcpus 1 \\ --cpu host \\ --memory 2048,maxmemory=4096 \\ --os-variant virtio26 \\ --cdrom rancheros.iso \\ --network network=default,model=virtio \\ --disk pool=ytera,size=10,format=qcow2,bus=virtio \\ --graphics none storage pool 使ったことなかったので使ってみた。\n$ virsh pool-list Name State Autostart ------------------------------------------- default active yes Downloads active yes tmp active yes ytera active yes $ virsh net-list Name State Autostart Persistent ---------------------------------------------------------- default active yes yes インストール # ros install コマンドでディスクにインストールします\n$ sudo ros help install NAME: ros install - install RancherOS to disk USAGE: ros install [command options] [arguments...] OPTIONS: --image value, -i value install from a certain image (e.g., 'rancher/os:v0.7.0') use 'ros os list' to see what versions are available. --install-type value, -t value generic: (Default) Creates 1 ext4 partition and installs RancherOS (syslinux) amazon-ebs: Installs RancherOS and sets up PV-GRUB gptsyslinux: partition and format disk (gpt), then install RancherOS and setup Syslinux --cloud-config value, -c value cloud-config yml file - needed for SSH authorized keys --device value, -d value storage device --partition value, -p value partition to install to --statedir value install to rancher.state.directory --force, -f [ DANGEROUS! Data loss can happen ] partition/format without prompting --no-reboot do not reboot after install --append value, -a value append additional kernel parameters --kexec, -k reboot using kexec --debug Run installer with debug output SSH でログインするために公開鍵を cloud-config で設定する必要があります。最小でこの程度。\n#cloud-config ssh_authorized_keys: - ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBE3YdsjZWcGkO8g0gCypD1ampRaZunb8VHJD0zzqjc8NxW+H192uOxwXSBJZQWAVY3yH5VIjKhnTdp83/swhajc= hostname: rancher cloud-config ファイルは事前にチェックできます\n$ sudo ros config validate -i cloud-config.yml YAML フォーマットの正しさだじゃなくて typo などの不明な設定も見つけてくれます\n$ sudo ros config validate -i cloud-config.yml \u0026gt; ERRO[0000] hogehoge: Additional property hogehoge is not allowed cloud-config で設定可能なリストは ドキュメント で。ネットワーク設定 で固定IPにしたり bonding や Tag VLAN 設定もできます。 インストールするバージョンを -i rancher/os:v1.0.1 などと指定できます。指定可能なリストは ros os list で確認できます。\n$ sudo ros os list rancher/os:v1.0.1 remote latest running rancher/os:v1.0.0 remote available rancher/os:v0.9.2 remote available rancher/os:v0.9.1 remote available rancher/os:v0.9.0 remote available (snip) インストールの実行。なぜか Continue [y/N]: へのキーボードからの入力が効かなかったので yes コマンドで渡して逃げました。\n$ yes | sudo ros install -d /dev/vda -c cloud-config.yml -a \u0026quot;console=tty0 console=ttyS0,115200n8\u0026quot; \u0026gt; INFO[0000] No install type specified...defaulting to generic Installing from rancher/os:v1.0.1 Continue [y/N]: \u0026gt; INFO[0000] start !isoinstallerloaded \u0026gt; INFO[0000] trying to load /bootiso/rancheros/installer.tar.gz 23b9c7b43573: Loading layer 4.23 MB/4.23 MB f90562450ed7: Loading layer 14.96 MB/14.96 MB 94dbf06f5a27: Loading layer 4.608 kB/4.608 kB 52125c070e5e: Loading layer 18.08 MB/18.08 MB 051a95b6eaa9: Loading layer 1.636 MB/1.636 MB a6fd95b21434: Loading layer 1.536 kB/1.536 kB c52e27553689: Loading layer 2.56 kB/2.56 kB 7b425eeedf72: Loading layer 3.072 kB/3.072 kB \u0026gt; INFO[0003] Loaded images from /bootiso/rancheros/installer.tar.gz \u0026gt; INFO[0003] starting installer container for rancher/os-installer:latest (new) Installing from rancher/os-installer:latest mount: /dev/sr0 is write-protected, mounting read-only Continue with reboot [y/N]: \u0026gt; INFO[0005] Rebooting 起動しました。 😊\n\u0026gt; INFO[0009] [15/16] [docker]: Started \u0026gt; INFO[0010] [16/16] [preload-user-images]: Started \u0026gt; INFO[0010] Project [os]: Project started \u0026gt; INFO[0011] RancherOS v1.0.1 started , , ______ _ _____ _____TM ,------------|'------'| | ___ \\ | | / _ / ___| / . '-' |- | |_/ /__ _ _ __ ___| |__ ___ _ __ | | | \\ '--. \\/| | | | // _' | '_ \\ / __| '_ \\ / _ \\ '__' | | | |'--. \\ | .________.'----' | |\\ \\ (_| | | | | (__| | | | __/ | | \\_/ /\\__/ / | | | | \\_| \\_\\__,_|_| |_|\\___|_| |_|\\___|_| \\___/\\____/ \\___/ \\___/ Linux 4.9.24-rancher RancherOS v1.0.1 rancher ttyS0 docker-sys: 172.18.42.2 eth0: 192.168.122.129 lo: 127.0.0.1 rancher login: ssh rancher@192.168.122.129 でログインできます。 インストール先のディスクが AWS EBS や GPT の場合は ros install に -t で type の指定が必要。\n仮想サーバーの削除 # お掃除\n$ virsh destroy rancher $ virsh undefine rancher --remove-all-storage Domain rancher has been undefined Volume 'vda'(/home/ytera/rancher.qcow2) removed. ","date":"2017年5月13日","permalink":"/2017/05/installing-rancheros-on-kvm/","section":"Posts","summary":"CD Boot # virt-install で ISO ファイルから仮想ゲストを作成＆起動します $ virt-install \\ --name rancher \\ --vcpus 1 \\ --cpu host \\ --memory 2048,maxmemory=4096 \\ --os-variant virtio26 \\ --cdrom rancheros.iso \\ --network network=default,model=virtio \\ --disk pool=ytera,size=10,format=qcow2,bus=virtio \\ --graphics none storage pool 使ったことなかったので使っ","title":"KVMゲストとしてRancherOSをインストール"},{"content":"","date":"2017年5月13日","permalink":"/tags/rancheros/","section":"Tags","summary":"","title":"RancherOS"},{"content":"$ lsb_release -a No LSB modules are available. Distributor ID:\tUbuntu Description:\tUbuntu 17.04 Release:\t17.04 Codename:\tzesty $ uname -r 4.10.0-20-generic 必要なパッケージをインストール\n$ sudo apt install qemu-kvm virt-manager virt-top virt-manager を使うために（たぶんほぼ使わないけど） libvirt グループに入っていれば操作できるようにする\n$ sudoedit /etc/libvirt/libvirtd.conf # Default allows only owner (root), do not change it unless you are # sure to whom you are exposing the access to. #unix_sock_admin_perms = \u0026quot;0700\u0026quot; ここの unix_sock_admin_perms を 0770 に書き換えて libvirtd を再起動する\n","date":"2017年5月13日","permalink":"/2017/05/kvm-on-ubuntu-17-04-laptop/","section":"Posts","summary":"$ lsb_release -a No LSB modules are available. Distributor ID: Ubuntu Description: Ubuntu 17.04 Release: 17.04 Codename: zesty $ uname -r 4.10.0-20-generic 必要なパッケージをインストール $ sudo apt install qemu-kvm virt-manager virt-top virt-manager を使うために（たぶんほぼ使わないけど） libvirt グループに","title":"Ubuntu 17.04 に KVM をインストール"},{"content":"RancherOS は syslogd も docker container で稼働しています。\nOS のサービスとして動かすコンテナは system-docker コマンドで操作します。\n$ sudo system-docker ps --format 'table {{.ID}}\\t{{.Image}}\\t{{.Command}}\\t{{.Names}}' CONTAINER ID IMAGE COMMAND NAMES 00d574dcd02a rancher/os-docker:1.12.6 \u0026quot;ros user-docker\u0026quot; docker a086f9e01e66 rancher/os-console:v1.0.1 \u0026quot;/usr/bin/ros entrypo\u0026quot; console d860b0783517 rancher/os-base:v1.0.1 \u0026quot;/usr/bin/ros entrypo\u0026quot; ntp 61a493aa2cd2 rancher/os-base:v1.0.1 \u0026quot;/usr/bin/ros entrypo\u0026quot; network 53b33a360900 rancher/os-base:v1.0.1 \u0026quot;/usr/bin/ros entrypo\u0026quot; udev dbac7f916cc2 rancher/os-base:v1.0.1 \u0026quot;/usr/bin/ros entrypo\u0026quot; syslog bf69300b6715 rancher/os-acpid:v1.0.1 \u0026quot;/usr/bin/ros entrypo\u0026quot; acpid syslog という名前のコンテナがいますね。\n$ sudo system-docker ps --no-trunc -f name=syslog --format 'table {{.Image}}\\t{{.Command}}\\t{{.Names}}' IMAGE COMMAND NAMES rancher/os-base:v1.0.1 \u0026quot;/usr/bin/ros entrypoint rsyslogd -n\u0026quot; syslog rsyslogd の -n は\n Avoid auto-backgrounding. This is needed especially if the rsyslogd is started and controlled by init(8).\n ということで普通に rsyslogd が起動しているだけっぽいので /etc/rsyslog.conf を読んでいるはず、ホストの /etc か /etc/rsyslog.conf をマウントしてるのかな？と思って確認してみる。\n$ sudo system-docker inspect --format '{{range .Mounts}}{{printf \u0026quot;%-36s -\u0026gt; %s\\n\u0026quot; .Source .Destination}}{{end}}' syslog /usr/share/ros -\u0026gt; /usr/share/ros /lib/modules -\u0026gt; /lib/modules /run -\u0026gt; /run /etc/selinux -\u0026gt; /etc/selinux /usr/bin/ros -\u0026gt; /usr/bin/ros /dev -\u0026gt; /host/dev /var/log -\u0026gt; /var/log /var/lib/rancher/cache -\u0026gt; /var/lib/rancher/cache /var/lib/rancher -\u0026gt; /var/lib/rancher /lib/firmware -\u0026gt; /lib/firmware /etc/docker -\u0026gt; /etc/docker /etc/resolv.conf -\u0026gt; /etc/resolv.conf /etc/ssl/certs/ca-certificates.crt -\u0026gt; /etc/ssl/certs/ca-certificates.crt.rancher /etc/hosts -\u0026gt; /etc/hosts /var/lib/rancher/conf -\u0026gt; /var/lib/rancher/conf /var/run -\u0026gt; /var/run マウントしてない。 けれどもホストの /etc/rsyslog.conf を書き換えて再起動してみる。\n*.* @syslog-server を追記して再起動するのですが、再起動方法は\n$ sudo system-docker restart syslog です。再起動してみるも syslog-server に送ってくれない。ということでコンテナ内のファイルを書き換えて再起動します。\n$ sudo system-docker exec -it syslog vi /etc/rsyslog.conf rsyslog.conf には\n$IncludeConfig /etc/rsyslog.d/*.conf という記述があるので /etc/rsyslog.d/relay.conf などに書くこともできます。 コンテナ内のファイルを書き換えて再起動することで無事ログの転送ができました。めでたしめでたし。\nおまけ # rsyslog は UDP だけでなく TCP での転送に対応しています。@ 1個だと UDP で、TCP で送りたい場合は @@ と2個にします。 CentOS だと次のようなコメントが rsyslog.conf に書いてあります。TCP を使う場合は接続が切れた場合にログを出力するプログラムが止まってしまわないように Queue を有効にしておくべきです。繋がらない場合にディスクに溜めておいてくれます。復旧時には多くのサーバーから一斉に送られるとサーバー側に負荷がかかりすぎたりしないようにゆっくりと流れるようになってるみたいです。（復旧したのにログが流れてこない！！って悩んだことがあります）\n# ### begin forwarding rule ### # The statement between the begin ... end define a SINGLE forwarding # rule. They belong together, do NOT split them. If you create multiple # forwarding rules, duplicate the whole block! # Remote Logging (we use TCP for reliable delivery) # # An on-disk queue is created for this action. If the remote host is # down, messages are spooled to disk and sent when it is up again. #$WorkDirectory /var/lib/rsyslog # where to place spool files #$ActionQueueFileName fwdRule1 # unique name prefix for spool files #$ActionQueueMaxDiskSpace 1g # 1gb space limit (use as much as possible) #$ActionQueueSaveOnShutdown on # save messages to disk on shutdown #$ActionQueueType LinkedList # run asynchronously #$ActionResumeRetryCount -1 # infinite retries if host is down # remote host is: name/ip:port, e.g. 192.168.0.1:514, port optional #*.* @@remote-host:514 # ### end of the forwarding rule ### ","date":"2017年5月8日","permalink":"/2017/05/syslog-setting-on-rancheros/","section":"Posts","summary":"RancherOS は syslogd も docker container で稼働しています。 OS のサービスとして動かすコンテナは system-docker コマンドで操作します。 $ sudo system-docker ps --format 'table {{.ID}}\\t{{.Image}}\\t{{.Command}}\\t{{.Names}}' CONTAINER ID IMAGE COMMAND NAMES 00d574dcd02a rancher/os-docker:1.12.6 \u0026quot;ros user-docker\u0026quot; docker a086f9e01e66 rancher/os-console:v1.0.1 \u0026quot;/usr/bin/ros entrypo\u0026quot; console d860b0783517 rancher/os-base:v1.0.1 \u0026quot;/usr/bin/ros entrypo\u0026quot;","title":"RancherOSでsyslogを別サーバーに送る"},{"content":"前回「RancherのKubernetesにサービスをデプロイしてみる」の続きです。\n前回は guestbook-all-in-one.yaml の type: LoadBalancer をアンコメントして Rancher の Load Balancer サービスが自動で構築されるようにしてみましたが、それぞれの Pod が NodePort で外部にポートを公開する必要はないんじゃないかなということで今回はアンコメントしないでそのままデフォルトの ClustrIP のサービスとして構築してみます。 変更の必要がないので YAML ファイルの指定に GitHub の URL を直接使えます\n$ kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/guestbook/all-in-one/guestbook-all-in-one.yaml service \u0026quot;redis-master\u0026quot; created deployment \u0026quot;redis-master\u0026quot; created service \u0026quot;redis-slave\u0026quot; created deployment \u0026quot;redis-slave\u0026quot; created service \u0026quot;frontend\u0026quot; created deployment \u0026quot;frontend\u0026quot; created Deployments は前回と変わりません\nRancher Kubernetes Deployments  Services を見ると frontend の External endpoints が空っぽです\nRancher Kubernetes Serivces no External endpoints  ここで Rancher の「KUBERNETES」→「Infrastructure Stacks」から「kubernetes-ingress-lbs」で「Add Service」の「Add Load Balancer」を選択します。\nRancher Kubernetes ingress lbs  「Name」を適当に入力して「Target」に「frontend」を選択して作成すれば外部からアクセス可能になります\nRancher Add Load Balancer  「Scale」で任意のコンテナ数を指定可能ですが、「Always run one instance of this container on every host」を選択すれば全てのホストで1コンテナずつ起動されます。 この LoadBalancer は HAProxy で Host ヘッダーや Path、Port で proxy 先 Service を切替可能なのでこれ一つで複数のサービスに対応できます。Kubernetes のサービスは ClusterIP で作成してこの kubernetes-ingress-lbs で受ければ良さそうです。HAProxy からの proxy 先は1つの CLusterIP ではなく、そこに紐付いている各 Pod のです。今回の例では3つの frontend Pod が起動しているのでそこへ振り分けられます。HTTPS 対応も可能です、証明書は「INFRASTRUCTURE」→「Certificates」で登録したものから選択します。複数の証明書を指定可能です。\n HTTPS LoadBalancer  ","date":"2017年5月6日","permalink":"/2017/05/deploy-services-on-k8s-with-rancher-2/","section":"Posts","summary":"前回「RancherのKubernetesにサービスをデプロイしてみる」の続きです。 前回は guestbook-all-in-one.yaml の type: LoadBalancer をアンコメントして Rancher の Load Balancer サービスが自動で","title":"RancherのKubernetesにサービスをデプロイしてみる(2)"},{"content":"「さくらのクラウドRancherOSでKubernetes環境を構築」の続きです。さくらのクラウドで Rancher + RancherOS を使って構築した Kubernetes 環境にサービスをデプロイしてみます。Kubernetes への deploy 自体は minikube でやったことがある（Kubernetes Secrets を使って minikube に netbox を deploy してみる）ので Rancher を使った場合のネットワーク構成とかを調査していきたい。\nCaddy で Rancher の HTTPS 化 # Kubernetes の前に、前回は Rancher サーバーに直接アクセスしていましたが、HTTPS 化のために Caddy を入れてみました。勝手に Let\u0026rsquo;s Encrypt っから証明書を取得して設定してくれるので便利です。\nCaddy については先日「Caddy という高機能 HTTPS サーバー」を書きました。\n適当な Dockerfile を書いて Docker Hub に push して使いました。実行時に Caddyfile をテンプレートから生成したかったので Entrykit を使いました。（Entrykit の使い方）\nFROMalpineEXPOSE80 443ENV CADDYPATH /etc/ssl/caddySTOPSIGNALSIGQUITCOPY ./caddy /usr/bin/caddyCOPY ./entrykit /usr/bin/entrykitCOPY ./Caddyfile.tmpl /etc/Caddyfile.tmplRUN mkdir -p /usr/share/caddy/html; mkdir -p /etc/ssl/caddy; chmod 755 /usr/bin/caddy /usr/bin/entrykit; /usr/bin/entrykit --symlink; apk --update add ca-certificates; rm -fr /var/cache/apkENTRYPOINT [\u0026#34;/usr/bin/render\u0026#34;, \u0026#34;/etc/Caddyfile\u0026#34;, \\  \u0026#34;--\u0026#34;, \\  \u0026#34;/usr/bin/caddy\u0026#34;, \\  \u0026#34;-log=stdout\u0026#34;, \u0026#34;-agree=true\u0026#34;, \\  \u0026#34;-conf=/etc/Caddyfile\u0026#34;, \u0026#34;-root=/usr/share/caddy/html\u0026#34;]普通の Reverse Proxy で良いのだろうと、こんな出来上がりになるようにしてみたところ、Rancher Agent からのアクセスは WebSocket が通る必要がありました。\nrancher.teraoka.me { proxy / 172.17.0.2:8080 { header_upstream Host {host} header_upstream X-Forwarded-Proto {scheme} } } そこで -e RANCHER_USE_WEBSOCKET=true とした場合に1行 websocket と追加されるようにしました。\nrancher.teraoka.me { proxy / 172.17.0.2:8080 { header_upstream Host {host} header_upstream X-Forwarded-Proto {scheme} websocket } } これで無事ブラウザからも Rancher Agent からのアクセスもできるようになりました。\n無駄骨・・・ # わざわざ別サーバーを間に入れなくても Rancher サーバーは 8080/tcp で HTTP にも HTTPS にも両方対応しているのでした！！Caddy サーバーをセットアップした後に気づきました・・・ 😢\nKubectl で Kubernetes にアクセス # Rancher 上部の「KUBERNETES」から「CLI」を選択すると次の画面になるのでここでブラウザから kubectl コマンドを実行することもできますが、「Generate Config」ボタンをクリックして生成される設定を ~/.kube/config にコピペすればローカル PC から kubectl コマンドでアクセスできるようになります。\nRancher Kubernetes CLI  ブラウザ内のコンソールから kubectl version を実行した出力\n# Run kubectl commands inside here # e.g. kubectl get rc \u0026gt; kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;5\u0026quot;, GitVersion:\u0026quot;v1.5.4\u0026quot;, GitCommit:\u0026quot;7243c69eb523aa4377bce883e7c0dd76b84709a1\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2017-03-07T23:53:09Z\u0026quot;, GoVersion:\u0026quot;go1.7.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;5+\u0026quot;, GitVersion:\u0026quot;v1.5.4-rancher1\u0026quot;, GitCommit:\u0026quot;6ed2b64b2e1df9637661077d877a0483c58a6ae5\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2017-03-17T16:58:04Z\u0026quot;, GoVersion:\u0026quot;go1.7.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} ローカル PC から試した出力（クライアントのバージョンが 1.6.0）\n$ kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;6\u0026quot;, GitVersion:\u0026quot;v1.6.0\u0026quot;, GitCommit:\u0026quot;fff5156092b56e6bd60fff75aad4dc9de6b6ef37\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2017-03-28T16:36:33Z\u0026quot;, GoVersion:\u0026quot;go1.7.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;5+\u0026quot;, GitVersion:\u0026quot;v1.5.4-rancher1\u0026quot;, GitCommit:\u0026quot;6ed2b64b2e1df9637661077d877a0483c58a6ae5\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2017-03-17T16:58:04Z\u0026quot;, GoVersion:\u0026quot;go1.7.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} kubectl は\n$ source \u0026lt;(kubectl completion bash) とすれば補完が効いて便利になるようだ。zsh なら bash のところを zsh すればよし。\nGuestbook Example アプリを Kubernetes にデプロイしてみる # https://github.com/kubernetes/kubernetes/tree/master/examples/guestbook にある Guestbook アプリをデプロイしてみる（Kubernetes の紹介で時々見かけるやつですね）。 guestbook-all-in-one.yaml を使うと一発でできちゃうんですが一箇所だけ修正します。 コメントアウトされている type: LoadBalancer をアンコメントします。\napiVersion:v1kind:Servicemetadata:name:frontendlabels:app:guestbooktier:frontendspec:# if your cluster supports it, uncomment the following to automatically create# an external load-balanced IP for the frontend service.type:LoadBalancerports:# the port that this service should serve on- port:80selector:app:guestbooktier:frontend$ kubectl create -f guestbook-all-in-one.yaml --record service \u0026quot;redis-master\u0026quot; created deployment \u0026quot;redis-master\u0026quot; created service \u0026quot;redis-slave\u0026quot; created deployment \u0026quot;redis-slave\u0026quot; created service \u0026quot;frontend\u0026quot; created deployment \u0026quot;frontend\u0026quot; created Kubernetes の Dashboard から Deployments を確認すると frontend という Apache + mod_php のアプリ Container (Pod) が3つと redis のマスターが1つ、 redis のレプリカが2つ起動しているのが確認できます。\nKubernetes Dashboard Deployments - guestbook  Services を確認するとそれぞれの Cluster IP が確認できます。\nKubernetes Dashboard Services - guestbook  External Endpoints に表示されているIPアドレス、ポート番号にブラウザからアクセスすると Guestbook アプリにアクセスできます。次のような表示になります。\nguestbook-app  名前解決 # guestbook.php の中身は次のようになっており redis のサーバー名は GET_HOSTS_FROM という環境変数が env の場合は環境変数 REDIS_MASTER_SERVICE_HOST, REDIS_SLAVE_SERVICE_HOST から取得し、そうでない場合は redis-master, redis-slave という名前で DNS によって解決しています。guestbook-all-in-one.yaml では GET_HOSTS_FROM は dns になっていますから DNS ですね。rancher-dns ってのが動いてるっぽいけど resolv.conf にある 10.43.0.10 というアドレスがどこにどう定義されているのか要調査。\n\u0026lt;?php error_reporting(E_ALL); ini_set(\u0026#39;display_errors\u0026#39;, 1); require \u0026#39;Predis/Autoloader.php\u0026#39;; Predis\\Autoloader::register(); if (isset($_GET[\u0026#39;cmd\u0026#39;]) === true) { $host = \u0026#39;redis-master\u0026#39;; if (getenv(\u0026#39;GET_HOSTS_FROM\u0026#39;) == \u0026#39;env\u0026#39;) { $host = getenv(\u0026#39;REDIS_MASTER_SERVICE_HOST\u0026#39;); } header(\u0026#39;Content-Type: application/json\u0026#39;); if ($_GET[\u0026#39;cmd\u0026#39;] == \u0026#39;set\u0026#39;) { $client = new Predis\\Client([ \u0026#39;scheme\u0026#39; =\u0026gt; \u0026#39;tcp\u0026#39;, \u0026#39;host\u0026#39; =\u0026gt; $host, \u0026#39;port\u0026#39; =\u0026gt; 6379, ]); $client-\u0026gt;set($_GET[\u0026#39;key\u0026#39;], $_GET[\u0026#39;value\u0026#39;]); print(\u0026#39;{\u0026#34;message\u0026#34;: \u0026#34;Updated\u0026#34;}\u0026#39;); } else { $host = \u0026#39;redis-slave\u0026#39;; if (getenv(\u0026#39;GET_HOSTS_FROM\u0026#39;) == \u0026#39;env\u0026#39;) { $host = getenv(\u0026#39;REDIS_SLAVE_SERVICE_HOST\u0026#39;); } $client = new Predis\\Client([ \u0026#39;scheme\u0026#39; =\u0026gt; \u0026#39;tcp\u0026#39;, \u0026#39;host\u0026#39; =\u0026gt; $host, \u0026#39;port\u0026#39; =\u0026gt; 6379, ]); $value = $client-\u0026gt;get($_GET[\u0026#39;key\u0026#39;]); print(\u0026#39;{\u0026#34;data\u0026#34;: \u0026#34;\u0026#39; . $value . \u0026#39;\u0026#34;}\u0026#39;); } } else { phpinfo(); } ?\u0026gt;LoadBalancer # guestbook-all-in-one.yaml の type: LoadBalancer 行をアンコメントしましたが、これによって何ができたかというと「KUBERNETES」の「Infrastructure Stacks」を確認すると「kubernetes loadbalancers」に次のような表示が確認できます。\n  \nこの中で lb-a9a2059bd2efb11e7a82402a939d3449 を見てみると次のような情報も確認できます。「Ports」ではどのホストのIPアドレスで外からのアクセスを受け付けるようになっているかが確認できます。今回の例では k8s-01 のIPアドレスになっています。\n  \nBalancer Rules タブではどのホストのどのポート (container) に転送するかがわかります。\n  \nこの Load Balancer は HAProxy コンテナで実装されています。haproxy.cfg を確認してみると次のようになっていました。proxy 先は Global IP Address なのですね。ホストがインターネットに晒されている場合は iptables や手間でのどこかで閉じていないとここに直接アクセスできてしまいますね。Service の Cluster IP に転送するのかと思っていたが違っていたようだ。Cluster IP は Kubernetes 内でアクセスするためのアドレスだから外から転送するには NodePort を使わざるを得ないということか。\nglobal chroot /var/lib/haproxy daemon group haproxy maxconn 4096 maxpipes 1024 ssl-default-bind-ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:ECDHE-RSA-DES-CBC3-SHA:ECDHE-ECDSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA ssl-default-bind-options no-sslv3 no-tlsv10 ssl-default-server-ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:ECDHE-RSA-DES-CBC3-SHA:ECDHE-ECDSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA tune.ssl.default-dh-param 2048 user haproxy defaults errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http maxconn 4096 mode tcp option forwardfor option http-server-close option redispatch retries 3 timeout client 50000 timeout connect 5000 timeout server 50000 resolvers rancher nameserver dnsmasq 169.254.169.250:53 listen default bind *:42 frontend 80 bind *:80 mode tcp default_backend 80_ backend 80_ acl forwarded_proto hdr_cnt(X-Forwarded-Proto) eq 0 acl forwarded_port hdr_cnt(X-Forwarded-Port) eq 0 http-request add-header X-Forwarded-Port %[dst_port] if forwarded_port http-request add-header X-Forwarded-Proto https if { ssl_fc } forwarded_proto mode tcp server 9c51f1b00fd9e1eb2211cde0ed46d9456d0213f5 153.120.129.113:31877 server aa73f7199039e58a2c5c6081e56edf6d27e06c31 153.120.82.8:31877 server cf4c62546250ba397c98196c8ce9c08ceef88342 133.242.49.48:31877 この LB は1台のホストでしか稼働していないのでその1台が止まってしまうとこまります。でも Service ページの左側にある「Scale」欄の「+ / -」で増減できます。3に増やすことで k8s-01, k8s-02, k8s-03 のどのサーバーでも受けられるようにできます。\nLB 1台の状態で当該ホストを強制シャットダウンしたら別の当該ホストで稼働していた他のコンテナ同様に別のホストでえ起動してきました。Rancher の Proxy 経由でアクセスする Kubernetes の dashboard はなぜかなかなか切り替わってくれなかったけど、kubectl でアクセスする方はすぐに切り替わってました。\n続き「RancherのKubernetesにサービスをデプロイしてみる(2)」\n","date":"2017年5月2日","permalink":"/2017/05/deploy-services-on-k8s-with-rancher/","section":"Posts","summary":"「さくらのクラウドRancherOSでKubernetes環境を構築」の続きです。さくらのクラウドで Rancher + RancherOS を使って構築した Kubernetes 環境にサービスを","title":"RancherのKubernetesにサービスをデプロイしてみる"},{"content":"","date":"2017年5月2日","permalink":"/tags/sacloud/","section":"Tags","summary":"","title":"sacloud"},{"content":"「さくらのクラウドで提供されたRancherOSを試す」の続きです。Rancher で Kubernetes 環境を作ってアプリをデプロイしてみます。\nRancher Server セットアップ # Kubernetes 環境を構築するためのテストなの Rancher Server は冗長化などは考えず\ndocker run -d --restart=unless-stopped -p 8080:8080 rancher/server で起動させます。 起動したらまずは画面上部の「ADMIN」→「Access Control」で認証設定を行います。Active Directory とか LDAP とか GitHub などを使えますがテストなので「Local Authentication」で。\nKubernetes Environment 追加 # Rancher は1つのサーバーで複数の Docker クラスタ(Orchestration)を管理することができ、クラスタ毎にアクセス権の管理ができたりします。クラスタ化も Rancher 独自の Cattle の他、ここで試す Kubernetes や Docker Swarm、Mesos にも対応しておりそれぞれを簡単に構築することが可能です。 左上の「Default」（デフォルトの Environment 名）と表示されているメニューから「Manage Environments」にアクセスします。「Add Environment」ボタンから作成画面へ移動し、「Environment Template」で「Kubernetes」を選択して任意の名前を入力して「Create」ボタンをクリックするだけで準備完了です。今回は「k8s」という名前にしました。後は、この環境にホストを追加すれば Kubernetes 環境ができてしまいます。 また「Default」のメニューから今作成した「k8s」を選択することで環境を切り替えます。 「Setting up Kubernetes」と表示され何やらセットアップが進んでる風に見えますが、進みません。上部に\n Before adding your first service or launching a container, you\u0026rsquo;ll need to add a Linux host with a supported version or Docker. Add a host\n と表示されており、Add a host のリンクからホストを追加する必要があります。\nKubernetes 用サーバーのセットアップ # 前回 と同じように RancherOS イメージを使って 2CPU, 2GB メモリのサーバーを3台起動させます。Rancher Server と通信できる必要がありますし、構築するサービスに外部からアクセスするのに別途 Load Balancer などを使わないのであればインターネットに接続させます。 k8s-01, k8s-02, k8s-03 というサーバーをセットアップしました。\nRancherOS の更新 # 前回試さなかったのですが ros コマンドで OS の更新ができます。\n$ sudo ros os NAME: ros os - operating system upgrade/downgrade USAGE: ros os command [arguments...] COMMANDS: upgrade upgrade to latest version list list the current available versions version show the currently installed version 更新可能なバージョンが無いかと確認してみると\n$ sudo ros os list rancher/os:v1.0.1 remote latest rancher/os:v1.0.0 remote available running rancher/os:v0.9.2 remote available rancher/os:v0.9.1 remote available ...snip... v1.0.1 が存在するようなので更新してみます。\n$ sudo ros os upgrade Upgrading to rancher/os:v1.0.1 Continue [y/N]: y Pulling os-upgrade (rancher/os:v1.0.1)... v1.0.1: Pulling from rancher/os 627beaf3eaaf: Pull complete 56ecb7539042: Pull complete ab6a6aa500c0: Pull complete 237fe36f0593: Pull complete 959d9773a286: Pull complete f62d8177237f: Pull complete 25a6fb770b97: Pull complete 6235a630e44b: Pull complete fb3adec6ce09: Pull complete c7354f67942a: Pull complete Digest: sha256:6656686f65c3820a8399ec64f80b2511cc0441d9202dba445d8d4cab7dfd85e0 Status: Downloaded newer image for rancher/os:v1.0.1 os-upgrade_1 | Installing from :v1.0.1 Continue with reboot [y/N]: y \u0026gt; INFO[0034] Rebooting とっても簡単に更新できました。-f オプションをつければプロンプトも出さずに reboot して更新が完了します。\nDocker のバージョン変更 # ホストを追加せよってメッセージのところに supported version というリンクがありました。そうです、Kubernetes はまだ最新の Docker には対応していません。 RancherOS に入っている Docker のバージョンはいくつでしょうか？\n$ docker version Client: Version: 17.03.1-ce API version: 1.27 Go version: go1.7.5 Git commit: c6d412e Built: Tue Mar 28 00:40:02 2017 OS/Arch: linux/amd64 Server: Version: 17.03.1-ce API version: 1.27 (minimum version 1.12) Go version: go1.7.5 Git commit: c6d412e Built: Tue Mar 28 00:40:02 2017 OS/Arch: linux/amd64 Experimental: false 17.03.1-ce では新しすぎますね。RancherOS は Docker のバージョンも ros コマンドで簡単に変更できます。sudo ros engine list で使用可能な一覧が確認できます。\n$ sudo ros engine list disabled docker-1.10.3 disabled docker-1.11.2 disabled docker-1.12.6 disabled docker-1.13.1 current docker-17.03.1-ce disabled docker-17.04.0-ce Kubernetes でサポートされているのは 1.12 までなので docker-1.12.6 を使うように変更します。\n$ sudo ros engine switch docker-1.12.6 \u0026gt; INFO[0001] Project [os]: Starting project \u0026gt; INFO[0001] [0/16] [docker]: Starting Pulling docker (rancher/os-docker:1.12.6)... 1.12.6: Pulling from rancher/os-docker 52160511971f: Pull complete Digest: sha256:1916540f838dbef62602e0565541a3e25dcb66649369ed697266326fc3cd615c Status: Downloaded newer image for rancher/os-docker:1.12.6 \u0026gt; INFO[0007] Recreating docker \u0026gt; INFO[0008] [1/16] [docker]: Started \u0026gt; INFO[0008] Project [os]: Project started $ docker version Client: Version: 1.12.6 API version: 1.24 Go version: go1.6.4 Git commit: 78d1802 Built: Wed Jan 11 00:23:16 2017 OS/Arch: linux/amd64 Server: Version: 1.12.6 API version: 1.24 Go version: go1.6.4 Git commit: 78d1802 Built: Wed Jan 11 00:23:16 2017 OS/Arch: linux/amd64 Docker のバージョンも簡単に切り替えることができました。それでは Rancher にホストを追加しましょう。\nRancher の Environment にホストを追加 # いよいよホストの追加です。Add a host のリンクか上部メニューの「INFRASTRUCTURE」→「Hosts」から追加します。 Rancher は認証情報を設定してやれば各クラウドサービスのAPIを使って自動で AWS EC2 や Azure VM、DigitalOcean の VPS インスタンスを起動し、Docker をインストールしホストとして組み込むところまでやってくれますが、今回は自前でセットアップした RancherOS を登録するので Custom を選択します。\nRancher add a host  IPSec のために追加するホスト同士で 500/udp, 4500/udp が開いていれば、ホストのIPアドレスを入力して（省略したら Rancher Server が接続元のIPアドレスを使う）、次のテキストエリアに表示されている docker コマンドをホストで実行するだけで完了です。\nsudo docker run \\ -e CATTLE_AGENT_IP=\u0026quot;{node-ip-address}\u0026quot; \\ --rm --privileged \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /var/lib/rancher:/var/lib/rancher rancher/agent:v1.2.2 \\ http://rancher-server/v1/scripts/7BC9F0A0ECE7DD1C2EF7:1483142400000:0gH7Zulb3WzXMgclidhtiXZ8Ag こんな感じのコマンドを実行するだけ。 3台登録するとこんな感じになります。\nRancher Kubernetes Host  これは全てのコンテナの状態が緑ですが、3台起動する過程で停止して移動させたりもろもろあるので赤も混ざってきます。stop 済みの赤いコンテナは手動で削除しました。 「INFRASTRUCTURE」→「Containers」では次のような表示になります。\nRancher Kubernetes Containers  ホストは Deactivate でそれ以上 container を起動しなくなりますが、動いていた container はそのまま残ります。Deactivate 状態のホストは削除することが可能です。削除されたホストで稼働していた container は必要ではれば別のホストで起動されます。ホストが3台以上になると etcd が3台構成となりますが、etcd は同一ホストで複数起動させられないため、3台未満では etcd の冗長性が担保できなくなります。\n次回は Kubernetes 上にサービスをデプロイしてみます\n","date":"2017年5月1日","permalink":"/2017/05/build-kubernetes-using-rancheros-on-sakura-cloud/","section":"Posts","summary":"「さくらのクラウドで提供されたRancherOSを試す」の続きです。Rancher で Kubernetes 環境を作ってアプリをデプロイしてみます。 Rancher Server セットアッ","title":"さくらのクラウドRancherOSでKubernetes環境を構築"},{"content":"  https://caddyserver.com/ なにやら大変便利そうなものがありました。\n HTTP/HTTPS サーバー TLS 証明書の自動更新が可能 (HTTP, DNS 両方対応) HTTP2, QUIC, WebSocket にも対応 Go で書かれているのでマルチプラットフォーム対応 単純なディレクトリの公開 Markdown のレンダリングが可能 ダイナミックな証明書取得 (Let\u0026rsquo;s Encrypt はワイルドカード証明書に対応していないが、リクエストを受けた時点でそのドメインの証明書を取得するということでワイルドカードっぽく使える) リバースプロキシ ロードバランス Basic認証 他 Plugin による拡張  IPアドレス制限 ratelimit ファイルアップロード、削除 CGI などなど    https://github.com/caddyserver/examples ここに各種設定例があります。でも古いかも。 設定ファイルのデフォルトはカレントディレクトリの Caddyfile です。 ファイルはなくても引数や標準入力からでも渡せます。 何も指定せずに単に caddy と実行すればカレントディレクトリをドキュメントルートとして 2015/tcp で http サーバーが起動します。 caddy browse とすればディレクトリを指定した場合にファイルのリストが表示されます。\ncaddy file browser  caddy browse markdown とすれば更に、アクセスしたファイルが .md だった場合に markdown を HTML にして表示してくれます。（独立したオプションなので browse を外せばファイルのリストは表示されませんが、markdown の HTML 化はされます）\nこのサイトに Caddy を導入してみる # Apache + PHP の手前に nginx を置いた構成なので、nginx を caddy に置き換えるだけです。 Caddyfile を次のようにし、ダウンロードした caddy の tar.gz に入っている init/linux-systemd/caddy.service を使い ExecStart に -quic を追加し、実行ユーザーを変えただけです。\nblog.1q77.com { proxy / localhost:8080 { header_upstream Host {host} header_upstream X-Forwarded-Proto {scheme} } log / /var/log/caddy/access.log \u0026quot;remote_addr:{remote}\ttime:{when_iso}\tmethod:{method}\turi:{uri}\tprotocol:{proto}scheme:{scheme}\tstatus:{status}\thost:{host}\th_host:{\u0026gt;Host}\tsize:{size}\tlatency:{latency}\tmitm:{mitm}\tua:{\u0026gt;User-Agent}\tcookie:{\u0026gt;Cookie}\u0026quot; { rotate_size 100 # rotate after 100MB rotate_age 30 # keep 30 days } } ログは LTSV にしてみました。placeholder の一覧。 X-Forwarded-Proto を追加していますが X-Forwarded-For は header_upstream 設定なしで付きます。 証明書は環境変数 CADDYPATH で /etc/ssl/caddy が指定してあるのでここにファイルができていました。\n# find /etc/ssl/caddy/ -type f /etc/ssl/caddy/acme/acme-v01.api.letsencrypt.org/users/default/default.json /etc/ssl/caddy/acme/acme-v01.api.letsencrypt.org/users/default/default.key /etc/ssl/caddy/acme/acme-v01.api.letsencrypt.org/sites/blog.1q77.com/blog.1q77.com.crt /etc/ssl/caddy/acme/acme-v01.api.letsencrypt.org/sites/blog.1q77.com/blog.1q77.com.key /etc/ssl/caddy/acme/acme-v01.api.letsencrypt.org/sites/blog.1q77.com/blog.1q77.com.json /etc/ssl/caddy/ocsp/blog.1q77.com-815f526b QUIC プロトコル # UDP でより効率的な TCP を再設計（TCP を改善するには標準化や各OSでの対応を待つ必要があり、大変時間がかかるのでアプリケーションレイヤーｄやってしまおうという）したようなプロトコルで主に Google のサイトとモバイル端末の間で使われているプロトコルですが、Caddy では実験的なサポートがされています。 -quic オプションをつけて起動すると有効になります。 次のようなヘッダーが返るようになって動作しているみたいですが\nAlt-Svc: quic=\u0026quot;:443\u0026quot;; ma=2592000; v=\u0026quot;36,35\u0026quot; Alternate-Protocol: 443:quic Chrome で見ると Broken と表示されてしまいました。なんでだろ？  chrome://net-internals/#alt-svc https://github.com/mholt/caddy/wiki/QUIC\nDNS を使った証明書取得 # これが便利だなと思っています。Let\u0026rsquo;s Encrypt のサーバーからアクセスできるサーバーであれば certbot なり何なりで自動化は容易ですが、アクセスできない場合は dns-01 によるドメインの所有確認が必要です。certbot とスクリプトを組み合わせることでこれもできるようですが Web サーバーだけで簡単にできちゃうのは便利そうです。まだ試してないけど沢山の DNS プロバイダに対応しています。\nhttps://github.com/caddyserver/dnsproviders\nCertbotでDNSによる認証(DNS-01)で無料のSSL/TLS証明書を取得する | 本日も乙\nRoute53 は IAM で編集可能な範囲を制限するのが zone 単位なので example.com を持っていて www.example.com を dns-01 で所有確認するのであれば www.example.com を別 zone に切り出して権限を付与するのが安全でしょう、万が一 IAM 情報が漏れてしまっても影響範囲を限定できます。\nまとめ # パフォーマンスとか安定性は良くわからないけど社内向けサーバーなんかで使ってみるのはいいんじゃないかなと。特に外からアクセスできないような場合。\n","date":"2017年4月29日","permalink":"/2017/04/caddy/","section":"Posts","summary":"https://caddyserver.com/ なにやら大変便利そうなものがありました。 HTTP/HTTPS サーバー TLS 証明書の自動更新が可能 (HTTP, DNS 両方対応) HTTP2, QUIC, WebSocket にも対応 Go で書かれているのでマルチプラットフ","title":"Caddy という高機能 HTTPS サーバー"},{"content":"ちゃんと公式ドキュメントを読めっていう話なのですが HAProxy にて X-Forwarded-For をセットするという options forwardfor 設定は通常期待される動作とは違います。ググって見つけた答えはちゃんと検証しましょう。\nX-Forwarded-For: xxx.xxx.xxx.xxx がすでにリクエストに入っていた場合、次のように HAProxy の動作するサーバーの IP アドレスがカンマ区切りで追加されることを期待しますが\nX-Forwarded-For: xxx.xxx.xxx.xxx, yyy.yyy.yyy.yyy 実際には次のようにヘッダーの最後に新たに X-Forwarded-For ヘッダーが追加されます。\nX-Forwarded-For: xxx.xxx.xxx.xxx その他の Header X-Forwarded-For: yyy.yyy.yyy.yyy これを受け取ったサーバーがどのように扱うかは実装依存っぽいですね。 Rails 5.0.2 で確認すると\nHTTP_X_FORWARDED_FOR:xxx.xxx.xxx.xxx, yyy.yyy.yyy.yyy となりました。Flask 0.12.1 では最後の X-Forwarded-For だけになりました。HAProxy のドキュメントには同名ヘッダーを受け取ったサーバーは最後値を使うべきだと書いてあるから Flask の方が正しいのかな。 他のサーバーでどのように処理されるかどうかは不明。後で調べて追記予定。\n 公式ドキュメントの option forwardfor 項\noption forwardfor [ except ] [ header ] [ if-none ] という構文なので except で X-Forwarded-For を追加しない条件を指定したり、header でヘッダー名を X-Forwarded-For ではないものにしたり、if-none で X-Forwarded-For が存在しない場合にのみ追加するといった指定が可能です。\n","date":"2017年4月27日","permalink":"/2017/04/haproxy-option-forwardfor/","section":"Posts","summary":"ちゃんと公式ドキュメントを読めっていう話なのですが HAProxy にて X-Forwarded-For をセットするという options forwardfor 設定は通常期待される動作とは違います。ググって見つけた答えは","title":"HAProxy の X-Forwarded-For 実装の罠"},{"content":"2017.04.20 に Rancher OSのアーカイブ提供を開始いたしました | さくらのクラウドニュース というニュースが出ていました。\nKubernetes を気軽に立てたり捨てたりする環境として Rancher は便利そうなので気になっていました。どの OS 使うべきか悩みますしね (Dockerの本番運用 | インフラ・ミドルウェア | POSTD)。RancherOS は Rancher 用に最小のパッケージングで提供される OS です。そして全てを docker で実行します。ntpd も syslog も docker コンテナで稼働してます。udev や acpid なんてのもいます。\nA simplified Linux distribution built from containers, for containers RancherOS は 4/12 に GA が出たばかりです [Press Release] RancherOS Hits General Availability Dockerコンテナに特化した「RancherOS」正式版リリース。Linuxカーネル上でDockerを実行、システムもユーザーもすべてをコンテナ空間に － Publickey ログインして sudo system-docker ps を実行すると OS の必要機能として起動している docker コンテナが確認できます。\n$ sudo system-docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b67a69f4b088 rancher/os-docker:17.03.1 \u0026quot;ros user-docker\u0026quot; 13 minutes ago Up 13 minutes docker 79a01794e57b rancher/os-console:v1.0.0 \u0026quot;/usr/bin/ros entrypo\u0026quot; 13 minutes ago Up 13 minutes console df1815599bbf rancher/os-base:v1.0.0 \u0026quot;/usr/bin/ros entrypo\u0026quot; 13 minutes ago Up 13 minutes ntp 30b6333158ca rancher/os-base:v1.0.0 \u0026quot;/usr/bin/ros entrypo\u0026quot; 13 minutes ago Up 13 minutes network ab5b3abf4e74 rancher/os-base:v1.0.0 \u0026quot;/usr/bin/ros entrypo\u0026quot; 13 minutes ago Up 13 minutes udev 517a848c2a36 rancher/os-acpid:v1.0.0 \u0026quot;/usr/bin/ros entrypo\u0026quot; 13 minutes ago Up 13 minutes acpid 79d1abc559d2 rancher/os-base:v1.0.0 \u0026quot;/usr/bin/ros entrypo\u0026quot; 13 minutes ago Up 13 minutes syslog system-docker は /usr/bin/ros への symbolic link で通常の docker コマンド (/usr/bin/docker は /var/lib/rancher/engine/docker への symbolic link) とは別物になっています。\nさくらのクラウドで RancherOS を起動してみる # https://secure.sakura.ad.jp/cloud/ からログインします。 サーバーの「追加」で「2. ディスク」の「アーカイブ選択」で「[20GB] RancherOS v1.0.0 LTS #112900470901」を選択します。 すると次のような表示が出ます。\n 管理ユーザ名は「rancher」です。 サーバ作成後、rancherユーザでログインしてください。 [注意事項] サーバへの接続の際には公開鍵の登録が必須となります。 リモートコンソールからのログインは出来ません。 こちらは20GB固定サイズのアーカイブです。 20GBより大きいディスクを作成する場合、 パーティションをリサイズする cloud-config をご利用ください。 https://docs.rancher.com/os/configuration/resizing-device-partition/\n これに従って「4. ディスクの修正」では「公開鍵」で「入力」か「選択」で公開鍵をセットします。 「配置する スタートアップスクリプト」は yaml_cloud_config しか使えません。 プリセットの「[public] Switching Consoles for RancherOS #112900473840」が選択できますが必要であれば自作できます。 https://docs.rancher.com/os/configuration/ SSH の公開鍵登録登録の例 (SSH Keys)\n#cloud-configssh_authorized_keys:- ssh-rsa AAA...ZZZ example1@rancher- ssh-rsa BBB...ZZZ example2@rancherファイルを作成する例 (Writing Files)\n#cloud-configwrite_files:- path:/etc/rc.localpermissions:\u0026#34;0755\u0026#34;owner:rootcontent:|#!/bin/bash echo \u0026#34;I\u0026#39;m doing things on start\u0026#34;コマンド実行の例 (Running Commands)\n#cloud-configruncmd:- [touch, /home/rancher/test1 ]- echo \u0026#34;test\u0026#34; \u0026gt; /home/rancher/test2その他 IP アドレスやらいろいろ設定できます\nros コマンド # 設定変更や OS の upgrade / downgrade ができるようです\n$ sudo ros -v ros version v1.0.0 $ sudo ros help NAME: ros - Control and configure RancherOS USAGE: ros [global options] command [command options] [arguments...] VERSION: v1.0.0 AUTHOR(S): Rancher Labs, Inc. COMMANDS: config, c configure settings console manage which console container is used engine manage which Docker engine is used service, s Command line interface for services and compose. os operating system upgrade/downgrade tls setup tls configuration install install RancherOS to disk selinux Launch SELinux tools container. help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --help, -h show help --version, -v print the version docker コマンドを実行してみる # $ docker run -p 80:80 -d nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 36a46ebd5019: Pull complete 57168433389f: Pull complete 332ec8285c50: Pull complete Digest: sha256:c15f1fb8fd55c60c72f940a76da76a5fccce2fefa0dd9b17967b9e40b0355316 Status: Downloaded newer image for nginx:latest ce6cec3a61a7a74c071fab1a11db0f291845cb58863d1e44f3194c50c8526924 これで普通に port 80 で nginx にアクセスできます\ndocker version # $ docker version Client: Version: 17.03.1-ce API version: 1.27 Go version: go1.7.5 Git commit: c6d412e Built: Tue Mar 28 00:40:02 2017 OS/Arch: linux/amd64 Server: Version: 17.03.1-ce API version: 1.27 (minimum version 1.12) Go version: go1.7.5 Git commit: c6d412e Built: Tue Mar 28 00:40:02 2017 OS/Arch: linux/amd64 Experimental: false Rancher Server を起動してみる # とりあえずお試しなのでこれで LAUNCHING RANCHER SERVER - SINGLE CONTAINER (NON-HA)\n$ docker run -d --restart=unless-stopped -p 8080:8080 rancher/server Unable to find image 'rancher/server:latest' locally latest: Pulling from rancher/server 6599cadaf950: Pull complete 23eda618d451: Pull complete (snip) 58bafb65736d: Pull complete 232b8325e66b: Pull complete Digest: sha256:eeeab5bd80f707e2523c11c7fa437315d56fe97113eed7ad2dc058aa26555db0 Status: Downloaded newer image for rancher/server:latest 1c45fd639bba2d07180786dd6ed95e3e6afb01d84637bf6d3c75db312301d672 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1c45fd639bba rancher/server \u0026quot;/usr/bin/entry /u...\u0026quot; 47 seconds ago Up 46 seconds 3306/tcp, 0.0.0.0:8080-\u0026gt;8080/tcp upbeat_elion 起動しました。port 8080 でアクセスできました。 起動してしまえば後は過去記事と同じ\n DigitalOcean にて Rancher を試す – その1 DigitalOcean にて Rancher を試す – その2 (HA構成)  クーポン # インフラ技術を極めろ！クラウドマスター認定試験｜teratail（テラテイル） でいただいた 20,000円分のクーポン を使わせていただいております。ありがとうございます。 ちなみにこのブログでも頻繁に登場する DigitalOcean については http://docs.rancher.com/os/running-rancheros/cloud/do/ に「Running RancherOS on DigitalOcean is not yet supported.」と書いてありました。\n次回 # 次回は RancherOS で Kubernetes Environment を作ってみます。\n","date":"2017年4月26日","permalink":"/2017/04/rancheros-on-sacloud/","section":"Posts","summary":"2017.04.20 に Rancher OSのアーカイブ提供を開始いたしました | さくらのクラウドニュース というニュースが出ていました。 Kubernetes を気軽に立てたり捨てたりする環境として","title":"さくらのクラウドで提供されたRancherOSを試す"},{"content":"Kubernetes お試し中です。Rancher で構築するべきか悩み中。 今回は YAML を書いて Deployment, Service を作成して Kubernetes 上に netbox を構築してみます。(netbox は Django + PostgreSQL のデータセンターファシリティ管理ツールです、Wordpress のセットアップは飽きた) パスワードなどは Kubernetes Secrets を使います。 minikube の起動は\n$ minikube start たったこれだけ。詳細は以前の投稿で。\n構成図 # こんな構成にしてみます\nkubernetes-netbox 構成図  Version # minikube の version は 0.16.0\n$ minikube version minikube version: v0.16.0 Kubernetes の version は\n$ kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;5\u0026quot;, GitVersion:\u0026quot;v1.5.1\u0026quot;, GitCommit:\u0026quot;82450d03cb057bab0950214ef122b67c83fb11df\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2016-12-14T00:57:05Z\u0026quot;, GoVersion:\u0026quot;go1.7.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;5\u0026quot;, GitVersion:\u0026quot;v1.5.2\u0026quot;, GitCommit:\u0026quot;08e099554f3c31f6e6f07b448ab3ed78d0520507\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;1970-01-01T00:00:00Z\u0026quot;, GoVersion:\u0026quot;go1.7.1\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} kubectl 更新しなきゃ\nSecret を作成 # https://kubernetes.io/docs/user-guide/secrets/ 実際に試した時にはまずは Secrets なしでやったので順序が違いますが、Secrets を使うとなったらまずはこれを作るところから。 秘密なのでコマンドラインの履歴に残らないように --from-file を使いました。 YAML や JSON からでも作れますが、値を Base64 に encode する手間が必要です。直接文字列を渡すには --from-literal=username=testuser という指定を使います。\n$ kubectl create secret generic netbox-secret \\ --from-file=secret_key=secrets/secret_key.txt \\ --from-file=email_address=secrets/email_address.txt \\ --from-file=email_password=secrets/email_password.txt \\ --from-file=netbox_password=secrets/netbox_password.txt \\ --from-file=db_password=secrets/db_password.txt \\ --from-file=superuser_password=secrets/superuser_password.txt secret \u0026quot;netbox-secret\u0026quot; created ファイルで渡す場合は末尾の改行も含まれてしまうため、改行が入らないようの echo -n などで作ります。 あれ？ echo コマンドが履歴に・・・\n$ echo -n 'string' \u0026gt; secret.txt netbox-secret という名前の secret ができているので確認してみます。\n$ kubectl get secrets netbox-secret NAME TYPE DATA AGE netbox-secret Opaque 6 52s $ kubectl describe secrets netbox-secret Name:\tnetbox-secret Namespace:\tdefault Labels:\t\u0026lt;none\u0026gt; Annotations:\t\u0026lt;none\u0026gt; Type:\tOpaque Data ==== superuser_password:\t5 bytes db_password:\t16 bytes email_address:\t27 bytes email_password:\t16 bytes netbox_password:\t5 bytes secret_key:\t49 bytes ひとつの secret に複数の key / value のセットを登録されていることが確認できます。\nこれを volume としてマウントてファイルのようにしてアクセスしたり、環境変数として渡すことができます。今回は環境変数として設定します。 Docker 1.13 の secret では一度設定した値を更新できませんでしたが、Kubernetes の場合には更新が可能で、ファイルとしてアクセスする場合には定期的に更新がチェックされ、反映されます。 マウント時にパーミッションも指定できるようなのでコンテナ側から更新できるのかな？試してないけど。\nファイルから登録してファイルとして見せられるので TLS の証明書と秘密鍵とか SSH の private key などを渡すのにも便利に使えそうです。\nドキュメントの Risks に書かれていることは理解しておいたほうが良いです。Kubernetes で任意のコンテナを起動できる人には Secrets は丸見えだとか、etcd の中には平文で入っているので etcd のファイルにアクセスできてしまうとダメだとか書かれています。\nさらに、Kubernetes の Dashboard で値が見れちゃう、ええぇぇっ！ ということは\n$ kubectl get secret netbox-secret -o yaml とかで YAML や JSON には Base64 の文字列が含まれてるから簡単に値が読めちゃう。API にアクセス出来ちゃう人には丸見えってことだな。\nPostgreSQL Deployment の YAML ファイル作成 # 近頃の Kubernetes では Pod を直接作るのではなく、Deployment というものを使うみたいですね。 Deployment は template として設定した Pod を何個起動させるかを定義する感じです。\nマスタの DB は1個で良いので replicas: 1 です。\nPod のコンテナもここでは postgres の1個だけです。\napiVersion:extensions/v1beta1kind:Deploymentmetadata:name:netbox-db-deploymentspec:replicas:1template:metadata:labels:app:netbox-dbspec:containers:- name:netbox-dbimage:postgres:9.6.2env:- name:POSTGRES_USERvalue:netbox- name:POSTGRES_PASSWORD# netbox-secret という secret volume から key で指定した値を環境変数に設定valueFrom:secretKeyRef:name:netbox-secretkey:db_password- name:POSTGRES_DBvalue:netbox# 5432/tcp を公開ports:- containerPort:5432volumeMounts:# /var/lib/postgresql にデータ用 volume をマウント- mountPath:/var/lib/postgresqlname:netbox-db-datarestartPolicy:Alwaysvolumes:# データファイル用の volume を定義- name:netbox-db-dataemptyDir:{}# netbox-secret という名前の secret を netbox-secret という volume 名で定義- name:netbox-secretsecret:secretName:netbox-secretアプリの Deployment の YAML ファイル作成 # こちらの Pod には Reveerse Proxy としての nginx と Django で書かれた netbox アプリの2つのコンテナを入れてあります。\nDjango 側で持っている制定ファイルを nginx が直接返したいために volume を共有してあります。\nまた、冗長化、負荷分散として replicas: 2 として2セット立ち上げるようにしてあります。 replicas は起動後にも増減が可能です。\nnetbox は docker-entrypoint.sh で起動されるようになっています。\nnetbox の repository には docker-compose.yml が置いてあるのでこれを参考に YAML ファイルを作りました。 Docker イメージは公開されていないようなので作りました。yteraoka/netbox, yteraoka/netbox-nginx\n環境変数は文字列として定義しないとダメというのになかなか気付けずにしばらくハマってしまいました。\napiVersion:extensions/v1beta1kind:Deploymentmetadata:name:netbox-app-deploymentspec:replicas:2template:metadata:labels:app:netbox-appspec:containers:# nginx コンテナ- name:netbox-nginximage:yteraoka/netbox-nginx:1.11.10command:- nginx# 80/tcp を公開するports:- containerPort:80# アプリ側と共有する volume を /opt/netbox/netbox/static にマウントする# /static/ はここのファイルを返すように nginx.conf で指定してあるvolumeMounts:- mountPath:/opt/netbox/netbox/staticname:netbox-static-files# netbox アプリコンテナ- name:netbox-appimage:yteraoka/netbox:1.8.3# 環境変数指定env:- name:SUPERUSER_NAMEvalue:admin- name:SUPERUSER_EMAIL# netbox-secret volume から key で指定した secret の値を環境変数として指定valueFrom:secretKeyRef:name:netbox-secretkey:email_address- name:SUPERUSER_PASSWORDvalueFrom:secretKeyRef:name:netbox-secretkey:superuser_password- name:ALLOWED_HOSTSvalue:\u0026#39;*\u0026#39;- name:DB_NAMEvalue:netbox- name:DB_USERvalue:netbox- name:DB_PASSWORDvalueFrom:secretKeyRef:name:netbox-secretkey:db_password- name:DB_HOSTvalue:netbox-db- name:SECRET_KEYvalueFrom:secretKeyRef:name:netbox-secretkey:secret_key- name:EMAIL_SERVERvalue:smtp.gmail.com- name:EMAIL_PORT# ハマりポイント！環境変数は文字列として定義する必要があるので数値はクオートが必要value:\u0026#34;587\u0026#34;- name:EMAIL_USERNAMEvalueFrom:secretKeyRef:name:netbox-secretkey:email_address- name:EMAIL_PASSWORDvalueFrom:secretKeyRef:name:netbox-secretkey:email_password- name:EMAIL_TIMEOUTvalue:\u0026#34;10\u0026#34;- name:EMAIL_FROMvalueFrom:secretKeyRef:name:netbox-secretkey:email_address- name:NETBOX_USERNAMEvalue:guest- name:NETBOX_PASSWORDvalueFrom:secretKeyRef:name:netbox-secretkey:netbox_passwordvolumeMounts:- mountPath:/opt/netbox/netbox/staticname:netbox-static-filesrestartPolicy:Alwaysvolumes:- name:netbox-static-filesemptyDir:{}# netbox-secret という名前の secret を netbox-secret という volume 名で定義- name:netbox-secretsecret:secretName:netbox-secret作った YAML ですぐに Deployment を作成しても良いのですが、後に回して次の Service の定義を作成します\nService の作成 # Deployment だけではコンテナは起動するものの外部からのアクセスはもちろん Pod 間の通信すらできません。 これを可能にするために受け口である Service を作成する必要があります。\nPostgreSQL の Service\nkind:ServiceapiVersion:v1metadata:name:netbox-dbspec:selector:# 紐付ける pod (deployment) を label で指定app:netbox-dbports:- protocol:TCPport:5432targetPort:5432アプリ側の Service\nkind:ServiceapiVersion:v1metadata:name:netbox-appspec:selector:# 紐付ける pod (deployment) を label で指定app:netbox-appports:- protocol:\u0026#34;TCP\u0026#34;port:80# minikube なので node の port にマッピングします# minikube node の ephemeral port が割り当てられますtype:NodePortDeployment や Service の YAML は --- 行を挟むことで複数をひとつのファイルにまとめることができるのでDBとアプリそれぞれを1つのファイルにして netbox-db-deployment.yaml netbox-app-deployment.yaml というファイルにしました。\nService, Deployment の作成 # Secret を作成し、Service と Deployment の定義ができたのでいよいよデプロイしてみます。\nDB の起動\n$ kubectl create -f netbox-db-deployment.yaml --record service \u0026quot;netbox-db\u0026quot; created deployment \u0026quot;netbox-db-deployment\u0026quot; created アプリの起動\n$ kubectl create -f netbox-app-deployment.yaml --record service \u0026quot;netbox-app\u0026quot; created deployment \u0026quot;netbox-app-deployment\u0026quot; created describe コマンドで確認してみます\nDB の Deployment\n$ kubectl describe deployment netbox-db Name:\tnetbox-db-deployment Namespace:\tdefault CreationTimestamp:\tWed, 22 Feb 2017 23:00:03 +0900 Labels:\tapp=netbox-db Selector:\tapp=netbox-db Replicas:\t1 updated | 1 total | 1 available | 0 unavailable StrategyType:\tRollingUpdate MinReadySeconds:\t0 RollingUpdateStrategy:\t1 max unavailable, 1 max surge Conditions: Type\tStatus\tReason ----\t------\t------ Available True\tMinimumReplicasAvailable OldReplicaSets:\tNewReplicaSet:\tnetbox-db-deployment-3568492614 (1/1 replicas created) Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 1m\t1m\t1\t{deployment-controller }\tNormal\tScalingReplicaSet\tScaled up replica set netbox-db-deployment-3568492614 to 1 DB の Service\n$ kubectl describe service netbox-db Name:\tnetbox-db Namespace:\tdefault Labels:\tSelector:\tapp=netbox-db Type:\tClusterIP IP:\t10.0.0.94 Port:\t5432/TCP Endpoints:\t172.17.0.4:5432 Session Affinity:\tNone No events. アプリの Deployment\n$ kubectl describe deployment netbox-app Name:\tnetbox-app-deployment Namespace:\tdefault CreationTimestamp:\tWed, 22 Feb 2017 23:00:16 +0900 Labels:\tapp=netbox-app Selector:\tapp=netbox-app Replicas:\t2 updated | 2 total | 2 available | 0 unavailable StrategyType:\tRollingUpdate MinReadySeconds:\t0 RollingUpdateStrategy:\t1 max unavailable, 1 max surge Conditions: Type\tStatus\tReason ----\t------\t------ Available True\tMinimumReplicasAvailable OldReplicaSets:\tNewReplicaSet:\tnetbox-app-deployment-205279487 (2/2 replicas created) Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 59s\t59s\t1\t{deployment-controller }\tNormal\tScalingReplicaSet\tScaled up replica set netbox-app-deployment-205279487 to 2 アプリの Service Endpoints に2つの登録があるので2つのサーバーにアクセスが割り振られそうです。実際に Pod のログを確認すると2つに割り振られていました。 NodePort に 31631 とありますので http://192.168.99.100:31631/ で netbox にアクセスができます。 IPアドレスは minikube ip コマンドで確認できます。\n$ kubectl describe service netbox-app Name:\tnetbox-app Namespace:\tdefault Labels:\tSelector:\tapp=netbox-app Type:\tNodePort IP:\t10.0.0.214 Port:\t80/TCP NodePort:\t31631/TCP Endpoints:\t172.17.0.5:80,172.17.0.6:80 Session Affinity:\tNone No events. 前に教えてもらった minikube service list の方が便利なのでした\n$ minikube service list |-------------|----------------------|-----------------------------| | NAMESPACE | NAME | URL | |-------------|----------------------|-----------------------------| | default | kubernetes | No node port | | default | netbox-app | http://192.168.99.100:31631 | | default | netbox-db | No node port | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | http://192.168.99.100:30000 | |-------------|----------------------|-----------------------------| Pod の状況\n$ kubectl get pods NAME READY STATUS RESTARTS AGE netbox-app-deployment-205279487-1lx6p 2/2 Running 0 1h netbox-app-deployment-205279487-r2ksc 2/2 Running 0 1h netbox-db-deployment-3568492614-qwblg 1/1 Running 0 1h netbox にアクセスしてログインできました！！\nKubernetes Netbox Screenshot  あとは rolling update や healthcheck、実際の network 環境での Service 設定、ログ収集の仕組みとかいろいろあるなあ\nお掃除 # 作ったものの削除です。minikube をまるっと消すなら minikube delete で全部消えます。\nService の削除\n$ kubectl get services NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.0.0.1 443/TCP 12m netbox-app 10.0.0.117 80:32678/TCP 6m netbox-db 10.0.0.140 5432/TCP 6m $ kubectl delete service netbox-app service \u0026quot;netbox-app\u0026quot; deleted $ kubectl delete service netbox-db service \u0026quot;netbox-db\u0026quot; deleted Deployment の削除\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE netbox-app-deployment 2 2 2 0 6m netbox-db-deployment 1 1 1 1 6m $ kubectl delete deployment netbox-app-deployment deployment \u0026quot;netbox-app-deployment\u0026quot; deleted $ kubectl delete deployment netbox-db-deployment deployment \u0026quot;netbox-db-deployment\u0026quot; deleted これで Pod も削除されますが、完全に消えるまでにはちょっと時間がかかります。\nSecret の削除\n$ kubectl get secrets NAME TYPE DATA AGE default-token-8674j kubernetes.io/service-account-token 3 52m netbox-secret Opaque 6 41m $ kubectl delete secrets netbox-secret secret \u0026quot;netbox-secret\u0026quot; deleted ","date":"2017年2月22日","permalink":"/2017/02/deploy-netbox-on-minikube/","section":"Posts","summary":"Kubernetes お試し中です。Rancher で構築するべきか悩み中。 今回は YAML を書いて Deployment, Service を作成して Kubernetes 上に netbox を構築してみます。(netbox は Django + PostgreSQL のデータ","title":"Kubernetes Secrets を使って minikube に netbox を deploy してみる"},{"content":"","date":"2017年2月22日","permalink":"/tags/netbox/","section":"Tags","summary":"","title":"netbox"},{"content":"Introducing Docker Secrets Management で紹介されているパスワードなどの機密情報管理の仕組みを試してみました。\nサーバー3台を起動 # いつものように DigitalOcean で Docker 1.13 on Ubuntu 16.04 のサーバーを3台立ち上げます``` $ doctl compute droplet ls ID\tName\tPublic IPv4\tPublic IPv6\tMemory\tVCPUs\tDisk\tRegion\tImage\tStatus\tTags 39553388\tdocker1\t128.199.166.69\t2048\t2\t40\tsgp1\tUbuntu Docker 1.13.0 on 16.04\tactive 39553389\tdocker2\t128.199.166.74\t2048\t2\t40\tsgp1\tUbuntu Docker 1.13.0 on 16.04\tactive 39553390\tdocker3\t128.199.166.111\t2048\t2\t40\tsgp1\tUbuntu Docker 1.13.0 on 16.04\tactive\n ### Swarm cluster を作成する root@docker1:~# docker swarm init \u0026ndash;listen-addr eth1 \u0026ndash;advertise-addr eth1 Swarm initialized: current node (belgib92ppl2xq75aliayfpfu) is now a manager.\nTo add a worker to this swarm, run the following command:\ndocker swarm join \\\\ --token SWMTKN-1-3avyjkw67sm3c6mpkmspb1g68vn3pp2xjd60e20j591qpaingi-eo53e4b2q029frqg868n9errv \\\\ 10.130.6.155:2377  To add a manager to this swarm, run \u0026lsquo;docker swarm join-token manager\u0026rsquo; and follow the instructions.\n2377/tcp で待ち受けるよってことですが、 root@docker1:~# ufw status Status: active\nTo Action From\n 22 LIMIT Anywhere 2375/tcp ALLOW Anywhere 2376/tcp ALLOW Anywhere 22 (v6) LIMIT Anywhere (v6) 2375/tcp (v6) ALLOW Anywhere (v6) 2376/tcp (v6) ALLOW Anywhere (v6)\n開いてないので開けましょう (3台とも) # ufw allow in on eth1 proto tcp to any port 2377 Rule added Rule added (v6)\nufw allow out on eth1 proto tcp to any port 2377 # Rule added Rule added (v6)\n2台目、3台目を worker として swarm の join させる root@docker2:~# docker swarm join \u0026ndash;listen-addr eth1 \u0026ndash;advertise-addr eth1 \\\n --token SWMTKN-1-3avyjkw67sm3c6mpkmspb1g68vn3pp2xjd60e20j591qpaingi-eo53e4b2q029frqg868n9errv \\\\ 10.130.6.155:2377   This node joined a swarm as a worker.\nroot@docker3:~# docker swarm join --listen-addr eth1 --advertise-addr eth1 \\\\ \u0026gt; --token SWMTKN-1-3avyjkw67sm3c6mpkmspb1g68vn3pp2xjd60e20j591qpaingi-eo53e4b2q029frqg868n9errv \\\\ \u0026gt; 10.130.6.155:2377 This node joined a swarm as a worker. ```3台の swarm cluster ができました``` root@docker1:~# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 49tnhxfj2f935pzf0oa6hiia5 docker2 Ready Active belgib92ppl2xq75aliayfpfu \\* docker1 Ready Active Leader o71mxiuvxb1uxqzsc9n84ymud docker3 Ready Active ``` ### secret の作成 [Manage sensitive data with Docker secrets](https://docs.docker.com/engine/swarm/secrets/) にいろんな例がありますが標準入力からの文字列を登録してみます。``` root@docker1:~# echo \u0026quot;This is a secret\u0026quot; | docker secret create my\\_secret\\_data - j64qa46dokdye82drqmjiy0z7 root@docker1:~# docker secret ls ID NAME CREATED UPDATED j64qa46dokdye82drqmjiy0z7 my_secret_data About a minute ago About a minute ago\nroot@docker1:~# docker secret inspect my\\_secret\\_data \\[ { \u0026quot;ID\u0026quot;: \u0026quot;j64qa46dokdye82drqmjiy0z7\u0026quot;, \u0026quot;Version\u0026quot;: { \u0026quot;Index\u0026quot;: 21 }, \u0026quot;CreatedAt\u0026quot;: \u0026quot;2017-02-10T14:19:29.170169762Z\u0026quot;, \u0026quot;UpdatedAt\u0026quot;: \u0026quot;2017-02-10T14:19:29.170169762Z\u0026quot;, \u0026quot;Spec\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;my\\_secret\\_data\u0026quot; } } \\] ``` ### コンテナから secret にアクセスする なんでもいいけど redis のサービスを **\\--secret=\u0026quot;my\\_secret\\_data\u0026quot;** つきで作成する``` root@docker1:~# docker service create --name=\u0026quot;redis\u0026quot; --secret=\u0026quot;my\\_secret\\_data\u0026quot; redis:alpine t7pnaiho5b26uilcz58q7ej8x root@docker1:~# docker service ls ID NAME MODE REPLICAS IMAGE t7pnaiho5b26 redis replicated 1/1 redis:alpine\nroot@docker1:~# docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS sxiapghqb6ev redis.1 redis:alpine docker1 Running Running 4 minutes ago ```docker1 上で起動しているので docker1 サーバー上で **docker exec** します``` root@docker1:~# docker exec $(docker ps --filter name=redis -q) ls -l /run/secrets total 4 -r--r--r-- 1 root root 17 Feb 10 14:20 my\\_secret\\_data ```**/run/secrets/my\\_secret\\_data** というファイルが確認できます 中身を見てみます``` root@docker1:~# docker exec $(docker ps --filter name=redis -q) cat /run/secrets/my\\_secret\\_data This is a secret ```**docker secret create** に渡した **This is a secret** ができました。 コンテナの **/run/secrets** ディレクトリに secret の名前のファイルができるわけですね。 コンテナ起動中には使われている secret は削除できないようです``` root@docker1:~# docker secret rm my\\_secret\\_data Error response from daemon: rpc error: code = 3 desc = secret 'my\\_secret\\_data' is in use by the following service: redis ```コンテナから消したい場合は service の update を行う必要があるようです。``` root@docker1:~# docker service update --secret-rm=\u0026quot;my\\_secret\\_data\u0026quot; redis redis root@docker1:~# docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 9i9axnogc4cz redis.1 redis:alpine docker2 Running Running 2 seconds ago sxiapghqb6ev \\\\\\_ redis.1 redis:alpine docker1 Shutdown Shutdown 6 seconds ago ```今度は docker2 サーバーですね``` root@docker2:~# docker exec $(docker ps --filter name=redis -q) cat /run/secrets/my\\_secret\\_data cat: can't open '/run/secrets/my\\_secret\\_data': No such file or directory ```当該ファイルにアクセスできなくなりました ### secret 名を別名で渡す [Manage sensitive data with Docker secrets](https://docs.docker.com/engine/swarm/secrets/) の例に Wordpress + MySQL のものがあります。``` $ docker service create \\\\ --name mysql \\\\ --replicas 1 \\\\ --network mysql\\_private \\\\ --mount type=volume,source=mydata,destination=/var/lib/mysql \\\\ --secret source=mysql\\_root\\_password,target=mysql\\_root\\_password \\\\ --secret source=mysql\\_password,target=mysql\\_password \\\\ -e MYSQL\\_ROOT\\_PASSWORD\\_FILE=\u0026quot;/run/secrets/mysql\\_root\\_password\u0026quot; \\\\ -e MYSQL\\_PASSWORD\\_FILE=\u0026quot;/run/secrets/mysql\\_password\u0026quot; \\\\ -e MYSQL\\_USER=\u0026quot;wordpress\u0026quot; \\\\ -e MYSQL\\_DATABASE=\u0026quot;wordpress\u0026quot; \\\\ mysql:latest $ docker service create \\ \u0026ndash;name wordpress \\ \u0026ndash;replicas 1 \\ \u0026ndash;network mysql_private \\ \u0026ndash;publish 30000:80 \\ \u0026ndash;mount type=volume,source=wpdata,destination=/var/www/html \\ \u0026ndash;secret source=mysql_password,target=wp_db_password,mode=0400 \\ -e WORDPRESS_DB_USER=\u0026ldquo;wordpress\u0026rdquo; \\ -e WORDPRESS_DB_PASSWORD_FILE=\u0026quot;/run/secrets/wp_db_password\u0026quot; \\ -e WORDPRESS_DB_HOST=\u0026ldquo;mysql:3306\u0026rdquo; \\ -e WORDPRESS_DB_NAME=\u0026ldquo;wordpress\u0026rdquo; \\ wordpress:latest\n","date":"2017年2月10日","permalink":"/2017/02/docker-1-13-secrets/","section":"Posts","summary":"Introducing Docker Secrets Management で紹介されているパスワードなどの機密情報管理の仕組みを試してみました。 サーバー3台を起動 # いつものように DigitalOcean で Docker 1.13 on Ubuntu 16.04 のサーバーを3","title":"docker 1.13 の secrets を試す"},{"content":"もう10年近く使っていた東芝の加湿器の湿度センサーがお亡くなりになったのか一晩中フル稼働して窓から結露の水が外に流れていくほどになっていたので買い換えました。加湿器なしでは乾燥がひどくて喉がつらい。 東芝加湿器 KA-E55DX 去年の冬には安かったのでシャープの HV-E50 を子供部屋用に買いました。    今回はその時ヨドバシでどっちにしようか悩んだダイニチの加湿器を購入しました。確か一番売れてるとか言っていた。 ダイニチハイブリッド式加湿器（木造8.5畳まで/プレハブ洋室14畳まで）    この3つの加湿器はどれもおぼ同じ作りで、何が違うの？って感じですが、東芝のは電源ケーブルが取り外し可能でかつマグネットで本体にくっつくタイプなので足を引っ掛けたりした場合も倒して水浸しになることを避けることができました。 シャープのにはプラズマクラスターというどうでもよいものがついてます。 ダイニチのはとっても静音です。通常運転でも静かなのにさらに静かな静音モードがついてます。\nただ、いかんせん作りが安っぽい・・・そこが残念\nまあ外観は悪くないですけどね 静かなのが欲しい人にはおすすめです。\n","date":"2017年2月1日","permalink":"/2017/02/i-bought-dainichi-humidifier/","section":"Posts","summary":"もう10年近く使っていた東芝の加湿器の湿度センサーがお亡くなりになったのか一晩中フル稼働して窓から結露の水が外に流れていくほどになっていたの","title":"ダイニチの加湿器を買ってみた"},{"content":"","date":"2017年2月1日","permalink":"/tags/%E5%8A%A0%E6%B9%BF%E5%99%A8/","section":"Tags","summary":"","title":"加湿器"},{"content":"Docker を Kubernetes や Swarm でサービスを実行する場合は JSON や YAML などで事前に定義したものを実行するだけなので知っているけどそれほど脅威ではありませんでしたが docker を便利コマンドや batch として自由に任意のコマンドを実行したいと言われるととたんに -v /:/rootfs とか、--privileged されたらどうしようという壁にぶち当たるわけです。 しかし、安全に使えたら便利なのでなんとかならないかなと CentOS 7 の公式リポジトリの Docker なら SELinux でなんとかする方法があるかもしれないなと調べてみました。 公式リポジトリの docker は古いんじゃないかと疑ってましたが 1.12.5 だったのでこれなら悪くない。クラスタの方では 1.11.1 使ってるし。``` $ rpm -q docker docker-1.12.5-14.el7.centos.x86_64\n* [https://github.com/projectatomic/docker-storage-setup](https://github.com/projectatomic/docker-storage-setup) * [第8章 DOCKER フォーマットコンテナーを使用したストレージの管理](https://access.redhat.com/documentation/ja/red-hat-enterprise-linux-atomic-host/7/paged/getting-started-with-containers/chapter-8-managing-storage-with-docker-formatted-containers) kickstart の後処理で SELinux を無効にしてしまっていたので戻します。ごめんなさい。🙇``` $ sudo sed -i 's/^SELINUX=.\\*$/SELINUX=enforcing/' /etc/selinux/config $ sudo touch /.autorelabel $ sudo shutdown -r now ```さて、SELinux が有効な docker 環境が出来たので早速例のコマンドを試してみます。 (実際には SELinux を有効にした後に docker のインストール以降を行っています)``` \\# docker run -it -v /:/rootfs ubuntu bash root@c78efbd64991:/# cat /rootfs/etc/shadow cat: /rootfs/etc/shadow: Permission denied root@c78efbd64991:/# echo test \u0026gt;\u0026gt; /rootfs/etc/passwd bash: /rootfs/etc/passwd: Permission denied ```/etc/passwd には書き込めないし、/etc/shadow は中身を見ることすらできません。素晴らしい。 audit.log を確認してみる``` \\# grep denied /var/log/audit/audit.log type=AVC msg=audit(1485879393.821:7029): avc: denied { read } for pid=10530 comm=\u0026quot;cat\u0026quot; name=\u0026quot;shadow\u0026quot; dev=\u0026quot;dm-0\u0026quot; ino=67685 scontext=system\\_u:system\\_r:svirt\\_lxc\\_net\\_t:s0:c217,c489 tcontext=system\\_u:object\\_r:shadow\\_t:s0 tclass=file type=AVC msg=audit(1485879415.350:7030): avc: denied { append } for pid=10500 comm=\u0026quot;bash\u0026quot; name=\u0026quot;passwd\u0026quot; dev=\u0026quot;dm-0\u0026quot; ino=67691 scontext=system\\_u:system\\_r:svirt\\_lxc\\_net\\_t:s0:c217,c489 tcontext=system\\_u:object\\_r:passwd\\_file\\_t:s0 tclass=file ```**しかーし！**これでも **\\--privileged** をつけてしまうと制限が効かなくなり /etc/shadow だって書き換えられちゃいます。libc.so だって消せちゃう。(昔々、何を思ったのかこれを消しちまった人がいるんですよー、なぁにぃ？！やっちまったなぁ！)``` \\# docker run -it -v /:/rootfs --rm --privileged ubuntu bash root@36103222099e:/# cd /rootfs/ root@36103222099e:/rootfs# echo aaa \u0026gt; etc/shadow root@36103222099e:/rootfs# cat etc/shadow aaa root@36103222099e:/rootfs# rm lib64/libc-2.17.so root@36103222099e:/rootfs# ```きゃー😱 ところで、docker.com にあるインストール手順などでは **usermod -aG docker username** で docker グループに参加させれば docker.sock にアクセスできて sudo 要らずで便利だよと書かれている。しかしながら Enterprise 仕様の RHEL とその仲間の Fedora, CentOS ではこれが出来ません。``` \\# ls -l /var/run/docker.sock srw-rw----. 1 root root 0 Feb 1 11:18 /var/run/docker.sock ```証跡が残らないので sudo を使えという方針のようです。 * [Why we don't let non-root users run Docker in CentOS, Fedora, or RHEL](http://www.projectatomic.io/blog/2015/08/why-we-dont-let-non-root-users-run-docker-in-centos-fedora-or-rhel/) **sudo** を使おうが使うまいが例のオプションが指定できてしまっては困るのでなんとか防ぐ方法を見つけたい。**man sudoers** を読んでみた。「**!**」をつければ除外できるようである。``` %users ALL=(root) NOPASSWD: /usr/bin/docker, \\\\ !/usr/bin/docker \\* --privileged \\*, \\\\ !/usr/bin/docker \\* --security-opt\\* \\*, \\\\ !/usr/bin/docker \\* --cap-add\\* \\* ```こうすることで **\\--privileged** をつけては docker コマンドを実行できなくなりました``` $ sudo docker run -it --rm -v /:/rootfs --privileged ubuntu bash Sorry, user ytera is not allowed to execute '/bin/docker run -it --rm -v /:/rootfs --privileged ubuntu bash' as root on localhost.localdomain. ```いぇい！😎 抜け道があったらぜひ教えてください ただ、SELinux を有効にしたことで次のように``` $ mkdir $HOME/work $ sudo docker run -it --rm -v $HOME/work:/work ubuntu bash root@8dffed1e3583:/# ls work ls: cannot open directory 'work': Permission denied root@8dffed1e3583:/# echo test \u0026gt; work/test.txt bash: work/test.txt: Permission denied root@8dffed1e3583:/# ```全然ホスト側のディスクにアクセスできません 😢 (/etc 配下は ReadOnly でアクセスできる) コンテナ間で共有したいのであれば volume を使うのが良いと思います (**docker volume create volume-name**) どうしてもホスト側のディレクトリをマウントしたいという場合は **chcon** で **svirt\\_sandbox\\_file\\_t** という SELinux security context に設定します``` $ ls -ldZ work drwxr-xr-x. ytera users unconfined\\_u:object\\_r:user\\_home\\_t:s0 work $ chcon -t svirt\\_sandbox\\_file\\_t work $ ls -ldZ work drwxr-xr-x. ytera users unconfined\\_u:object\\_r:svirt\\_sandbox\\_file\\_t:s0 work $ sudo docker run -it --rm -v $HOME/work:/work ubuntu bash root@4f4cb02951b3:/# ls work root@4f4cb02951b3:/# echo test \u0026gt; work/test.txt root@4f4cb02951b3:/# cat work/test.txt test root@4f4cb02951b3:/# exit $ cat work/test.txt test $ ls -lZ work/test.txt -rw-r--r--. root root system\\_u:object\\_r:svirt\\_sandbox\\_file\\_t:s0 work/test.txt ```relabel とかも機能するようにするには **semanage** で設定します。 参考資料 * [Practical SELinux and Containers](http://www.projectatomic.io/blog/2016/03/dwalsh_selinux_containers/) * [Using Volumes with Docker can Cause Problems with SELinux](http://www.projectatomic.io/blog/2015/06/using-volumes-with-docker-can-cause-problems-with-selinux/) * [コンテナーセキュリティーガイド](https://access.redhat.com/documentation/ja/red-hat-enterprise-linux-atomic-host/7/paged/container-security-guide/) * [SELinux と Docker と OpenShift v3](http://qiita.com/nak3/items/361b62595601828bd354) * [Docker privileged オプションについて](http://qiita.com/muddydixon/items/d2982ab0846002bf3ea8)","date":"2017年2月1日","permalink":"/2017/02/docker-with-selinux/","section":"Posts","summary":"Docker を Kubernetes や Swarm でサービスを実行する場合は JSON や YAML などで事前に定義したものを実行するだけなので知っているけどそれほど脅威ではありませんでしたが docker を便","title":"Docker 使うなら石川さんごめんなさいしてる場合ではない"},{"content":"","date":"2017年2月1日","permalink":"/tags/selinux/","section":"Tags","summary":"","title":"SELinux"},{"content":"今回は Rancher Server を HA 構成でセットアップしてみます。\nhttps://docs.rancher.com/rancher/v1.3/en/installing-rancher/installing-server/#multi-nodes これまた簡単でした。\n前回試した Quick Start Guide では\ndocker run -d --restart=unless-stopped -p 8080:8080 rancher/server とするだけでした。この Container の中で MySQL も稼働しているのですが、HA 構成とするためには複数のサーバーで共有するための MySQL サーバーを別途用意し、サーバーはお互いに 9345/tcp で通信できるようにします。ただそれだけで、複数台のどのサーバーにアクセスしても良い状態となるためこれらを Load Balancer に入れます。 今回も DigitalOcean です。OS は Ubuntu 16.04 サーバー間の通信には Private Network を使うこととします。\nMySQL サーバーのセットアップ # MySQL サーバーのインストール\n$ sudo apt-get install mysql-server $ sudo sed -i 's/bind-address.*/bind-address = 0.0.0.0/' /etc/mysql/mysql.conf.d/mysqld.cnf $ sudo systemctl restart mysql firewall 設定\n$ sudo ufw allow ssh $ sudo ufw enable $ sudo ufw allow from 10.130.0.0/16 to any port mysql MySQL の DB とユーザーを作成\nmysql\u0026gt; CREATE DATABASE IF NOT EXISTS cattle COLLATE = 'utf8_general_ci' CHARACTER SET = 'utf8'; mysql\u0026gt; GRANT ALL ON cattle.* TO 'cattle'@'%' IDENTIFIED BY 'cattle-pass'; mysql\u0026gt; GRANT ALL ON cattle.* TO 'cattle'@'localhost' IDENTIFIED BY 'cattle-pass'; Rancher Server の起動 # $ sudo docker run -d \\ --restart=unless-stopped \\ -p 8080:8080 \\ -p 9345:9345 \\ rancher/server \\ --db-host 10.130.37.252 \\ --db-port 3306 \\ --db-user cattle \\ --db-pass cattle-pass \\ --db-name cattle \\ --advertise-address $(curl -s http://169.254.169.254/metadata/v1/interfaces/private/0/ipv4/address)  外部の MySQL サーバーを使うように指定 (--db-*) お互いの通信のために expose する port に 9345 が追加 (-p 9345:9345) お互いの通信のためにここに接続しろと DB に登楼するアドレスを --advertise-address で指定 (private address を DigitalOcean の metadata API から取得している)  mysql\u0026gt; select id, name, uuid, heartbeat, config, hex(clustered) from cluster_membership\\G *************************** 1. row *************************** id: 5 name: NULL uuid: 772d6569-11ba-416a-a057-2dc7693d4571 heartbeat: 1485617842672 config: {\u0026quot;advertiseAddress\u0026quot;:\u0026quot;10.130.48.220:9345\u0026quot;,\u0026quot;httpPort\u0026quot;:8080,\u0026quot;clustered\u0026quot;:true} hex(clustered): 1 *************************** 2. row *************************** id: 6 name: NULL uuid: 4b7a0e7d-c31c-4dbe-904b-461204c7d017 heartbeat: 1485617846379 config: {\u0026quot;advertiseAddress\u0026quot;:\u0026quot;10.130.48.23:9345\u0026quot;,\u0026quot;httpPort\u0026quot;:8080,\u0026quot;clustered\u0026quot;:true} hex(clustered): 1 2 rows in set (0.00 sec) MySQL よくわかんないけど clustered の値がなんかおかしい？ bit 型って mysql コマンドでは普通にテキストで表示できないのかな？\nmysql\u0026gt; desc cluster_membership; +-----------+--------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-----------+--------------+------+-----+---------+----------------+ | id | bigint(20) | NO | PRI | NULL | auto_increment | | name | varchar(255) | YES | MUL | NULL | | | uuid | varchar(128) | NO | UNI | NULL | | | heartbeat | bigint(20) | YES | | NULL | | | config | mediumtext | YES | | NULL | | | clustered | bit(1) | NO | | b'0' | | +-----------+--------------+------+-----+---------+----------------+ 6 rows in set (0.01 sec) 2つの Rancher Server を起動した状態です。Admin → High Availability で確認ができます。\nRancher HA  今回はロードバランサーの設定は行っていませんが、どちらのサーバー (8080/tcp) へアクセスしても問題ありませんでした。一方で Kubernetes クラスタの追加を行って、そのセットアップ状況をもう一方で確認することができました。\nまとめ # MySQL の冗長化は MHA などで別途対応する必要がありますが、Rancher Server の冗長化はとても簡単に行えることが確認できました。次は Node が突然死んだ場合とか、メンテナンスのために止めたいときにそこで動いているコンテナがどうなるのかを確認したいと思います。\n","date":"2017年1月28日","permalink":"/2017/01/rancher-on-digitalocean-part2/","section":"Posts","summary":"今回は Rancher Server を HA 構成でセットアップしてみます。 https://docs.rancher.com/rancher/v1.3/en/installing-rancher/installing-server/#multi-nodes これまた簡単でした。 前回試した Quick Start Guide では docker run -d --restart=unless-stopped -p 8080:8080 rancher/server とするだけでした。この Container の中で MySQL も稼働し","title":"DigitalOcean にて Rancher を試す - その2 (HA構成)"},{"content":"LINE MOBILE を使ってみようかなと思って IIJmio から乗り換えることにした。 FREETEL の 200kbps であれば3日間制限とかなく 20GB まで使い放題ってのも魅力ではあったものの LINE にしてみた。FREETEL はとりかえ〜るなんてのも魅力的ではある。 IIJmio はパケットの追加が割高なんですよね。息子が入院した際にテザリングで YouTube 見せてたら使いきっちゃって、さらに3日の通信量制限にもひっかかってしまった。この3日の制限は非常に厳しくてまったくもってまともにブラウジングもできない、パケットを追加購入するのも困難なほどです。Google Play Music とか Amazon Prime Music を 200kbps で垂れ流そうとしたときにもそんな制限を知らなくてひっかかってしまった。 端末は2015年に買った docomo SO-01G (Xperia Z3) もう Android 7 に更新できなくて辛い。ハードウェア的に不満は無いのに。 IIJmio は2014年10月に au から MNP で乗り換えてました。 ちなみに LINE MOBILE は2017年2月1日から400円追加で必要になるようです。 初期費用の改定について（SIMカード発行手数料400円が2/1より追加） さて、申し込みページにアクセスしてみると謎の「エントリーコード」なるものが存在するようです。\n ググッてみると Amazon でLINEモバイル 音声通話SIMエントリーパックというものが購入できるようです。 このエントリーパックは990円で購入することになるわけですが、これを使うと契約事務手数料3,000円が不要になるため2,000円ほど安くなります。 というわけで、さっそくこれを購入しました。 1月21日の午後に注文して22日に到着しました。 さて、それでは申し込もうと思ったところ IIJ から MNP の予約番号を取得する必要があるのを忘れてたことに気づきました。 するとなんと MNP 転出手数料として3,000円必要とあるじゃないですか！！！マジか、盲点だったわー。 でもまあ仕方ないかと・・・ IIJからの転出 (https://www.iijmio.jp/hdd/miofone/mnp.jsp#portout) そんでまた「IIJmioユーザーは要注意！MNP転出に15日もかかる！」 てなブログを見つけてマジか！？と思ったわけですが今回はとってもスムーズに切り替えできましたよ。 22日の夜に予約番号発行の申請をし、23日の夜に発行されました。 そこからすぐに23日中に LINE MOBILE の申し込みを済ませました。 申し込みに必要な情報は次のようになっています。本人確認資料をカメラで取ったりしてアップロードする必要があります。 クレジットカードが必要、デビットカードは不可とありますが LINE PAY でも良いらしい。\n 翌日の24日には SIM カード発送の通知メールが届きました。そして25日には SIM カードが届きました。\n SIM カードが届いたら LINE MOBILE の「MNP開通受付窓口」に電話をすると2時間後くらいに開通します。 この窓口の受付時間が「10:00～19:00 (年中無休)」なので帰宅後では間に合わなかったので26日に電話して無事切り替えが完了しました。 エントリーパックの購入は除外して LINE MOBILE への申し込み(23日)から開通(26日)で切り替え完了でした。 切替えに伴う通信ができない時間もわずかでスムーズでした。（ちょうどタイミング悪く切り替え完了後に私が SIM カードを入れ替えてなくて、妻からの発熱で辛いという連絡が受け取れなかったという問題はありましたが\u0026hellip;） IIJmio に切り替えた時は BIC カメラのカウンターへ行ったけど店側も慣れていなくってちょーー待たされたのを思い出した。 今はきっとそんなことは無いと思いますが。 IIJmio では「みおふぉんダイアル」を使っていたけど、IIJmio 契約がないと使えないので「楽天でんわ」に切り替えました。\n  LINE 通話もパケットカウントされないっていうのが LINE MOBILE の売りなんだけど、普通の電話とちがってこれだと相手側の通信料も気にしないといけなくって困ったなという感じです。LINE Out を今度試してみよう。\n ","date":"2017年1月26日","permalink":"/2017/01/switched-to-line-mobile/","section":"Posts","summary":"LINE MOBILE を使ってみようかなと思って IIJmio から乗り換えることにした。 FREETEL の 200kbps であれば3日間制限とかなく 20GB まで使い放題ってのも魅力ではあったものの LINE にして","title":"IIJmio から LINE Mobile へ MNP で乗り換えた"},{"content":"","date":"2017年1月26日","permalink":"/tags/line/","section":"Tags","summary":"","title":"LINE"},{"content":"","date":"2017年1月26日","permalink":"/tags/mnp/","section":"Tags","summary":"","title":"MNP"},{"content":"","date":"2017年1月26日","permalink":"/tags/mvno/","section":"Tags","summary":"","title":"MVNO"},{"content":"","date":"2017年1月23日","permalink":"/tags/gzip/","section":"Tags","summary":"","title":"gzip"},{"content":"","date":"2017年1月23日","permalink":"/tags/lambda/","section":"Tags","summary":"","title":"Lambda"},{"content":"","date":"2017年1月23日","permalink":"/tags/nodejs/","section":"Tags","summary":"","title":"nodejs"},{"content":"AWS Lambda で Application Load Balancer (ALB) のログを Elasticsearch へ投入して Kibana でビジュアライズしようと思い nodejs で Lambda Function を書いてみました。 https://github.com/awslabs/amazon-elasticsearch-lambda-samples/blob/master/src/s3_lambda_es.js ここにサンプルがあって、おっ？これはほぼこのまま使えるのか？って思ってやってみたら\n 使われている clf-parser は Apache の Common Log Format にしか対応していない Elasticsearch への投入に Bulk API を使ってない、Lambda には時間制限があるのに1行ずつ登録していては時間が掛かり過ぎる S3 のファイルが gzip されてない前提  という残念な感じだった\u0026hellip; 初めて書く nodejs でよくわからないながらも stream で処理するようにしてみてやっとできたと思って喜んでみたのもつかの間、何故か 1,000 アクセス、10,000 アクセスしてみても 170 件程度しか Elasticsearch に入らないのです。 あら？なにか stream の使い方がおかしいのかな？と思いファイルをまるっと処理するようにしてみたけど結果が変わらず\u0026hellip; さらに調査を進めると同じファイルは常に同じ件数しか登録されない、そもそも gzip から取り出せてないということに気づきました。 もしかしてこれはあれじゃないか？ということでその gzip ファイルを od コマンドで開いて見たらやっぱり複数の gzip chunk が1つのファイルに連結されていました。そして AWS Lambda で使われている nodejs 4.3.2 では zlib がこれに対応していなかったのです\u0026hellip; orz``` $ echo 123 | gzip \u0026gt; a.gz $ echo 456 | gzip \u0026raquo; a.gz $ echo 789 | gzip \u0026raquo; a.gz\n\u0026gt; [@yteraoka](https://twitter.com/yteraoka) これですかね。Node 6.0.0 からの模様。 [https://t.co/BeC9SWf2F8](https://t.co/BeC9SWf2F8) [https://t.co/WfqUtatYYh](https://t.co/WfqUtatYYh) \u0026gt; \u0026gt; — Shuhei Kagawa (@shuheikagawa) [2017年1月23日](https://twitter.com/shuheikagawa/status/823499642092613633) Lambda は python で書き直しましたとさ。 みんな困ってないのかな？ AWS のサポートにリクエストしておこう。","date":"2017年1月23日","permalink":"/2017/01/concatinated-gzip-with-nodejs/","section":"Posts","summary":"AWS Lambda で Application Load Balancer (ALB) のログを Elasticsearch へ投入して Kibana でビジュアライズしようと思い nodejs で Lambda Function を書いてみました。 https://github.com/awslabs/amazon-elasticsearch-lambda-samples/blob/master/src/s3_lambda_es.js ここにサンプルがあって、おっ？これはほぼこのまま","title":"nodejs がサポートしてなかった結合 gzip ファイル"},{"content":"いつか使いたくなった時のためにメモ。 nginx には ngx_http_limit_req_module というモジュールがあり、リクエストレートをコントロールすることができる。\nlimit_req_zone $binary_remote_addr zone=one:10m rate=1r/s; とすればクライアントのIPアドレス単位でリクエストのレートがコントロールされる\n$binary_remote_addr は 192.168.1.1 のような文字列として扱うよりもメモリを節約できる。zone=one:10m は one という名前で10MBの管理領域を確保する。rate=1r/s で $binary_remote_addr あたり1秒に1リクエストしか処理しないように制限する。 このルールを使って実際に制限するには server 内で limit_req zone=zone1 burst=1; などと設定する。location で制限する場所を指定することもできるburst=1 によって同時アクセスが1までであれば r/s で指定したレートに落としてレスポンスを返すようになる。同時に2つ以上のリクエストが来ると limit_req_status で設定したコードを返す。デフォルトは 503 (Service Unavailable) を返す。limit_req_status 429 として 429 (Too Many Requests) を返すのが良いのではないかと思う。 burst を設定しない場合は r/s を超える頻度でのアクセスには即座に拒否を返す。 IP アドレスだけでの制限であった場合、通常 HTML から CSS や画像を呼び出すのでこれらは除外しておかないと制限にかかりまくります。redirect で別の path へとか、http から https への redirect でも制限にかかってしまいます。 制限に使用する key は自由に指定可能で、その値が空だった場合は制限がかからない。 IPアドレスだけでは携帯キャリアの proxy を経由したアクセスなどが制限されまくりということになってしまうため、利用者単位で制限をかけられることが望ましい。 アプリが session id を cookie で扱っていればこれを使うことができる。API などにクライアントに識別子を送ってくる場合はそれを使うこともできる。 cookie であれば $cookie_xxx, QUERY_STRING であれば $arg_xxx が使える。 ブラウザがなにかおかしな挙動をして同一 URL に対して大量のアクセスをしてくるものを防ぎたい場合の例。\nhttp { ... limit_req_zone $limit_key zone=zone1:100m rate=5r/s; limit_req_status 429; upstream backend { server ...; ... } server { ... set $limit_key \u0026#34;\u0026#34;; # sessionid cookie があれば url + sessionid 単位で  if ($cookie_sessionid) { set $limit_key $cookie_sessionid$uri; } # さらに xxx パラメータがあればそれも key に追加  if ($arg_xxx) { set $limit_key $cookie_sessionid$uri$arg_xxx; } location /some/limit/path { limit_req zone=zone1 burst=1; ... proxy_pass http://backend; } } } ","date":"2017年1月22日","permalink":"/2017/01/ngx_http_limit_req_module/","section":"Posts","summary":"いつか使いたくなった時のためにメモ。 nginx には ngx_http_limit_req_module というモジュールがあり、リクエストレートをコントロールすることができる。 limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s; とすればクライア","title":"ngx_http_limit_req_module でリクエストレートをコントロール"},{"content":"","date":"2017年1月16日","permalink":"/tags/prometheus/","section":"Tags","summary":"","title":"Prometheus"},{"content":"前回の「Prometheus + Grafana + cAdvisor で Docker container のリソースモニタリング」では prometheus.yml に直接ターゲットサーバーのリストを書きましたが、file_sd_config と consul_sd_config を使ってみます。\nfile_sd_config # ファイルを使った Service Discovery は次のように file_sd_configs に複数の JSON または YAML ファイルを指定します。ひとつだけ「\\」を含んだ some_dir/*.yml と glob っぽく指定することも可能です。ファイルフォーマットはファイル名の拡張子から判断されます。\n- job_name:\u0026#39;myapp\u0026#39;metrics_path:\u0026#39;/api/prometheus\u0026#39;file_sd_configs:- files:- \u0026#39;tgroups/myapp1-servers.yml\u0026#39;- \u0026#39;tgroups/myapp2-servers.yml\u0026#39;ファイルの中身は static_config で、次のようなフォーマットで記述します。\n[ { \u0026#34;targets\u0026#34;: [ \u0026#34;\u0026#34;, ... ], \u0026#34;labels\u0026#34;: { \u0026#34;\u0026#34;: \u0026#34;\u0026#34;, ... } }, ... ] YAML ではこのようになります。\n- targets:- 192.168.2.95:31234- 192.168.2.96:32768- 192.168.2.97:35395labels:aaa:AAAbbb:BBBなんだファイルに書くなら prometheus.yml と何が違うんだ？というおとになりますが、fsnotify で変更を検知して自動でリロードしてくれます。また、対象のリストを含むだけなので自動更新するのも簡単です。次に説明する consul_sd_config で consul から直接リストを取り出せますが、それは consul の service 部分だけなので node の情報を使いたい場合は consul-template を使うなどして更新することができます。 他のリソースから cron などで更新しても良いです。\nconsul_sd_config # Consul の catalog API の services, service を使ってターゲットを更新します。\nindex と wait パラメータを使って変更があるまで最大で wait の秒数待つけど、その間に変更があったらすぐにレスポンスが変えるようになっています。\nservice API では Consul から次のようなレスポンスが返ります。\n[ { \u0026#34;Node\u0026#34;: \u0026#34;docker-nodeqa1\u0026#34;, \u0026#34;Address\u0026#34;: \u0026#34;192.168.2.97\u0026#34;, \u0026#34;ServiceID\u0026#34;: \u0026#34;docker-host1:myapp_1:8000\u0026#34;, \u0026#34;ServiceName\u0026#34;: \u0026#34;myapp\u0026#34;, \u0026#34;ServiceTags\u0026#34;: [], \u0026#34;ServiceAddress\u0026#34;: \u0026#34;10.0.0.8\u0026#34;, \u0026#34;ServicePort\u0026#34;: 32768, \u0026#34;ServiceEnableTagOverride\u0026#34;: false, \u0026#34;CreateIndex\u0026#34;: 9181194, \u0026#34;ModifyIndex\u0026#34;: 9181194 } ] デフォルトでは Address よりも ServiceAddress が優先されるため、上記の例では 10.0.0.8:32768 をターゲットとして登録します。 私の今回の環境ではこの ServiceAddress は Docker の内部ネットワークであり、Docker クラスタの外にある Prometheus サーバーからはアクセスできませんでした。Address を使う方法はないか？と GitHub の issue で聞いてみたら relabel_config でできるよってことだったので調べてみたら次の設定でできました。ラベルっていう名称からグルーピングに使うようなラベルだけかと思ったら違ったのですね。\n- job_name:\u0026#39;myapp\u0026#39;metrics_path:\u0026#39;/api/prometheus\u0026#39;consul_sd_configs:- server:\u0026#39;127.0.0.1:8500\u0026#39;datacenter:\u0026#39;dc1\u0026#39;services:- \u0026#39;myapp1\u0026#39;- \u0026#39;myapp2\u0026#39;relabel_configs:- source_labels:[\u0026#39;__meta_consul_address\u0026#39;,\u0026#39;__meta_consul_service_port\u0026#39;]separator:\u0026#39;:\u0026#39;regex:\u0026#39;(.*)\u0026#39;target_label:\u0026#39;__address__\u0026#39;replacement:\u0026#39;$1\u0026#39;127.0.0.1:8500 の consul サーバーに問い合わせて、myapp1、myapp2 のサービスを対象とし Address と ServiceAddress を : で join したものを regex でマッチさせ、カッコで囲んだ \u0026lsquo;$1\u0026rsquo;, \u0026lsquo;$2\u0026rsquo; などを使って target_label で指定するラベルにセットします。__address__ がターゲットとなるIPアドレスとポートになります。https://github.com/prometheus/prometheus/issues/2342\n正しくターゲットが取得できているかどうかは Prometheus UI の上部のメニューにある「Status▼」から「Targets」で確認できます。\nいーーーーーっじょう！！\n次回は Grafana でのグラフの作成方法かな。\n","date":"2017年1月16日","permalink":"/2017/01/service-discovery-of-prometheus/","section":"Posts","summary":"前回の「Prometheus + Grafana + cAdvisor で Docker container のリソースモニタリング」では prometheus.yml に直接ターゲットサーバーのリストを書きましたが、file_sd_co","title":"Prometheus の Service Discovery"},{"content":"","date":"2017年1月14日","permalink":"/tags/grafana/","section":"Tags","summary":"","title":"Grafana"},{"content":"Docker 1.11 の Swarm クラスタのリソースモニタリングをどうしようかなということでひとまず各 Docker ホストに monitoringartist/zabbix-agent-xxl-limited/ をインストールし、zabbix で一応見れるようにしていました。\n「Zabbix 3.0をDocker Composeで一度に実行する方法」にお世話になりました。\nでも zabbix に慣れないのもあるし不便だったのでずっと課題ではありました。\nDocker に限らず Prometheus は前から使いたいと思っていたし、今後 Swarm をやめて Kubernetes に切り替えようかなというのもあってやっぱり Prometheus にトライしてみようということにしました。\nPrometheus は pull 型のサーバーで、監視対象（target）に exporter という metrics を HTTP で提供する endpoint を用意する必要があります。公式の exporter は https://prometheus.io/download/ からダウンロードできます。\n blackbox_exporter haproxy_exporter memcached_exporter mysqld_exporter node_exporter statsd_exporter  が、公式には Docker コンテナの情報を返す exporter が存在しない。\n検索してみると cAdvisor が Prometheus 用の endpoint を持っているようなのでこれを各 Docker ホストで起動させます。 公式、サーボパーティ合わせて https://prometheus.io/docs/instrumenting/exporters/ ここにもっと沢山載ってました。\ncAdvisor の設定 # 設定というほどのことはなくって、Docker ホストなので cAdvisor もコンテナとして起動するだけです。 起動方法も GitHub の README.md に書いてあります。\nsudo docker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:rw \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ google/cadvisor:latest でもなぜか起動時に頻繁に panic でコケるので --restart=on-failure:30 を追加したのでした。それでも起動しなかったりするので調査中。（docker rm で device or resource busy となって rm -f でないと消せない問題なんかもあるのでコンテナではなく通常のプロセスとして起動させようと試したら今度は docker daemon がだんまりになって reboot しても直らない問題が発生したのでした\u0026hellip;）\nこれで Docker ホストのポート 8080 で Prometheus 用の metrics が提供されます。\ncurl -s http://localhost:8080/metrics で確認できます。Prometheus サーバーからもアクセスできることを確認しておきましょう。\n今回の環境は ubuntu 14.04 + docker 1.11 だったので https://github.com/google/cadvisor/blob/master/docs/running.md の Debian のところにある https://github.com/google/cadvisor/issues/432 を見て\nsudo sed -i 's/GRUB_CMDLINE_LINUX=.*/GRUB_CMDLINE_LINUX=\u0026quot;cgroup_enable=memory\u0026quot;/' /etc/default/grub sudo update-grub2 を実行してみた。\nnode_exporter の設定 # コンテナだけじゃなくてホストの状態も確認する必要があるので node_exporter も起動させます。Linu x ホストで一般的な各種メトリクスを返してくれます。対応していない特殊なメトリクスもテキストファイルに別途書き出す処理を用意してやればその値を返すようにできます。\n起動はこれも docker run するだけです。\ndocker run -d -p 9100:9100 \\ -v \u0026quot;/proc:/host/proc\u0026quot; \\ -v \u0026quot;/sys:/host/sys\u0026quot; \\ -v \u0026quot;/:/rootfs\u0026quot; \\ --net=\u0026quot;host\u0026quot; \\ --name=\u0026quot;node-exporter\u0026quot; \\ quay.io/prometheus/node-exporter \\ -collector.procfs /host/proc \\ -collector.sysfs /host/sys \\ -collector.filesystem.ignored-mount-points \u0026quot;^/(sys|proc|dev|host|etc)($|/)|docker\u0026quot; コンテナが起動している環境だとマウントポイントが山ほど出てきちゃうので -collector.filesystem.ignored-mount-points をちょっといじって docker が含まれるものを除外するようにしました。\ncurl -s http://localhost:9100/metrics で確認します。\nPrometheus の設定 # Prometheus サーバーは Go で書かれているので GitHub のリリースページ から最新版をダウンロードして展開すると prometheus というバイナリがあるので、そのディレクトリに移動して\n./prometheus とすれば起動します。デフォルトではカレントディレクトリの prometheus.yml を設定ファイルとして data ディレクトリをデータの保存場所として動作します。console_libraries, consoles もカレントディレクトリにある前提で動作するので場所を変更したい場合は引数で指定する必要があります。\nもちろん Docker Image もあるので docker run するだけでも起動します。設定ファイルを書いて指定する必要はあります。 設定ファイルはコメントを削るとデフォルトで次の様に書いてありました\nglobal:scrape_interval:15sevaluation_interval:15sexternal_labels:monitor:\u0026#39;codelab-monitor\u0026#39;scrape_configs:- job_name:\u0026#39;prometheus\u0026#39;static_configs:- targets:[\u0026#39;localhost:9090\u0026#39;]で、この scrape_configs にリストでモニタリング対象を設定します。Prometheus 自身の metrics を収集する設定がデフォルトで入っています job_name: 'prometheus'。これにならって Docker コンテナ用 (cAdvisor) の設定を追加します。\n- job_name:\u0026#39;docker\u0026#39;static_configs:- targets:- \u0026#39;docker-host1:8080\u0026#39;- \u0026#39;docker-host2:8080\u0026#39;- \u0026#39;docker-host3:8080\u0026#39;labels:group:\u0026#39;docker-container\u0026#39;labels は任意のラベルを指定できます。グラフ生成時にこれを使ってグループ化したりできます。job_name もラベルと同様に使えるので上記の様に同じ値を設定するのはあまり意味がない。 metrics を返す URL の path がデフォルトの /metrics でない場合は job_name と同じ階層に metrics_path: '/some/where' と指定します。 次に node_exporter 用の設定です\n- job_name:\u0026#39;docker-host\u0026#39;static_configs:- targets:- \u0026#39;docker-host1:9100\u0026#39;- \u0026#39;docker-host2:9100\u0026#39;- \u0026#39;docker-host3:9100\u0026#39;labels:group:\u0026#39;docker-host\u0026#39;ポート番号が違うだけですね。ラベルは適当。 また、ここでは static_configs を使っているので直接 targets に対象をつらつらを書き並べているのでホストの増減に合わせて書き換える必要があります。\nPrometheus にはいくつかの Service Discovery 機能があるためこれを利用すれば動的に監視対象を更新できます。\nhttps://prometheus.io/docs/operating/configuration/\n azure_sd_config consul_sd_config dns_sd_config ec2_sd_config file_sd_config gce_sd_config kubernetes_sd_config marathon_sd_config nerve_sd_config serverset_sd_config  consul_sd_config, file_sd_config は使ったので次のエントリで書こうと思います。\nport 9090 にブラウザでアクセスすれば Prometheus サーバーにアクセスできます。 上部のメニューにある「Status▼」から「Targets」にアクセスすると job と target のリストが確認できます。\nService Discovery で見つかった target も表示されます。\nPrometheus Targets  Grafana # の設定 Grafana は DEB も RPM も APT, YUM の repository も提供されているのでパッケージを使ってインストールします。 設定ファイルは省略（後で書くかも） いろんな認証方法に対応してますね。 Grafana は port 3000 を listen します。 起動したらブラウザでそこにアクセスします。 まずは DataSource として Prometheus を追加します。\nGrafana add datasource  Access には direct か proxy かを指定しますが。これは Prometheus へのアクセスをブラウザから直接行うか、Grafana に proxy させるかの指定です。ブラウザから直接アクセスできるかどうかとか、認証をどするかによって指定します。 次にダッシュボードを作る必要があります。一から作るのは大変そうですが collection of shared dashboards というものがあるので、ここから Docker and system monitoring をダウンロードして、「Dashboard」→「Import」で json ファイルをインポートします。 あら簡単、こんな Dashboard ができちゃいました\nGrafana doker dashboard  node_exporter を使っているので Node Exporter Server Metrics, Node exporter server stats あたりを使ってホストの状態を確認することもできます。 これらを参考にして必要に合わせてオリジナルの Dashboard を作るのが良いかと思います。各グラフのタイトル部分をクリックして「Edit」を選択すると設定内容が確認できます。 対象サーバーや対象コンテナをリストから選んで表示するようにするには Template を使います。\n次回は Consul と File の Service Discovery について書こうと思います。\n書きました「Prometheus の Service Discovery」\n","date":"2017年1月14日","permalink":"/2017/01/docker-container-resource-monitoring-using-prometheus-and-grafana-and-cadvisor/","section":"Posts","summary":"Docker 1.11 の Swarm クラスタのリソースモニタリングをどうしようかなということでひとまず各 Docker ホストに monitoringartist/zabbix-agent-xxl-limited/ をインストールし、zabbix で一応見れるようにして","title":"Prometheus + Grafana + cAdvisor で Docker container のリソースモニタリング"},{"content":"Docker 1.11 の Swarm クラスタを運用しているものの、Kubernetes に切り替えようかなと考えていて Rancher って便利なのかな？と思い Rancher Meetup Tokyo #3 に参加してきました。 Rancher Labs の VP of Sales である Shannon Williams さんが多くの導入事例を発表されました。 youtube に Case Study が沢山あるようです。 Version 2.0 では RBAC が導入されるようでエンタープライズ環境に入れやすくなりそうです。 さて、DigitalOcean のチュートリアルに How To Manage Multi-Node Deployments with Rancher and Docker Machine on Ubuntu 16.04 というものがありました。 Rancher で何ができるのかを確認するためにこれをなぞってみます。 Rancher は Server と Agent があり、どちらもコンテナとして起動し、Agent が Server に接続にいく感じらしい。\nRancher Server の起動 # チュートリアルにある手順ではブラウザから操作するようになっていますが、ここでは doctl を使います。 （サーバー1台起動させるだけなのでブラウザからの操作で十分ではあります） Docker インストール済みの Ubuntu 16.04 のイメージを使います CLI で指定するイメージ名を確認します\nytera@vaio:~$ doctl compute image list | awk '(NR == 1 || /Docker/) {print $0}' ID\tName\tType\tDistribution\tSlug\tPublic\tMin Disk 20842141\tDocker 1.12.2 on 14.04\tsnapshot\tUbuntu\tdocker\ttrue\t20 21543639\tDocker 1.12.4 on 16.04\tsnapshot\tUbuntu\ttrue\t20 21968259\tDocker 1.12.5 on 16.04\tsnapshot\tUbuntu\tdocker-16-04\ttrue\t20 docker-16-04 ですね 1GB メモリのインスタンスを起動するので size は 1gb と指定します\n$ doctl compute size list Slug\tMemory\tVCPUs\tDisk\tPrice Monthly\tPrice Hourly 512mb\t512\t1\t20\t5.00\t0.007440 1gb\t1024\t1\t30\t10.00\t0.014880 2gb\t2048\t2\t40\t20.00\t0.029760 4gb\t4096\t2\t60\t40.00\t0.059520 8gb\t8192\t4\t80\t80.00\t0.119050 16gb\t16384\t8\t160\t160.00\t0.238100 m-16gb\t16384\t2\t30\t120.00\t0.178570 32gb\t32768\t12\t320\t320.00\t0.476190 m-32gb\t32768\t4\t90\t240.00\t0.357140 48gb\t49152\t16\t480\t480.00\t0.714290 m-64gb\t65536\t8\t200\t480.00\t0.714290 64gb\t65536\t20\t640\t640.00\t0.952380 m-128gb\t131072\t16\t340\t960.00\t1.428570 m-224gb\t229376\t32\t500\t1680.00\t2.500000 region は一番近いシンガポールにするので spg1\n$ doctl compute region list | awk '(NR == 1 || /Singapore/) {print $0}' Slug\tName\tAvailable sgp1\tSingapore 1\ttrue Rancher サーバーは docker コンテナとして起動するだけなので次のスクリプトを user data として渡せばサーバーの作成とともに Rancher サーバーまで起動できます。\n#!/bin/bash docker run -d --name rancher-server -p 80:8080 rancher/server これを user-data.txt として保存します。rancher/server イメージでホストの port 80 にコンテナを port 8080 をマップして起動させるだけ。 ssh の public key 指定は doctl compute ssh-key list で ID を確認して指定します いよいよサーバーの作成です\n$ doctl compute droplet create rancher-server --image docker-16-04 --region sgp1 --size 1gb --ssh-keys 76364 --user-data-file user-data.txt --wait Notice: extracting volumes from []string{} ID\tName\tPublic IPv4\tPublic IPv6\tMemory\tVCPUs\tDisk\tRegion\tImage\tStatus\tTags 37021557\trancher-server\t128.199.xxx.yyy\t1024\t1\t30\tsgp1\tUbuntu Docker 1.12.5 on 16.04\tactive\t起動しました。\n$ doctl compute ssh rancher-server Welcome to Ubuntu 16.04.1 LTS (GNU/Linux 4.4.0-57-generic x86_64) ... Last login: Thu Jan 12 10:22:56 2017 from 124.211.xxx.yyy root@rancher-server:~# イメージの pull にすこし時間がかかりますが待っていると Rancher server が起動します\nroot@rancher-server:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES da3eda013332 rancher/server \u0026quot;/usr/bin/entry /usr/\u0026quot; 43 seconds ago Up 41 seconds 3306/tcp, 0.0.0.0:80-\u0026gt;8080/tcp rancher-server ブラウザで port 80 にアクセスしてみます。\nWelcome to Rancher  Rancher user stacks  こんな画面が表示されます。\n認証設定 # 初期状態では認証もなく誰でもアクセスできてしまうので、「ADMIN」→「Access Control」で認証設定を行います。\nRancher Access Control   Active Directory Azure AD GitHub Local OpenLDAP Shibboleth  から選択できます。\nhttps://docs.rancher.com/rancher/v1.3/en/configuration/access-control/ ここではチュートリアルに沿って GitHub 認証を設定してみます GitHub にはアカウントがすでにある前提です。https://github.com/settings/developers で新しいアプリケーションを作成します。\nGitHub Register a new OAuth application  Homepage URL, Authorization callback URL にはさきほど作成した Rancher server の URL を指定します。これでアプリケーションを作成すると Client ID, Client Secret が得られるので、これを Rancher 側に設定すれば完了です。\nRancher Access Control GitHub enabled  右下の English とあるところを 日本語 にすれば表示が日本語になります。\nRancher Access Control GitHub enabled (ja)  環境の追加 # Rancher はアプリ（コンテナ）のデプロイ先クラスタを Environment と呼び、これを複数管理することができるようです。 \u0026ldquo;dev\u0026rdquo;, \u0026ldquo;test\u0026rdquo;, \u0026ldquo;production\u0026rdquo; などをひとつの Rancher server から管理することができるということですね。 それぞれが別のクラウドサービスであってもそこから Rancher server へアクセスできれば問題なさそうです。\nここではその Environment を追加してみます。\n上部メニューの Default とあるところが Environment のメニューで、そこにある Manage Environments を選択します。\nRancher Manage Environments  Add Environment というボタンをクリック\nRancher Add Environtment  5つのテンプレートがあります Swarm は Swarmkit とあるので Docker 1.12 以降の swarm mode ですね\n   Name Orchestration Framework Networking     Cattle Cattle Network Services, Scheduler, Healthcheck Service Rancher IPsec   Kubernetes Kubernetes Network Services, Healthcheck Service Rancher IPsec   Mesos Mesos Network Services, Scheduler, Healthcheck Service Rancher IPsec   Swarm SwarmKit Network Services, Scheduler, Healthcheck Service Rancher IPsec   Windows Windows - -    テンプレートは次のようなかたちで追加することができます\nRancher Add Template  Name を k8s として Kubernetes テンプレートで Environment を作成してみました\nRancher Environment Added  Environment を作成したらそこへホストを追加して Kubernetes を構築する必要がありますが、ホストさえ用意すれば後はテンプレートに従って Rancher がやってくれます 上部メニューの Default 部分を今作成した k8s に切り替えます すると、次のような表示になりました\nRancher Setting up Kubernetes  ぐるぐる回ってますが、ここで待っててもホストがないのでセットアップは進みません。上部メニューの INFRASTRUCTURE から Hosts を選択します\nRancher Hosts  まだホストがないので Add Host で追加します\nRancher Add DigitalOcean Host  今回は DigitalOcean なのでそれを選択し、Access Token を入力して次へ進みます Access Token は https://cloud.digitalocean.com/settings/api/tokens で取得します 上の図に表示されていないクラウドサービス用の Machine Driver もあります\nRancher Machine Drivers  が、これは置いておいて DigitalOcean で進めます\nRancher Add DigitalOcean Host (2)  Name（数字部分は数にあわせてインクリメントされます）、Quantity（数）、Region、Image、Size 等を指定して Create をクリック\nRancher Add DigitalOcean Host (3)  このような感じで状態がどんどん変わってゆき、ついに Kubernetes クラスタが完成するのです\nRancher Add DigitalOcean Host (4)  ただただインスタンス（Droplet）のサイズと数を指定しただけで Kubernetes クラスタが完成しました。便利すぎる。\netcd が 3 つ必要なので 3 台用意しましたが、リソースさえ足りていれば2台でも1台でも大丈夫なようです。もちろんホストの故障を考えたら 3 台以上が必要です。\nが、、なぜか Kubernetes へのコンテナのデプロイができないな\u0026hellip; どこがおかしいのか\u0026hellip; やっぱり、こういう場合に困るよなぁ\nKUBERNETES → Dashboard から Kubernetes Dashboard へアクセスできるはずですが Service Unavailable になってしまう ちなみにこの Kubernetes Dashboard は直接外からはアクセスできないため、Rancher server が proxy するようです。 Kubernetes を動かすには Droplet のサイズが小さすぎたかな？ （後で 2GB メモリのインスタンス 3 台で試したら Kubernetes Dashboard にもアクセスできました）\nRancher Host Info  ちょっと Kubernetes 環境は置いておいて Default Environment にホストを追加してためしてみます。Default は Cattle なので単体の Docker ホストを追加して使うようです。Rancher の agent がコンテナとして稼働しています。\nカタログから Wordpress stack を追加してみました\nRancher WordPress stack  Public Port を 80 に指定してある\nRancher WordPress stack  起動しました。外からアクセスできました。ホストアドレスの Public Port で指定した port 80 でアクセスできました。\nということなので2つの Wordpress をひとつのホストでどちらも port 80 というわけにはいかないということですね。 Scheduling Services に scheduling policies が書かれていました\n port conflicts shared volumes host tagging shared network stack: –net=container:dependency strict and soft affinity/anti-affinity rules by using both env var (Swarm) and labels (Rancher)  Rancher の Load Balancer というのがどういうものか気になっている 夜も更けてきたので続きは次回\n","date":"2017年1月12日","permalink":"/2017/01/rancher-on-digitalocean-part1/","section":"Posts","summary":"Docker 1.11 の Swarm クラスタを運用しているものの、Kubernetes に切り替えようかなと考えていて Rancher って便利なのかな？と思い Rancher Meetup Tokyo #3 に参加してきました","title":"DigitalOcean にて Rancher を試す - その1"},{"content":"minikube を試す – その2 の続きです。\nデプロイメントのスケーリング # 同じアプリ(Pod)を沢山並べたい時は、数を指定するだけで増減できて、異常終了などしても指定した数をキープするように再起動してくれたりするやつですね。 前回の続きなので 1 Pod だけのこの状態です。\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 1 1 1 1 19h ここで表示されている DESIRED, CURRENT, UP-TO-DATE, AVAILABLE という状態はスケーリングに関係するものです。\n DESIRED  望んでいる指定した Pod の数   CURRENT  現状の Pod の数   UP-TO-DATE  ローリングアップデートや設定変更を行った際に更新済みとなっている Pod 数   AVAILABLE  起動処理、ヘルスチェックが完了してリクエストを受けられる状態の Pod の数    ここで DESIRED を 4 にしてみます。\n$ kubectl scale deployments/kubernetes-bootcamp --replicas=4 deployment \u0026quot;kubernetes-bootcamp\u0026quot; scaled 1秒間隔で kubectl get deployments を実行してみました。 変化のあったところだけを抜き出すと次のようになっていました。\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 1 1 1 1 19h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 4 4 4 1 19h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 4 4 4 4 19h AVAILABLE になるところは時間がかかるけれども、そこまでは一瞬で進んでしまいました。 1 node であるために docker image はすでにローカルに持っているしプロセス起動するだけだからかな。\nkubectl get pods -o wide とした方が状況が分かりやすかったです。\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE kubernetes-bootcamp-390780338-3tl9b 1/1 Running 0 6m 172.17.0.5 minikube kubernetes-bootcamp-390780338-42692 1/1 Running 0 20h 172.17.0.4 minikube kubernetes-bootcamp-390780338-6lkqt 1/1 Running 0 6m 172.17.0.6 minikube kubernetes-bootcamp-390780338-lf295 1/1 Running 0 6m 172.17.0.7 minikube minikube ssh して node へログインし、docker kill で pod のコンテナを止めてみたら自動で restart がかかり、RESTARTS の値が増えていきました。\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE kubernetes-bootcamp-390780338-3tl9b 1/1 Running 1 10m minikube kubernetes-bootcamp-390780338-42692 1/1 Running 0 20h 172.17.0.4 minikube kubernetes-bootcamp-390780338-6lkqt 1/1 Running 2 10m 172.17.0.6 minikube kubernetes-bootcamp-390780338-lf295 1/1 Running 1 10m 172.17.0.7 minikube リスタート直後はまだ IP が決まっておらず \u0026quot;\u0026quot; となっています。\n$ kubectl scale deployments/kubernetes-bootcamp --replicas=1 deployment \u0026quot;kubernetes-bootcamp\u0026quot; scaled でレプリカの数を1に戻すと deployments で見える数は瞬時に 1 に戻りますが\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 1 1 1 1 20h pods を確認するとしばらくは Terminating という状態で残っていました。\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE kubernetes-bootcamp-390780338-3tl9b 1/1 Terminating 2 12m 172.17.0.5 minikube kubernetes-bootcamp-390780338-42692 1/1 Running 0 20h 172.17.0.4 minikube kubernetes-bootcamp-390780338-6lkqt 1/1 Terminating 2 12m 172.17.0.6 minikube kubernetes-bootcamp-390780338-lf295 1/1 Terminating 1 12m 172.17.0.7 minikube 数十秒後には消えます。\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE kubernetes-bootcamp-390780338-42692 1/1 Running 0 20h 172.17.0.4 minikube ロードバランシング # kubectl expose で外部からアクセスできるようにします。\n$ kubectl expose deployment/kubernetes-bootcamp --type=\u0026quot;NodePort\u0026quot; --port 8080 service \u0026quot;kubernetes-bootcamp\u0026quot; exposed $ kubectl describe services/kubernetes-bootcamp Name:\tkubernetes-bootcamp Namespace:\tdefault Labels:\trun=kubernetes-bootcamp Selector:\trun=kubernetes-bootcamp Type:\tNodePort IP:\t10.0.0.100 Port:\t8080/TCP NodePort:\t30334/TCP Endpoints:\t172.17.0.4:8080,172.17.0.5:8080,172.17.0.6:8080 + 1 more... Session Affinity:\tNone No events. また例によって環境変数に NodePort を入れておきます\n$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}') $ echo NODE_PORT=$NODE_PORT NODE_PORT=30334 kubernetes-bootcamp のアプリは GET / で Pod 名を返しているのでいろんな Pod に振り分けられていることがわかります。単純な Roundrobin ではなさそう。\n$ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-ds1nl | v=1 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-n9pnv | v=1 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-42692 | v=1 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-n9pnv | v=1 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-42692 | v=1 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-ds1nl | v=1 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-42692 | v=1 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-n9pnv | v=1 kubectl describe services/kubernetes-bootcamp でわかるように Service の IP は 10.0.0.100 ですから minikube ssh で node に入って、そこから curl でアクセスすればバランシングされます。\ndocker@minikube:~$ curl 10.0.0.100:8080 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-ds1nl | v=1 docker@minikube:~$ curl 10.0.0.100:8080 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-42692 | v=1 docker@minikube:~$ curl 10.0.0.100:8080 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-8mnjl | v=1 docker@minikube:~$ curl 10.0.0.100:8080 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-8mnjl | v=1 docker@minikube:~$ curl 10.0.0.100:8080 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-n9pnv | v=1 docker@minikube:~$ curl 10.0.0.100:8080 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-ds1nl | v=1 minikube では type=NodePort しか使えないようですが ClusterIP (Default), NodePort, LoadBalancer http://kubernetes.io/docs/user-guide/kubectl/kubectl_expose/ expose はいろんな使い方があるようだ。\nアプリの更新 # $ kubectl describe deployments Name:\tkubernetes-bootcamp Namespace:\tdefault CreationTimestamp:\tTue, 10 Jan 2017 00:05:32 +0900 Labels:\trun=kubernetes-bootcamp Selector:\trun=kubernetes-bootcamp Replicas:\t4 updated | 4 total | 4 available | 0 unavailable StrategyType:\tRollingUpdate MinReadySeconds:\t0 RollingUpdateStrategy:\t1 max unavailable, 1 max surge Conditions: Type\tStatus\tReason ----\t------\t------ Available True\tMinimumReplicasAvailable OldReplicaSets:\tNewReplicaSet:\tkubernetes-bootcamp-390780338 (4/4 replicas created) No events. RollingUpdateStrategy: 1 max unavailable, 1 max surge とあるのでローリングアップデート時に DESIRED を 1 つ超えるだけ増やし、1 つを UNAVAILABLE にすることで許されるので 1 つ止め、追加で 1 つ新しいのを入れることで 2 つづつ置き換えていくことができます。 イメージを入れ替える v1 タグだったものを v2 に更新します\n$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2 deployment \u0026quot;kubernetes-bootcamp\u0026quot; image updated $ kubectl describe pods | grep Image: Image:\tjocatalin/kubernetes-bootcamp:v2 Image:\tjocatalin/kubernetes-bootcamp:v2 Image:\tdocker.io/jocatalin/kubernetes-bootcamp:v1 Image:\tdocker.io/jocatalin/kubernetes-bootcamp:v1 Image:\tdocker.io/jocatalin/kubernetes-bootcamp:v1 Image:\tdocker.io/jocatalin/kubernetes-bootcamp:v1 2 個ずつ更新されてるようですね\n$ kubectl describe pods | grep Image: Image:\tjocatalin/kubernetes-bootcamp:v2 Image:\tjocatalin/kubernetes-bootcamp:v2 Image:\tjocatalin/kubernetes-bootcamp:v2 Image:\tjocatalin/kubernetes-bootcamp:v2 Image:\tdocker.io/jocatalin/kubernetes-bootcamp:v1 Image:\tdocker.io/jocatalin/kubernetes-bootcamp:v1 Image:\tdocker.io/jocatalin/kubernetes-bootcamp:v1 Image:\tdocker.io/jocatalin/kubernetes-bootcamp:v1 古い Pod が入れ替わりました。Pod 名も変わりました。\n kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-2100875782-415gm 1/1 Running 0 25s kubernetes-bootcamp-2100875782-l7525 1/1 Running 0 25s kubernetes-bootcamp-2100875782-q8c18 1/1 Running 0 19s kubernetes-bootcamp-2100875782-wgtv0 1/1 Running 0 17s kubernetes-bootcamp-390780338-42692 1/1 Terminating 0 22h kubernetes-bootcamp-390780338-8mnjl 1/1 Terminating 0 2h kubernetes-bootcamp-390780338-ds1nl 1/1 Terminating 0 2h kubernetes-bootcamp-390780338-n9pnv 1/1 Terminating 0 2h Terminating はその後消えます\n$ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-2100875782-415gm 1/1 Running 0 2m kubernetes-bootcamp-2100875782-l7525 1/1 Running 0 2m kubernetes-bootcamp-2100875782-q8c18 1/1 Running 0 2m kubernetes-bootcamp-2100875782-wgtv0 1/1 Running 0 2m Service 経由でアクセスしてみます、v=2 が返って来てます。\n$ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-wgtv0 | v=2 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-l7525 | v=2 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-415gm | v=2 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-q8c18 | v=2 $ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-2100875782-415gm | v=2 ロールバック # 今度は v10 タグへの更新ですがこんなイメージファイルは存在しないようです\n$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v10 deployment \u0026quot;kubernetes-bootcamp\u0026quot; image updated 更新が始まりました、さきほどと同じく 1 つ停止して 2 つ追加されてます。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-1951388213-1b5q1 0/1 ContainerCreating 0 3s kubernetes-bootcamp-1951388213-vp1tp 0/1 ContainerCreating 0 3s kubernetes-bootcamp-2100875782-415gm 1/1 Running 0 8m kubernetes-bootcamp-2100875782-l7525 1/1 Running 0 8m kubernetes-bootcamp-2100875782-q8c18 1/1 Running 0 8m kubernetes-bootcamp-2100875782-wgtv0 1/1 Terminating 0 8m Image の取得に失敗してしまったようです\n$ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-1951388213-1b5q1 0/1 ErrImagePull 0 14s kubernetes-bootcamp-1951388213-vp1tp 0/1 ErrImagePull 0 14s kubernetes-bootcamp-2100875782-415gm 1/1 Running 0 8m kubernetes-bootcamp-2100875782-l7525 1/1 Running 0 8m kubernetes-bootcamp-2100875782-q8c18 1/1 Running 0 8m kubernetes-bootcamp-2100875782-wgtv0 1/1 Terminating 0 8m DESIRED が 4 で unavailable が 1 つの状態なのでこれ以上は進みません\n$ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-1951388213-1b5q1 0/1 ImagePullBackOff 0 2m kubernetes-bootcamp-1951388213-vp1tp 0/1 ImagePullBackOff 0 2m kubernetes-bootcamp-2100875782-415gm 1/1 Running 0 11m kubernetes-bootcamp-2100875782-l7525 1/1 Running 0 11m kubernetes-bootcamp-2100875782-q8c18 1/1 Running 0 11m kubectl describe pods でログを確認\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------------\t------- 5m\t5m\t1\t{default-scheduler }\tNormal\tScheduled\tSuccessfully assigned kubernetes-bootcamp-1951388213-1b5q1 to minikube 5m\t2m\t5\t{kubelet minikube}\tspec.containers{kubernetes-bootcamp}\tNormal\tPulling\tpulling image \u0026quot;jocatalin/kubernetes-bootcamp:v10\u0026quot; 5m\t1m\t5\t{kubelet minikube}\tspec.containers{kubernetes-bootcamp}\tWarningFailed\tFailed to pull image \u0026quot;jocatalin/kubernetes-bootcamp:v10\u0026quot;: Tag v10 not found in repository docker.io/jocatalin/kubernetes-bootcamp 5m\t1m\t5\t{kubelet minikube}\tWarningFailedSync\tError syncing pod, skipping: failed to \u0026quot;StartContainer\u0026quot; for \u0026quot;kubernetes-bootcamp\u0026quot; with ErrImagePull: \u0026quot;Tag v10 not found in repository docker.io/jocatalin/kubernetes-bootcamp\u0026quot; 5m\t7s\t17\t{kubelet minikube}\tspec.containers{kubernetes-bootcamp}\tNormal\tBackOff\tBack-off pulling image \u0026quot;jocatalin/kubernetes-bootcamp:v10\u0026quot; 5m\t7s\t17\t{kubelet minikube}\tWarning\tFailedSync\tError syncing pod, skipping: failed to \u0026quot;StartContainer\u0026quot; for \u0026quot;kubernetes-bootcamp\u0026quot; with ImagePullBackOff: \u0026quot;Back-off pulling image \\\u0026quot;jocatalin/kubernetes-bootcamp:v10\\\u0026quot;\u0026quot; さて、にっちもさっちもいかなくなりましたので元に戻さなくてはなりません kubectl rollout undo で戻せるんですって\n$ kubectl rollout undo deployments/kubernetes-bootcamp deployment \u0026quot;kubernetes-bootcamp\u0026quot; rolled back $ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-2100875782-0v3ql 0/1 ContainerCreating 0 2s kubernetes-bootcamp-2100875782-415gm 1/1 Running 0 23m kubernetes-bootcamp-2100875782-l7525 1/1 Running 0 23m kubernetes-bootcamp-2100875782-q8c18 1/1 Running 0 23m $ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-2100875782-0v3ql 1/1 Running 0 10s kubernetes-bootcamp-2100875782-415gm 1/1 Running 0 23m kubernetes-bootcamp-2100875782-l7525 1/1 Running 0 23m kubernetes-bootcamp-2100875782-q8c18 1/1 Running 0 23m 戻った\n$ kubectl describe pods | grep Image: Image:\tjocatalin/kubernetes-bootcamp:v2 Image:\tjocatalin/kubernetes-bootcamp:v2 Image:\tjocatalin/kubernetes-bootcamp:v2 Image:\tjocatalin/kubernetes-bootcamp:v2 さて、つぎは multi node な Kubernetes Cluster を作らねば。\n","date":"2017年1月10日","permalink":"/2017/01/minikube-part3/","section":"Posts","summary":"minikube を試す – その2 の続きです。 デプロイメントのスケーリング # 同じアプリ(Pod)を沢山並べたい時は、数を指定するだけで増減できて、異常終了など","title":"minikube を試す - その3"},{"content":"minikubeでKubernetesのチュートリアルをやってみた というお役立ち記事をみたので前回 (minikube でローカルでのテスト用 Kubernetes を構築)の続きをやってみる\n新しいバージョンが出てたのでまずは更新\nThere is a newer version of minikube available (v0.14.0). Download it here: https://github.com/kubernetes/minikube/releases/tag/v0.14.0 To disable this notification, add WantUpdateNotification: False to the json config file at /home/ytera/.minikube/config (you may have to create the file config.json in this folder if you have no previous configuration) $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.14.0/minikube-linux-amd64 \\ \u0026amp;\u0026amp; chmod +x minikube \\ \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ $ minikube version minikube version: v0.14.0 $ minikube get-k8s-versions The following Kubernetes versions are available: - v1.5.1 - v1.4.5 - v1.4.3 - v1.4.2 - v1.4.1 - v1.4.0 - v1.3.7 - v1.3.6 - v1.3.5 - v1.3.4 - v1.3.3 - v1.3.0 最新バージョンが 1.5.1 になってます。\n$ minikube start Starting local Kubernetes cluster... Kubectl is now configured to use the cluster. $ minikube status minikubeVM: Running localkube: Running やっぱ楽ちん\n$ minikube dashboard Opening kubernetes dashboard in default browser... 既存のブラウザ セッションに新しいウィンドウが作成されました。 kubectl も更新\n$ curl -Lo kubectl http://storage.googleapis.com/kubernetes-release/release/v1.5.1/bin/linux/amd64/kubectl \\ \u0026amp;\u0026amp; chmod +x kubectl \\ \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/ $ kubectl cluster-info Kubernetes master is running at https://192.168.99.100:8443 KubeDNS is running at https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-system/services/kube-dns kubernetes-dashboard is running at https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl get nodes NAME STATUS AGE minikube Ready 4m アプリのデプロイ # kubernets-bootcamp という名前の deployment を作成 image と port を指定するだけなのですね。昔は pod 用の YAML を用意する必要があったきがするけど。\n$ kubectl run kubernetes-bootcamp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --port=8080 deployment \u0026quot;kubernetes-bootcamp\u0026quot; created $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 1 1 1 1 37s Dashboard ではこんな感じ\nWorkload  deployment  アプリへのアクセス # kubectl run だけでは kubernetes 内に閉じているので外部から直接はアクセスできません。 kubectl proxy と実行することで kubernetes 内へアクセスするための proxy サーバーが起動します。 まずは POD 名を取得 何度も使うので環境変数に入れておきます\n$ export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\u0026quot;\\n\u0026quot;}}{{end}}') $ echo Name of the Pod: $POD_NAME Name of the Pod: kubernetes-bootcamp-390780338-42692 proxy の起動（終了までプロンプトが戻らないのでバックグラウンドで）\n$ kubectl proxy \u0026amp; [1] 9009 Starting to serve on 127.0.0.1:8001 次のような URL で proxy 経由で POD のアプリにアクセスできます\n$ curl http://localhost:8001/api/v1/proxy/namespaces/default/pods/$POD_NAME/ Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-42692 | v=1 default という namespace の kubernetes-bootcamp-390780338-42692 という pod にアクセスしてます。 でもこれは動作確認に使う程度かな？\nPod を Dashboard で見てみるとこんな感じ\nPod  Pod の情報確認 # $ kubectl get pods NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-390780338-42692 1/1 Running 0 24m $ kubectl describe pods Name:\tkubernetes-bootcamp-390780338-42692 Namespace:\tdefault Node:\tminikube/192.168.99.100 Start Time:\tTue, 10 Jan 2017 00:05:32 +0900 Labels:\tpod-template-hash=390780338 run=kubernetes-bootcamp Status:\tRunning IP:\t172.17.0.4 Controllers:\tReplicaSet/kubernetes-bootcamp-390780338 Containers: kubernetes-bootcamp: Container ID:\tdocker://a503a23e393ffa15ea98c0f0ef28e87739b6b6f55dd0c2629fe1b50dd9b5b213 Image:\tdocker.io/jocatalin/kubernetes-bootcamp:v1 Image ID:\tdocker://sha256:8fafd8af70e9aa7c3ab40222ca4fd58050cf3e49cb14a4e7c0f460cd4f78e9fe Port:\t8080/TCP State:\tRunning Started:\tTue, 10 Jan 2017 00:06:07 +0900 Ready:\tTrue Restart Count:\t0 Volume Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-t611k (ro) Environment Variables:\tConditions: Type\tStatus Initialized True Ready True PodScheduled True Volumes: default-token-t611k: Type:\tSecret (a volume populated by a Secret) SecretName:\tdefault-token-t611k QoS Class:\tBestEffort Tolerations:\tEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 25m\t25m\t1\t{default-scheduler }\tNormal\tScheduled\tSuccessfully assigned kubernetes-bootcamp-390780338-42692 to minikube 25m\t25m\t1\t{kubelet minikube}\tspec.containers{kubernetes-bootcamp}\tNormal\tPulling\tpulling image \u0026quot;docker.io/jocatalin/kubernetes-bootcamp:v1\u0026quot; 24m\t24m\t1\t{kubelet minikube}\tspec.containers{kubernetes-bootcamp}\tNormal\tPulled\tSuccessfully pulled image \u0026quot;docker.io/jocatalin/kubernetes-bootcamp:v1\u0026quot; 24m\t24m\t1\t{kubelet minikube}\tspec.containers{kubernetes-bootcamp}\tNormal\tCreated\tCreated container with docker id a503a23e393f; Security:[seccomp=unconfined] 24m\t24m\t1\t{kubelet minikube}\tspec.containers{kubernetes-bootcamp}\tNormal\tStarted\tStarted container with docker id a503a23e393f ログの確認 さっき proxy 経由でアクセスしたものが表示されてます\n$ kubectl logs $POD_NAME Kubernetes Bootcamp App Started At: 2017-01-09T15:06:07.713Z | Running On: kubernetes-bootcamp-390780338-42692 Running On: kubernetes-bootcamp-390780338-42692 | Total Requests: 1 | App Uptime: 461.145 seconds | Log Time: 2017-01-09T15:13:48.859Z docker exec 的なやつ # kubectl exec で docker exec 的なことができます コンテナ内で env コマンドを実行\n$ kubectl exec $POD_NAME env PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=kubernetes-bootcamp-390780338-42692 KUBERNETES_PORT_443_TCP_ADDR=10.0.0.1 KUBERNETES_SERVICE_HOST=10.0.0.1 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT=tcp://10.0.0.1:443 KUBERNETES_PORT_443_TCP=tcp://10.0.0.1:443 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP_PORT=443 NPM_CONFIG_LOGLEVEL=info NODE_VERSION=6.3.1 HOME=/root コンテナ内で bash を実行し、アプリ（nodejs）のファイルを覗いてみる そして curl で localhost:8080 にアクセスしてみる\n$ kubectl exec -ti $POD_NAME bash root@kubernetes-bootcamp-390780338-42692:/# head server.js var http = require('http'); var requests=0; var podname= process.env.HOSTNAME; var startTime; var host; var handleRequest = function(request, response) { response.setHeader('Content-Type', 'text/plain'); response.writeHead(200); response.write(\u0026quot;Hello Kubernetes bootcamp! | Running on: \u0026quot;); response.write(host); root@kubernetes-bootcamp-390780338-42692:/# curl localhost:8080 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-42692 | v=1 root@kubernetes-bootcamp-390780338-42692:/# exit exit いまコンテナ内からアクセスしたログも kubectl logs で確認できます\n$ kubectl logs $POD_NAME Kubernetes Bootcamp App Started At: 2017-01-09T15:06:07.713Z | Running On: kubernetes-bootcamp-390780338-42692 Running On: kubernetes-bootcamp-390780338-42692 | Total Requests: 1 | App Uptime: 461.145 seconds | Log Time: 2017-01-09T15:13:48.859Z Running On: kubernetes-bootcamp-390780338-42692 | Total Requests: 2 | App Uptime: 1982.054 seconds | Log Time: 2017-01-09T15:39:09.767Z Service を使って外部からアクセスする # まずは今の状態で service を確認してみる\n$ kubectl get services NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.0.0.1 443/TCP 46m $ kubectl describe services/kubernetes Name:\tkubernetes Namespace:\tdefault Labels:\tcomponent=apiserver provider=kubernetes Selector:\tType:\tClusterIP IP:\t10.0.0.1 Port:\thttps\t443/TCP Endpoints:\t10.0.2.15:8443 Session Affinity:\tClientIP No events.  Service  kubernetes-bootcamp deployment に対して service を作成する --type=\u0026quot;NodePort\u0026quot; とすることで node (minikube) のIPアドレスで先ほどの pod (deployment) の port 8080 にアクセスできるようになります\n$ kubectl expose deployment/kubernetes-bootcamp --type=\u0026quot;NodePort\u0026quot; --port 8080 service \u0026quot;kubernetes-bootcamp\u0026quot; exposed kubernetes-bootcamp という service が追加されました\n$ kubectl get services NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.0.0.1 443/TCP 52m kubernetes-bootcamp 10.0.0.235 8080:31168/TCP 2m describe で node のどの port で listen しているか確認できます。\n$ kubectl describe services/kubernetes-bootcamp Name:\tkubernetes-bootcamp Namespace:\tdefault Labels:\trun=kubernetes-bootcamp Selector:\trun=kubernetes-bootcamp Type:\tNodePort IP:\t10.0.0.235 Port:\t8080/TCP NodePort:\t31168/TCP Endpoints:\t172.17.0.4:8080 Session Affinity:\tNone No events.  Service (2)  Service (3)  後で使うため node の port を環境変数にいれておきます\n$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}') $ echo NODE_PORT=$NODE_PORT NODE_PORT=31168 node は minikube なのでその IP address を確認します\n$ minikube ip 192.168.99.100 node の IP address と service port がわかったところで curl で外からアクセスしてみます\n$ curl $(minikube ip):$NODE_PORT Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-390780338-42692 | v=1 アクセスできました。\nminikube service コマンド (2017/1/23追記) # minikube service というコマンドを教えてもらいました。\n@yteraoka ブログ上でコメントできるところがなかったのでこちらで失礼します。https://t.co/H0AoFVY5iI\nminikube service my-nginx --url でもアプリアクセスできるipが参照できました。\n\u0026mdash; みんちゃん🍀 (@kkam0907) January 18, 2017  minikube service list コマンドで service の URL が簡単に確認できます。\n$ minikube service list |-------------|----------------------|-----------------------------| | NAMESPACE | NAME | URL | |-------------|----------------------|-----------------------------| | default | kubernetes | No node port | | default | kubernetes-bootcamp | http://192.168.99.100:30628 | | kube-system | kube-dns | No node port | | kube-system | kubernetes-dashboard | http://192.168.99.100:30000 | |-------------|----------------------|-----------------------------| さらに minikube service kubernetes-bootcamp と service 名を指定すればブラウザでその URL を開いてくれます。minikube dashboard と同じ感じです。 ブラウザで開いてもらわなくて良い場合は --url をつけるとこう表示されます。\n$ minikube service kubernetes-bootcamp --url http://192.168.99.100:30628 ラベル # kubernetes の各リソースにはラベルがついています。 今回作成した deployment には run=kubernetes-bootcamp というラベルがついていることが describe で確認できます。 Dashboard からポチポチと各リソースを確認してみてもわかります\n$ kubectl describe deployment Name:\tkubernetes-bootcamp Namespace:\tdefault CreationTimestamp:\tTue, 10 Jan 2017 00:05:32 +0900 Labels:\trun=kubernetes-bootcamp Selector:\trun=kubernetes-bootcamp Replicas:\t1 updated | 1 total | 1 available | 0 unavailable StrategyType:\tRollingUpdate MinReadySeconds:\t0 RollingUpdateStrategy:\t1 max unavailable, 1 max surge Conditions: Type\tStatus\tReason ----\t------\t------ Available True\tMinimumReplicasAvailable OldReplicaSets:\tNewReplicaSet:\tkubernetes-bootcamp-390780338 (1/1 replicas created) Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 55m\t55m\t1\t{deployment-controller }\tNormal\tScalingReplicaSet\tScaled up replica set kubernetes-bootcamp-390780338 to 1 -l パラメーターでラベルを使ったクエリが行えます\n$ kubectl get pods -l run=kubernetes-bootcamp NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-390780338-42692 1/1 Running 0 58m Pod は1つしか作ってないので未指定の場合と変わらないけど services の方はマッチするもだけが表示されてることがわかります\n$ kubectl get services -l run=kubernetes-bootcamp NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-bootcamp 10.0.0.235 8080:31168/TCP 19m kubectl label コマンドで任意のラベルを追加することもできます\n$ kubectl describe pods $POD_NAME Name:\tkubernetes-bootcamp-390780338-42692 Namespace:\tdefault Node:\tminikube/192.168.99.100 Start Time:\tTue, 10 Jan 2017 00:05:32 +0900 Labels:\tapp=v1 pod-template-hash=390780338 run=kubernetes-bootcamp Status:\tRunning IP:\t172.17.0.4 Controllers:\tReplicaSet/kubernetes-bootcamp-390780338 ... $ kubectl get pods -l app=v1 NAME READY STATUS RESTARTS AGE kubernetes-bootcamp-390780338-42692 1/1 Running 0 1h サービスの削除 # $ kubectl delete service -l run=kubernetes-bootcamp service \u0026quot;kubernetes-bootcamp\u0026quot; deleted $ kubectl get services NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.0.0.1 443/TCP 1h 削除されました。 先ほどの NodePort にももうアクセスできません。\n$ curl $(minikube ip):$NODE_PORT curl: (7) Failed to connect to 192.168.99.100 port 31168: Connection refused その3 に続く\n","date":"2017年1月9日","permalink":"/2017/01/minikube-part2/","section":"Posts","summary":"minikubeでKubernetesのチュートリアルをやってみた というお役立ち記事をみたので前回 (minikube でローカルでのテスト用 Kubernetes を構築)の続きを","title":"minikube を試す - その2"},{"content":"$ /usr/lib/mozc/mozc_tool --mode=dictionary_tool で辞書ツールを起動して登録する 入力メソッド設定ツールの起動は\n$ fcitx-configtool ","date":"2016年11月26日","permalink":"/2016/11/ubuntu-16-04-mozc-dictionary-tool/","section":"Posts","summary":"$ /usr/lib/mozc/mozc_tool --mode=dictionary_tool で辞書ツールを起動して登録する 入力メソッド設定ツールの起動は $ fcitx-configtool","title":"Ubuntu 16.04 で mozc の辞書登録"},{"content":"Chrome や Firefox、gnome-terminal の内部で表示するフォントはそれぞれのアプリの設定で可能だが、Window Title やアプリ側で設定できない部分のフォントの変更には unity-tweak-tool を使う\n$ sudo apt-get update $ sudo apt-get install unity-tweak-tool $ unity-tweak-tool Keepass で日本語表示するために Keepass 内で変更可能なフォントを変更したら 1366x768 の画面では縦がおさまらなくなってしまったため変更したかった。\n","date":"2016年11月26日","permalink":"/2016/11/ubuntu-16-04-how-to-change-system-font/","section":"Posts","summary":"Chrome や Firefox、gnome-terminal の内部で表示するフォントはそれぞれのアプリの設定で可能だが、Window Title やアプリ側で設定で","title":"Ubuntu 16.04 でのシステムフォントの変更"},{"content":"Kubernetes を調査しようかなということで minikube を使ったセットアップを試してみる 環境は Ubuntu 16.04 の Note PC minikube は single node の Kubernetes を 1 コマンドでセットアップすることができるツールです。Windows でも使えます。 KVM または VirtualBox で仮想サーバをたてて、そこで Kubernetes 環境が構築されます。\nkubectl のインストール # Kubernetes の操作には kubectl が必要なのでまずはこれをインストール\ncurl -Lo kubectl http://storage.googleapis.com/kubernetes-release/release/v1.4.0/bin/linux/amd64/kubectl \\  \u0026amp;\u0026amp; chmod +x kubectl \\  \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/ Mac なら linux/amd64 の部分が darwin/amd64 ですかね GitHub のリリースページからまるっとダウンロードした中からも取り出せますが 1GB もダウンロードしないといけない kubectl だけ取り出すならこんな感じ\nsudo tar xvf kubernetes.tar.gz -C /usr/local/bin --strip-components=4 kubernetes/platforms/linux/amd64/kubectl sudo chmod 755 /usr/local/bin/kubectl minikube のインストール # minikube は GitHub の release ページからダウンロード https://github.com/kubernetes/minikube/releases\ncurl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.11.0/minikube-linux-amd64 \\  \u0026amp;\u0026amp; chmod +x minikube \\  \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ $ minikube version minikube version: v0.11.0 Kubernetes をセットアップ # Linux なので KVM が使えるのですが、Vagrant などで VirtualBox を使っているし、KVM 用のパッケージはまだインストールしてなかったので他の環境でも使えるしということで VirtualBox を使うことにしました。 使いたいバージョンを選んでセットアップすることができます。今回は 1.4.0 を使ってみました。\n$ minikube get-k8s-versions The following Kubernetes versions are available: - v1.5.0-alpha.0 - v1.4.0 - v1.3.7 - v1.3.6 - v1.3.5 - v1.3.4 - v1.3.3 - v1.3.0 次のコマンドだけで Kubernetes がセットアップできます\n$ minikube start Starting local Kubernetes cluster... Kubectl is now configured to use the cluster. $ minikube status minikubeVM: Running localkube: Running VM の CPU の数やメモリサイズ、ディスクサイズなども指定できます\n$ minikube start --help Starts a local kubernetes cluster using Virtualbox. This command assumes you already have Virtualbox installed. Usage: minikube start [flags] Flags: --container-runtime string The container runtime to be used --cpus int Number of CPUs allocated to the minikube VM (default 1) --disk-size string Disk size allocated to the minikube VM (format: [], where unit = b, k, m or g) (default \u0026quot;20g\u0026quot;) --docker-env stringSlice Environment variables to pass to the Docker daemon. (format: key=value) --extra-config ExtraOption A set of key=value pairs that describe configuration that may be passed to different components. The key should be '.' separated, and the first part before the dot is the component to apply the configuration to. Valid components are: kubelet, apiserver, controller-manager, etcd, proxy, scheduler. --host-only-cidr string The CIDR to be used for the minikube VM (only supported with Virtualbox driver) (default \u0026quot;192.168.99.1/24\u0026quot;) --insecure-registry stringSlice Insecure Docker registries to pass to the Docker daemon --iso-url string Location of the minikube iso (default \u0026quot;https://storage.googleapis.com/minikube/minikube-0.7.iso\u0026quot;) --kubernetes-version string The kubernetes version that the minikube VM will (ex: v1.2.3) OR a URI which contains a localkube binary (ex: https://storage.googleapis.com/minikube/k8sReleases/v1.3.0/localkube-linux-amd64) (default \u0026quot;v1.4.0+$Format:%h$\u0026quot;) --memory int Amount of RAM allocated to the minikube VM (default 1024) --network-plugin string The name of the network plugin --registry-mirror stringSlice Registry mirrors to pass to the Docker daemon --vm-driver string VM driver is one of: [virtualbox kvm] (default \u0026quot;virtualbox\u0026quot;) Global Flags: --alsologtostderr log to standard error as well as files --log_backtrace_at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log_dir string If non-empty, write log files in this directory --logtostderr log to standard error instead of files --show-libmachine-logs Whether or not to show logs from libmachine. --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level log level for V logs --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging vagrant や docker-machine の様に ssh で Kubernetes のサーバーにログインできます。\n$ minikube ssh ## . ## ## ## == ## ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _ | |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __ | '_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ '__| | |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| \u0026lt; __/ | |_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_| Boot2Docker version 1.11.1, build master : 901340f - Fri Jul 1 22:52:19 UTC 2016 Docker version 1.11.1, build 5604cbe docker@minikube:~$ Dashboard # Kubernetes の Dashboard も使えます minikube dashboard コマンドを実行すればブラウザで開いてくれます\n$ minikube dashboard Opening kubernetes dashboard in default browser... 既存のブラウザ セッションに新しいウィンドウが作成されました。 直接ローカルのブラウザで開けない場合は次のようにすればアクセスするための URL が表示されます\n$ minikube dashboard --url=true http://192.168.99.100:30000  Kubernetes Dashboard  kubectl からアクセスしてみる # kubectl でアクセスできることを確認してみます\n$ kubectl cluster-info Kubernetes master is running at https://192.168.99.100:8443 kubernetes-dashboard is running at https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;4\u0026quot;, GitVersion:\u0026quot;v1.4.0\u0026quot;, GitCommit:\u0026quot;a16c0a7f71a6f93c7e0f222d961f4675cd97a46b\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2016-09-26T18:16:57Z\u0026quot;, GoVersion:\u0026quot;go1.6.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;4\u0026quot;, GitVersion:\u0026quot;v1.4.0\u0026quot;, GitCommit:\u0026quot;a16c0a7f71a6f93c7e0f222d961f4675cd97a46b\u0026quot;, GitTreeState:\u0026quot;dirty\u0026quot;, BuildDate:\u0026quot;1970-01-01T00:00:00Z\u0026quot;, GoVersion:\u0026quot;go1.7.1\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} $ kubectl get nodes NAME STATUS AGE minikube Ready 10h $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kube-addon-manager-minikube 1/1 Running 0 10h kube-system kubernetes-dashboard-6ommh 1/1 Running 0 10h $ kubectl get namespaces NAME STATUS AGE default Active 10h kube-system Active 10h はて？特に何も設定してないのにどうして kubectl は接続先を知っているのでしょう？ どうやら minikube は ~/.kube/config も設定してくれているようです\n$ cat ~/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority: /home/ytera/.minikube/ca.crt server: https://192.168.99.100:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/ytera/.minikube/apiserver.crt client-key: /home/ytera/.minikube/apiserver.key Kubernetes の Docker daemon にアクセスしてみる # docker-machine のように次のようにすることで docker コマンドで直接 Kubernetes の Docker daemon にアクセスできます\n$ minikube docker-env export DOCKER_TLS_VERIFY=\u0026quot;1\u0026quot; export DOCKER_HOST=\u0026quot;tcp://192.168.99.100:2376\u0026quot; export DOCKER_CERT_PATH=\u0026quot;/home/ytera/.minikube/certs\u0026quot; export DOCKER_API_VERSION=\u0026quot;1.23\u0026quot; # Run this command to configure your shell: # eval $(minikube docker-env) $ eval $(minikube docker-env) $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4590a8c95cc3 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.0 \u0026quot;/dashboard --port=90\u0026quot; 10 hours ago Up 10 hours k8s_kubernetes-dashboard.17d7dac9_kubernetes-dashboard-6ommh_kube-system_af24e8e2-9274-11e6-b79f-9e8b7626e0db_d484a526 3a3e728bfb83 gcr.io/google_containers/pause-amd64:3.0 \u0026quot;/pause\u0026quot; 10 hours ago Up 10 hours k8s_POD.2225036b_kubernetes-dashboard-6ommh_kube-system_af24e8e2-9274-11e6-b79f-9e8b7626e0db_7c3eb331 919ed2887873 gcr.io/google-containers/kube-addon-manager-amd64:v2 \u0026quot;/opt/kube-addons.sh\u0026quot; 10 hours ago Up 10 hours k8s_kube-addon-manager.a1c58ca2_kube-addon-manager-minikube_kube-system_3e8322eb546e1d90d2fb7cac24d6d6a2_6ddcb7b8 80fe85c54a84 gcr.io/google_containers/pause-amd64:3.0 \u0026quot;/pause\u0026quot; 10 hours ago Up 10 hours Kubernetes でコンテナを起動してみる # YAML を書かないでコンテナを起動することもできるようだ http://kubernetes.io/docs/user-guide/simple-nginx/ nginx を2つ起動させる\n$ kubectl run my-nginx --image=nginx --replicas=2 --port=80 deployment \u0026quot;my-nginx\u0026quot; created docker pull など起動の準備中\n$ kubectl get pods NAME READY STATUS RESTARTS AGE my-nginx-379829228-9tr21 0/1 ContainerCreating 0 9s my-nginx-379829228-s5d06 0/1 ContainerCreating 0 9s 1つ目が起動\n$ kubectl get pods NAME READY STATUS RESTARTS AGE my-nginx-379829228-9tr21 1/1 Running 0 47s my-nginx-379829228-s5d06 0/1 ContainerCreating 0 47s 2つ目も起動\n$ kubectl get pods NAME READY STATUS RESTARTS AGE my-nginx-379829228-9tr21 1/1 Running 0 1m my-nginx-379829228-s5d06 1/1 Running 0 1m deployments というものになるようだ\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE my-nginx 2 2 2 2 3m 削除\n$ kubectl delete deployment my-nginx deployment \u0026quot;my-nginx\u0026quot; deleted Service にしてみる # さっきと同じように2つの nginx を起動させる\n$ kubectl run my-nginx --image=nginx --replicas=2 --port=80 deployment \u0026quot;my-nginx\u0026quot; created 起動中\n$ kubectl get pods NAME READY STATUS RESTARTS AGE my-nginx-379829228-08jvp 1/1 Running 0 6s my-nginx-379829228-9wudu 0/1 ContainerCreating 0 6s $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE my-nginx 2 2 2 2 9s deployment を LoadBalancer を使って expose する\n$ kubectl expose deployment my-nginx --port=80 --type=LoadBalancer service \u0026quot;my-nginx\u0026quot; exposed $ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE my-nginx 2 2 2 2 37s Service ができた\n$ kubectl get services NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.0.0.1 443/TCP 11h my-nginx 10.0.0.88 80/TCP 14s しかし、この IP アドレスではアクセスできない、EXTERNAL-IP が pending だからここをなんとかする必要があるようだ 今後調査\n$ kubectl expose --help Expose a resource as a new Kubernetes service. Looks up a deployment, service, replica set, replication controller or pod by name and uses the selector for that resource as the selector for a new service on the specified port. A deployment or replica set will be exposed as a service only if its selector is convertible to a selector that service supports, i.e. when the selector contains only the matchLabels component. Note that if no port is specified via --port and the exposed resource has multiple ports, all will be re-used by the new service. Also if no labels are specified, the new service will re-use the labels from the resource it exposes. Possible resources include (case insensitive): pod (po), service (svc), replicationcontroller (rc), deployment (deploy), replicaset (rs) Examples: # Create a service for a replicated nginx, which serves on port 80 and connects to the containers on port 8000. kubectl expose rc nginx --port=80 --target-port=8000 # Create a service for a replication controller identified by type and name specified in \u0026quot;nginx-controller.yaml\u0026quot;, which serves on port 80 and connects to the containers on port 8000. kubectl expose -f nginx-controller.yaml --port=80 --target-port=8000 # Create a service for a pod valid-pod, which serves on port 444 with the name \u0026quot;frontend\u0026quot; kubectl expose pod valid-pod --port=444 --name=frontend # Create a second service based on the above service, exposing the container port 8443 as port 443 with the name \u0026quot;nginx-https\u0026quot; kubectl expose service nginx --port=443 --target-port=8443 --name=nginx-https # Create a service for a replicated streaming application on port 4100 balancing UDP traffic and named 'video-stream'. kubectl expose rc streamer --port=4100 --protocol=udp --name=video-stream # Create a service for a replicated nginx using replica set, which serves on port 80 and connects to the containers on port 8000. kubectl expose rs nginx --port=80 --target-port=8000 # Create a service for an nginx deployment, which serves on port 80 and connects to the containers on port 8000. kubectl expose deployment nginx --port=80 --target-port=8000 Options: --cluster-ip='': ClusterIP to be assigned to the service. Leave empty to auto-allocate, or set to 'None' to create a headless service. --container-port='': Synonym for --target-port --create-external-load-balancer=false: If true, create an external load balancer for this service (trumped by --type). Implementation is cloud provider dependent. Default is 'false'. --dry-run=false: If true, only print the object that would be sent, without sending it. --external-ip='': Additional external IP address (not managed by Kubernetes) to accept for the service. If this IP is routed to a node, the service can be accessed by this IP in addition to its generated service IP. -f, --filename=[]: Filename, directory, or URL to a file identifying the resource to expose a service --generator='service/v2': The name of the API generator to use. There are 2 generators: 'service/v1' and 'service/v2'. The only difference between them is that service port in v1 is named 'default', while it is left unnamed in v2. Default is 'service/v2'. -l, --labels='': Labels to apply to the service created by this call. --load-balancer-ip='': IP to assign to the Load Balancer. If empty, an ephemeral IP will be created and used (cloud-provider specific). --name='': The name for the newly created object. --no-headers=false: When using the default or custom-column output format, don't print headers. -o, --output='': Output format. One of: json|yaml|wide|name|custom-columns=...|custom-columns-file=...|go-template=...|go-template-file=...|jsonpath=...|jsonpath-file=... See custom columns [http://kubernetes.io/docs/user-guide/kubectl-overview/#custom-columns], golang template [http://golang.org/pkg/text/template/#pkg-overview] and jsonpath template [http://kubernetes.io/docs/user-guide/jsonpath]. --output-version='': Output the formatted object with the given group version (for ex: 'extensions/v1beta1'). --overrides='': An inline JSON override for the generated object. If this is non-empty, it is used to override the generated object. Requires that the object supply a valid apiVersion field. --port='': The port that the service should serve on. Copied from the resource being exposed, if unspecified --protocol='': The network protocol for the service to be created. Default is 'TCP'. --record=false: Record current kubectl command in the resource annotation. If set to false, do not record the command. If set to true, record the command. If not set, default to updating the existing annotation value only if one already exists. -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory. --save-config=false: If true, the configuration of current object will be saved in its annotation. This is useful when you want to perform kubectl apply on this object in the future. --selector='': A label selector to use for this service. Only equality-based selector requirements are supported. If empty (the default) infer the selector from the replication controller or replica set. --session-affinity='': If non-empty, set the session affinity for the service to this; legal values: 'None', 'ClientIP' -a, --show-all=false: When printing, show all resources (default hide terminated pods.) --show-labels=false: When printing, show all labels as the last column (default hide labels column) --sort-by='': If non-empty, sort list types using this field specification. The field specification is expressed as a JSONPath expression (e.g. '{.metadata.name}'). The field in the API resource specified by this JSONPath expression must be an integer or a string. --target-port='': Name or number for the port on the container that the service should direct traffic to. Optional. --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview]. --type='': Type for this service: ClusterIP, NodePort, or LoadBalancer. Default is 'ClusterIP'. Usage: kubectl expose (-f FILENAME | TYPE NAME) [--port=port] [--protocol=TCP|UDP] [--target-port=number-or-name] [--name=name] [--external-ip=external-ip-of-service] [--type=type] [options] Use \u0026quot;kubectl options\u0026quot; for a list of global command-line options (applies to all commands). 作った Kubernetes 環境の削除 # 不要になったら削除する（stop しなくても delete できるし速い）\n$ minikube stop Stopping local Kubernetes cluster... Machine stopped. $ minikube status minikubeVM: Stopped localkube: N/A $ minikube delete Deleting local Kubernetes cluster... Machine deleted. $ minikube status minikubeVM: Does Not Exist localkube: N/A Kubernetes の調査を続けたい\n続き minikube を試す – その2、 minikube を試す – その3 を書いた\n","date":"2016年10月15日","permalink":"/2016/10/setup-kubernetes-1-4-using-minikube/","section":"Posts","summary":"Kubernetes を調査しようかなということで minikube を使ったセットアップを試してみる 環境は Ubuntu 16.04 の Note PC minikube は single node の Kubernetes を 1 コマンドでセットアップすることができるツール","title":"minikube でローカルでのテスト用 Kubernetes を構築"},{"content":"","date":"2016年9月17日","permalink":"/tags/boxcutter/","section":"Tags","summary":"","title":"boxcutter"},{"content":"","date":"2016年9月17日","permalink":"/tags/packer/","section":"Tags","summary":"","title":"Packer"},{"content":"前にも作ってみたが、もっとイカした作成方法が無いかな？ってのと自分用の Vagrant box を Github に置こうかなと思って調べてたら https://atlas.hashicorp.com/boxcutter という便利ツールの存在を知った https://github.com/misheska/basebox-packer から https://github.com/box-cutter に移り、今は https://github.com/boxcutter となっているようだ そんでもって https://atlas.hashicorp.com/boxcutter にほぼ最小構成の box が揃ってるから欲しかったのはこれだということでわざわざ作らなくても良くなった\u0026hellip; カスタマイズしたくなったら各 distribution の repository を clone して編集して build すれば良さそう。CentOS なら kickstart 用のファイル をいじるか custom-script.sh をいじってビルドすれば良い。ubuntu の場合は preseed.cfg か custom-script.sh で良さそう。 どちらの場合も custom-script.sh でできることはこっちでやるのがスジだろう\nbin/box build centos72 virtualbox などとするだけで出来ちゃう！！素晴らしい！！\nboxcutter で公開されている box を使う方法 # $ vagrant box add https://atlas.hashicorp.com/boxcutter/boxes/centos72 ==\u0026gt; box: Loading metadata for box 'https://atlas.hashicorp.com/boxcutter/boxes/centos72' This box can work with multiple providers! The providers that it can work with are listed below. Please review the list and choose the provider you will be working with. 1) parallels 2) virtualbox 3) vmware_desktop Enter your choice: 2 ==\u0026gt; box: Adding box 'boxcutter/centos72' (v2.0.15) for provider: virtualbox box: Downloading: https://atlas.hashicorp.com/boxcutter/boxes/centos72/versions/2.0.15/providers/virtualbox.box box: Progress: 84% (Rate: 4661k/s, Estimated time remaining: 0:00:17) ==\u0026gt; box: Successfully added box 'boxcutter/centos72' (v2.0.15) for 'virtualbox'! $ vagrant init boxcutter/centos72 A `Vagrantfile` has been placed in this directory. You are now ready to `vagrant up` your first virtual environment! Please read the comments in the Vagrantfile as well as documentation on `vagrantup.com` for more information on using Vagrant. $ vagrant up Bringing machine 'default' up with 'virtualbox' provider... ==\u0026gt; default: Importing base box 'boxcutter/centos72'... ==\u0026gt; default: Matching MAC address for NAT networking... ==\u0026gt; default: Checking if box 'boxcutter/centos72' is up to date... ==\u0026gt; default: Setting the name of the VM: test_default_1474101995245_97755 ==\u0026gt; default: Clearing any previously set network interfaces... ==\u0026gt; default: Preparing network interfaces based on configuration... default: Adapter 1: nat ==\u0026gt; default: Forwarding ports... default: 22 (guest) =\u0026gt; 2222 (host) (adapter 1) ==\u0026gt; default: Booting VM... ==\u0026gt; default: Waiting for machine to boot. This may take a few minutes... default: SSH address: 127.0.0.1:2222 default: SSH username: vagrant default: SSH auth method: private key default: Warning: Remote connection disconnect. Retrying... default: default: Vagrant insecure key detected. Vagrant will automatically replace default: this with a newly generated keypair for better security. default: default: Inserting generated public key within guest... default: Removing insecure key from the guest if it's present... default: Key inserted! Disconnecting and reconnecting using new SSH key... ==\u0026gt; default: Machine booted and ready! ==\u0026gt; default: Checking for guest additions in VM... ==\u0026gt; default: Mounting shared folders... default: /vagrant =\u0026gt; /home/ytera/test Ubuntu 16.04 で VirtualBox 5.1.6 ではエラーで起動せず、vagrant も 1.8.5 には authorized_keys のパーミッション設定に問題があるということでログインできずということでハマってしまったが、それぞれ 5.0 と 1.8.4 にすることで回避\n","date":"2016年9月17日","permalink":"/2016/09/how-to-create-vagrant-box/","section":"Posts","summary":"前にも作ってみたが、もっとイカした作成方法が無いかな？ってのと自分用の Vagrant box を Github に置こうかなと思って調べてたら https://atlas.hashicorp.com/boxcutter という便利ツールの存在を知った","title":"Vagrant box 作成方法"},{"content":"","date":"2016年9月1日","permalink":"/tags/s3/","section":"Tags","summary":"","title":"s3"},{"content":"PostgreSQL の Backup / Restore ツールとして heroku で開発されたとされる WAL-E がある。 フィジカル（物理）バックアップと WAL のアーカイブを S3 互換のオブジェクトストレージや Azure BLOB Storage や Google Cloud Storage へ保存でき、そこからのリストアもできる便利ツールです。 AWS で EC2 上の PostgreSQL のバックアップ/リストアを WAL-E で行ってみます。OS は Ubuntu 14.04 (Trusty) を使います。\nS3 側の準備 # バックアップ先となる S3 の Bucket を用意します。既存のものでもかまいません。 Bucket 内の指定のディレクトリ(Path)配下に保存します。 IAM のポリシーを作成\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListAllMyBuckets\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::YOUR_BUCKET_NAME\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::YOUR_BUCKET_NAME/*\u0026#34; } ] } EC2 のインスタンス Role を使わない場合は AWS_ACCESS_KEY_ID と AWS_SECRET_ACCESS_KEY を取得します。 この後の例はインスタンス Role を使っているため \u0026ndash;aws-instance-profile オプションを指定してありますが、クレデンシャルを使う場合はこれを削る必要があります。\nwal-e のインストール # wal-e は Python で書かれており pip でインストールできます まずは apt で必要なパッケージをインストールします\nsudo apt-get update sudo apt-get install lzop pv python-pip python-dev daemontools -y daemontools に含まれる envdir コマンドで AWS のクレデンシャルを環境変数として読み込んでコマンドを実行するようにします lzop はファイルの圧縮に使われます pv は disk i/o のレート制限に使われます（バックアップ処理の負荷で影響がでないように） requests と six が古いので更新して wal-e をインストールします\nsudo pip install -U requests sudo pip install -U six sudo pip install wal-e $ wal-e -h usage: wal-e [-h] [-k AWS_ACCESS_KEY_ID | --aws-instance-profile] [-a WABS_ACCOUNT_NAME] [--s3-prefix S3_PREFIX] [--wabs-prefix WABS_PREFIX] [--gpg-key-id GPG_KEY_ID] [--terse] {version,backup-fetch,backup-list,backup-push,wal-push,wal-fetch,wal-prefetch,delete} ... WAL-E is a program to assist in performing PostgreSQL continuous archiving on S3 or Windows Azure Blob Service (WABS): it handles pushing and fetching of WAL segments and base backups of the PostgreSQL data directory. optional arguments: -h, --help show this help message and exit -k AWS_ACCESS_KEY_ID, --aws-access-key-id AWS_ACCESS_KEY_ID public AWS access key. Can also be defined in an environment variable. If both are defined, the one defined in the programs arguments takes precedence. --aws-instance-profile Use the IAM Instance Profile associated with this instance to authenticate with the S3 API. -a WABS_ACCOUNT_NAME, --wabs-account-name WABS_ACCOUNT_NAME Account name of Windows Azure Blob Service account. Can also be defined in an environmentvariable. If both are defined, the one definedin the programs arguments takes precedence. --s3-prefix S3_PREFIX S3 prefix to run all commands against. Can also be defined via environment variable WALE_S3_PREFIX. --wabs-prefix WABS_PREFIX Storage prefix to run all commands against. Can also be defined via environment variable WALE_WABS_PREFIX. --gpg-key-id GPG_KEY_ID GPG key ID to encrypt to. (Also needed when decrypting.) Can also be defined via environment variable WALE_GPG_KEY_ID --terse Only log messages as or more severe than a warning. subcommands: {version,backup-fetch,backup-list,backup-push,wal-push,wal-fetch,wal-prefetch,delete} version print the wal-e version backup-fetch fetch a hot backup from S3 or WABS backup-list list backups in S3 or WABS backup-push pushing a fresh hot backup to S3 or WABS wal-push push a WAL file to S3 or WABS wal-fetch fetch a WAL file from S3 or WABS wal-prefetch Prefetch WAL delete operators to destroy specified data in S3 or WABS PostgreSQL のインストール # sudo apt-get install postgresql-9.3 -y Ubuntu の場合、データベースの設定ファイルは /etc/postgresql/9.3/main/ 配下に、データは /var/lib/postgresql/9.3/main/ 配下に設置されます アーカイブモードの設定を行います\nsudoedit /etc/postgresql/9.3/main/postgresql.conf wal_level = archive # hot_standby でも可 archive_mode = on archive_command = '/usr/bin/envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile wal-push %p' archive_timeout = 60 # WAL ファイルがいっぱいにならなくてもこの秒数が経過すればログスイッチしてアーカイブする 反映には再起動が必要ですがそれは envdir 設定の後で\nenvdir 用設定 # 場所に決まりはありませんが /etc/wal-e.d/env/ ディレクトリを作成し、そこに設定したい環境変数名のファイルを作成し値を入れます\nsudo mkdir -p /etc/wal-e.d/env sudoedit /etc/wal-e.d/env/AWS_ACCESS_KEY_ID # インスタンス Role を使う場合は不要 sudoedit /etc/wal-e.d/env/AWS_SECRET_ACCESS_KEY # インスタンス Role を使う場合は不要 sudoedit /etc/wal-e.d/env/AWS_REGION # 東京なので ap-northeast-1 sudoedit /etc/wal-e.d/env/WALE_S3_PREFIX # s3://YOUR_BUCKET_NAME/SUBDIR sudo chown -R root:postgres /etc/wal-e.d sudo chmod -R o-rwx /etc/wal-e.d バックアップの実行 # sudo service postgresql restart archive_command が正常に実行されるかどうかログ (/var/log/postgresql/postgresql-9.3-main.log) を確認します\nwal_e.main INFO MSG: starting WAL-E DETAIL: The subcommand is \u0026quot;wal-push\u0026quot;. STRUCTURED: time=2016-09-01T14:39:52.182896-00 pid=23763 wal_e.worker.upload INFO MSG: begin archiving a file DETAIL: Uploading \u0026quot;pg_xlog/000000010000000000000002\u0026quot; to \u0026quot;s3://YOUR_BUCKET_NAME/SUBDIR/wal_005/000000010000000000000002.lzo\u0026quot;. STRUCTURED: time=2016-09-01T14:39:52.234031-00 pid=23763 action=push-wal key=s3://YOUR_BUCKET_NAME/SUBDIR/wal_005/000000010000000000000002.lzo prefix=SUBDIR/ seg=000000010000000000000002 state=begin wal_e.worker.upload INFO MSG: completed archiving to a file DETAIL: Archiving to \u0026quot;s3://YOUR_BUCKET_NAME/SUBDIR/wal_005/000000010000000000000002.lzo\u0026quot; complete at 609.625KiB/s. STRUCTURED: time=2016-09-01T14:39:52.462079-00 pid=23763 action=push-wal key=s3://YOUR_BUCKET_NAME/SUBDIR/wal_005/000000010000000000000002.lzo prefix=SUBDIR/ rate=609.625 seg=000000010000000000000002 state=complete sudo -u postgres envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile backup-push /var/lib/postgresql/9.3/main これでデータベース全体のバックアップが行われます\n$ sudo -u postgres envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile backup-push /var/lib/postgresql/9.3/main wal_e.main INFO MSG: starting WAL-E DETAIL: The subcommand is \u0026quot;backup-push\u0026quot;. STRUCTURED: time=2016-09-01T14:39:51.941209-00 pid=23737 wal_e.operator.backup INFO MSG: start upload postgres version metadata DETAIL: Uploading to s3://YOUR_BUCKET_NAME/SUBDIR/basebackups_005/base_000000010000000000000003_00000040/extended_version.txt. STRUCTURED: time=2016-09-01T14:39:52.237635-00 pid=23737 wal_e.operator.backup INFO MSG: postgres version metadata upload complete STRUCTURED: time=2016-09-01T14:39:52.368084-00 pid=23737 wal_e.worker.upload INFO MSG: beginning volume compression DETAIL: Building volume 0. STRUCTURED: time=2016-09-01T14:39:52.399796-00 pid=23737 wal_e.worker.upload INFO MSG: begin uploading a base backup volume DETAIL: Uploading to \u0026quot;s3://YOUR_BUCKET_NAME/SUBDIR/basebackups_005/base_000000010000000000000003_00000040/tar_partitions/part_00000000.tar.lzo\u0026quot;. STRUCTURED: time=2016-09-01T14:39:52.720335-00 pid=23737 wal_e.worker.upload INFO MSG: finish uploading a base backup volume DETAIL: Uploading to \u0026quot;s3://YOUR_BUCKET_NAME/SUBDIR/basebackups_005/base_000000010000000000000003_00000040/tar_partitions/part_00000000.tar.lzo\u0026quot; complete at 8799.85KiB/s. STRUCTURED: time=2016-09-01T14:39:53.112379-00 pid=23737 NOTICE: pg_stop_backup complete, all required WAL segments have been archived backup-list サブコマンドでベースバックアップのリストが確認できまう\n$ sudo -u postgres envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile backup-list wal_e.main INFO MSG: starting WAL-E DETAIL: The subcommand is \u0026quot;backup-list\u0026quot;. STRUCTURED: time=2016-09-01T14:50:11.054832-00 pid=24023 name last_modified expanded_size_bytes wal_segment_backup_start wal_segment_offset_backup_start wal_segment_backup_stop wal_segment_offset_backup_stop base_000000010000000000000003_00000040 2016-09-01T14:39:55.000Z 000000010000000000000003 00000040 base_000000010000000000000007_00000040 2016-09-01T14:49:02.000Z 000000010000000000000007 00000040 base_000000010000000000000009_00000040 2016-09-01T14:49:27.000Z 000000010000000000000009 00000040 --detail をつけるともう少し情報が増えますが、それぞれについて S3 にアクセスすることになるので大量にある時間がかかります\n$ sudo -u postgres envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile backup-list --detail wal_e.main INFO MSG: starting WAL-E DETAIL: The subcommand is \u0026quot;backup-list\u0026quot;. STRUCTURED: time=2016-09-01T14:51:08.766735-00 pid=24049 name last_modified expanded_size_bytes wal_segment_backup_start wal_segment_offset_backup_start wal_segment_backup_stop wal_segment_offset_backup_stop base_000000010000000000000003_00000040 2016-09-01T14:39:55.000Z 19603869 000000010000000000000003 00000040 000000010000000000000003 00000184 base_000000010000000000000007_00000040 2016-09-01T14:49:02.000Z 26102357 000000010000000000000007 00000040 000000010000000000000007 00000184 base_000000010000000000000009_00000040 2016-09-01T14:49:27.000Z 26110549 000000010000000000000009 00000040 000000010000000000000009 00000184 リストアする # PostgreSQL を停止してデータディレクトリを空っぽにする\nsudo service postgresql stop sudo rm -fr /var/lib/postgresql/9.3/main sudo install -o postgres -g postgres -m 0700 -d /var/lib/postgresql/9.3/main backup-fetch サブコマンドでベースバックアップを取得する\n$ sudo -u postgres envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile backup-fetch /var/lib/postgresql/9.3/main LATEST wal_e.main INFO MSG: starting WAL-E DETAIL: The subcommand is \u0026quot;backup-fetch\u0026quot;. STRUCTURED: time=2016-09-01T15:01:12.084425-00 pid=24131 wal_e.worker.s3.s3_worker INFO MSG: beginning partition download DETAIL: The partition being downloaded is part_00000000.tar.lzo. HINT: The absolute S3 key is waletest2/basebackups_005/base_000000010000000000000009_00000040/tar_partitions/part_00000000.tar.lzo. STRUCTURED: time=2016-09-01T15:01:12.439743-00 pid=24131 recovery.conf を作成する restore_command に wal-fetch サブコマンドを指定します、これでベースバックアップ後の WAL ファイルを順に取得して最後の WAL archive 時点まで復旧されます\ncat \u0026lt;\u0026lt;_EOF_ | sudo -u postgres tee /var/lib/postgresql/9.3/main/recovery.conf restore_command = '/usr/bin/envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile wal-fetch \u0026quot;%f\u0026quot; \u0026quot;%p\u0026quot;' _EOF_ PostgreSQL の起動\nsudo service postgresql start ログ (/var/log/postgresql/postgresql-9.3-main.log) を確認 recovery.conf が recovery.done になっててデータが復元されていることを確認\n指定の時刻の状態に復元する # さっきは LATEST 指定で最新のベースバックアップを取得したが、もっと前の状態に戻したいのでバックアップの名前指定で取得する\nsudo service postgresql stop sudo rm -fr /var/lib/postgresql/9.3/main sudo install -o postgres -g postgres -m 0700 -d /var/lib/postgresql/9.3/main sudo -u postgres envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile backup-fetch /var/lib/postgresql/9.3/main base_000000010000000000000003_00000040 cat \u0026lt;\u0026lt;_EOF_ | sudo -u postgres tee /var/lib/postgresql/9.3/main/recovery.conf restore_command = '/usr/bin/envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile wal-fetch \u0026quot;%f\u0026quot; \u0026quot;%p\u0026quot;' recovery_target_time = '2016-09-01 14:49:30' _EOF_ sudo service postgresql start これで 2016-09-01 14:49:30 時点の状態に戻りました。recovery_target_time はミリ秒まで指定可能。\n定期実行 # ベースバックアップは cron などで日次や週次で定期的に実行します 毎日夜中の3時にベースバックアップを取得する設定\n0 3 * * * /usr/bin/envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile backup-push /var/lib/postgresql/9.3/main バックアップの削除 # ベースバックアップは圧縮されてはいるもののフルバックアップなので沢山持つと大きなサイズになっていまいます。 そこからの WAL ファイルを全部持つのも大きなサイズになるので定期的に古いものを削除することになると思います。 次のように delete retain 3 とすることで最新の3つ残して古いものを削除することができます これも cron などで定期実行すると良いでしょう\n/usr/bin/envdir /etc/wal-e.d/env /usr/local/bin/wal-e --aws-instance-profile delete --confirm retain 3 delete everything で全部、delete before base_000000010000000000000009_00000040 で指定のバックアップより古いもの（指定したものは残る）を削除できます \u0026ndash;confirm を付けない場合は削除対象が表示されるだけです\nファイルの暗号化 # gpg を使ってファイルを暗号化して保存することもできます\nWAL-E を使った replication # マスタ側の archive_command で backup-push し、レプリカ側の recovery.conf で wal-fetch するようにしておけば S3 などを経由したレプリケーションを組むことも可能です。 Streaming Replication を組めないネットワーク環境では便利かも。\n","date":"2016年9月1日","permalink":"/2016/09/postgressql-backup-and-restore-using-wal-e/","section":"Posts","summary":"PostgreSQL の Backup / Restore ツールとして heroku で開発されたとされる WAL-E がある。 フィジカル（物理）バックアップと WAL のアーカイブを S3 互換のオブジェクトストレージや Azure BLOB Storage","title":"WAL-E で PostgreSQL の Backup / Restore"},{"content":"","date":"2016年8月31日","permalink":"/tags/entrykit/","section":"Tags","summary":"","title":"Entrykit"},{"content":"Docker コンテナで何かを実行する場合に便利なのが Entrykit 前に使い方を調べたはずなのに忘れてしまったので再度まとめてメモっておく インストールは release ページから最新版をダウンロードし /bin/entrykit に設置する その後 entrykit \u0026ndash;symlink で symlink をはる、entrykit はどの名前で実行されたかによって動作が変わります\n# /bin/entrykit --symlink Creating symlink /bin/entrykit ... Creating symlink /bin/codep ... Creating symlink /bin/prehook ... Creating symlink /bin/render ... Creating symlink /bin/switch ... codep # 複数のプロセスを同時に起動させます\ncodep \u0026quot;sleep 100\u0026quot; \u0026quot;sleep 200\u0026quot; \u0026quot;nginx\u0026quot; \u0026quot;nginx -v\u0026quot; とすると 4 つのプロセスが実行されるかと思いきゃ \u0026ldquo;sleep 200\u0026rdquo; と \u0026ldquo;nginx -v\u0026rdquo; しか実行されません。同じ実行ファイルのプロセスは後で指定したものだけが実行されるようです\ncodep \u0026quot;sleep 100\u0026quot; \u0026quot;/bin/echo test\u0026quot; \u0026quot;nginx -v\u0026quot; であれば 3 つとも実行されます ドキュメントにあるように\nENTRYPOINT [\u0026quot;codep\u0026quot;, \\ \u0026quot;/bin/config-reloader\u0026quot;, \\ \u0026quot;/usr/sbin/nginx\u0026quot; ] と書けば /bin/config-reloader と /usr/sbin/nginx が実行されます、どちらか一方でも終了すればコンテナごと停止します\nENTRYPOINT [\u0026quot;codep\u0026quot;, \\ \u0026quot;/bin/config-reloader\u0026quot;, \\ \u0026quot;/usr/sbin/nginx\u0026quot;, \\ \u0026quot;sleep 100\u0026quot; ] とすれば100秒で sleep が終了したタイミングでコンテナが停止します\nrender # render は sigil という template エンジンを使ってファイルを生成します\nrender /etc/nginx.conf -- /usr/sbin/nginx とすれば /etc/nginx.conf.tmpl から /etc/nginx.conf を生成した後に nginx を起動させます\nENTRYPOINT [\u0026quot;render\u0026quot;, \u0026quot;/etc/nginx.conf\u0026quot;, \\ \u0026quot;--\u0026quot;, \u0026quot;/usr/sbin/nginx\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot;] switch # switch shell=/bin/bash ps=/bin/ps \u0026quot;version=nginx -v\u0026quot; -- /usr/sbin/nginx -g 'daemon off;' version とすれば nginx -v が実行される。末尾の version を外せば \u0026ndash; 以降のコマンドが実行される version の代わりに shell とすれば bash が起動する つまり\nENTRYPOINT [\u0026quot;switch\u0026quot;, \\ \u0026quot;shell=/bin/bash\u0026quot;, \\ \u0026quot;ps=/bin/ps\u0026quot;, \\ \u0026quot;version=nginx -v\u0026quot;, \\ \u0026quot;--\u0026quot;, \\ \u0026quot;/usr/sbin/nginx\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot; ] としておけば\ndocker run -it xxx version とすれば nginx -v が実行され、\ndocker run -it xxx shell とすれば bash が実行されます\ndocker run -it xxx とすれば /usr/sbin/nginx -g \u0026lsquo;daemon off;\u0026rsquo; が実行されます name=command として指定されていないものを引数に渡すとこの後ろにつけて実行されます\ndocker run -it xxx -V とすれば /usr/sbin/nginx -g \u0026lsquo;daemon off;\u0026rsquo; -V が実行されるのでバージョン情報やビルド情報が表示されます\nprehook # prehook で指定したコマンドを実行した後にアプリなどを起動させます\nENTRYPOINT [\u0026quot;prehook\u0026quot;, \u0026quot;nginx -V\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;/usr/sbin/nginx\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;/nginx.conf\u0026quot;] \u0026ldquo;\u0026ndash;\u0026rdquo; の後はリストで引数を渡しますが prehook コマンドをリストで渡すことはできません\nENTRYPOINT [\u0026quot;prehook\u0026quot;, \u0026quot;nginx\u0026quot;, \u0026quot;-V\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;/usr/sbin/nginx\u0026quot;] また、prehook で指定したコマンドは正常終了しないと後のコマンドを実行しません よって、次のようにすると nginx は起動しません\nENTRYPOINT [\u0026quot;prehook\u0026quot;, \u0026quot;/bin/false\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;/usr/sbin/nginx\u0026quot;] 合わせ技 # これまでに出てきたものを組み合わせることが可能です\nENTRYPOINT [ \\ \u0026quot;switch\u0026quot;, \\ \u0026quot;shell=/bin/sh\u0026quot;, \\ \u0026quot;version=nginx -v\u0026quot;, \u0026quot;--\u0026quot;, \\ \u0026quot;render\u0026quot;, \u0026quot;/demo/nginx.conf\u0026quot;, \u0026quot;--\u0026quot;, \\ \u0026quot;prehook\u0026quot;, \u0026quot;nginx -V\u0026quot;, \u0026quot;--\u0026quot;, \\ \u0026quot;codep\u0026quot;, \\ \u0026quot;/bin/reloader 3\u0026quot;, \\ \u0026quot;/usr/sbin/nginx -c /demo/nginx.conf\u0026quot; ] 環境変数で指定する # 次のように環境変数で指定することも可能なので Dockerfile に書いたものを実行時に上書きすることも可能です\nENV SWITCH_SHELL=/bin/sh ENV RENDER_CONFIG=/etc/nginx.conf ENV CODEP_NGINX=nginx -g ENV CODEP_CONFD=confd ENV PREHOOK_HTPASSWD=htpasswd -bc /etc/nginx/htpasswd $HTPASSWD ENTRYPOINT [\u0026quot;entrykit -e\u0026quot;] 後でテンプレート機能のところをもうちょい掘り下げよう\n","date":"2016年8月31日","permalink":"/2016/08/how-to-use-entrykit/","section":"Posts","summary":"Docker コンテナで何かを実行する場合に便利なのが Entrykit 前に使い方を調べたはずなのに忘れてしまったので再度まとめてメモっておく インストールは release ページから","title":"Entrykit の使い方"},{"content":"https://dcos.io/docs/1.7/administration/installing/custom/advanced/ を参考に DC/OS をセットアップしてみる 環境はいつものように DigitalOcean CoreOS で master 3台、agent 1台 別 OS で良かったのかどうかわからないけど Bootstrap 用サーバーを Ubuntu で1台\nBootstrap サーバーのセットアップ # Bootstrap サーバーはセットアップ用のパッケージやスクリプトの配布サーバーです。 まずは Docker Engine のインストール\n$ curl -fsSL https://get.docker.com/ | sh 任意の場所に genconf というディレクトリを作成\n$ mkdir -p genconf genconf/config.yaml を作成\n---bootstrap_url: http://: cluster_name:\u0026#39;\u0026#39;exhibitor_storage_backend:staticip_detect_filename:/genconf/ip-detectmaster_list:- - - resolvers:- 8.8.4.4- 8.8.8.8\u0026lt;bootstrap_public_ip\u0026gt; は master, agent の各サーバーからアクセス可能な bootstrap サーバーのIPアドレス \u0026lt;your_port\u0026gt; は任意のポート、Docker で起動する nginx の publish port です /genconf/ip-deect はこの後 genconf/ip-detect というスクリプトファイルを作成します  は任意のクラスタ名 resolvers の DNS サーバーも他に自前のものがあればそれでも問題なし genconf/ip-detect というファイルで自ホストのIPアドレスを取得するスクリプトを作成します DigitalOcean なので metadata API から取得します\n#!/bin/bash  curl -fsSL http://169.254.169.254/metadata/v1/interfaces/private/0/ipv4/address DC/OS installer のダウンロード\n$ curl -O https://downloads.dcos.io/dcos/EarlyAccess/commit/14509fe1e7899f439527fb39867194c7a425c771/dcos_generate_config.sh ここで dcos_generate_config.sh を実行します\n$ sudo bash dcos_generate_config.sh Extracting image from this script and loading into docker daemon, this step can take a few minutes dcos-genconf.14509fe1e7899f4395-3a2b7e03c45cd615da.tar c56b7dabbc7a: Loading layer [==================================================\u0026gt;] 5.041 MB/5.041 MB cb9346f72a60: Loading layer [==================================================\u0026gt;] 22.73 MB/22.73 MB bc3f3016e472: Loading layer [==================================================\u0026gt;] 4.063 MB/4.063 MB 24e0af39909a: Loading layer [==================================================\u0026gt;] 129.5 MB/129.5 MB fd56668380be: Loading layer [==================================================\u0026gt;] 2.048 kB/2.048 kB 90755ec2374c: Loading layer [==================================================\u0026gt;] 415.4 MB/415.4 MB 58ae10cff6df: Loading layer [==================================================\u0026gt;] 4.608 kB/4.608 kB Loaded image: mesosphere/dcos-genconf:14509fe1e7899f4395-3a2b7e03c45cd615da Running mesosphere/dcos-genconf docker with BUILD_DIR set to /root/genconf ====\u0026gt; EXECUTING CONFIGURATION GENERATION Generating configuration files... Final arguments:{ ... } Package filename: packages/dcos-config/dcos-config--setup_caecbbeb5649b58b74977a1adbd6512480245c9a.tar.xz Package filename: packages/dcos-metadata/dcos-metadata--setup_caecbbeb5649b58b74977a1adbd6512480245c9a.tar.xz Generating Bash configuration files for DC/OS 次のような状態となります\n. ├── dcos-genconf.14509fe1e7899f4395-3a2b7e03c45cd615da.tar ├── dcos_generate_config.sh └── genconf ├── cluster_packages.json ├── config.yaml ├── ip-detect ├── serve │ ├── bootstrap │ │ ├── 3a2b7e03c45cd615da8dfb1b103943894652cd71.active.json │ │ └── 3a2b7e03c45cd615da8dfb1b103943894652cd71.bootstrap.tar.xz │ ├── bootstrap.latest │ ├── cluster-package-info.json │ ├── dcos_install.sh │ ├── fetch_packages.sh │ └── packages │ ├── dcos-config │ │ └── dcos-config--setup_caecbbeb5649b58b74977a1adbd6512480245c9a.tar.xz │ └── dcos-metadata │ └── dcos-metadata--setup_caecbbeb5649b58b74977a1adbd6512480245c9a.tar.xz └── state 7 directories, 13 files ここで genconf/serve を DocumentRoot として nginx でファイルを公開します\n$ sudo docker run -d -p :80 -v $PWD/genconf/serve:/usr/share/nginx/html:ro nginx Bootstrap サーバーのセットアップはこれで完了\nMaster サーバー3台のセットアップ # 3台それぞれで実行します 作業ディレクトリの作成と移動\n$ mkdir /tmp/dcos \u0026amp;\u0026amp; cd /tmp/dcos Bootstrap サーバーからセットアップ用スクリプトのダウンロード\n$ curl -O http://:/dcos_install.sh Master サーバーとしてセットアップされるように引数に master を指定して実行\n$ sudo bash dcos_install.sh master これでしばらく待っていれば完了です 大量のサービスが登録されています\n$ systemctl | grep dcos dcos-adminrouter.service loaded active running Admin Router: A high performance web server and a reverse proxy server dcos-cosmos.service loaded active running Package Service: DC/OS Packaging API dcos-ddt.service loaded active running Diagnostics: DC/OS Distributed Diagnostics Tool Master API and Aggregation Service dcos-epmd.service loaded active running Erlang Port Mapping Daemon: DC/OS Erlang Port Mapping Daemon dcos-exhibitor.service loaded active running Exhibitor: Zookeeper Supervisor Service dcos-history-service.service loaded active running Mesos History: DC/OS Resource Metrics History Service/API dcos-marathon.service loaded active running Marathon: DC/OS Init System dcos-mesos-dns.service loaded active running Mesos DNS: DNS based Service Discovery dcos-mesos-master.service loaded active running Mesos Master: DC/OS Mesos Master Service dcos-minuteman.service loaded active running Layer 4 Load Balancer: DC/OS Layer 4 Load Balancing Service dcos-oauth.service loaded active running OAuth: OAuth Authentication Service dcos-spartan.service loaded active running DNS Dispatcher: An RFC5625 Compliant DNS Forwarder dcos.target loaded active active dcos.target dcos-adminrouter-reload.timer loaded active waiting Admin Router Reloader Timer: Periodically reload admin router nginx config to pickup new dns dcos-gen-resolvconf.timer loaded active waiting Generate resolv.conf Timer: Periodically update systemd-resolved for mesos-dns dcos-logrotate.timer loaded active waiting Logrotate Timer: Timer to trigger every 2 minutes dcos-signal.timer loaded active waiting Signal Timer: Timer for DC/OS Signal Service dcos-spartan-watchdog.timer loaded active waiting DNS Dispatcher Watchdog Timer: Periodically check is Spartan is working Agent サーバーのセットアップ # Master とほぼ同じですが最後のスクリプト実行の引数が異なります\n$ sudo bash dcos_install.sh slave Exhibitor for ZooKeeper にアクセスしてみる # Master サーバーの port 8181 で Exhibitor が起動しています\nExhibitor for ZooKeeper  DC/OS Dashboard へアクセスしてみる # Master サーバーの port 80 で DC/OS コンソールが起動しています OAuth でログインします\nDC/OS login  Dashboard 画面（Marathon で1つのコンテナを実行中）\nDC/OS Dashboard  Services 画面では Marathon が稼働していることが確認できます ここから Marathon のインターフェースにアクセスできます\nDC/OS Services  Marathon のインターフェース、ここで Application として Docker コンテナなどを実行できます\nDC/OS Marathon  Nodes では Agent ノードの状況が確認できます、ここでは1つしか Agent をセットアップしていないので1つしか見えません\nDC/OS Nodes  Universe では簡単に（ワンクリック？）追加可能なサービスが並んでいます\nDC/OS Universe Packages  System では DC/OS を構成する各コンポーネントの状況が確認できます\nDC/OS System  非常に沢山のコンポーネントがあります\n Admin Router Admin Router Reloader Admin Router Reloader Timer Cluster ID Diagnostics DNS Dispatcher DNS Dispatcher Watchdog DNS Dispatcher Watchdog Timer Erlang Port Mapping Daemon Exhibitor Generate resolv.conf Generate resolv.conf Timer Keepalived Layer 4 Load Balancer Logrotate Logrotate Timer Marathon Mesos Agent Mesos DNS Mesos History Mesos Master Mesos Persistent Volume Discovery OAuth Package Service Signal Signal Timer  Chronos を追加してみる # Universe の Chronos アイコンの下にある Install Package をクリックすると、確認が表示されます\nDC/OS Chronos install confirm  再度 Install Package をクリックするとインストールが始まります、これだけでインストール完了です\nDC/OS Chronos install finished  しばらくすると Services に Chronos が追加されていることが確認できます\nDC/OS Services に Chronos が追加されている  とりあえず動かすことはできたが調べるべきことが盛り沢山だ\n","date":"2016年8月21日","permalink":"/2016/08/install-dcos/","section":"Posts","summary":"https://dcos.io/docs/1.7/administration/installing/custom/advanced/ を参考に DC/OS をセットアップしてみる 環境はいつものように DigitalOcean CoreOS で master 3台、agent 1台 別 OS で良かったのかどうかわからないけど Bootstrap 用サーバーを Ubuntu で1","title":"DC/OS をセットアップしてみる"},{"content":"","date":"2016年8月21日","permalink":"/tags/dcos/","section":"Tags","summary":"","title":"DCOS"},{"content":"","date":"2016年8月21日","permalink":"/tags/mesos/","section":"Tags","summary":"","title":"Mesos"},{"content":"","date":"2016年8月21日","permalink":"/tags/doctl/","section":"Tags","summary":"","title":"doctl"},{"content":"DigitalOcean のコマンドラインツール doctl の使い方 1ヶ月ほど前からなんでだか使えないなぁと思ってたら削除した token が環境変数に残ってたからだった\u0026hellip; どうして気づかなかったんだろうか\nインストール # go で書かれているので GitHub の release ページからバイナリをダウンロードして PATH の通ったところに置く 2016/8/20 現在での最新版は 1.4.0\n初期設定 # 事前に DigitalOcean のサイトで token を取得して、doctl auth init で token を入力します（自動でブラウザ起動するやつは無くなったのかな？）``` $ doctl auth init DigitalOcean access token: ****************************************************************\nValidating token: OK\n1.4.0 から設定ファイルが $HOME/.doctlcfg から $HOME/.config/doctl/config.yaml に変わり、内容も大幅に増えています compute.ssh.ssh-key-path で doctl compute ssh の際のデフォルトの private key を指定できます compute.ssh.ssh-user ではその際の ssh ユーザーを指定できます。標準のイメージから起動する場合、CentOS や Ubuntu は root でログインすることになりますが、CoreOS の場合は core です。実行時のオプションで指定すれば設定ファイルの値よりも優先されます doctl compute ssh --ssh-user core \u0026lt;droplet\u0026gt; droplet.create.image, droplet.create.size, droplet.create.enable-private-networking, droplet.create.ssh-keys など droplet.create.\\* では doctl compute droplet create の際のデフォルト値を設定できます droplet 作成時のオプションをいくつも設定するのは面倒なのでこれは便利 テストで使う場合はいつも日本から近そうなシンガポールの DC を使うので droplet.create.region: \u0026quot;sgp1\u0026quot; とします doctl コマンドはいちいち長すぎて tab 補完が効いても面倒なので適当な alias を設定しました alias docreate=\u0026ldquo;doctl compute droplet create\u0026rdquo; alias dols=\u0026ldquo;doctl compute droplet ls\u0026rdquo; alias dossh=\u0026ldquo;doctl compute ssh\u0026rdquo; alias dorm=\u0026ldquo;doctl compute droplet delete\u0026rdquo; alias doimgs=\u0026ldquo;doctl compute image list-distribution\u0026rdquo;\n","date":"2016年8月21日","permalink":"/2016/08/doctl/","section":"Posts","summary":"DigitalOcean のコマンドラインツール doctl の使い方 1ヶ月ほど前からなんでだか使えないなぁと思ってたら削除した token が環境変数に残ってたからだった\u0026hellip;","title":"doctl"},{"content":"https://developers.digitalocean.com/documentation/metadata/ にドキュメントがあります。 （AWS EC2 での インスタンスメタデータ です）``` curl -s http://169.254.169.254/metadata/v1/interfaces/public/0/ipv4/address\nとすればパブリックIPアドレスが取得できます。 curl -s http://169.254.169.254/metadata/v1.json\nとすれば JSON ですべて取れます。 { \u0026ldquo;droplet_id\u0026rdquo;:2756294, \u0026ldquo;hostname\u0026rdquo;:\u0026ldquo;sample-droplet\u0026rdquo;, \u0026ldquo;vendor_data\u0026rdquo;:\u0026quot;#cloud-config\\ndisable_root: false\\nmanage_etc_hosts: true\\n\\ncloud_config_modules:\\n - ssh\\n - set_hostname\\n - [ update_etc_hosts, once-per-instance ]\\n\\ncloud_final_modules:\\n - scripts-vendor\\n - scripts-per-once\\n - scripts-per-boot\\n - scripts-per-instance\\n - scripts-user\\n\u0026quot;, \u0026ldquo;public_keys\u0026rdquo;:[\u0026ldquo;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCcbi6cygCUmuNlB0KqzBpHXf7CFYb3VE4pDOf/RLJ8OFDjOM+fjF83a24QktSVIpQnHYpJJT2pQMBxD+ZmnhTbKv+OjwHSHwAfkBullAojgZKzz+oN35P4Ea4J78AvMrHw0zp5MknS+WKEDCA2c6iDRCq6/hZ13Mn64f6c372JK99X29lj/B4VQpKCQyG8PUSTFkb5DXTETGbzuiVft+vM6SF+0XZH9J6dQ7b4yD3sOder+M0Q7I7CJD4VpdVD/JFa2ycOS4A4dZhjKXzabLQXdkWHvYGgNPGA5lI73TcLUAueUYqdq3RrDRfaQ5Z0PEw0mDllCzhk5dQpkmmqNi0F sammy@digitalocean.com\u0026rdquo;], \u0026ldquo;region\u0026rdquo;:\u0026ldquo;nyc3\u0026rdquo;, \u0026ldquo;interfaces\u0026rdquo;:{ \u0026ldquo;private\u0026rdquo;:[ { \u0026ldquo;ipv4\u0026rdquo;:{ \u0026ldquo;ip_address\u0026rdquo;:\u0026ldquo;10.132.255.113\u0026rdquo;, \u0026ldquo;netmask\u0026rdquo;:\u0026ldquo;255.255.0.0\u0026rdquo;, \u0026ldquo;gateway\u0026rdquo;:\u0026ldquo;10.132.0.1\u0026rdquo; }, \u0026ldquo;mac\u0026rdquo;:\u0026ldquo;04:01:2a:0f:2a:02\u0026rdquo;, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;private\u0026rdquo; } ], \u0026ldquo;public\u0026rdquo;:[ { \u0026ldquo;ipv4\u0026rdquo;:{ \u0026ldquo;ip_address\u0026rdquo;:\u0026ldquo;104.131.20.105\u0026rdquo;, \u0026ldquo;netmask\u0026rdquo;:\u0026ldquo;255.255.192.0\u0026rdquo;, \u0026ldquo;gateway\u0026rdquo;:\u0026ldquo;104.131.0.1\u0026rdquo; }, \u0026ldquo;ipv6\u0026rdquo;:{ \u0026ldquo;ip_address\u0026rdquo;:\u0026ldquo;2604:A880:0800:0010:0000:0000:017D:2001\u0026rdquo;, \u0026ldquo;cidr\u0026rdquo;:64, \u0026ldquo;gateway\u0026rdquo;:\u0026ldquo;2604:A880:0800:0010:0000:0000:0000:0001\u0026rdquo; }, \u0026ldquo;mac\u0026rdquo;:\u0026ldquo;04:01:2a:0f:2a:01\u0026rdquo;, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;public\u0026rdquo;} ] }, \u0026ldquo;floating_ip\u0026rdquo;: { \u0026ldquo;ipv4\u0026rdquo;: { \u0026ldquo;active\u0026rdquo;: false } }, \u0026ldquo;dns\u0026rdquo;:{ \u0026ldquo;nameservers\u0026rdquo;:[ \u0026ldquo;2001:4860:4860::8844\u0026rdquo;, \u0026ldquo;2001:4860:4860::8888\u0026rdquo;, \u0026ldquo;8.8.8.8\u0026rdquo; ] } }\n","date":"2016年8月20日","permalink":"/2016/08/digitalocean-metadata-api/","section":"Posts","summary":"https://developers.digitalocean.com/documentation/metadata/ にドキュメントがあります。 （AWS EC2 での インスタンスメタデータ です）``` curl -s http://169.254.169.254/metadata/v1/interfaces/public/0/ipv4/address とすればパブリックIPアドレスが取得できます。 curl -s http://169.254.169.254/metadata/v1.json とすれ","title":"DigitalOcean のメタデータ API"},{"content":"そろそろ Bash on Ubuntu on Linux (https://msdn.microsoft.com/ja-jp/commandline/wsl/about) を試してみようかなと。 普段 Ubuntu 16.04 のインストールされた VAIO を使っているので特に何かやりたいというわけではないけど\u0026hellip; インストールの手順は https://msdn.microsoft.com/commandline/wsl/install_guide に書いてある通り。ただ、英語環境向けなので日本語環境では検索時のキーワードが違ってたりする。\n前提条件の確認 #  2016年8月2日に適用可能となった Windows 10 Anniversary Update - build 14393 が適用されていること 64bit CPU であること AMD/Intel x64 互換の CPU であること Windows Insider Program に参加し、できれば Fast-Ring を使うこと  Fast-Ring については http://ascii.jp/elem/000/001/099/1099995/ がわかりやすい。が、Beta 版 OS を使うということになるので、普段使いの PC では有効にしないほうが無難。\nスタートメニューから歯車アイコンの設定を選択  「システム」を選択  バージョン情報を確認  開発者モードを有効にする # 設定から「更新とセキュリティ」を選択  開発者モードにチェックを入れる  確認が出るのでリスクを許容する  「設定」の検索窓で「Windows の機能の有効化または無効化」を探す  「Windows Subsystem for Linux (Beta)」にチェックを入れて「OK」をクリック  再起動 # ここでいったん再起動\nBash の起動 # 「Windows + R」でコマンドプロンプトを起動し、「Bash」を起動  インストール完了  $ cat /etc/os-release NAME=\u0026quot;Ubuntu\u0026quot; VERSION=\u0026quot;14.04.4 LTS, Trusty Tahr\u0026quot; ID=ubuntu ID_LIKE=debian PRETTY_NAME=\u0026quot;Ubuntu 14.04.4 LTS\u0026quot; VERSION_ID=\u0026quot;14.04\u0026quot; HOME_URL=\u0026quot;http://www.ubuntu.com/\u0026quot; SUPPORT_URL=\u0026quot;http://help.ubuntu.com/\u0026quot; BUG_REPORT_URL=\u0026quot;http://bugs.launchpad.net/ubuntu/\u0026quot; 更新 # スタートメニューから「Bash on Ubuntu on Windows」を起動してみると\n52 個のパッケージがアップデート可能です。 30 個のアップデートはセキュリティアップデートです。 と表示されるので sudo apt-get update, sudo apt-get upgrade -y で更新しました。 sudo のパスワードは Bash インストール時に設定したものを入力します\n次は？ # 普段使いするにはコマンドプロンプトはつらいので何か良い代替ソフトを探す必要がありますね\n","date":"2016年8月20日","permalink":"/2016/08/using-bash-on-ubuntu-on-windows/","section":"Posts","summary":"そろそろ Bash on Ubuntu on Linux (https://msdn.microsoft.com/ja-jp/commandline/wsl/about) を試してみようかなと。 普段 Ubuntu 16.04 のインストールされた VAIO を使っているので特に何かやりたいというわけではないけど\u0026hellip","title":"Bash on Ubuntu on Windows をセットアップする"},{"content":"そろそろ 1.12 の正式版がリリースされる予定ですが DigitalOcean に Ubuntu 14.04 を4台用意して Docker 1.12-rc5 をインストールします。\nroot@sm01:~# curl -fsSL https://test.docker.com/ | sh root@sm01:~# docker version Client: Version: 1.12.0-rc5 API version: 1.24 Go version: go1.6.3 Git commit: a3f2063 Built: Tue Jul 26 13:32:56 2016 OS/Arch: linux/amd64 Server: Version: 1.12.0-rc5 API version: 1.24 Go version: go1.6.3 Git commit: a3f2063 Built: Tue Jul 26 13:32:56 2016 OS/Arch: linux/amd64 root@sm01:~# root@sm01:~# docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 1.12.0-rc5 Storage Driver: aufs Root Dir: /var/lib/docker/aufs Backing Filesystem: extfs Dirs: 0 Dirperm1 Supported: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: null host bridge overlay Swarm: inactive Runtimes: runc Default Runtime: runc Security Options: apparmor Kernel Version: 4.4.0-31-generic Operating System: Ubuntu 14.04.4 LTS OSType: linux Architecture: x86\\_64 CPUs: 2 Total Memory: 1.954 GiB Name: sm01 ID: MGAP:D3I3:LLZT:4RRL:WVUI:HR6U:6XJ3:ON3D:OERN:RBRR:7H6A:I3YG Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ WARNING: No swap limit support Insecure Registries: 127.0.0.0/8 root@sm01:~# 1.12 といえば Swarm mode なので早速\nroot@sm01:~# docker swarm init --help Usage: docker swarm init [OPTIONS] Initialize a swarm Options: --advertise-addr string Advertised address (format: \u0026lt;ip|interface\u0026gt;[:port]) --cert-expiry duration Validity period for node certificates (default 2160h0m0s) --dispatcher-heartbeat duration Dispatcher heartbeat period (default 5s) --external-ca value Specifications of one or more certificate signing endpoints --force-new-cluster Force create a new cluster from current state. --help Print usage --listen-addr value Listen address (format: \u0026lt;ip|interface\u0026gt;[:port]) (default 0.0.0.0:2377) --task-history-limit int Task history retention limit (default 5) root@sm01:~# 4台の Private address は次の通り\nsm01 eth1 10.130.9.198 sm02 eth1 10.130.14.153 sm03 eth1 10.130.21.166 sm04 eth1 10.130.42.39 root@sm01:~# docker swarm init --listen-addr eth1 --advertise-addr eth1 Swarm initialized: current node (8y26xoif1ucu2dr4lnet0z4rk) is now a manager. To add a worker to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-4lz4k3n65d76akrxn934md0e0zrgq5bsq5jf2gq54icncq37qp-009402j3gts3zm1vvqrkqp79f \\ 10.130.9.198:2377 To add a manager to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-4lz4k3n65d76akrxn934md0e0zrgq5bsq5jf2gq54icncq37qp-2nxm00jx3vehtromd3xdgalgg \\ 10.130.9.198:2377 root@sm01:~# worker として join するか manager として join するかは --token の値によって決まるようです。rc4 までにあった --manager というオプションはなくなっています。--auto-accept もなくなりました。 init 時の --advertise-addr も必須になりました。これを明示しなくてハマる人が多かったようです。--task-history-limit は説明「Task history retention limit (default 5)」を読んでも謎ですね。これは Docker イメージの更新などで入れかわた container の履歴とそのボリューム(?)を何世代残すかという設定になります。これを超えてはじめて docker rm されることになります。 それでは manager x3 + worker x1 の構成にしてみます。\nroot@sm02:~# docker swarm join --listen-addr eth1 --advertise-addr eth1 \\ --token SWMTKN-1-4lz4k3n65d76akrxn934md0e0zrgq5bsq5jf2gq54icncq37qp-2nxm00jx3vehtromd3xdgalgg \\ 10.130.9.198:2377 This node joined a swarm as a manager. root@sm02:~# sm03 は sm02 と同じなので省略\nroot@sm04:~# docker swarm join --listen-addr eth1 --advertise-addr eth1 \\ --token SWMTKN-1-4lz4k3n65d76akrxn934md0e0zrgq5bsq5jf2gq54icncq37qp-009402j3gts3zm1vvqrkqp79f \\ 10.130.9.198:2377 This node joined a swarm as a worker. root@sm04:~# root@sm01:~# docker node ls ID HOSTNAME STATUS AVAILABILITY **MANAGER STATUS** 3v11smnwwulxd987mtix494so sm02 Ready Active **Reachable** 4mg10n4rirjylaaekiekybcnl sm03 Ready Active **Reachable** 4p396pp5d3qe3e8sns6er3bgx sm04 Ready Active 8y26xoif1ucu2dr4lnet0z4rk * sm01 Ready Active **Leader** root@sm01:~# join される側での accept 処理は不要になってます。もう完成してしまいました。 token を忘れたり、漏洩しちゃったらどうしましょう？ 忘れた場合は docker swarm join-token {manager|worker} で確認できます。\nroot@sm01:~# docker swarm join-token worker To add a worker to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-4lz4k3n65d76akrxn934md0e0zrgq5bsq5jf2gq54icncq37qp-009402j3gts3zm1vvqrkqp79f \\ 10.130.9.198:2377 root@sm01:~# docker swarm join-token manager To add a manager to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-4lz4k3n65d76akrxn934md0e0zrgq5bsq5jf2gq54icncq37qp-2nxm00jx3vehtromd3xdgalgg \\ 10.130.9.198:2377 root@sm01:~# 漏洩しちゃったり、しばらく使わないという場合は rotate させます。\nroot@sm01:~# docker swarm join-token --rotate worker To add a worker to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-4lz4k3n65d76akrxn934md0e0zrgq5bsq5jf2gq54icncq37qp-4igl65pp3mbgt29t8wy1visjk \\ 10.130.9.198:2377 root@sm01:~# docker swarm join-token --rotate manager To add a manager to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-4lz4k3n65d76akrxn934md0e0zrgq5bsq5jf2gq54icncq37qp-bbb6n7i318f6npco5wd5mg43y \\ 10.130.9.198:2377 root@sm01:~# これで新しい token が発行され、古いものは無効になりました。join のためのコマンドまで表示されてちょっとうざいっていう場合は -q を追加します。\nroot@sm01:~# docker swarm join-token -q worker SWMTKN-1-4lz4k3n65d76akrxn934md0e0zrgq5bsq5jf2gq54icncq37qp-4igl65pp3mbgt29t8wy1visjk root@sm01:~# docker swarm join-token -q manager SWMTKN-1-4lz4k3n65d76akrxn934md0e0zrgq5bsq5jf2gq54icncq37qp-bbb6n7i318f6npco5wd5mg43y root@sm01:~# root@sm01:~# docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 1.12.0-rc5 Storage Driver: aufs Root Dir: /var/lib/docker/aufs Backing Filesystem: extfs Dirs: 0 Dirperm1 Supported: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge null host overlay Swarm: active NodeID: 8y26xoif1ucu2dr4lnet0z4rk Is Manager: true Managers: 3 Nodes: 4 Node Address: 10.130.9.198 Runtimes: runc Default Runtime: runc Security Options: apparmor Kernel Version: 4.4.0-31-generic Operating System: Ubuntu 14.04.4 LTS OSType: linux Architecture: x86_64 CPUs: 2 Total Memory: 1.954 GiB Name: sm01 ID: MGAP:D3I3:LLZT:4RRL:WVUI:HR6U:6XJ3:ON3D:OERN:RBRR:7H6A:I3YG Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ WARNING: No swap limit support Insecure Registries: 127.0.0.0/8 root@sm01:~# Swarm: active NodeID: 8y26xoif1ucu2dr4lnet0z4rk Is Manager: true Managers: 3 Nodes: 4 Node Address: 10.130.9.198 docker-compose のインストール\nroot@sm01:~# curl -Lo /usr/bin/docker-compose https://github.com/docker/compose/releases/download/1.8.0/docker-compose-`uname -s`-`uname -m` % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 600 0 600 0 0 509 0 --:--:-- 0:00:01 --:--:-- 509 100 7783k 100 7783k 0 0 925k 0 0:00:08 0:00:08 --:--:-- 1660k root@sm01:~# chmod 755 /usr/bin/docker-compose root@sm01:~# https://docs.docker.com/compose/wordpress/ にある wordpress を実行するための docker-compose.yml を使ってコンテナを実行してみます\nroot@sm01:~/wordpress# vi docker-compose.yml root@sm01:~/wordpress# docker-compose up -d WARNING: The Docker Engine you're using is running in swarm mode. Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node. To deploy your application across the swarm, use the bundle feature of the Docker experimental build. More info: https://docs.docker.com/compose/bundles Creating network \u0026quot;wordpress_default\u0026quot; with the default driver Pulling db (mysql:5.7)... 5.7: Pulling from library/mysql 5c90d4a2d1a8: Pull complete ... b836883cb3d9: Pull complete Digest: sha256:a9a5b559f8821fe73d58c3606c812d1c044868d42c63817fa5125fd9d8b7b539 Status: Downloaded newer image for mysql:5.7 Pulling wordpress (wordpress:latest)... latest: Pulling from library/wordpress 5c90d4a2d1a8: Already exists 9e955ea56615: Pull complete ... 417100314774: Pull complete Digest: sha256:0f73fa5e20b0194c6cffc78143e9b9b966c952b57118be12526edb058726cd92 Status: Downloaded newer image for wordpress:latest Creating wordpress_db_1 Creating wordpress_wordpress_1 root@sm01:~/wordpress# docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------- wordpress_db_1 docker-entrypoint.sh mysqld Up 3306/tcp wordpress_wordpress_1 /entrypoint.sh apache2-for ... Up 0.0.0.0:8000-\u0026gt;80/tcp root@sm01:~/wordpress# docker ホストの 8000 番ポートにアクセスすると wordpress が起動しています。\nですが、次のような WARNING が出ています\nWARNING: The Docker Engine you\u0026rsquo;re using is running in swarm mode. Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node. To deploy your application across the swarm, use the bundle feature of the Docker experimental build. More info: https://docs.docker.com/compose/bundles\n1.11 の Swarm では docker-compose でも node をまたがって container が展開されていましたが、それをやりたかたら bundle を使えってことのようです。でも bundle ってまだ仕様も固まってないんじゃなかったかな？ 今回の docker-compose.yml には\nvolumes:- \u0026#34;./.data/db:/var/lib/mysql\u0026#34;てな定義もあるからどっちにしてもダメか.. ちょっと dockercon 2016 にあわせていろいろ詰め込み過ぎたんじゃないかと思うわけですが、Docker の release policy は次のようになっているので\u0026hellip; https://github.com/docker/docker/wiki\n The Docker follows a time-based release process, currently aiming at releasing a new major version every two months. We issue a code freeze several weeks before the scheduled release date during which nothing but bugfixes can be added to the release. Even though the process is time-based, the Docker Engine sets short-term goals for the upcoming release(s). However, time will always win over features: the code freeze will happen regardless of our goals being met.\n Service について\nroot@sm01:~/wordpress# docker service create --name nginx --replicas 5 nginx e28bh091n39zxvav6f3agmf1v これだけで nginx container を 5 つキープしてくれる が、docker run に比べて圧倒的に起動時のオプションが少ない\nroot@sm01:~/wordpress# docker service create --help Usage:\tdocker service create [OPTIONS] IMAGE [COMMAND] [ARG...] Create a new service Options: --constraint value Placement constraints (default []) --container-label value Container labels (default []) --endpoint-mode string Endpoint mode (vip or dnsrr) -e, --env value Set environment variables (default []) --help Print usage -l, --label value Service labels (default []) --limit-cpu value Limit CPUs (default 0.000) --limit-memory value Limit Memory (default 0 B) --log-driver string Logging driver for service --log-opt value Logging driver options (default []) --mode string Service mode (replicated or global) (default \u0026quot;replicated\u0026quot;) --mount value Attach a mount to the service --name string Service name --network value Network attachments (default []) -p, --publish value Publish a port as a node port (default []) --replicas value Number of tasks (default none) --reserve-cpu value Reserve CPUs (default 0.000) --reserve-memory value Reserve Memory (default 0 B) --restart-condition string Restart when condition is met (none, on-failure, or any) --restart-delay value Delay between restart attempts (default none) --restart-max-attempts value Maximum number of restarts before giving up (default none) --restart-window value Window used to evaluate the restart policy (default none) --stop-grace-period value Time to wait before force killing a container (default none) --update-delay duration Delay between updates --update-failure-action string Action on update failure (pause|continue) (default \u0026quot;pause\u0026quot;) --update-parallelism uint Maximum number of tasks updated simultaneously (0 to update all at once) (default 1) -u, --user string Username or UID --with-registry-auth Send registry authentication details to swarm agents -w, --workdir string Working directory inside the container root@sm01:~/wordpress# docker service inspect e2 [ { \u0026quot;ID\u0026quot;: \u0026quot;e28bh091n39zxvav6f3agmf1v\u0026quot;, \u0026quot;Version\u0026quot;: { \u0026quot;Index\u0026quot;: 45 }, \u0026quot;CreatedAt\u0026quot;: \u0026quot;2016-07-28T16:17:29.6671509Z\u0026quot;, \u0026quot;UpdatedAt\u0026quot;: \u0026quot;2016-07-28T16:17:29.6671509Z\u0026quot;, \u0026quot;Spec\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;nginx\u0026quot;, \u0026quot;TaskTemplate\u0026quot;: { \u0026quot;ContainerSpec\u0026quot;: { \u0026quot;Image\u0026quot;: \u0026quot;nginx\u0026quot; }, \u0026quot;Resources\u0026quot;: { \u0026quot;Limits\u0026quot;: {}, \u0026quot;Reservations\u0026quot;: {} }, \u0026quot;RestartPolicy\u0026quot;: { \u0026quot;Condition\u0026quot;: \u0026quot;any\u0026quot;, \u0026quot;MaxAttempts\u0026quot;: 0 }, \u0026quot;Placement\u0026quot;: {} }, \u0026quot;Mode\u0026quot;: { \u0026quot;Replicated\u0026quot;: { \u0026quot;Replicas\u0026quot;: 5 } }, \u0026quot;UpdateConfig\u0026quot;: { \u0026quot;Parallelism\u0026quot;: 1, \u0026quot;FailureAction\u0026quot;: \u0026quot;pause\u0026quot; }, \u0026quot;EndpointSpec\u0026quot;: { \u0026quot;Mode\u0026quot;: \u0026quot;vip\u0026quot; } }, \u0026quot;Endpoint\u0026quot;: { \u0026quot;Spec\u0026quot;: {} }, \u0026quot;UpdateStatus\u0026quot;: { \u0026quot;StartedAt\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot;, \u0026quot;CompletedAt\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot; } } ] root@sm01:~/wordpress# root@sm01:~/wordpress# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3063f0490040 nginx:latest \u0026quot;nginx -g 'daemon off\u0026quot; 2 minutes ago Up 2 minutes 80/tcp, 443/tcp nginx.1.4hxwvcyyk4wficzyqi3bmrxt3 fde5a780144e wordpress:latest \u0026quot;/entrypoint.sh apach\u0026quot; 2 hours ago Up 2 hours 0.0.0.0:8000-\u0026gt;80/tcp wordpress_wordpress_1 359e31c3465a mysql:5.7 \u0026quot;docker-entrypoint.sh\u0026quot; 2 hours ago Up 2 hours 3306/tcp wordpress_db_1 root@sm01:~/wordpress# docker inspect 30 [ { \u0026quot;Id\u0026quot;: \u0026quot;3063f04900407965e5d2a509fd24b15b54b7c1a1ed89712817507cdd3975e934\u0026quot;, \u0026quot;Created\u0026quot;: \u0026quot;2016-07-28T16:17:51.378771708Z\u0026quot;, \u0026quot;Path\u0026quot;: \u0026quot;nginx\u0026quot;, \u0026quot;Args\u0026quot;: [ \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot; ], \u0026quot;State\u0026quot;: { \u0026quot;Status\u0026quot;: \u0026quot;running\u0026quot;, \u0026quot;Running\u0026quot;: true, \u0026quot;Paused\u0026quot;: false, \u0026quot;Restarting\u0026quot;: false, \u0026quot;OOMKilled\u0026quot;: false, \u0026quot;Dead\u0026quot;: false, \u0026quot;Pid\u0026quot;: 6060, \u0026quot;ExitCode\u0026quot;: 0, \u0026quot;Error\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;StartedAt\u0026quot;: \u0026quot;2016-07-28T16:17:51.714182965Z\u0026quot;, \u0026quot;FinishedAt\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot; }, \u0026quot;Image\u0026quot;: \u0026quot;sha256:0d409d33b27e47423b049f7f863faa08655a8c901749c2b25b93ca67d01a470d\u0026quot;, \u0026quot;ResolvConfPath\u0026quot;: \u0026quot;/var/lib/docker/containers/3063f04900407965e5d2a509fd24b15b54b7c1a1ed89712817507cdd3975e934/resolv.conf\u0026quot;, \u0026quot;HostnamePath\u0026quot;: \u0026quot;/var/lib/docker/containers/3063f04900407965e5d2a509fd24b15b54b7c1a1ed89712817507cdd3975e934/hostname\u0026quot;, \u0026quot;HostsPath\u0026quot;: \u0026quot;/var/lib/docker/containers/3063f04900407965e5d2a509fd24b15b54b7c1a1ed89712817507cdd3975e934/hosts\u0026quot;, \u0026quot;LogPath\u0026quot;: \u0026quot;/var/lib/docker/containers/3063f04900407965e5d2a509fd24b15b54b7c1a1ed89712817507cdd3975e934/3063f04900407965e5d2a509fd24b15b54b7c1a1ed89712817507cdd3975e934-json.log\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;/nginx.1.4hxwvcyyk4wficzyqi3bmrxt3\u0026quot;, \u0026quot;RestartCount\u0026quot;: 0, \u0026quot;Driver\u0026quot;: \u0026quot;aufs\u0026quot;, \u0026quot;MountLabel\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;ProcessLabel\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;AppArmorProfile\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;ExecIDs\u0026quot;: null, \u0026quot;HostConfig\u0026quot;: { \u0026quot;Binds\u0026quot;: null, \u0026quot;ContainerIDFile\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;LogConfig\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;json-file\u0026quot;, \u0026quot;Config\u0026quot;: {} }, \u0026quot;NetworkMode\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;PortBindings\u0026quot;: null, \u0026quot;RestartPolicy\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;MaximumRetryCount\u0026quot;: 0 }, \u0026quot;AutoRemove\u0026quot;: false, \u0026quot;VolumeDriver\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;VolumesFrom\u0026quot;: null, \u0026quot;CapAdd\u0026quot;: null, \u0026quot;CapDrop\u0026quot;: null, \u0026quot;Dns\u0026quot;: null, \u0026quot;DnsOptions\u0026quot;: null, \u0026quot;DnsSearch\u0026quot;: null, \u0026quot;ExtraHosts\u0026quot;: null, \u0026quot;GroupAdd\u0026quot;: null, \u0026quot;IpcMode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Cgroup\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Links\u0026quot;: null, \u0026quot;OomScoreAdj\u0026quot;: 0, \u0026quot;PidMode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Privileged\u0026quot;: false, \u0026quot;PublishAllPorts\u0026quot;: false, \u0026quot;ReadonlyRootfs\u0026quot;: false, \u0026quot;SecurityOpt\u0026quot;: null, \u0026quot;UTSMode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;UsernsMode\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;ShmSize\u0026quot;: 67108864, \u0026quot;Runtime\u0026quot;: \u0026quot;runc\u0026quot;, \u0026quot;ConsoleSize\u0026quot;: [ 0, 0 ], \u0026quot;Isolation\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;CpuShares\u0026quot;: 0, \u0026quot;Memory\u0026quot;: 0, \u0026quot;CgroupParent\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;BlkioWeight\u0026quot;: 0, \u0026quot;BlkioWeightDevice\u0026quot;: null, \u0026quot;BlkioDeviceReadBps\u0026quot;: null, \u0026quot;BlkioDeviceWriteBps\u0026quot;: null, \u0026quot;BlkioDeviceReadIOps\u0026quot;: null, \u0026quot;BlkioDeviceWriteIOps\u0026quot;: null, \u0026quot;CpuPeriod\u0026quot;: 0, \u0026quot;CpuQuota\u0026quot;: 0, \u0026quot;CpusetCpus\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;CpusetMems\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Devices\u0026quot;: null, \u0026quot;DiskQuota\u0026quot;: 0, \u0026quot;KernelMemory\u0026quot;: 0, \u0026quot;MemoryReservation\u0026quot;: 0, \u0026quot;MemorySwap\u0026quot;: 0, \u0026quot;MemorySwappiness\u0026quot;: -1, \u0026quot;OomKillDisable\u0026quot;: false, \u0026quot;PidsLimit\u0026quot;: 0, \u0026quot;Ulimits\u0026quot;: null, \u0026quot;CpuCount\u0026quot;: 0, \u0026quot;CpuPercent\u0026quot;: 0, \u0026quot;IOMaximumIOps\u0026quot;: 0, \u0026quot;IOMaximumBandwidth\u0026quot;: 0 }, \u0026quot;GraphDriver\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;aufs\u0026quot;, \u0026quot;Data\u0026quot;: null }, \u0026quot;Mounts\u0026quot;: [], \u0026quot;Config\u0026quot;: { \u0026quot;Hostname\u0026quot;: \u0026quot;3063f0490040\u0026quot;, \u0026quot;Domainname\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;User\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;AttachStdin\u0026quot;: false, \u0026quot;AttachStdout\u0026quot;: false, \u0026quot;AttachStderr\u0026quot;: false, \u0026quot;ExposedPorts\u0026quot;: { \u0026quot;443/tcp\u0026quot;: {}, \u0026quot;80/tcp\u0026quot;: {} }, \u0026quot;Tty\u0026quot;: false, \u0026quot;OpenStdin\u0026quot;: false, \u0026quot;StdinOnce\u0026quot;: false, \u0026quot;Env\u0026quot;: [ \u0026quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026quot;, \u0026quot;NGINX_VERSION=1.11.1-1~jessie\u0026quot; ], \u0026quot;Cmd\u0026quot;: [ \u0026quot;nginx\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot; ], \u0026quot;Image\u0026quot;: \u0026quot;nginx:latest\u0026quot;, \u0026quot;Volumes\u0026quot;: null, \u0026quot;WorkingDir\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Entrypoint\u0026quot;: null, \u0026quot;OnBuild\u0026quot;: null, \u0026quot;Labels\u0026quot;: { \u0026quot;com.docker.swarm.node.id\u0026quot;: \u0026quot;8y26xoif1ucu2dr4lnet0z4rk\u0026quot;, \u0026quot;com.docker.swarm.service.id\u0026quot;: \u0026quot;e28bh091n39zxvav6f3agmf1v\u0026quot;, \u0026quot;com.docker.swarm.service.name\u0026quot;: \u0026quot;nginx\u0026quot;, \u0026quot;com.docker.swarm.task\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;com.docker.swarm.task.id\u0026quot;: \u0026quot;4hxwvcyyk4wficzyqi3bmrxt3\u0026quot;, \u0026quot;com.docker.swarm.task.name\u0026quot;: \u0026quot;nginx.1\u0026quot; } }, \u0026quot;NetworkSettings\u0026quot;: { \u0026quot;Bridge\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;SandboxID\u0026quot;: \u0026quot;e9163754dadff812153de58d8e2594ee41f2933baf2036e7d070020216c12dd0\u0026quot;, \u0026quot;HairpinMode\u0026quot;: false, \u0026quot;LinkLocalIPv6Address\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;LinkLocalIPv6PrefixLen\u0026quot;: 0, \u0026quot;Ports\u0026quot;: { \u0026quot;443/tcp\u0026quot;: null, \u0026quot;80/tcp\u0026quot;: null }, \u0026quot;SandboxKey\u0026quot;: \u0026quot;/var/run/docker/netns/e9163754dadf\u0026quot;, \u0026quot;SecondaryIPAddresses\u0026quot;: null, \u0026quot;SecondaryIPv6Addresses\u0026quot;: null, \u0026quot;EndpointID\u0026quot;: \u0026quot;500d153bda733300fd88145230a8bb0f50c83b494d1d4ea8fbf3cd6f96c26d54\u0026quot;, \u0026quot;Gateway\u0026quot;: \u0026quot;172.17.0.1\u0026quot;, \u0026quot;GlobalIPv6Address\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;GlobalIPv6PrefixLen\u0026quot;: 0, \u0026quot;IPAddress\u0026quot;: \u0026quot;172.17.0.2\u0026quot;, \u0026quot;IPPrefixLen\u0026quot;: 16, \u0026quot;IPv6Gateway\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;MacAddress\u0026quot;: \u0026quot;02:42:ac:11:00:02\u0026quot;, \u0026quot;Networks\u0026quot;: { \u0026quot;bridge\u0026quot;: { \u0026quot;IPAMConfig\u0026quot;: null, \u0026quot;Links\u0026quot;: null, \u0026quot;Aliases\u0026quot;: null, \u0026quot;NetworkID\u0026quot;: \u0026quot;aa10bf2517626132866dbf519bbd99163e65ed449e7ada028e1b723ee33f4b46\u0026quot;, \u0026quot;EndpointID\u0026quot;: \u0026quot;500d153bda733300fd88145230a8bb0f50c83b494d1d4ea8fbf3cd6f96c26d54\u0026quot;, \u0026quot;Gateway\u0026quot;: \u0026quot;172.17.0.1\u0026quot;, \u0026quot;IPAddress\u0026quot;: \u0026quot;172.17.0.2\u0026quot;, \u0026quot;IPPrefixLen\u0026quot;: 16, \u0026quot;IPv6Gateway\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;GlobalIPv6Address\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;GlobalIPv6PrefixLen\u0026quot;: 0, \u0026quot;MacAddress\u0026quot;: \u0026quot;02:42:ac:11:00:02\u0026quot; } } } } ] root@sm01:~/wordpress# この状態から nginx.3 を docker kill で止めてみる\nroot@sm01:~/wordpress# docker service tasks nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR 4hxwvcyyk4wficzyqi3bmrxt3 nginx.1 nginx sm01 Running Running 4 minutes ago dgv49gkq3dlwm620vpz03f0c5 nginx.2 nginx sm03 Running Running 4 minutes ago boscs727ouw0859osiiko5j3f nginx.3 nginx sm02 Running Running 4 minutes ago 80e990f922jaohfalmtg20pjc nginx.4 nginx sm04 Running Running 4 minutes ago 6tjgw5kqdoglzbk2ckgffr4dk nginx.5 nginx sm04 Running Running 4 minutes ago root@sm02:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7bbace95d718 nginx:latest \u0026quot;nginx -g 'daemon off\u0026quot; 4 minutes ago Up 4 minutes 80/tcp, 443/tcp nginx.3.boscs727ouw0859osiiko5j3f root@sm02:~# docker kill 7b 7b root@sm02:~# restart してくれた。 _nginx.3 となっているのが swram init のところででてきた history というやつですね、デフォルトではこれが 5 つまでずらずらと残ります。\nroot@sm01:~/wordpress# docker service tasks nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR 4hxwvcyyk4wficzyqi3bmrxt3 nginx.1 nginx sm01 Running Running 5 minutes ago dgv49gkq3dlwm620vpz03f0c5 nginx.2 nginx sm03 Running Running 5 minutes ago dd4tvbrsqc7zqelzqak4tar9c nginx.3 nginx sm02 Ready Preparing 2 seconds ago boscs727ouw0859osiiko5j3f \\_ nginx.3 nginx sm02 Shutdown Failed 2 seconds ago \u0026quot;task: non-zero exit (137)\u0026quot; 80e990f922jaohfalmtg20pjc nginx.4 nginx sm04 Running Running 5 minutes ago 6tjgw5kqdoglzbk2ckgffr4dk nginx.5 nginx sm04 Running Running 5 minutes ago root@sm01:~/wordpress# docker service tasks nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR 4hxwvcyyk4wficzyqi3bmrxt3 nginx.1 nginx sm01 Running Running 5 minutes ago dgv49gkq3dlwm620vpz03f0c5 nginx.2 nginx sm03 Running Running 5 minutes ago dd4tvbrsqc7zqelzqak4tar9c nginx.3 nginx sm02 Running Running 7 seconds ago boscs727ouw0859osiiko5j3f \\_ nginx.3 nginx sm02 Shutdown Failed 13 seconds ago \u0026quot;task: non-zero exit (137)\u0026quot; 80e990f922jaohfalmtg20pjc nginx.4 nginx sm04 Running Running 5 minutes ago 6tjgw5kqdoglzbk2ckgffr4dk nginx.5 nginx sm04 Running Running 5 minutes ago Leader node (sm01) を停止させてみる (SIGKILL)\nroot@sm02:~# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 3v11smnwwulxd987mtix494so * sm02 Ready Active Reachable 4mg10n4rirjylaaekiekybcnl sm03 Ready Active Reachable 4p396pp5d3qe3e8sns6er3bgx sm04 Ready Active 8y26xoif1ucu2dr4lnet0z4rk sm01 Ready Active Leader root@sm02:~# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 3v11smnwwulxd987mtix494so * sm02 Unknown Active Leader 4mg10n4rirjylaaekiekybcnl sm03 Unknown Active Reachable 4p396pp5d3qe3e8sns6er3bgx sm04 Unknown Active 8y26xoif1ucu2dr4lnet0z4rk sm01 Unknown Active Unreachable sm02 が Leader に昇格しました。Service, Task についても sm01 で実行されていた task が sm03 で起動されました。\nroot@sm02:~# docker service tasks nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR 4hxwvcyyk4wficzyqi3bmrxt3 nginx.1 nginx sm01 Running Running 15 minutes ago dgv49gkq3dlwm620vpz03f0c5 nginx.2 nginx sm03 Running Running 15 minutes ago dd4tvbrsqc7zqelzqak4tar9c nginx.3 nginx sm02 Running Running 9 minutes ago boscs727ouw0859osiiko5j3f \\_ nginx.3 nginx sm02 Shutdown Failed 10 minutes ago \u0026quot;task: non-zero exit (137)\u0026quot; 80e990f922jaohfalmtg20pjc nginx.4 nginx sm04 Running Running 15 minutes ago 6tjgw5kqdoglzbk2ckgffr4dk nginx.5 nginx sm04 Running Running 15 minutes ago root@sm02:~# docker service tasks nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR 22qjm1w4a0wkj2bkkbj7arji0 nginx.1 nginx sm03 Running Running 57 seconds ago 4hxwvcyyk4wficzyqi3bmrxt3 \\_ nginx.1 nginx sm01 Shutdown Complete 21 seconds ago dgv49gkq3dlwm620vpz03f0c5 nginx.2 nginx sm03 Running Running 16 minutes ago dd4tvbrsqc7zqelzqak4tar9c nginx.3 nginx sm02 Running Running 11 minutes ago boscs727ouw0859osiiko5j3f \\_ nginx.3 nginx sm02 Shutdown Failed 11 minutes ago \u0026quot;task: non-zero exit (137)\u0026quot; 80e990f922jaohfalmtg20pjc nginx.4 nginx sm04 Running Running 16 minutes ago 6tjgw5kqdoglzbk2ckgffr4dk nginx.5 nginx sm04 Running Running 16 minutes ago sm01 を再起動させたら何事もなかったかのようにクラスタに参加してきました\nroot@sm02:~# docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 3v11smnwwulxd987mtix494so * sm02 Ready Active Leader 4mg10n4rirjylaaekiekybcnl sm03 Ready Active Reachable 4p396pp5d3qe3e8sns6er3bgx sm04 Ready Active 8y26xoif1ucu2dr4lnet0z4rk sm01 Ready Active Reachable けれども勝手にリバランスされりはしないので nginx.4, nginx.5 の2つが稼働しえいる sm04 で nginx.5 を kill してみる\nroot@sm02:~# docker service tasks nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR 22qjm1w4a0wkj2bkkbj7arji0 nginx.1 nginx sm03 Running Running 11 minutes ago 4hxwvcyyk4wficzyqi3bmrxt3 \\_ nginx.1 nginx sm01 Shutdown Complete about a minute ago dgv49gkq3dlwm620vpz03f0c5 nginx.2 nginx sm03 Running Running 26 minutes ago dd4tvbrsqc7zqelzqak4tar9c nginx.3 nginx sm02 Running Running 21 minutes ago boscs727ouw0859osiiko5j3f \\_ nginx.3 nginx sm02 Shutdown Failed 21 minutes ago \u0026quot;task: non-zero exit (137)\u0026quot; 80e990f922jaohfalmtg20pjc nginx.4 nginx sm04 Running Running 26 minutes ago 6tjgw5kqdoglzbk2ckgffr4dk nginx.5 nginx sm04 Running Running 26 minutes ago sm01 に配置されました\nroot@sm01:~# docker service tasks nginx ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR 22qjm1w4a0wkj2bkkbj7arji0 nginx.1 nginx sm03 Running Running 13 minutes ago 4hxwvcyyk4wficzyqi3bmrxt3 \\_ nginx.1 nginx sm01 Shutdown Complete 3 minutes ago dgv49gkq3dlwm620vpz03f0c5 nginx.2 nginx sm03 Running Running 28 minutes ago dd4tvbrsqc7zqelzqak4tar9c nginx.3 nginx sm02 Running Running 23 minutes ago boscs727ouw0859osiiko5j3f \\_ nginx.3 nginx sm02 Shutdown Failed 23 minutes ago \u0026quot;task: non-zero exit (137)\u0026quot; 80e990f922jaohfalmtg20pjc nginx.4 nginx sm04 Running Running 28 minutes ago 6p152l80j7kdlbhhitowbzn1a nginx.5 nginx sm01 Running Running 12 seconds ago 6tjgw5kqdoglzbk2ckgffr4dk \\_ nginx.5 nginx sm04 Shutdown Failed 17 seconds ago \u0026quot;task: non-zero exit (137)\u0026quot; 簡単なテストは期待通りに動作したようです。 bundle について調査しなくっちゃ。\n","date":"2016年7月28日","permalink":"/2016/07/docker-1-12-rc5-part1/","section":"Posts","summary":"そろそろ 1.12 の正式版がリリースされる予定ですが DigitalOcean に Ubuntu 14.04 を4台用意して Docker 1.12-rc5 をインストールします。 root@sm01:~# curl -fsSL https://test.docker.com/ | sh root@sm01:~# docker version Client: Version: 1.12.0-rc5 API version: 1.24 Go version: go1.6.3 Git commit: a3f2063 Built: Tue Jul","title":"Docker 1.12-rc5 を試す - part1"},{"content":"後でやろうと思ってたら忘れてこのサイトの証明書の期限が切れてしまってました\u0026hellip;😢 ということで自動更新の方法をメモ。公式ツールの certbot はまだ名前変わる前のベータの時に試してみたけど個人的にはちょと大げさすぎて too much だなと思っていたので golang で書かれたシングルバイナリの lego を使うことにしました。（おもちゃの LEGO ではありませんが、nodejs のやつみたいに商標問題に発展してしまわないかすこし心配です） 以前試した Acmesmith も悪くないのですがこのサイトは AWS じゃないので外しました。 lego の使い方は簡単です まずはダウンロード、GitHub のリリースページから最新版をダウンロードし展開すれば準備完了\n$ curl -LO https://github.com/xenolf/lego/releases/download/v0.3.1/lego_linux_amd64.tar.xz $ tar xvf lego_linux_amd64.tar.xz lego/ lego/README.md lego/LICENSES.txt lego/CHANGELOG.md lego/lego $ lego/lego NAME: lego - Let's Encrypt client written in Go USAGE: lego/lego [global options] command [command options] [arguments...] VERSION: v0.3.1-0-g96a2477 COMMANDS: run\tRegister an account, then create and install a certificate revoke\tRevoke a certificate renew\tRenew a certificate dnshelp\tShows additional help for the --dns global option help, h\tShows a list of commands or help for one command GLOBAL OPTIONS: --domains, -d [--domains option --domains option] Add domains to the process --server, -s \u0026quot;https://acme-v01.api.letsencrypt.org/directory\u0026quot; CA hostname (and optionally :port). The server certificate must be trusted in order to avoid further modifications to the client. --email, -m Email used for registration and recovery contact. --accept-tos, -a By setting this flag to true you indicate that you accept the current Let's Encrypt terms of service. --key-type, -k \u0026quot;rsa2048\u0026quot; Key type to use for private keys. Supported: rsa2048, rsa4096, rsa8192, ec256, ec384 --path \u0026quot;/root/.lego\u0026quot; Directory to use for storing the data --exclude, -x [--exclude option --exclude option] Explicitly disallow solvers by name from being used. Solvers: \u0026quot;http-01\u0026quot;, \u0026quot;tls-sni-01\u0026quot;. --webroot Set the webroot folder to use for HTTP based challenges to write directly in a file in .well-known/acme-challenge --http Set the port and interface to use for HTTP based challenges to listen on. Supported: interface:port or :port --tls Set the port and interface to use for TLS based challenges to listen on. Supported: interface:port or :port --dns Solve a DNS challenge using the specified provider. Disables all other challenges. Run 'lego dnshelp' for help on usage. --help, -h show help --version, -v print the version lego は dns-01, http-01, tls-sni-01 に対応していますが、ここは Web サーバーなので --webroot を使ってドメイン認証を行うことにします。 --webroot で指定するディレクトリに .well-known/acme-challenge/UeXvRgha4wjQIaKKM31vNubrtvd3C2KrDZpFnscJTBU といったファイルが一時的に作成されてドメインの所有確認が行われます。 Let\u0026rsquo;s Encrypt のサーバーから確認のためのアクセスがくるためアクセスできるように nginx の設定を事前に行っておく必要があります。 次のように /.well-known/acme-challenge/ 配下へのアクセスが --webroot で指定したディレクトリを root とするようにします。\nlocation /.well-known/acme-challenge/ { root /etc/lego/webroot; } すでに HTTPS 化されており、HTTP でアクセスされたら HTTPS に redirect されるようになっていても問題ありません。redirect に対応していました。\nhost:66.133.109.36 time:2016-07-13T15:01:58+00:00 method:GET uri:/.well-known/acme-challenge/y9FLqXFbJ806v6qfWl-Qt-DR5mMscJ5nsnWODmXxUT0 protocol:HTTP/1.1 status:301 ref:- ua:Mozilla/5.0 (compatible; Let's Encrypt validation server; +https://www.letsencrypt.org) https: host:66.133.109.36 time:2016-07-13T15:01:59+00:00 method:GET uri:/.well-known/acme-challenge/y9FLqXFbJ806v6qfWl-Qt-DR5mMscJ5nsnWODmXxUT0 protocol:HTTP/1.1 status:200 ref:http://www.teraoka.me/.well-known/acme-challenge/y9FLqXFbJ806v6qfWl-Qt-DR5mMscJ5nsnWODmXxUT0 ua:Mozilla/5.0 (compatible; Let's Encrypt validation server; +https://www.letsencrypt.org) https:on さて、準備ができたところで初回の証明書発行です。\n# mkdir /etc/lego # mkdir /etc/lego/webroot # lego/lego --path /etc/lego \\ --email user@example.com \\ --domains www.example.com \\ --webroot /etc/lego/webroot \\ --accept-tos run 2016/07/13 14:06:31 No key found for account user@example.com. Generating a curve P384 EC key. 2016/07/13 14:06:31 Saved key to /etc/lego/accounts/acme-v01.api.letsencrypt.org/user@example.com/keys/user@example.com.key 2016/07/13 14:06:32 [INFO] acme: Registering account for user@example.com 2016/07/13 14:06:33 !!!! HEADS UP !!!! 2016/07/13 14:06:33 Your account credentials have been saved in your Let's Encrypt configuration directory at \u0026quot;/etc/lego/accounts/acme-v01.api.letsencrypt.org/user@example.com\u0026quot;. You should make a secure backup\tof this folder now. This configuration directory will also contain certificates and private keys obtained from Let's Encrypt so making regular backups of this folder is ideal. 2016/07/13 14:06:34 [INFO][www.example.com] acme: Obtaining bundled SAN certificate 2016/07/13 14:06:34 [INFO][www.example.com] acme: Trying to solve HTTP-01 2016/07/13 14:06:36 [INFO][www.example.com] The server validated our request 2016/07/13 14:06:36 [INFO][www.teraoka.me] acme: Validations succeeded; requesting certificates 2016/07/13 14:06:38 [INFO] acme: Requesting issuer cert from https://acme-v01.api.letsencrypt.org/acme/issuer-cert 2016/07/13 14:06:38 [INFO][www.example.com] Server responded with a certificate. # これだけでOK😄 --path で指定するディレクトリに certificates というディレクトリができ、そこへ www.example.com.crt, www.example.com.json, www.example.com.key というファイルができ、accounts ディレクトリ配下にアカウントの private key などが生成されます。 更新は renew コマンドです。\n# lego/lego --path /etc/lego \\ --email user@example.com \\ --domains www.example.com \\ --webroot /etc/lego/webroot \\ renew 不要になったり、漏洩しちゃったりしたら revoke しましょう\n# lego/lego --path /etc/lego \\ --email user@example.com \\ --domains www.example.com \\ revoke では更新を自動化しましょう。 雑に書くとこんな感じでいけます\n#!/bin/sh file=$(find /etc/lego/certificates/www.example.com.crt -type f -mtime +80) if [ -n \u0026quot;$file\u0026quot; ] ; then lego --path /etc/lego \\ --email user@example.com \\ --domains www.example.com \\ --webroot /etc/lego/webroot \\ renew nginx -s reload fi あまりに頻繁に renew すると拒否されるようになるので気をつけましょう\n2016/07/13 15:03:24 acme: Error 429 - urn:acme:error:rateLimited - Error creating new cert :: Too many certificates already issued for exact set of domains: www.example.com ","date":"2016年7月13日","permalink":"/2016/07/auto-update-certificate-with-lego/","section":"Posts","summary":"後でやろうと思ってたら忘れてこのサイトの証明書の期限が切れてしまってました\u0026hellip;😢 ということで自動更新の方法をメモ。公式ツールの certbot","title":"lego で Let's Encrypt の証明書を自動更新"},{"content":"","date":"2016年7月13日","permalink":"/tags/ssl/","section":"Tags","summary":"","title":"ssl"},{"content":"Infrastructure as a Newsletter — July 07, 2016 にて\n DO’s network engineering team created NetBox, a tool that manages both IP address management (IPAM) and datacenter infrastructure management (DCIM).\n とあったので早速試してみました。 Django と PostgreSQL で作られた IP アドレスと DataCenter のラック、サーバー、ネットワーク、電源などを管理するツールです。 NetBox の GitHub を覗いてみると docker-compose.yml があったので早速試してみようと\n$ curl -LO https://raw.githubusercontent.com/digitalocean/netbox/develop/docker-compose.yml $ docker-compose up -d としてみましたが\nPulling netbox (digitalocean/netbox:latest)... Pulling repository docker.io/digitalocean/netbox ERROR: Error: image digitalocean/netbox not found とエラーになってしまいました。repository が private なのかな？ netbox repository には Dockerfile もあったので clone して docker-compose.yml の\nimage: digitalocean/netbox を\nbuild: . に書き換えて再度\n$ docker-compose up -d したら無事起動しました。Docker 便利！！\nNetBoxHome  次のような構成になってます。\nDCIM #  Sites  Geographic locations   Racks  Equipment racks, optionally organized by group   Devices  Rack-mounted network equipment, servers, and other devices   Connections  Interfaces Console Power    Secrets #  Secrets  Sensitive data (such as passwords) which has been stored securely    IPAM #  Aggregates  Top-level IP allocations   Prefixes  IPv4 and IPv6 network assignments   IP Addresses  Individual IPv4 and IPv6 addresses   VLANs  Layer two domains, identified by VLAN ID    Circuits #  Providers  Organizations which provide circuit connectivity   Circuits  Communication links for Internet transit, peering, and other services    できることはだいたい racktables と同じ感じです。\nTackTablesHome  Site (DataCenter) があり、そこに Rack を並べ、Device (サーバーやストレージ、スイッチなど) を配置します。Device にはネットワークなどのインターフェースを登録し、それがどの Device のどのポートと接続されてるかも管理できます。（全部登録するのは結構面倒\u0026hellip;）\nRackTables を使ってるならわざわざ乗り換える必要は無い感じですかね。個人的にはデザインは NetBox の方が好きです。Device はロールごとに色を設定できるのでラックのマウント状況を確認する画面が見やすかったりもしますね。\n何ができるのか試していませんが https://github.com/digitalocean/go-netbox という golang で書かれた API クライアントがあるのは便利かもしれません。\nRackTables にも https://github.com/xing/racktables_api とかあるみたいですけど。\nRackTables は PHP と MySQL のシステムです。PHP + MySQL か Python (Django) + PostgreSQL かというのも選定ポイントかな。\n","date":"2016年7月9日","permalink":"/2016/07/digitalocean-netbox/","section":"Posts","summary":"Infrastructure as a Newsletter — July 07, 2016 にて DO’s network engineering team created NetBox, a tool that manages both IP address management (IPAM) and datacenter infrastructure management (DCIM). とあったので早速試してみました。 Django と PostgreSQL で作られた IP アドレスと DataCenter のラック、サ","title":"DigitalOcean が公開した NetBox を使ってみる"},{"content":"2016年7月14日にリリースされる予定の Docker 1.12 ですが RC3 に HEALTHCHECK 機能が入ったようなのでこれを試してみます。（1.12 のその他の目玉機能はDocker 1.12 の衝撃 [slideshare] をどうぞ） https://github.com/docker/docker/releases/tag/v1.12.0-rc3\n New HEALTHCHECK Dockerfile instruction to support user-defined healthchecks #23218\n RC版のインストールも簡単でcurl -fsSL https://test.docker.com/ | shだけ。 バージョン情報はこちら``` Client: Version: 1.12.0-rc3 API version: 1.24 Go version: go1.6.2 Git commit: 91e29e8 Built: Sat Jul 2 00:28:53 2016 OS/Arch: linux/amd64\nServer: Version: 1.12.0-rc3 API version: 1.24 Go version: go1.6.2 Git commit: 91e29e8 Built: Sat Jul 2 00:28:53 2016 OS/Arch: linux/amd64 [DigitalOcean](https://m.do.co/c/97e74a2e7336) を使って3台の swarm mode cluster をセットアップします。 root@docker01:~# docker swarm init \u0026ndash;listen-addr 10.130.13.161 Swarm initialized: current node (8a0gb55owih94q8lwum4b61us) is now a manager.\nroot@docker02:~# docker swarm join \u0026ndash;listen-addr 10.130.27.157 10.130.13.161 This node joined a Swarm as a worker.\nroot@docker03:~# docker swarm join \u0026ndash;listen-addr 10.130.44.213 10.130.13.161 This node joined a Swarm as a worker.\nroot@docker01:~# docker node ls ID HOSTNAME MEMBERSHIP STATUS AVAILABILITY MANAGER STATUS 3tv5prg22lgebh0rvjp8gu4o4 docker03 Accepted Ready Active 8a0gb55owih94q8lwum4b61us * docker01 Accepted Ready Active Leader ciezynqpmqa9ok5egpuf2sdjy docker02 Accepted Ready Active 簡単ですねぇ、たったこれだけで3台のクラスタができちゃいます。 （あら？worker node って RC2 の時から auto accept だったっけ？） テストにあたって nginx:1.11.1-alpine をベースに HEALTHCHECK の有効な docker image を build してみます。 まず、healthcheck.sh というスクリプトを準備 #!/bin/sh\nstatus=`curl -so /dev/null -w %{http_code} http://127.0.0.1/healthcheck` if [ $? -eq 0 -a \u0026ldquo;$status\u0026rdquo; = \u0026ldquo;200\u0026rdquo; ] ; then exit 0 else exit 1 fi healthcheck コマンドは OK の場合は 0、NG の場合は 1 で終了させます。起動中の場合は 2 で終了させますが、一度 healthy となった後は 1 も 2 も同じです。 このスクリプトでは nginx に GET /healthcheck でテストします。初期状態ではこの URL は 404 となるため 1 で終了します。 次に Dockerfile を準備 FROM nginx:1.11.1-alpine COPY healthcheck.sh / RUN apk update \u0026amp;\u0026amp; apk add curl \u0026amp;\u0026amp; chmod 755 /healthcheck.sh HEALTHCHECK \u0026ndash;interval=5s \u0026ndash;timeout=3s \u0026ndash;retries=1 CMD /healthcheck.sh 大事なのは `HEALTHCHECK --interval=5s --timeout=3s --retries=1 CMD /healthcheck.sh` の部分ですね。5秒おきに /healthcheck.sh が実行されます。ポートへの connect だけの確認であれば `HEALTHCHECK CONNECT TCP 7000` という書き方もできるようです。interval や timeout, retries は省略可能。 build して Docker Hub にアップロードします root@docker01:~# docker build \u0026ndash;tag yteraoka/nginx-healthcheck . root@docker01:~# docker login root@docker01:~# docker push yteraoka/nginx-healthcheck docker image の準備ができたところでコンテナ3つを維持するように service を起動させます root@docker01:~# docker service create \u0026ndash;name nginx \u0026ndash;replicas 3 -p 80 yteraoka/nginx-healthcheck 7dxakcqlnsqcoq4g9sqq6qyvc\nroot@docker01:~# docker service ls ID NAME REPLICAS IMAGE COMMAND 7dxakcqlnsqc nginx 3/3 yteraoka/nginx-healthcheck\nroot@docker01:~# docker service tasks nginx ID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE 0qwcjtu4pni4i5r8qn37d6nj4 nginx.1 nginx yteraoka/nginx-healthcheck Running About a minute Running docker01 9ajup0heqn1i9j894ntn7e17j nginx.2 nginx yteraoka/nginx-healthcheck Running About a minute Running docker03 0zbp7nniwimnj89v3f2d3i644 nginx.3 nginx yteraoka/nginx-healthcheck Running About a minute Running docker02 あら簡単！ healthcheck が機能しているかどうかログを確認します root@docker01:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac6a04048f48 yteraoka/nginx-healthcheck:latest \u0026ldquo;nginx -g \u0026lsquo;daemon off\u0026rdquo; About a minute ago Up About a minute (unhealthy) 80/tcp, 443/tcp nginx.1.4t4708e2hhijpk3i0oh6udoav\nroot@docker01:~# docker logs \u0026ndash;tail 5 ac6a04048f48 127.0.0.1 - - [05/Jul/2016:14:21:46 +0000] \u0026ldquo;GET /healthcheck HTTP/1.1\u0026rdquo; 404 169 \u0026ldquo;-\u0026rdquo; \u0026ldquo;curl/7.49.1\u0026rdquo; \u0026ldquo;-\u0026rdquo; 2016/07/05 14:21:51 [error] 6#6: *22 open() \u0026ldquo;/usr/share/nginx/html/healthcheck\u0026rdquo; failed (2: No such file or directory), client: 127.0.0.1, server: localhost, request: \u0026ldquo;GET /healthcheck HTTP/1.1\u0026rdquo;, host: \u0026ldquo;127.0.0.1\u0026rdquo; 127.0.0.1 - - [05/Jul/2016:14:21:51 +0000] \u0026ldquo;GET /healthcheck HTTP/1.1\u0026rdquo; 404 169 \u0026ldquo;-\u0026rdquo; \u0026ldquo;curl/7.49.1\u0026rdquo; \u0026ldquo;-\u0026rdquo; 2016/07/05 14:21:56 [error] 6#6: *23 open() \u0026ldquo;/usr/share/nginx/html/healthcheck\u0026rdquo; failed (2: No such file or directory), client: 127.0.0.1, server: localhost, request: \u0026ldquo;GET /healthcheck HTTP/1.1\u0026rdquo;, host: \u0026ldquo;127.0.0.1\u0026rdquo; 127.0.0.1 - - [05/Jul/2016:14:21:56 +0000] \u0026ldquo;GET /healthcheck HTTP/1.1\u0026rdquo; 404 169 \u0026ldquo;-\u0026rdquo; \u0026ldquo;curl/7.49.1\u0026rdquo; \u0026ldquo;-\u0026rdquo; 404 だから unhealthy のはずですね。さて、これはサービスにどう影響するのでしょう？ 期待としては unhealthy の状態では Load Balancer のメンバーに組み込まれないという状態ですが・・・ わかりやすいように /index.html にそれぞれ docker01, docker02, docker03 を書き込みます。 `docker exec -it ... /bin/sh` で `echo docker01 \u0026gt; /usr/share/nginx/html/index.html` てな具合に。 それから Load Balancer のポート番号を確認 root@docker01:~# docker service inspect nginx -f \u0026lsquo;{{range .Endpoint.Ports}}{{.PublishedPort}}{{end}}\u0026rsquo; 30000 30000 番なので http://localhost:30000/ に何度かアクセスしてみると root@docker01:~# curl http://localhost:30000/ docker02 root@docker01:~# curl http://localhost:30000/ docker03 root@docker01:~# curl http://localhost:30000/ docker01 root@docker01:~# curl http://localhost:30000/ docker02 root@docker01:~# curl http://localhost:30000/ docker03 root@docker01:~# あれ？3つとも有効になってるぞ 1つでも healthy にしてみたらなにか変わるだろうか？また docker exec で touch /usr/share/nginx/html/healthcheck して再度 curl で 30000 番にアクセスしてみるも変化なし おやおや？期待の動作じゃありませんね `docker inspect` で healthcheck の状態が確認できるようなので試してみます healthy な場合 root@docker01:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac6a04048f48 yteraoka/nginx-healthcheck:latest \u0026ldquo;nginx -g \u0026lsquo;daemon off\u0026rdquo; 9 minutes ago Up 9 minutes (healthy) 80/tcp, 443/tcp nginx.1.4t4708e2hhijpk3i0oh6udoav root@docker01:~# docker inspect -f {{.State.Health.Status}} ac6a04048f48 healthy root@docker01:~# docker inspect -f \u0026lsquo;{{json .State.Health}}\u0026rsquo; ac6a04048f48 { \u0026ldquo;Status\u0026rdquo;:\u0026ldquo;healthy\u0026rdquo;, \u0026ldquo;FailingStreak\u0026rdquo;:0, \u0026ldquo;Log\u0026rdquo;:[ { \u0026ldquo;Start\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:19.247502323-04:00\u0026rdquo;, \u0026ldquo;End\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:19.300936843-04:00\u0026rdquo;, \u0026ldquo;ExitCode\u0026rdquo;:0, \u0026ldquo;Output\u0026rdquo;:\u0026quot;\u0026quot; }, { \u0026ldquo;Start\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:24.301484104-04:00\u0026rdquo;, \u0026ldquo;End\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:24.386685529-04:00\u0026rdquo;, \u0026ldquo;ExitCode\u0026rdquo;:0, \u0026ldquo;Output\u0026rdquo;:\u0026quot;\u0026quot; }, { \u0026ldquo;Start\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:29.387079189-04:00\u0026rdquo;, \u0026ldquo;End\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:29.468869919-04:00\u0026rdquo;, \u0026ldquo;ExitCode\u0026rdquo;:0, \u0026ldquo;Output\u0026rdquo;:\u0026quot;\u0026quot; }, { \u0026ldquo;Start\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:34.469089038-04:00\u0026rdquo;, \u0026ldquo;End\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:34.545193782-04:00\u0026rdquo;, \u0026ldquo;ExitCode\u0026rdquo;:0, \u0026ldquo;Output\u0026rdquo;:\u0026quot;\u0026quot; }, { \u0026ldquo;Start\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:39.545724519-04:00\u0026rdquo;, \u0026ldquo;End\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:39.621370916-04:00\u0026rdquo;, \u0026ldquo;ExitCode\u0026rdquo;:0, \u0026ldquo;Output\u0026rdquo;:\u0026quot;\u0026quot; } ] } root@docker01:~# unhealthy な場合 root@docker02:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5c20b551e3b9 yteraoka/nginx-healthcheck:latest \u0026ldquo;nginx -g \u0026lsquo;daemon off\u0026rdquo; 9 minutes ago Up 9 minutes (unhealthy) 80/tcp, 443/tcp nginx.2.8bvv14ynpt9wvnruo6oukoy5u root@docker02:~# docker inspect -f {{.State.Health.Status}} 5c20b551e3b9 unhealthy root@docker02:~# docker inspect -f \u0026lsquo;{{json .State.Health}}\u0026rsquo; 5c20b551e3b9 { \u0026ldquo;Status\u0026rdquo;:\u0026ldquo;unhealthy\u0026rdquo;, \u0026ldquo;FailingStreak\u0026rdquo;:122, \u0026ldquo;Log\u0026rdquo;:[ { \u0026ldquo;Start\u0026rdquo;:\u0026ldquo;2016-07-05T10:29:58.986077293-04:00\u0026rdquo;, \u0026ldquo;End\u0026rdquo;:\u0026ldquo;2016-07-05T10:29:59.074625863-04:00\u0026rdquo;, \u0026ldquo;ExitCode\u0026rdquo;:1, \u0026ldquo;Output\u0026rdquo;:\u0026quot;\u0026quot; }, { \u0026ldquo;Start\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:04.075129735-04:00\u0026rdquo;, \u0026ldquo;End\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:04.160529007-04:00\u0026rdquo;, \u0026ldquo;ExitCode\u0026rdquo;:1, \u0026ldquo;Output\u0026rdquo;:\u0026quot;\u0026quot; }, { \u0026ldquo;Start\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:09.160904985-04:00\u0026rdquo;, \u0026ldquo;End\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:09.253501465-04:00\u0026rdquo;, \u0026ldquo;ExitCode\u0026rdquo;:1, \u0026ldquo;Output\u0026rdquo;:\u0026quot;\u0026quot; }, { \u0026ldquo;Start\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:14.253976633-04:00\u0026rdquo;, \u0026ldquo;End\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:14.32434033-04:00\u0026rdquo;, \u0026ldquo;ExitCode\u0026rdquo;:1, \u0026ldquo;Output\u0026rdquo;:\u0026quot;\u0026quot; }, { \u0026ldquo;Start\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:19.324644872-04:00\u0026rdquo;, \u0026ldquo;End\u0026rdquo;:\u0026ldquo;2016-07-05T10:30:19.395827854-04:00\u0026rdquo;, \u0026ldquo;ExitCode\u0026rdquo;:1, \u0026ldquo;Output\u0026rdquo;:\u0026quot;\u0026quot; } ] } root@docker02:~# healthcheck 自体は機能しているが、現状はまだこれによて Load Balancer の状態に反映されたりはしないようだ。コード読めってことですね、はい。 event としては通知されます。ドキュメントにもまだ event のことしか書かれていない。`docker events` コマンドで event を monitor できます。 2016-07-05T11:32:19.612553558-04:00 container health_status: unhealthy ac6a04048f488d66457edea65b168d5b51671cc9da9aab7a63719c78c6bb8d45 (com.docker.swarm.node.id=8a0gb55owih94q8lwum4b61us, com.docker.swarm.service.id=ecbqm2dd25ibcsf6fmwqbt6yl, com.docker.swarm.service.name=nginx, com.docker.swarm.task=, com.docker.swarm.task.id=4t4708e2hhijpk3i0oh6udoav, com.docker.swarm.task.name=nginx.1, image=yteraoka/nginx-healthcheck:latest, name=nginx.1.4t4708e2hhijpk3i0oh6udoav)\n2016-07-05T11:32:29.796809926-04:00 container health_status: healthy ac6a04048f488d66457edea65b168d5b51671cc9da9aab7a63719c78c6bb8d45 (com.docker.swarm.node.id=8a0gb55owih94q8lwum4b61us, com.docker.swarm.service.id=ecbqm2dd25ibcsf6fmwqbt6yl, com.docker.swarm.service.name=nginx, com.docker.swarm.task=, com.docker.swarm.task.id=4t4708e2hhijpk3i0oh6udoav, com.docker.swarm.task.name=nginx.1, image=yteraoka/nginx-healthcheck:latest, name=nginx.1.4t4708e2hhijpk3i0oh6udoav) ````docker runにも\u0026ndash;health-cmd, \u0026ndash;health-interval, \u0026ndash;health-retries, \u0026ndash;health-timeout, \u0026ndash;no-healthcheck` というオプションが追加されています。 ※更新※ 1.12.0 では Service で HEALTHCHECK が FAIL すると、その TASK は停止させれら、新しく起動されるようになっています おまけ\nDocker 1.12 の衝撃 from Yoshinori Teraoka\n","date":"2016年7月5日","permalink":"/2016/07/docker-healthcheck/","section":"Posts","summary":"2016年7月14日にリリースされる予定の Docker 1.12 ですが RC3 に HEALTHCHECK 機能が入ったようなのでこれを試してみます。（1.12 のその他の目玉機能はDocke","title":"Docker 1.12-RC3 の HEALTHCHECK を試す"},{"content":"","date":"2016年6月13日","permalink":"/tags/loadbalancer/","section":"Tags","summary":"","title":"LoadBalancer"},{"content":"docker 化をすすめるにあたり、consul-template と registrator で nginx の upstream を動的に更新しようと思いました。 ところで、\nupstream backend { server 10.1.2.3:3456; server 10.1.2.4:4567; server 10.1.2.5:5678; } という状態から1つ減って\nupstream backend { server 10.1.2.3:3456; server 10.1.2.4:4567; } となった場合、消えた 10.1.2.5:5678 とつながっていたクライアントとの通信はどうなるのでしょうか？ LoadBalancer には Connection Draining という機能・設定があります。 ELB にもあります（ロードバランサーの Connection Draining を設定する）。 nginx ではどうなるのか試してみました。version は 1.11.1 nginx で\nupstream backend { server 127.0.0.1:8080; server 127.0.0.1:8081; server 127.0.0.1:8082; server 127.0.0.1:8083; } とし、Apache にてざっとこんな感じで 8080, 8081, 8082, 8083 で CGI が動くようにし、\nListen 8080 Listen 8081 Listen 8082 Listen 8083 ScriptAlias /cgi-bin/ \u0026quot;/var/www/cgi-bin/\u0026quot; AllowOverride None Options None Order allow,deny Allow from all こんな CGI で各ポートに接続され、出力が流れている状態で nginx の upstream を減らして nginx -s reload すると通信が止まるかどうか\n#!/usr/bin/perl use strict; use warnings; $| = 1; print \u0026#34;Content-Type: text/plain\\n\\n\u0026#34;; for my $i (0 .. 60) { printf \u0026#34;%3d %s\\n\u0026#34;, $i, $ENV{SERVER_PORT}; sleep 1; } 結果は「通信は途切れない」、もちろん新規の接続は upstream に残った proxy 先にのみ振られます。 良かった良かった、これで安心して思う存分コンテナの入れ替えができます。\n","date":"2016年6月13日","permalink":"/2016/06/nginx-%E3%81%AE-connection-draining/","section":"Posts","summary":"docker 化をすすめるにあたり、consul-template と registrator で nginx の upstream を動的に更新しようと思いました。 ところで、 upstream backend { server 10.1.2.3:3456; server 10.1.2.4:4567; server 10.1.2.5:5678; } という状態か","title":"nginx の Connection Draining"},{"content":"DigitalOcean の API にアクセスするコマンドラインツールである doctl の使い方をメモ\nAccess Token の設定 # $ doctl auth login ```と実行すればブラウザが起動して DigitalOcean のログインフォームが表示されるのでログインすれば ~/.doctlcfg ファイルに``` access-token: 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef ```てな具合に保存されます。（私の手元では 1.0.2 ではこれが正常に動作したのですが、1.1.0, 1.2.0 では動作しませんでした） `.doctlcfg` に設定する以外に `DIGITALOCEAN_ACCESS_TOKEN` という環境変数に設定する方法もあります ### Droplet （仮想サーバー） の作成 作成には `docker compute droplet create` を使う、次のように `--image`, `--size`, `--region` が必須項目となっている``` $ doctl compute droplet create -h create droplet Usage: doctl compute droplet create NAME \\[NAME ...\\] \\[flags\\] Aliases: create, c Flags: --enable-backups Backup droplet --enable-ipv6 IPv6 support --enable-private-networking Private networking --format string Columns for output in a comma seperated list. Possible values: ID,Name,PublicIPv4,Memory,VCPUs,Disk,Region,Image,Status,Tags --image string Droplet image (required) --no-header hide headers --region string Droplet region (required) --size string Droplet size (required) --ssh-keys value SSH Keys or fingerprints (default \\[\\]) --tag-name string Tag name --user-data string User data --user-data-file string User data file --wait Wait for droplet to be created Global Flags: -t, --access-token string API V2 Access Token -c, --config string config file (default is $HOME/.doctlcfg) -o, --output string output format \\[text|json\\] (default \u0026quot;text\u0026quot;) --trace trace api access -v, --verbose verbose output $ doctl compute image list で起動可能なイメージの一覧が表示されます。ubuntu のイメージの一覧は次のようにして確認できる `create` の `--image` にはこの出力の `Slug` の値を指定する $ doctl compute image list | egrep \u0026lsquo;^ID|ubuntu\u0026rsquo; ID Name Type Distribution Slug Public Min Disk 17154032 14.04.4 x64 snapshot Ubuntu ubuntu-14-04-x64 true 20 17154107 14.04.4 x32 snapshot Ubuntu ubuntu-14-04-x32 true 20 17157155 12.04.5 x64 snapshot Ubuntu ubuntu-12-04-x64 true 20 17157433 12.04.5 x32 snapshot Ubuntu ubuntu-12-04-x32 true 20 15621816 15.10 x64 snapshot Ubuntu ubuntu-15-10-x64 true 20 15621817 15.10 x32 snapshot Ubuntu ubuntu-15-10-x32 true 20 17769086 16.04 x64 snapshot Ubuntu ubuntu-16-04-x64 true 20 17769845 16.04 x32 snapshot Ubuntu ubuntu-16-04-x32 true 20\nリージョンの一覧は次のようにして確認できます $ doctl compute region list Slug Name Available nyc1 New York 1 true sfo1 San Francisco 1 true nyc2 New York 2 true ams2 Amsterdam 2 true sgp1 Singapore 1 true lon1 London 1 true nyc3 New York 3 true ams3 Amsterdam 3 true fra1 Frankfurt 1 true tor1 Toronto 1 true blr1 Bangalore 1 true サイズは次のコマンドで。これも `--size` には `Slug` を指定します。 Slug はメモリのサイズになっていますが DigitalOcean ではこのようにメモリ、CPU、Diskのサイズがセットになっています。 $ doctl compute size list Slug Memory VCPUs Disk Price Monthly Price Hourly 512mb 512 1 20 5.00 0.007440 1gb 1024 1 30 10.00 0.014880 2gb 2048 2 40 20.00 0.029760 4gb 4096 2 60 40.00 0.059520 8gb 8192 4 80 80.00 0.119050 16gb 16384 8 160 160.00 0.238100 32gb 32768 12 320 320.00 0.476190 48gb 49152 16 480 480.00 0.714290 64gb 65536 20 640 640.00 0.952380 作成してみます。 $ doctl compute droplet create test01 \u0026ndash;image ubuntu-16-04-x64 \u0026ndash;size 512mb \u0026ndash;region sgp1 \u0026ndash;ssh-keys 76364 ID Name Public IPv4 Memory VCPUs Disk Region Image Status Tags 17218487 test01 512 1 20 sgp1 Ubuntu 16.04 x64 new 起動したら Status が `active` にかわります。 $ doctl compute droplet list ID Name Public IPv4 Memory VCPUs Disk Region Image Status Tags 17218487 test01 128.199.206.232 512 1 20 sgp1 Ubuntu 16.04 x64 active IPアドレスを確認せずとも次のようにして SSH でアクセスできます $ doctl compute ssh test01 ````createの\u0026ndash;ssh-keys` は必須ではないため省略可能です。省略するとパスワードがメールで送られてきます DigitalOcean の sshd はパスワード認証が有効なので要注意\nDroplet （仮想サーバー） の削除 # $ doctl compute droplet delete -h Delete droplet by id or name Usage: doctl compute droplet delete ID \\[ID|Name ...\\] \\[flags\\] Aliases: delete, d, del, rm Global Flags: -t, --access-token string API V2 Access Token -c, --config string config file (default is $HOME/.doctlcfg) -o, --output string output format \\[text|json\\] (default \u0026quot;text\u0026quot;) --trace trace api access -v, --verbose verbose output $ doctl compute droplet delete test01 deleted droplet 17218487\n","date":"2016年6月11日","permalink":"/2016/06/doctl-%E3%81%AE%E4%BD%BF%E3%81%84%E6%96%B9/","section":"Posts","summary":"DigitalOcean の API にアクセスするコマンドラインツールである doctl の使い方をメモ Access Token の設定 # $ doctl auth login ```と実行すればブラウザが起動して DigitalOcean のログインフォームが","title":"DigitalOcean の doctl の使い方"},{"content":"","date":"2016年6月10日","permalink":"/tags/certificate/","section":"Tags","summary":"","title":"Certificate"},{"content":"https://github.com/ehazlett/certm という TLS 証明書作成ツールを見つけたのでメモっておく OpenSSL での証明書作成については https://jamielinux.com/docs/openssl-certificate-authority/index.html がとても良く出来ているのであまりこのツールに頼ることはない気もするがテスト用の証明書をさくっと作りたい場合には使うかもしれない （docker を実行可能な環境であれば docker run ... と実行するだけで使えるっていうのは配布方法としても悪くないですね） まずはヘルプを見てみる``` $ docker run \u0026ndash;rm ehazlett/certm -h NAME: /bin/certm - certificate management\nUSAGE: /bin/certm [global options] command [command options] [arguments\u0026hellip;]\nVERSION: 0.1.2 (f7754d5)\nAUTHOR: @ehazlett\nCOMMANDS: ca\tCA certificate management server\tserver certificate management client\tclient certificate management bundle\tgenerate CA, server and client certs help, h\tShows a list of commands or help for one command\nGLOBAL OPTIONS: \u0026ndash;output-directory, -d output directory for certs \u0026ndash;debug, -D\tenable debug \u0026ndash;help, -h\tshow help \u0026ndash;version, -v\tprint the version\n ### CA の証明書を作成する $ mkdir certs $ docker run \u0026ndash;rm -v $(pwd)/certs:/certs ehazlett/certm -d /certs ca generate -o=local generating ca: org=local bits=2048 $ ls certs ca-key.pem ca.pem\nこんなのが生成されました。 $ openssl x509 -text -in certs/ca.pem -noout Certificate: Data: Version: 3 (0x2) Serial Number: 5c:4b:0e:8c:4a:28:20:68:36🇩🇪2d:a6:88:82:bf:f6 Signature Algorithm: sha256WithRSAEncryption Issuer: O=local Validity Not Before: Jun 10 13:42:00 2016 GMT Not After : May 26 13:42:00 2019 GMT Subject: O=local Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:bb:8f:c4:1c:53:c7:11:d8:fb:5d:d6:33:1c:cb: \u0026hellip; 26:17 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment, Key Agreement, Certificate Sign X509v3 Basic Constraints: critical CA:TRUE Signature Algorithm: sha256WithRSAEncryption 87:bb:4c:4a:d9:0b:a8:5d:88:ac:52:6b:96:b0:38:6c:b3:dc: \u0026hellip; fc:09:9f:2e\n鍵は RSA の 2048 bit Subject で指定可のなのは `O` の Organization だけ 鍵の bit 数は `-b` で指定可能 $ docker run \u0026ndash;rm ehazlett/certm ca -h NAME: generate - generate new certificate\nUSAGE: command generate [command options] [arguments\u0026hellip;]\nOPTIONS: \u0026ndash;org, -o \u0026ldquo;unknown\u0026rdquo;\torganization \u0026ndash;bits, -b \u0026ldquo;2048\u0026rdquo;\tnumber of bits in the key (default: 2048) \u0026ndash;overwrite\toverwrite existing certificates and keys\n ### サーバー証明書を作成する $ docker run \u0026ndash;rm -v $(pwd)/certs:/certs ehazlett/certm -d /certs server generate \u0026ndash;host localhost \u0026ndash;host 127.0.0.1 -o=local generating server certificate: org=local bits=2048 $ ls certs/ ca-key.pem ca.pem server-key.pem server.pem\n次のような証明書が作成されました $ openssl x509 -text -in certs/server.pem -noout Certificate: Data: Version: 3 (0x2) Serial Number: 54:e0:8b:ae:b9:94💿e3:09:fb:79:22:38:dd:a3:50 Signature Algorithm: sha256WithRSAEncryption Issuer: O=local Validity Not Before: Jun 10 13:48:00 2016 GMT Not After : May 26 13:48:00 2019 GMT Subject: O=local Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:ea:3c:c4:ac:46:23:09:29:e0:a1:65:35:d6:00: \u0026hellip; 85:ff Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment, Key Agreement X509v3 Extended Key Usage: TLS Web Client Authentication, TLS Web Server Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Alternative Name: DNS:localhost, IP Address:127.0.0.1 Signature Algorithm: sha256WithRSAEncryption 27:5e:4e:b9:89:87:93:f7:af:0f:a8:89:fc:87:25:82:2b:81: \u0026hellip; 9f:f2:b8:fa\nこちらも Subject は O=local (Organization) だけ Subject: O=local サーバー証明書作成モード (server) だけどクライアント証明書としても使えるようになっている X509v3 Extended Key Usage: TLS Web Client Authentication, TLS Web Server Authentication ````\u0026ndash;host localhost \u0026ndash;host 127.0.0.1` と指定したため subjectAltName に DNS:localhost,IP:127.0.0.1 が指定されている。もっと沢山ならべることも可能。IPアドレスでアクセスする場合は CommonName (CN) が使えないらしいので subjectAltName が使えることを知っておくと便利X509v3 Subject Alternative Name: DNS:localhost, IP Address:127.0.0.1こちらも指定できる subject は Organization だけですね。sbjectAltName が指定できるから CommonName が指定できなくても大丈夫なのかな。ダメなクライアントもありそうだけど。``` $ docker run \u0026ndash;rm ehazlett/certm server -h NAME: generate - generate new certificate\nUSAGE: command generate [command options] [arguments\u0026hellip;]\nOPTIONS: \u0026ndash;ca-cert CA certificate for signing (defaults to ca.pem in output dir) \u0026ndash;ca-key CA key for signing (defaults to ca-key.pem in output dir) \u0026ndash;cert certificate name (default: server.pem) \u0026ndash;key key name (default: server-key.pem) \u0026ndash;host [\u0026ndash;host option \u0026ndash;host option]\tSAN/IP SAN for certificate \u0026ndash;org, -o \u0026ldquo;unknown\u0026rdquo;\torganization \u0026ndash;bits, -b \u0026ldquo;2048\u0026rdquo;\tnumber of bits in the key (default: 2048) \u0026ndash;overwrite\toverwrite existing certificates and keys\n ### クライアント署名書を作成する クライアント証明書では `CommonName` が指定可能になってますね クライアントのアイデンティファイにも使えるようにかな``` $ docker run --rm ehazlett/certm client -h NAME: generate - generate new certificate USAGE: command generate \\[command options\\] \\[arguments...\\] OPTIONS: --ca-cert CA certificate for signing (defaults to ca.pem in output dir) --ca-key CA key for signing (defaults to ca-key.pem in output dir) --cert certificate name (default: cert.pem) --key key name (default: key.pem) --common-name, -c common name --org, -o \u0026quot;unknown\u0026quot;\torganization --bits, -b \u0026quot;2048\u0026quot;\tnumber of bits in the key (default: 2048) --overwrite\toverwrite existing certificates and keys $ docker run \u0026ndash;rm -v $(pwd)/certs:/certs ehazlett/certm -d /certs client generate \u0026ndash;common-name=ehazlett -o=local generating client certificate: cn=\u0026ldquo;ehazlett\u0026rdquo; org=local bits=2048 cert=\u0026quot;/certs/cert.pem\u0026quot; key=\u0026quot;/certs/key.pem\u0026quot;\n$ openssl x509 -text -noout -in certs/cert.pem Certificate: Data: Version: 3 (0x2) Serial Number: a8:5d:80:4a:77:ae:73:84:a1:e3:8b:82:43:28:c7:71 Signature Algorithm: sha256WithRSAEncryption Issuer: O=local Validity Not Before: Jun 10 14:10:00 2016 GMT Not After : May 26 14:10:00 2019 GMT Subject: O=local, CN=ehazlett Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:d2:39:09:4e:18:69:fe:17:3f:12:d9:22:7f:a5: ... 14:cb Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment, Key Agreement X509v3 Extended Key Usage: TLS Web Client Authentication, TLS Web Server Authentication X509v3 Basic Constraints: critical CA:FALSE Signature Algorithm: sha256WithRSAEncryption 97:02:cc:98:55:21:d5:3a:b5:75:8d:46:37:d7:79:75:a5:bc: ... dc:37:6f:20 ```CommonName (CN) がセットされてる``` Subject: O=local, CN=ehazlett ```クライアント認証用のコマンドだけどサーバー証明書としても使える``` X509v3 Extended Key Usage: TLS Web Client Authentication, TLS Web Server Authentication ``` ### bundle モード CA, Server, Client 証明書を1コマンドで作ってくれる でも CommonName が指定できない``` $ docker run --rm ehazlett/certm bundle -hNAME: generate - generate new bundle USAGE: command generate \\[command options\\] \\[arguments...\\] OPTIONS: --host \\[--host option --host option\\]\tSAN/IP SAN for certificate --org, -o \u0026quot;unknown\u0026quot;\torganization --bits, -b \u0026quot;2048\u0026quot;\tnumber of bits in the key (default: 2048) --overwrite\toverwrite existing certificates and keys $ docker run \u0026ndash;rm -v $(pwd)/certs:/certs ehazlett/certm -d /certs bundle generate \u0026ndash;host 127.0.0.1 -o=local generating ca: org=local bits=2048 $ ls certs ca-key.pem ca.pem cert.pem key.pem server-key.pem server.pem\n ### PKCS12 形式に変換 クライアント証明書を PKCS12 形式にする``` $ openssl pkcs12 -export -in certs/cert.pem -inkey certs/key.pem -out certs/cert.p12 -password pass:\u0026quot;\u0026quot; 使ってみる # bundle で作成したものを Apache で使ってみた SSLCertificateFile, SSLCertificateKeyFile, SSLCACertificateFile に certm で作成した証明書と鍵を指定し SSLVerifyClient を require にしてテストしました。 無事アクセスできました。ヨカッタネ\n","date":"2016年6月10日","permalink":"/2016/06/certm/","section":"Posts","summary":"https://github.com/ehazlett/certm という TLS 証明書作成ツールを見つけたのでメモっておく OpenSSL での証明書作成については https://jamielinux.com/docs/openssl-certificate-authority/index.html がとても良く出来ているのであまりこのツールに頼ることはない気","title":"CertM という TLS 証明書作成ツール"},{"content":"","date":"2016年6月3日","permalink":"/tags/jupyter/","section":"Tags","summary":"","title":"Jupyter"},{"content":"Udemy の 【世界で2万人が受講】実践 Python データサイエンスを受講してみました。 104講義、合計17.5時間もの動画で Jupyter を使ったデータ解析の説明が受けられます。 私が申し込んだ時は定価が2万円を超えていて、キャンペーン価格で6,000円でした。でも今は定価が6,000円さらにそこから30% OFF の4,200円なんですね\u0026hellip; 内容はまず anaconda で環境構築。anaconda だからハマらないし楽ちん。環境構築に時間を割かれなくて良いです。 その後は次のような内容となっていました。\n numpy の Array の説明 pandas の Series, DataFrame の説明 外部データの読み込み データ操作 matplotlib を使ったデータの可視化 seaborn を使ったデータの可視化 タイタニックの乗船者情報を用いたデータ解析 株価データを分析 アメリカ大統領選挙のデータを解析 scikit-learn を使った機械学習 機械学習 線形回帰 機械学習 ロジスティック回帰 機械学習 多クラス分類 ロジスティック回帰 機械学習 多クラス分類 k近傍法 機械学習 サポートベクトルマシン（SVM） 機械学習 ナイーブベイズ分類 機械学習 決定木とランダムフォレスト 統計入門 離散一様分布 統計入門 連続一様分布 統計入門 二項分布 統計入門 ポアソン分布 統計入門 正規分布 統計入門 標本と母集団 統計入門 t分布 統計入門 仮説検定 統計入門 カイ二乗分布を使った検定 統計入門 ベイズの定理 SQLとPython Webスクレイピング  動画は手を動かしながら見るためだと思われますが、非常にゆっくりです。 最初は気づきませんでしたが再生速度を変更できるため、手を動かさずに見るだけの場合は1.5倍速で見ていました。便利。 スマホアプリでは端末にダウンロードしておくことが可能なので通勤電車内でも快適です。 閲覧期限も無いのでいつでも何度でも見直すことができます（サービスが終了しなければ\u0026hellip;） この講義の難点は音が小さいこと。スマホでこの動画に合わせて音量を最大にしたままにしておくと、次に音楽を再生した際などに爆音で困ります\u0026hellip; 最初のうちは「へー、numpy や pandas って便利なんだな」と思いつつもちょっと退屈でしたが、タイタニックの回以降は「なるほどなるほど」という感じでした。株価の予測はそんなのでは儲からんでしょ？とも思いましたが、実際にどういう場合にどの機能を使うのかがわかってきます。 その後の「機械学習」はそれぞれの学習アルゴリズムの原理が説明されて、これまたなるほどぉと。そして SciKit Learn はなんて便利なんでしょうと。 先日 Microsoft Azure Machine Learning のハンズオンをやって、ブラウザ上で線をつなぐだけで機械学習の学習と Web API 化があっという間に出来上がるというのを体験しましたが、SciKit Learn 使えばそれと大差ないんじゃないか？ってくらい簡単に実装できそうです。Azure の方はサーバーとか不要だから運用とか考えたらもちろんもっとずっと簡単。 Jupyter の便利さがよくわかる講義でした。Jupyter は便利だけれども大きなデータセットでは実行にそれなりのリソースが必要です。こうなるとクラウドで BigQuery のデータも使える Google Cloud DataLab は超便利なんじゃない？ 当然ながらこれを受講したからといっていきなりデータサイエンティストになれたりはしませんが、興味をもつきっかけになります。Jupyter がどんなものか知らなかった私にとってはその便利さに驚きこれは使っていきたいと思いました。データ分析に必要な数学、統計学的な知識もさわりだけ紹介されるので興味を持ったらその先は自分で勉強する必要があります。良い練習用の課題なんかがあればもっと身につくものになるかもしれません。 Jupyter のファイル .ipynb は JSON のようですが GitHub では HTML にして描画してくれます。グラフなんかも埋め込んであればそのまま表示されます。Jupyter の実行環境がなくても結果の共有ができるんです！GitLab はまだ対応していないようで JSON として表示されます。\n","date":"2016年6月3日","permalink":"/2016/06/udemy-python-jp/","section":"Posts","summary":"Udemy の 【世界で2万人が受講】実践 Python データサイエンスを受講してみました。 104講義、合計17.5時間もの動画で Jupyter を使ったデータ解析の説明が受けら","title":"「実践 Python データサイエンス」を受講した"},{"content":"","date":"2016年6月3日","permalink":"/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/","section":"Tags","summary":"","title":"機械学習"},{"content":"","date":"2016年6月3日","permalink":"/tags/%E7%B5%B1%E8%A8%88/","section":"Tags","summary":"","title":"統計"},{"content":"td-agent-2.3.1-0.el6.x86_64 (v0.12.20) で確認 fluentd の out_file は TimeSlicedOutput を継承して作られており、日別や時間別（time_slice_format で指定可能）にファイルを出力できるようになっています。\n デフォルトで file buffer となっており buffer を flush する際に最終的なファイルに保存されます。flush される buffer ファイルがそのまま最終的なファイルとなるため、勝手にこれは rename されているのだろうと思ってました。 buffer_chunk_limit のデフォルトは 8MB でこのままだと1GBあたり128個というファイル数になってしまってちょっと多すぎるなということで 500MB まで増やしてみたのです。\nそうすると0時過ぎにすごく重くなるという現象が発生\u0026hellip; ありゃ？なんでじゃろ？もしかして と思って確認したら rename だと思ってたところはバッファーファイルから読み出して保存先にまるっと書き出していたのです。数百MBのコピーがガンガン走ってたのです。そりゃ重い。 まず \u0026lsquo;b\u0026rsquo; + id がファイル名に入った buffer に書き、queue に入れる際に \u0026lsquo;b\u0026rsquo; が \u0026lsquo;q\u0026rsquo; のファイルに rename されて flush 時にコピー処理がされるようです。\nてなことなので、out_file で buffer_chunk_limit を大きくするのはよろしくないようです。今回の目的はあまりファイル数を大きくしたくないという理由だったので append を使うのが良さそうです。buffer_chunk_limit を小さくして append を有効にする。ファイルへの保存が最終目的ならいっそのこと、buffer_type を memory にしてしまった方が良いのかもしれない。\n out_file で num_threads が使えるのかどうか知らないけれど、もし使えるとしたら append と一緒には使わない方が良いかもしれない buffer からのコピーは行単位ではない（16KB単位で read, write しているみたい）ようなので複数 thread で同じファイルに追記すると壊れる行がでそう。\nまた、append を使う場合、そのファイルはすべての append まで不完全なままとなる。まあ、buffer からのコピーが一時ファイルからの rename じゃないから append でなくてもそのファイルはコピーが完了してるのかどうかファイルの存在確認だけでは判断できないが。\n","date":"2016年5月15日","permalink":"/2016/05/fluentd-out_file-copy-on-flush/","section":"Posts","summary":"td-agent-2.3.1-0.el6.x86_64 (v0.12.20) で確認 fluentd の out_file は TimeSlicedOutput を継承して作られており、日別や時間別（time_slice_format で指定可能）にファイルを出力できるようになってい","title":"fluentd の out_file でファイル数を減らしたいなら append を使う"},{"content":"CentOS 7 にて``` em1 \u0026mdash;+ +\u0026mdash; vlan2 \u0026mdash; br0 +\u0026mdash; team0 \u0026mdash;+ em2 \u0026mdash;+ +\u0026mdash; vlan3 \u0026mdash; br1\n\u0026gt; team + vlan + bridge 構成にしようとすると NetworkManager が邪魔をするんですよねえ / “RHEL7/CentOS7 NetworkManager徹底入門” [http://t.co/zc2WHznEB8](http://t.co/zc2WHznEB8) \u0026gt; \u0026gt; — yteraoka (@yteraoka) [2015年1月18日](https://twitter.com/yteraoka/status/556616999803969536) 中井さんが調べて Bugzilla に登録したりしてくださり、まとめページまでつくっていただけたので2016年に再チャレンジしたときにはすんなりできました。ありがとうございます。 \u0026gt; teaming + tag vlan + bridge がすんなりできた、ありがたや / “RHEL7/CentOS7のnmcliコマンドでBonding/VLAN/ブリッジを組み合わせる方法 - めもめも” [https://t.co/ZbSZOAJi8J](https://t.co/ZbSZOAJi8J) \u0026gt; \u0026gt; — yteraoka (@yteraoka) [2016年4月13日](https://twitter.com/yteraoka/status/720106581300547584) ががが、テスト用の環境でセットアップした際には期待通りに動作していたはずなのに実環境に持って行ったらなぜか vlan が片方だけしか Up しないという問題が発生... \u0026gt; CentOS7のteaming-vlan-bridge構成で起動時にvlanが1つbring upでtimeoutする謎 \u0026gt; nmcli d で connecting (prepare) と表示されており nmcli d connect vlan2 とするとつながる \u0026gt; \u0026gt; — yteraoka (@yteraoka) [2016年4月21日](https://twitter.com/yteraoka/status/722974379210989568) $ sudo systemctl status network.service -l ● network.service - LSB: Bring up/down networking Loaded: loaded (/etc/rc.d/init.d/network) Active: failed (Result: exit-code) since Thu 2016-04-21 09:31:10 JST; 1min 35s ago Docs: man:systemd-sysv-generator(8) Process: 11093 ExecStop=/etc/rc.d/init.d/network stop (code=exited, status=0/SUCCESS) Process: 11775 ExecStart=/etc/rc.d/init.d/network start (code=exited, status=1/FAILURE)\nApr 21 09:31:09 localhost network[11775]: Bringing up interface vlan-vlan2: Error: Timeout 90 sec expired. Apr 21 09:31:09 localhost network[11775]: [FAILED] Apr 21 09:31:09 localhost network[11775]: Bringing up interface vlan-vlan3: Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/14) Apr 21 09:31:09 localhost network[11775]: [ OK ] Apr 21 09:31:09 localhost network[11775]: Bringing up interface bridge-br0: [ OK ] Apr 21 09:31:10 localhost network[11775]: Bringing up interface bridge-br1: [ OK ] Apr 21 09:31:10 localhost systemd[1]: network.service: control process exited, code=exited status=1 Apr 21 09:31:10 localhost systemd[1]: Failed to start LSB: Bring up/down networking. Apr 21 09:31:10 localhost systemd[1]: Unit network.service entered failed state. Apr 21 09:31:10 localhost systemd[1]: network.service failed.\n```調べたところ L2 switch との相性の問題なのかちょっと待ってから inteface を up させれば良いということが判明したので次のように service を作成して対応しました。 `/etc/systemd/system/wait-vlan-ready.service```` [Unit] Description=Wait for VLAN ready state Before=network.service\n[Service] Type=oneshot RemainAfterExit=yes ExecStart=/bin/sleep 20\n[Install] WantedBy=multi-user.target\nsudo systemctl daemon-reload sudo systemctl enable wait-vlan-ready ````Before=network.servce` としてあるので network サービスを起動させる前に20秒 sleep することになります。","date":"2016年5月14日","permalink":"/2016/05/centos7-teaming-vlan-bridge/","section":"Posts","summary":"CentOS 7 にて``` em1 \u0026mdash;+ +\u0026mdash; vlan2 \u0026mdash; br0 +\u0026mdash; team0 \u0026mdash;+ em2 \u0026mdash;+ +\u0026mdash; vlan3 \u0026mdash; br1 \u0026gt; team + vlan + bridge 構成にしようとすると NetworkManager が邪魔をするんですよねえ / “RHEL7/CentOS7 Netw","title":"CentOS 7 の Teaming + VLAN + Bridge でハマる"},{"content":"docker-machine コマンドは \u0026ndash;driver digitalocean で簡単に DigitalOcean に Docker Machine を作れます。 https://docs.docker.com/machine/drivers/digital-ocean/``` Options:\n\u0026ndash;digitalocean-access-token Digital Ocean access token [$DIGITALOCEAN_ACCESS_TOKEN]\n\u0026ndash;digitalocean-backups enable backups for droplet [$DIGITALOCEAN_BACKUPS]\n\u0026ndash;digitalocean-image \u0026ldquo;ubuntu-15-10-x64\u0026rdquo; Digital Ocean Image [$DIGITALOCEAN_IMAGE]\n\u0026ndash;digitalocean-ipv6 enable ipv6 for droplet [$DIGITALOCEAN_IPV6]\n\u0026ndash;digitalocean-private-networking enable private networking for droplet [$DIGITALOCEAN_PRIVATE_NETWORKING]\n\u0026ndash;digitalocean-region \u0026ldquo;nyc3\u0026rdquo; Digital Ocean region [$DIGITALOCEAN_REGION]\n\u0026ndash;digitalocean-size \u0026ldquo;512mb\u0026rdquo; Digital Ocean size [$DIGITALOCEAN_SIZE]\n\u0026ndash;digitalocean-ssh-key-fingerprint SSH key fingerprint [$DIGITALOCEAN_SSH_KEY_FINGERPRINT]\n\u0026ndash;digitalocean-ssh-port \u0026ldquo;22\u0026rdquo; SSH port [$DIGITALOCEAN_SSH_PORT]\n\u0026ndash;digitalocean-ssh-user \u0026ldquo;root\u0026rdquo; SSH username [$DIGITALOCEAN_SSH_USER]\n\u0026ndash;digitalocean-userdata path to file with cloud-init user-data [$DIGITALOCEAN_USERDATA]\nregion や size は [doctl](https://github.com/digitalocean/doctl) コマンドで確認できます。実行のたびに変わらないものは環境変数にセットしておきます。（doctl は golang で書かれた one binary なので GitHub の release ページからダウンロートして PATH の通った場所に置いて使います）。 $ doctl compute region list Slug Name Available nyc1 New York 1 true sfo1 San Francisco 1 true nyc2 New York 2 true ams2 Amsterdam 2 true sgp1 Singapore 1 true lon1 London 1 true nyc3 New York 3 true ams3 Amsterdam 3 true fra1 Frankfurt 1 true tor1 Toronto 1 true\n$ doctl compute size list Slug Memory VCPUs Disk Price Monthly Price Hourly 512mb 512 1 20 5.00 0.007440 1gb 1024 1 30 10.00 0.014880 2gb 2048 2 40 20.00 0.029760 4gb 4096 2 60 40.00 0.059520 8gb 8192 4 80 80.00 0.119050 16gb 16384 8 160 160.00 0.238100 32gb 32768 12 320 320.00 0.476190 48gb 49152 16 480 480.00 0.714290 64gb 65536 20 640 640.00 0.952380 ```SSH公開鍵のフィンガープリントは DigitalOcean の [Settings](https://cloud.digitalocean.com/settings/security) でも確認できますが、ローカルある鍵については ssh-keygen コマンドで取得できます 最近の ssh はデフォルトの HASH アルゴリズムが SHA256 になっているので、次のような出力だった場合は -E md5 を指定する必要があります。``` $ ssh-keygen -l -f ~/.ssh/id\\_rsa.pub 2048 SHA256:XfGdbFbCEr/DkiONISd2V3fjpjYJddbJHVOfkau9qBA ytera@mypc (RSA) $ ssh-keygen -l -E md5 -f ~/.ssh/id_rsa.pub 2048 MD5:f2:f2:76:35:b0:54:54:0d:8c:67:37:59:b0:0b:43:51 ytera@mypc (RSA)\n公開鍵を消しちゃってる場合は ssh-keygen -y で秘密鍵から作れます。 MD5: の後の部分 (f2:f2:76:35:b0:54:54:0d:8c:67:37:59:b0:0b:43:51) を環境変数 DIGITALOCEAN\\_SSH\\_KEY\\_FINGERPRINT にセットしておきます。 $ docker-machine create \\ \u0026ndash;driver digitalocean \\ \u0026ndash;digitalocean-size 2gb \\ test1 Running pre-create checks\u0026hellip; Creating machine\u0026hellip; (test1) Creating SSH key\u0026hellip; (test1) Creating Digital Ocean droplet\u0026hellip; (test1) Waiting for IP address to be assigned to the Droplet\u0026hellip; Waiting for machine to be running, this may take a few minutes\u0026hellip; Detecting operating system of created instance\u0026hellip; Waiting for SSH to be available\u0026hellip; Detecting the provisioner\u0026hellip; Provisioning with ubuntu(systemd)\u0026hellip; Installing Docker\u0026hellip; Copying certs to the local machine directory\u0026hellip; Copying certs to the remote machine\u0026hellip; Setting Docker configuration on the remote daemon\u0026hellip; Checking connection to Docker\u0026hellip; Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env test1\n簡単に Docker Machine ができました。 $ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS test1 - digitalocean Running tcp://188.166.211.7:2376 v1.11.0\n$ docker-machine ssh test1 Welcome to Ubuntu 15.10 (GNU/Linux 4.2.0-27-generic x86\\_64) \\* Documentation: https://help.ubuntu.com/ Last login: Fri Apr 15 08:38:46 2016 from 124.211.178.241 root@test1:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES root@test1:~# $ eval $(docker-machine env test1)\n$ docker run -d -p 80:80 nginx ```とすれば外から port 80 にアクセスできちゃいました。 プライベートネットワークを有効にして試してみるとどうなるだろうか。``` $ docker-machine stop test1 Stopping \u0026quot;test1\u0026quot;... Machine \u0026quot;test1\u0026quot; was stopped. $ docker-machine rm test1 About to remove test1 Are you sure? (y/n): y Successfully removed test1 $ docker-machine create \\ \u0026ndash;driver digitalocean \\ \u0026ndash;digitalocean-size 2gb \\ \u0026ndash;digitalocean-private-networking \\ test2\nこれで同じく nginx コンテナを起動してみたら $ docker run -d -p 80:80 nginx\n","date":"2016年4月15日","permalink":"/2016/04/docker-machine-and-digitalocean/","section":"Posts","summary":"docker-machine コマンドは \u0026ndash;driver digitalocean で簡単に DigitalOcean に Docker Machine を作れます。 https://docs.docker.com/machine/drivers/digital-ocean/``` Options: \u0026ndash;digitalocean-access-token Digital Ocean access token [$DIGITALOCEAN_ACCESS_TOKEN] \u0026ndash;digitalocean-backups enable backups for droplet [$DIGITALOCEAN_BACKUPS] \u0026ndash;digitalocean-image \u0026ldquo;ubuntu-15-10-x64\u0026rdquo; Digital Ocean Image [$DIGITALOCEAN_IMAGE] \u0026ndash;digitalocean-ipv6 enable ipv6 for droplet [$DIGITALOCEAN_IPV6] \u0026ndash;digitalocean-private-networking enable private networking for droplet [$DIGITALOCEAN_PRIVATE_NETWORKING] \u0026ndash;digitalocean-region \u0026ldquo;nyc3\u0026rdquo; Digital Ocean region [$DIGITALOCEAN_REGION] \u0026ndash;digitalocean-size \u0026ldquo;512mb\u0026rdquo; Digital Ocean size [$DIGITALOCEAN_SIZE] \u0026ndash;digitalocean-ssh-key-fingerprint SSH key","title":"docker-machine で DigitalOcean を使う"},{"content":"Let\u0026rsquo;s Encrypt が Beta 期間を無事終了し正式公開となったようですが、Oracle の JRE/JDK が Trusted root CA として Let\u0026rsquo;s Encrypt で使われているものを含んでいません。 先週 LINE の BOT API が公開されて多くの方がこぞって試されていたようですが Let\u0026rsquo;s Encrypt の証明書を使った場合には Callback へのアクセスがないと報告されています。私も試しましたが、ダメでした。 これもおそらく Oracle の Java が Let\u0026rsquo;s Encrypt で使われている DST Root CA X3 も ISRG Root X1 も含んでいないからではないかと勝手に推測してます。 Community にもいくつか thread が立ってます Will the cross root cover trust by the default list in the JDK/JRE? 私は Java のコードを書けませんが、簡単なテストコードが公開されていたのでこれを試してみました。 http://alvinalexander.com/blog/post/java/simple-https-example\n$ mkdir foo $ vi foo/JavaHttpsExample.java $ javac foo/JavaHttpsExample.java $ java foo/JavaHttpsExample Oracle の JRE/JDK では次のようなエラーとなります。\nException in thread \u0026quot;main\u0026quot; javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target OpenJDK ではアクセスできました。\n$ java -version openjdk version \u0026quot;1.8.0_77\u0026quot; OpenJDK Runtime Environment (build 1.8.0_77-b03) OpenJDK 64-Bit Server VM (build 25.77-b03, mixed mode) クライアント側が keystore に登録すればアクセスできるわけですが、Let\u0026rsquo;s Encrypt を使おうとされているサーバーに Java のクライアントがいる場合は要注意です。 LINE Bot でもうちょい遊びたいから RapidSSL 更新しなきゃ。\n","date":"2016年4月14日","permalink":"/2016/04/oracle-jre-jdk-does-not-contain-lets-encrypt-root-ca/","section":"Posts","summary":"Let\u0026rsquo;s Encrypt が Beta 期間を無事終了し正式公開となったようですが、Oracle の JRE/JDK が Trusted root CA として Let\u0026rsquo;s Encrypt で使われているものを含んでいません。 先週 LINE の BOT API が公開","title":"Oracle JRE/JDK が Let's Encrypt に対応してない件"},{"content":"","date":"2016年4月10日","permalink":"/tags/bot/","section":"Tags","summary":"","title":"Bot"},{"content":"","date":"2016年4月10日","permalink":"/tags/flask/","section":"Tags","summary":"","title":"Flask"},{"content":"流行りに乗って LINE BOT API を試してみた。\n 仕組みとしては誰かが LINE からのアクセスを受ける口 (Callback URL) を準備して待っていれば、「友達登録（ブロック解除）」、「ブロック」、「メッセージ」が送られてくるというもの。\n友だち登録してもらえればそのアカウント情報(mid?)が得られるので宛先に指定すればメッセージを送ることが可能。\nブロックされたことがわかるので知らずにメッセージを送り続けるという無駄がなくせます。友達解除のことがブロックと等価なのかな。 メッセージには「テキスト」、「画像」、「動画」、「位置情報」といったパターンがある。\nmid からユーザー情報を得るための API もあり、次のような結果が得られるのでメッセージに名前を入れたり友達管理に名前が使えます。\n{ \u0026#34;contacts\u0026#34;: [ { \u0026#34;displayName\u0026#34;: \u0026#34;浦島太郎\u0026#34;, \u0026#34;mid\u0026#34;: \u0026#34;********************************\u0026#34;, \u0026#34;pictureUrl\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;statusMessage\u0026#34;: \u0026#34;玉手箱開けちゃった\u0026#34; } ], \u0026#34;count\u0026#34;: 1, \u0026#34;display\u0026#34;: 1, \u0026#34;pagingRequest\u0026#34;: { \u0026#34;start\u0026#34;: 1, \u0026#34;display\u0026#34;: 1, \u0026#34;sortBy\u0026#34;: \u0026#34;MID\u0026#34; }, \u0026#34;start\u0026#34;: 1, \u0026#34;total\u0026#34;: 1 } 今は Callback API のタイムアウトが10秒というゆるい設定のようなのですし、友達登録可能な上限が50と少ないのでリクエストを受けた処理の中でそのまま返信することができますが、沢山リクエストが来るようになるとそれでは捌き切れなかったり、エラーハンドリングで困るので Callback で受けたデータはシグネチャの検証だけやって job queue に突っ込むという設計にすべきのようです。\n事前共有鍵を使い HMAC SHA256 で X-LINE-ChannelSignature ヘッダーの値と一致することを検証\nimport hmac, hashlib, base64 if request.headers.get(\u0026#39;X-LINE-ChannelSignature\u0026#39;) == base64.b64encode(hmac.new(CHANNEL_SECRET, request.get_data(), hashlib.sha256).digest()): return True メッセージの送信は white list に登録した IP アドレスからであれば自由に送信できます。\ncurl -X POST https://trialbot-api.line.me/v1/events \\ -H \u0026quot;Content-Type: application/json; charser=UTF-8\u0026quot; \\ -H \u0026quot;X-Line-ChannelID: **********\u0026quot; \\ -H \u0026quot;X-Line-ChannelSecret: ********************************\u0026quot; \\ -H \u0026quot;X-Line-Trusted-User-With-ACL: u********************************\u0026quot; \\ -d ' { \u0026quot;to\u0026quot;: [ \u0026quot;u********************************\u0026quot; ], \u0026quot;toChannel\u0026quot;: \u0026quot;1383378250\u0026quot;, \u0026quot;eventType\u0026quot;: \u0026quot;138311608800106203\u0026quot;, \u0026quot;content\u0026quot;: { \u0026quot;contentType\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;toType\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;おはようございます、月曜日の朝です\u0026quot; } } ' こんな結果が返ってきます。\n{ \u0026#34;failed\u0026#34;: [], \u0026#34;messageId\u0026#34;: \u0026#34;1460327400008\u0026#34;, \u0026#34;timestamp\u0026#34;: 1460327400008, \u0026#34;version\u0026#34;: 1 } White list に登録されていない IP アドレスから送信しようとすると次のように拒否されます\n{ \u0026#34;statusCode\u0026#34;: \u0026#34;427\u0026#34;, \u0026#34;statusMessage\u0026#34;: \u0026#34;Your ip address [xxx.xxx.xxx.xxx] is not allowed to access this API.\u0026#34; } Flask で簡単な返信までするコード書いちゃったけど Celery 使ったジョブキュー方式に書き直そう。Celery の勉強を兼ねて。\n","date":"2016年4月10日","permalink":"/2016/04/using-line-bot-api/","section":"Posts","summary":"流行りに乗って LINE BOT API を試してみた。 仕組みとしては誰かが LINE からのアクセスを受ける口 (Callback URL) を準備して待っていれば、「友達登録（ブロック解除）」、「","title":"LINE BOT API を試してみた"},{"content":"前回 DigitalOcean にて OpenVPN サーバーをセットアップしました。ずっと起動させっぱなしでも$5/月なわけですが、必要なときにしか起動させない予定です。\nshutdown しておくだけだと費用がかかるため snapshot を取得して仮想サーバーは削除してしまいます。必要になったら snapshot から起動させれば IP アドレス以外は元通りになります。\nVPN サーバーなので IP アドレスが変わるたびにクライアントの設定を変更するのは面倒です。そこで DDNS っぽいサービスを使って起動時に毎度 DNS を更新することにします。\n使うのは https://www.noip.com/ にしてみました。 こんな簡単なスクリプトを /etc/update-noip.sh として書いて /etc/rc.local に書きました。\n#!/bin/sh  USERNAME=foobar PASSWORD=secret HOSTNAME=*****.noip.me MYIP=$(curl -s http://httpbin.org/ip | grep origin | awk \u0026#39;{print $2}\u0026#39; | sed -e \u0026#39;s/\u0026#34;//g\u0026#39;) curl -s -u $USERNAME:$PASSWORD -o /dev/null \\  \u0026#34;https://dynupdate.no-ip.com/nic/update?hostname=$HOSTNAME\u0026amp;myip=$MYIP\u0026#34; 90日間更新しないとホスト名が消えちゃうので長期間使わない場合はログインして更新してあげるなりなんなりしてあげる必要があります。有料プランに切り替えればこの制限はなくなります。\nDNS は AWS の Route53 を使っているので AWS の API で更新しようかとも思いましたが、IAM でリソース単位の制御はできないということで、ドメインまるごと更新可能なシークレットキーを置いておくのも嫌かなということで今回の構成としました。\n","date":"2016年3月28日","permalink":"/2016/03/noip-com/","section":"Posts","summary":"前回 DigitalOcean にて OpenVPN サーバーをセットアップしました。ずっと起動させっぱなしでも$5/月なわけですが、必要なときにしか起動させない予定です。 shutdown しておく","title":"noip.com で DDNS 設定"},{"content":"Docker の実行環境としてどれがいいかな？と Swarm や Mesos などを試している（今後は Kubernetes や Nomad も試してみたい）が手元の PC の Virtualbox だけではリソースが足りないため DigitalOcean を使うことにしたのだが、Global IP Address 側でクラスタ通信させるのも気持ちが悪いので Private Network を使い、PC からも直接そのセグメントにアクセスしたいので OpenVPN で接続することにしたのでそのメモ。 かつて CentOS で OpenVPN サーバーをセットアップし、人（証明書）によって違うセグメントを割り当てたり、固定IPを割り振ったり Windows クライアントのセットアップを行ったりしたことはあるが遠い昔の記憶だ。 今回は How To Set Up an OpenVPN Server on Ubuntu 14.04 を参考にセットアップしてみる。 ほぼ丸写しするだけでできるけど\u0026hellip; クライアントは ubuntu PC である。こちらは\nsudo apt-get install network-manager-openvpn した後、GUI でちょちょっと設定するだけで使える。楽ちん。 それではサーバーのセットアップ\nDroplet (VM) の作成 # $5/月のサーバーを作成、リージョンは一番近いシンガポールを選択\nPrivate Networking にチェックを入れる  OpenVPN のインストール # サーバーが起動したら SSH でログインして OpenVPN をインストールする リポジトリ情報の更新\napt-get update OpenVPN と Easy-RSA をインストール\napt-get install openvpn easy-rsa OpenVPN サーバーの設定 # 設定例が一緒にインストールされているのでこれを土台にする\nzcat /usr/share/doc/openvpn/examples/sample-config-files/server.conf.gz \u0026gt; /etc/openvpn/server.conf dhparam は今時は 1024 では短いので 2048 に変更\n# Diffie hellman parameters. # Generate your own with: # openssl dhparam -out dh1024.pem 1024 # Substitute 2048 for 1024 if you are using # 2048 bit keys. sed -i -e 's/^dh.\\*/dh dh2048.pem/' /etc/openvpn/server.conf openssl dhparam -out /etc/openvpn/dh2048.pem 2048 クライアントの Default Gateway を VPN に向けたいなら\n;push \u0026quot;redirect-gateway def1 bypass-dhcp\u0026quot; をアンコメントするらしいが、クライアントの設定にも依存するようだ。 この設定がなくてもクライアントが route を設定すればそうなるし、そうしなければそうならない。 今回の用途では不要。 VPN 接続時に DNS サーバーを切り替えさせたいなら\n;push \u0026quot;dhcp-option DNS 208.67.222.222\u0026quot; ;push \u0026quot;dhcp-option DNS 208.67.220.220\u0026quot; をアンコメントして DNS サーバーのアドレスを実際のものに書き換えます。 これも今回はやらない 実行ユーザーを非特権ユーザーにするため、次の行をアンコメントする\n;user nobody ;group nogroup 追加の routing を設定 DigitalOcean の Private Network は 10.130.0.0/16 のようなので\npush \u0026quot;route 10.130.0.0 255.255.0.0\u0026quot; を追記 VPN で使われるネットワークセグメントは次の設定によって決まっており\n# Configure server mode and supply a VPN subnet # for OpenVPN to draw client addresses from. # The server will take 10.8.0.1 for itself, # the rest will be made available to clients. # Each client will be able to reach the server # on 10.8.0.1. Comment this line out if you are # ethernet bridging. See the man page for more info. server 10.8.0.0 255.255.255.0 サーバーは 10.8.0.1 を使うことになり、残りからクライアント側に払い出される セキュリティをより強化するために TLS-AUTH の設定を行います。\n# For extra security beyond that provided # by SSL/TLS, create an \u0026quot;HMAC firewall\u0026quot; # to help block DoS attacks and UDP port flooding. # # Generate with: # openvpn --genkey --secret ta.key # # The server and each client must have # a copy of this key. # The second parameter should be '0' # on the server and '1' on the clients. ;tls-auth ta.key 0 # This file is secret 次のコマンドで鍵を作成します\nopenvpn --genkey --secret /etc/openvpn/ta.key tls-auth にこの鍵を指定します、鍵の次の値はサーバ側が 0 でクライアントは 1 を指定します\nsed -i -r -e 's/^;?tls-auth.*/tls-auth ta.key 0/' /etc/openvpn/server.conf 使われる cipher をよりセキュアなものにする\n# Select a cryptographic cipher. # This config item must be copied to # the client config file as well. ;cipher BF-CBC # Blowfish (default) ;cipher AES-128-CBC # AES ;cipher DES-EDE3-CBC # Triple-DES sed -i -r -e 's/^;?cipher AES-128-CBC.*/cipher AES-128-CBC/' /etc/openvpn/server.conf Packet の forwarding を有効にする # sysctl -w net.ipv4.ip_forward=1 再起動しても設定されるように /etc/sysctl.conf にも設定しておく\nsed -i -e 's/^#?net.ipv4.ip_forward.*/net.ipv4.ip_forward=1/' /etc/sysctl.conf Firewall (UFW) の設定 # SSH は許可する\nufw allow ssh OpenVPN で使うポートも許可する\nufw allow 1194/udp forwarding を許可する\nsed -e 's/DEFAULT_FORWARD_POLICY=.*/DEFAULT_FORWARD_POLICY=\u0026quot;ACCEPT\u0026quot;/' /etc/default/ufw NAT 設定 /etc/ufw/before.rules の *filter 設定の手前に次の設定を入れる\n# START OPENVPN RULES # NAT table rules *nat :POSTROUTING ACCEPT [0:0] # Allow traffic from OpenVPN client to eth0 -A POSTROUTING -s 10.8.0.0/8 -o eth0 -j MASQUERADE # Allow traffic from OpenVPN client to eth1 -A POSTROUTING -s 10.8.0.0/8 -o eth1 -j MASQUERADE COMMIT # END OPENVPN RULES 通常は Internet 側の eth0 だけで良いかもしれないが、今回はこの VPN サーバーも必要なときにしか起動させない（snapshot を取っておいて destroy する）ため都度 IP アドレスが変わる予定で、Private Network 内での routing 設定に困るので eth1 (private 側) も NAT させます。 ufw を有効にする\nufw enable 確認\nufw status NAT の確認\niptables -t nat -vnL 証明書まわりの設定 # Easy-RSA のスクリプトを openvpn 用にコピーする\ncp -r /usr/share/easy-rsa/ /etc/openvpn key を置くディレクトリの作成\nmkdir /etc/openvpn/easy-rsa/keys /etc/openvpn/easy-rsa/vars ファイルを編集する。 openssl コマンド実行時のデフォルト値を設定する\nvim /etc/openvpn/easy-rsa/vars 適当にそれらしく設定しておきます（個人でやってるぶんにはどーでもいい）\nexport KEY_COUNTRY=\u0026quot;JP\u0026quot; export KEY_PROVINCE=\u0026quot;Tokyo\u0026quot; export KEY_CITY=\u0026quot;Minato-ku\u0026quot; export KEY_ORG=\u0026quot;My Company Name\u0026quot; export KEY_EMAIL=\u0026quot;momotaro@example.com\u0026quot; export KEY_OU=\u0026quot;\u0026quot; サーバー用の鍵や証明書は server.key, server.crt と /etc/openvpn/server.conf で指定してあるので\nexport KEY_NAME=\u0026quot;server\u0026quot; も設定しておく スクリプトが期待通りにどうさするよう、ディレクトリを移動\ncd /etc/openvpn/easy-rsa 変数の読み込み\n. ./vars 初回は念の為掃除\n./clean-all ca.crt を作成する\n./build-ca サーバー用のキーペアを作成する\n./build-key-server server yes/no を聞かれる場合は yes (y) を、それ以外は Enter で。 keys ディレクトリ内に server.crt, server.key, ca.crt ができているので /etc/openvpn/ にコピーする\ncp /etc/openvpn/easy-rsa/keys/{server.crt,server.key,ca.crt} /etc/openvpn サーバー側はこれで完了 起動させる\nservice openvpn start service openvpn status クライアント用の証明書作成 # cd /etc/openvpn/easy-rsa ここでは client1 という Common Name で作成しようとしている。 どの（だれの）端末だかわかるような名前をつけましょう\n./build-key client1 これで /etc/openvpn/easy-rsa/keys/client1.crt, /etc/openvpn/easy-rsa/keys/client1.key ができたので /etc/openvpn/ca.crt, /etc/openvpn/ta.key とともにクライアント側にコピーして、後はクライアント側の設定を行います。\nクライアント設定 (ubuntu) # ノートPCの ubuntu での設定例です。 最初にも書いたが network-manager-openvpn をインストールします\nsudo apt-get install network-manager-openvpn  タスクバーのネットワークメニューからVPN接続の設定に入る  追加ボタンでVPN設定を追加する  OpenVPN を選択する  ゲートウェイにサーバーのIPアドレスを指定、証明書認証でサーバーからコピーした証明書を指定する  サーバーと設定を合わせるために LZO 圧縮を有効にする  TLS認証でサーバーからコピーした ta.key を選択、鍵の方向は 1  IPv4 設定でルートボタンをクリック  そのネットワーク上のリソースのためにのみこの接続を使用にチェックを入れる  スマホからの接続 # 後で Android からの接続を試してみよう。 試してみました OpenVPN for Android がスマホで設定できて便利でした。 OpenVPN Connect は設定ファイルを作ってインポートして上げる必要があるみたいです。\n","date":"2016年3月26日","permalink":"/2016/03/connect-to-private-network-on-digitalocean-using-openvpn/","section":"Posts","summary":"Docker の実行環境としてどれがいいかな？と Swarm や Mesos などを試している（今後は Kubernetes や Nomad も試してみたい）が手元の PC の Virtualbox だけではリソースが足りないため DigitalOcean を使う","title":"OpenVPN で DigitalOcean の Private Network へアクセスする"},{"content":"今回は Get started with multi-host networking に沿ってマルチホストでのオーバーレイネットワークを試してみます。\nKVS として Consul を立ち上げる # まず mh-keystore という名前（名前はなんでも良い）の docker-machine を作成します。\n$ docker-machine create -d virtualbox mh-keystore Running pre-create checks... Creating machine... (mh-keystore) Copying /home/ytera/.docker/machine/cache/boot2docker.iso to /home/ytera/.docker/machine/machines/mh-keystore/boot2docker.iso... (mh-keystore) Creating VirtualBox VM... (mh-keystore) Creating SSH key... (mh-keystore) Starting the VM... (mh-keystore) Check network to re-create if needed... (mh-keystore) Waiting for an IP... Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with boot2docker... Copying certs to the local machine directory... Copying certs to the remote machine... Setting Docker configuration on the remote daemon... Checking connection to Docker... Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env mh-keystore 環境変数を設定して\n$ eval \u0026quot;$(docker-machine env mh-keystore)\u0026quot; Consul コンテナを起動します\n$ docker run -d \\ \u0026gt; -p \u0026quot;8500:8500\u0026quot; \\ \u0026gt; -h \u0026quot;consul\u0026quot; \\ \u0026gt; progrium/consul -server -bootstrap Unable to find image 'progrium/consul:latest' locally latest: Pulling from progrium/consul c862d82a67a2: Pull complete 0e7f3c08384e: Pull complete 0e221e32327a: Pull complete 09a952464e47: Pull complete 60a1b927414d: Pull complete 4c9f46b5ccce: Pull complete 417d86672aa4: Pull complete b0d47ad24447: Pull complete fd5300bd53f0: Pull complete a3ed95caeb02: Pull complete d023b445076e: Pull complete ba8851f89e33: Pull complete 5d1cefca2a28: Pull complete Digest: sha256:8cc8023462905929df9a79ff67ee435a36848ce7a10f18d6d0faba9306b97274 Status: Downloaded newer image for progrium/consul:latest 2fe1bbb97506bd1aa975230125f26b0b1fd666a54f819769a28a99a981aceb21 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2fe1bbb97506 progrium/consul \u0026quot;/bin/start -server -\u0026quot; 25 seconds ago Up 24 seconds 53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500-\u0026gt;8500/tcp admiring_brahmagupta Consul を使った Swarm クラスタを立ち上げる # Docker Swarm を試す – その１ で試した Swarm クラスタは docker-machine を作成した後にその上で swarm コンテナを起動しましたが、今回は dcoker-machine create コマンドで swarm まで起動しちゃうようです\n$ docker-machine create \\ \u0026gt; -d virtualbox \\ \u0026gt; --swarm --swarm-master \\ \u0026gt; --swarm-discovery=\u0026quot;consul://$(docker-machine ip mh-keystore):8500\u0026quot; \\ \u0026gt; --engine-opt=\u0026quot;cluster-store=consul://$(docker-machine ip mh-keystore):8500\u0026quot; \\ \u0026gt; --engine-opt=\u0026quot;cluster-advertise=eth1:2376\u0026quot; \\ \u0026gt; mhs-demo0 Running pre-create checks... Creating machine... (mhs-demo0) Copying /home/ytera/.docker/machine/cache/boot2docker.iso to /home/ytera/.docker/machine/machines/mhs-demo0/boot2docker.iso... (mhs-demo0) Creating VirtualBox VM... (mhs-demo0) Creating SSH key... (mhs-demo0) Starting the VM... (mhs-demo0) Check network to re-create if needed... (mhs-demo0) Waiting for an IP... Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with boot2docker... Copying certs to the local machine directory... Copying certs to the remote machine... Setting Docker configuration on the remote daemon... Configuring swarm... Checking connection to Docker... Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env mhs-demo0 master と agent の2つのコンテナが起動しています\n$ DOCKER_HOST=$(docker-machine ip mhs-demo0):2376 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 20681dcbb76e swarm:latest \u0026quot;/swarm join --advert\u0026quot; 39 seconds ago Up 39 seconds swarm-agent 51e3f21e92f2 swarm:latest \u0026quot;/swarm manage --tlsv\u0026quot; 44 seconds ago Up 43 seconds swarm-agent-master もう一個 docker-machine を作成する\n$ docker-machine create -d virtualbox \\ \u0026gt; --swarm \\ \u0026gt; --swarm-discovery=\u0026quot;consul://$(docker-machine ip mh-keystore):8500\u0026quot; \\ \u0026gt; --engine-opt=\u0026quot;cluster-store=consul://$(docker-machine ip mh-keystore):8500\u0026quot; \\ \u0026gt; --engine-opt=\u0026quot;cluster-advertise=eth1:2376\u0026quot; \\ \u0026gt; mhs-demo1 Running pre-create checks... Creating machine... (mhs-demo1) Copying /home/ytera/.docker/machine/cache/boot2docker.iso to /home/ytera/.docker/machine/machines/mhs-demo1/boot2docker.iso... (mhs-demo1) Creating VirtualBox VM... (mhs-demo1) Creating SSH key... (mhs-demo1) Starting the VM... (mhs-demo1) Check network to re-create if needed... (mhs-demo1) Waiting for an IP... Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with boot2docker... Copying certs to the local machine directory... Copying certs to the remote machine... Setting Docker configuration on the remote daemon... Configuring swarm... Checking connection to Docker... Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env mhs-demo1 こちらは agent がひとつだけ\n$ DOCKER_HOST=$(docker-machine ip mhs-demo1):2376 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 94dc04993723 swarm:latest \u0026quot;/swarm join --advert\u0026quot; 5 seconds ago Up 5 seconds swarm-agent mhs-demo0 が SWARM の master になっているようです\n$ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS mh-keystore * virtualbox Running tcp://192.168.99.103:2376 v1.10.3 mhs-demo0 - virtualbox Running tcp://192.168.99.104:2376 mhs-demo0 (master) v1.10.3 mhs-demo1 - virtualbox Running tcp://192.168.99.105:2376 mhs-demo0 v1.10.3 docker-machine env に --swarm をつけると Swarm クラスタとしてアクセスするための値が返されます\n$ docker-machine env --swarm mhs-demo0 export DOCKER_TLS_VERIFY=\u0026quot;1\u0026quot; export DOCKER_HOST=\u0026quot;tcp://192.168.99.104:3376\u0026quot; export DOCKER_CERT_PATH=\u0026quot;/home/ytera/.docker/machine/machines/mhs-demo0\u0026quot; export DOCKER_MACHINE_NAME=\u0026quot;mhs-demo0\u0026quot; # Run this command to configure your shell: # eval $(docker-machine env --swarm mhs-demo0) --swarm をつけないとこんな感じで docker-machine 単体となる\n$ docker-machine env mhs-demo0 export DOCKER_TLS_VERIFY=\u0026quot;1\u0026quot; export DOCKER_HOST=\u0026quot;tcp://192.168.99.104:2376\u0026quot; export DOCKER_CERT_PATH=\u0026quot;/home/ytera/.docker/machine/machines/mhs-demo0\u0026quot; export DOCKER_MACHINE_NAME=\u0026quot;mhs-demo0\u0026quot; # Run this command to configure your shell: # eval $(docker-machine env mhs-demo0) master ではない node で --swarm をつけるとエラーになる\n$ docker-machine env --swarm mhs-demo1 Error checking TLS connection: \u0026quot;mhs-demo1\u0026quot; is not a swarm master. The --swarm flag is intended for use with swarm masters クラスタ情報の確認 2台のクラスタになっていることが確認できます\n$ eval $(docker-machine env --swarm mhs-demo0) $ docker info Containers: 3 Running: 3 Paused: 0 Stopped: 0 Images: 2 Server Version: swarm/1.1.3 Role: primary Strategy: spread Filters: health, port, dependency, affinity, constraint Nodes: 2 mhs-demo0: 192.168.99.104:2376 └ Status: Healthy └ Containers: 2 └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs └ Error: (none) └ UpdatedAt: 2016-03-18T16:01:05Z mhs-demo1: 192.168.99.105:2376 └ Status: Healthy └ Containers: 1 └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs └ Error: (none) └ UpdatedAt: 2016-03-18T16:00:59Z Plugins: Volume: Network: Kernel Version: 4.1.19-boot2docker Operating System: linux Architecture: amd64 CPUs: 2 Total Memory: 2.043 GiB Name: mhs-demo0 オーバーレイネットワークの作成 # $ eval $(docker-machine env --swarm mhs-demo0) $ docker network ls NETWORK ID NAME DRIVER 566f51fbecf4 mhs-demo0/bridge bridge 408cc854d4e2 mhs-demo1/bridge bridge f84f99d835f6 mhs-demo1/none null cecd093423fe mhs-demo1/host host 5fa9569a1b20 mhs-demo0/none null c7871366f2d5 mhs-demo0/host host $ DOCKER_HOST=$(docker-machine ip mhs-demo0):2376 docker network ls NETWORK ID NAME DRIVER 566f51fbecf4 bridge bridge 5fa9569a1b20 none null c7871366f2d5 host host $ DOCKER_HOST=$(docker-machine ip mhs-demo1):2376 docker network ls NETWORK ID NAME DRIVER f84f99d835f6 none null cecd093423fe host host 408cc854d4e2 bridge bridge $ eval $(docker-machine env --swarm mhs-demo0) $ docker network create --driver overlay --subnet=10.0.9.0/24 my-net 5fa661090e5d418c4c87fcfb90b87468cb8b9a027239090cd16d020c8a183016 $ docker network ls NETWORK ID NAME DRIVER 408cc854d4e2 mhs-demo1/bridge bridge f84f99d835f6 mhs-demo1/none null 5fa9569a1b20 mhs-demo0/none null 5fa661090e5d my-net overlay c7871366f2d5 mhs-demo0/host host 566f51fbecf4 mhs-demo0/bridge bridge cecd093423fe mhs-demo1/host host --subnet はちゃんと考えて指定しましょうとのこと。省略すると自動で採番されるけど、既存ネットワークとかぶると通信できないよと。 個別の docker-machine で見ても overlay の my-net が確認できます\n$ DOCKER_HOST=$(docker-machine ip mhs-demo0):2376 docker network ls NETWORK ID NAME DRIVER 5fa661090e5d my-net overlay 566f51fbecf4 bridge bridge 5fa9569a1b20 none null c7871366f2d5 host host $ DOCKER_HOST=$(docker-machine ip mhs-demo1):2376 docker network ls NETWORK ID NAME DRIVER 5fa661090e5d my-net overlay cecd093423fe host host 408cc854d4e2 bridge bridge f84f99d835f6 none null nginx を mhs-demo0 で実行します。--env=\u0026quot;constraint:node==mhs-demo0\u0026quot; で実行ノードを指定することができるんですね\n$ docker run -itd --name=web --net=my-net --env=\u0026quot;constraint:node==mhs-demo0\u0026quot; nginx bf918047d0dd862ef5d60dc119ae846c8abe8c114165a45bf889164903ad2712 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES bf918047d0dd nginx \u0026quot;nginx -g 'daemon off\u0026quot; 12 seconds ago Up 11 seconds 80/tcp, 443/tcp mhs-demo0/web mhs-demo1 で busybox を起動してその中から wget で先の nginx にアクセスします\n$ docker run -it --rm --net=my-net --env=\u0026quot;constraint:node==mhs-demo1\u0026quot; busybox wget -O- http://web Connecting to web (10.0.9.2:80) \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026quot;http://nginx.org/\u0026quot;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026quot;http://nginx.com/\u0026quot;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; busybox には wget も入ってるんですね。あのサイズで。 web という名前で 10.0.9.2 にアクセスできています。これは誰が DNS サーバーとして返してくれているのだろうか？ /etc/resolv.conf は次のようになっています。\nnameserver 127.0.0.11 options ndots:0 127.0.0.11 ってことは loopback デバイスだから同一ホスト上にいるということだから docker か swarm-agent に DNS サーバー機能があるっぽい\nDocker embedded DNS server というものがあるようで、docker だったようです。\nDocker の名前解決関連だと /etc/resolv.conf や /etc/hosts などはどうやって書き換えてるのかなって思ってたけど、こんなことになってるんですね\n/dev/sda1 on /etc/resolv.conf type ext4 (rw,relatime,data=ordered) /dev/sda1 on /etc/hostname type ext4 (rw,relatime,data=ordered) /dev/sda1 on /etc/hosts type ext4 (rw,relatime,data=ordered) 試しに ubuntu コンテナを起動して dig で問い合わせてみる。www.google.com でも答えてくれる\n# dig +short @127.0.0.11 web a 10.0.9.2 # dig +short @127.0.0.11 www.google.com a 216.58.197.196 ところで、docker コンテナを起動させたら docker_gwbridge というネットワークが現れましたね\n$ docker network ls NETWORK ID NAME DRIVER 9fbf2f30b09b my-net overlay efde1746d319 mhs-demo0/bridge bridge 7ace1816bf3e mhs-demo0/docker_gwbridge bridge dac21ff6debc mhs-demo1/docker_gwbridge bridge ca19cb9ed0fe mhs-demo1/bridge bridge 2f3cec420273 mhs-demo0/none null 9eca025e3f23 mhs-demo0/host host fdb80d5fdc3a mhs-demo1/none null 6533ca5a6b39 mhs-demo1/host host それぞれの docker-machine にできてます\n$ DOCKER_HOST=$(docker-machine ip mhs-demo1):2376 docker network ls NETWORK ID NAME DRIVER ca19cb9ed0fe bridge bridge fdb80d5fdc3a none null 6533ca5a6b39 host host dac21ff6debc docker_gwbridge bridge 9fbf2f30b09b my-net overlay $ DOCKER_HOST=$(docker-machine ip mhs-demo0):2376 docker network ls NETWORK ID NAME DRIVER 9fbf2f30b09b my-net overlay efde1746d319 bridge bridge 2f3cec420273 none null 9eca025e3f23 host host 7ace1816bf3e docker_gwbridge bridge それぞれを docker network inspect で見てみます\n$ docker network inspect mhs-demo0/docker_gwbridge [ { \u0026quot;Name\u0026quot;: \u0026quot;docker_gwbridge\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;7ace1816bf3e76eac53393808478bef4c6a6dfa91f86fbba413990489fff7955\u0026quot;, \u0026quot;Scope\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;bridge\u0026quot;, \u0026quot;IPAM\u0026quot;: { \u0026quot;Driver\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;Options\u0026quot;: null, \u0026quot;Config\u0026quot;: [ { \u0026quot;Subnet\u0026quot;: \u0026quot;172.18.0.0/16\u0026quot;, \u0026quot;Gateway\u0026quot;: \u0026quot;172.18.0.1/16\u0026quot; } ] }, \u0026quot;Containers\u0026quot;: {}, \u0026quot;Options\u0026quot;: { \u0026quot;com.docker.network.bridge.enable_icc\u0026quot;: \u0026quot;false\u0026quot;, \u0026quot;com.docker.network.bridge.enable_ip_masquerade\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;com.docker.network.bridge.name\u0026quot;: \u0026quot;docker_gwbridge\u0026quot; } } ] $ docker network inspect mhs-demo0/bridge [ { \u0026quot;Name\u0026quot;: \u0026quot;bridge\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;efde1746d3196225e68f47508882d7d5b3d118aef886b6040f322a58647a430a\u0026quot;, \u0026quot;Scope\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;bridge\u0026quot;, \u0026quot;IPAM\u0026quot;: { \u0026quot;Driver\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;Options\u0026quot;: null, \u0026quot;Config\u0026quot;: [ { \u0026quot;Subnet\u0026quot;: \u0026quot;172.17.0.0/16\u0026quot; } ] }, \u0026quot;Containers\u0026quot;: {}, \u0026quot;Options\u0026quot;: { \u0026quot;com.docker.network.bridge.default_bridge\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;com.docker.network.bridge.enable_icc\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;com.docker.network.bridge.enable_ip_masquerade\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;com.docker.network.bridge.host_binding_ipv4\u0026quot;: \u0026quot;0.0.0.0\u0026quot;, \u0026quot;com.docker.network.bridge.name\u0026quot;: \u0026quot;docker0\u0026quot;, \u0026quot;com.docker.network.driver.mtu\u0026quot;: \u0026quot;1500\u0026quot; } } ] $ docker network inspect my-net [ { \u0026quot;Name\u0026quot;: \u0026quot;my-net\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;9fbf2f30b09bbe17457a116c64af2d59291b61efc11c34a006a2013583bb54e4\u0026quot;, \u0026quot;Scope\u0026quot;: \u0026quot;global\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;overlay\u0026quot;, \u0026quot;IPAM\u0026quot;: { \u0026quot;Driver\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;Options\u0026quot;: null, \u0026quot;Config\u0026quot;: [ { \u0026quot;Subnet\u0026quot;: \u0026quot;10.0.9.0/24\u0026quot; } ] }, \u0026quot;Containers\u0026quot;: {}, \u0026quot;Options\u0026quot;: {} } ] $ docker network inspect mhs-demo0/host [ { \u0026quot;Name\u0026quot;: \u0026quot;host\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;9eca025e3f23c5fb7b66063dd09b74dbb0a19e0ab5f1438f65ff9d83c28a2bf6\u0026quot;, \u0026quot;Scope\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;host\u0026quot;, \u0026quot;IPAM\u0026quot;: { \u0026quot;Driver\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;Options\u0026quot;: null, \u0026quot;Config\u0026quot;: [] }, \u0026quot;Containers\u0026quot;: { \u0026quot;0a0c6293b4d31f439ff131ce57809e28feb91ce7a6f59a5d47d4fa22d28c1114\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;swarm-agent-master\u0026quot;, \u0026quot;EndpointID\u0026quot;: \u0026quot;22ff3f4019ce59162b87964078ae9328660a52b1c364043509bae3a427c402e4\u0026quot;, \u0026quot;MacAddress\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;IPv4Address\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;IPv6Address\u0026quot;: \u0026quot;\u0026quot; }, \u0026quot;4614fed35665803693ac4702793b004ed89ca5f11e8f1673974ebe307dabee41\u0026quot;: { \u0026quot;Name\u0026quot;: \u0026quot;swarm-agent\u0026quot;, \u0026quot;EndpointID\u0026quot;: \u0026quot;4b1464dc76486835a42702281ed7714ef65bf900a385164c47bc6d44837113e1\u0026quot;, \u0026quot;MacAddress\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;IPv4Address\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;IPv6Address\u0026quot;: \u0026quot;\u0026quot; } }, \u0026quot;Options\u0026quot;: {} } ] $ docker network inspect mhs-demo0/none [ { \u0026quot;Name\u0026quot;: \u0026quot;none\u0026quot;, \u0026quot;Id\u0026quot;: \u0026quot;2f3cec4202730f0191ff6547f73da9ffa695367418fe34e98a3375d53abe71bd\u0026quot;, \u0026quot;Scope\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;null\u0026quot;, \u0026quot;IPAM\u0026quot;: { \u0026quot;Driver\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;Options\u0026quot;: null, \u0026quot;Config\u0026quot;: [] }, \u0026quot;Containers\u0026quot;: {}, \u0026quot;Options\u0026quot;: {} } ] nginx コンテナで interface を確認してみます。my-net と docker_gwbridge が割り当てられています\n$ docker exec web ip addr 1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 9: eth0@if10: mtu 1450 qdisc noqueue state UP group default link/ether 02:42:0a:00:09:02 brd ff:ff:ff:ff:ff:ff inet 10.0.9.2/24 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:aff:fe00:902/64 scope link valid_lft forever preferred_lft forever 12: eth1@if13: mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff inet 172.18.0.2/16 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe12:2/64 scope link valid_lft forever preferred_lft forever $ docker exec web ip r default via 172.18.0.1 dev eth1 10.0.9.0/24 dev eth0 proto kernel scope link src 10.0.9.2 172.18.0.0/16 dev eth1 proto kernel scope link src 172.18.0.2 $ docker exec -t web ping -c 3 8.8.8.8 PING 8.8.8.8 (8.8.8.8): 56 data bytes 64 bytes from 8.8.8.8: icmp_seq=0 ttl=61 time=13.327 ms 64 bytes from 8.8.8.8: icmp_seq=1 ttl=61 time=13.003 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=61 time=13.236 ms --- 8.8.8.8 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max/stddev = 13.003/13.189/13.327/0.136 ms my-net を使わない場合は 172.17.0.0/16 のアドレスなので bridge が割り当てられています\n$ docker run -it --rm busybox ip a 1: lo: mtu 65536 qdisc noqueue link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 44: eth0@if45: mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link tentative valid_lft forever preferred_lft forever さて、オーバーレイ・ネットワークがない場合とどう違うの？というところですが\n$ docker run -itd --name=web2 --env=\u0026quot;constraint:node==mhs-demo0\u0026quot; nginx c3c0131cb7881652f2e27768184a29767728d33cc3be6d66c0fdbc79d8c66459 $ docker run -it --rm --env=\u0026quot;constraint:node==mhs-demo1\u0026quot; busybox wget -O- http://web2 wget: bad address 'web2' 名前解決ができませんね、IP アドレスで試してみましょう\n$ docker exec -it web2 ip a 1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 14: eth0@if15: mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link valid_lft forever preferred_lft forever $ docker run -it --rm --env=\u0026quot;constraint:node==mhs-demo1\u0026quot; busybox wget -O - http://172.17.0.2 Connecting to 172.17.0.2 (172.17.0.2:80) wget: can't connect to remote host (172.17.0.2): Connection refused IP アドレスで指定しても通信できませんね。 nginx と wget を実行するコンテナを同一ホスト(docker-machine)で実行するとどうでしょうか\n$ docker run -it --rm --env=\u0026quot;constraint:node==mhs-demo0\u0026quot; busybox wget -O - http://172.17.0.2 Connecting to 172.17.0.2 (172.17.0.2:80) Welcome to nginx! body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! ================= If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to [nginx.org](http://nginx.org/). Commercial support is available at [nginx.com](http://nginx.com/). _Thank you for using nginx._ - 100% |*******************************| 612 0:00:00 ETA つながりましたね。\nオーバーレイ・ネットワークがない状態の場合、2つの docker-machine がそれぞれ同じネットワークセグメントを独立して持っているためにお互いに通信ができませんでした。名前解決もできません。複数台の docker-machine を使う場合はオーバーレイ・ネットワークがとても便利です。\n","date":"2016年3月20日","permalink":"/2016/03/evaluate-docker-swarm-part4/","section":"Posts","summary":"今回は Get started with multi-host networking に沿ってマルチホストでのオーバーレイネットワークを試してみます。 KVS として Consul を立ち上げる # まず mh-keystore という名前（名前はなんでも良い","title":"Docker Swarm を試す – その4"},{"content":"Docker Swarm を試す – その１ で swarm の agent の join はどうやって manager (master?) を探しているのだろう？マルチキャスト？と書いた部分ですが Docker Swarm Discovery https://docs.docker.com/swarm/discovery/ に書いてありました。 「Docker Hub as a hosted discovery service」 だったようです。Docker Hub で提供されているサービスを利用していたのでした。 swarm create がこのサービスで使う token を発行コマンドだったのです。 Hosted discovery service はインターネット越しでのアクセスにもなるしテスト用なので Production 環境では libkv がサポートする consul, etcd, zookeeper を使いましょうということのようです。 create で得た共通の token を使ってクラスタのリストが管理されているのでした。\n","date":"2016年3月17日","permalink":"/2016/03/evaluate-docker-swarm-part3/","section":"Posts","summary":"Docker Swarm を試す – その１ で swarm の agent の join はどうやって manager (master?) を探しているのだろう？マルチキャスト？と書いた部分ですが Docker Swarm Discovery https://docs.docker.com/swarm/discovery/ に書いてありました。 「Dock","title":"Docker Swarm を試す – その3"},{"content":"自分の理解を整理できたのでメモ nginx を次のような設定で reverse proxy とした場合、起動時に名前解決した upstream.example.com の IP アドレスをずっと使い続けるため、IP アドレスが変更されるとアクセスできなくなってしまいます。 proxy 先が AWS の ELB だったりすると頻繁に IP アドレスが変わるためすぐに問題が顕在化します。\nlocation / { proxy_pass http://upstream.example.com; } よくある問題なのでググると解決方法の書かれたサイトが沢山出てきます。 proxy 先のホスト名を変数にセットして使えば良いというものもあれば、$request_uri と resolver 設定の valid を短く設定しろというものなど。\nresolver 192.168.0.1 192.168.0.2; set $upstream_server upstream.example.com; location / { proxy_pass http://$upstream_server; } resolver 192.168.0.1 192.168.0.2 valid=30s; location / { proxy_pass http://upstream.example.com$request_uri; } パターン1の設定でも DNS の TTL に合わせて問い合わせが発生するので ELB の自動的な IP アドレス変更であれば resolver に短い valid 設定をする必要はない。予期せぬ IP アドレスの変更への対応が必要であれば短くしておくに越したことはない。 パターン2でも valid で指定した時間を超えると DNS への問い合わせが発生するためホスト名を変数にする必要はない。nginx がリクエストに対して設定する変数を使うことでも良い。 よって次のように書くことでも TTL にしたがって DNS への問い合わせてくれます。だた、https で受けて http で proxy するといった場合には困ります。\nresolver 192.168.0.1 192.168.0.2; location / { proxy_pass $scheme://upstream.example.com; } proxy_pass に変数を使うことで困るのは /foo/var.html へのアクセスを /var.html へ proxy するといった場合です。変数を使わない場合は次のように書けますが、\nlocation /foo/ { proxy_pass http://upstream.example.com/; } 変数を使うと /foo が削られないまま proxy されてしまいます。こんな場合は\nlocation /foo/ { rewrite ^/foo(/.*) $1 break; proxy_pass $scheme://upstream.example.com; } または\nlocation /foo/ { rewrite ^/foo(/.*) $1 break; proxy_pass http://upstream.example.com$uri; } とすることで実現できます。$request_uri は rewrite では書き換わらないので注意。 resolver を忘れずに。valid はお好みで。 結局のところ、ホスト名を変数に入れるのが一番わかりやすいのかな？ ※ 2016/09/01 追記 upstream を使った場合は効かなかった\u0026hellip;\nupstream backend { server be1.example.com; server be2.example.com; server be3.example.com; } などとし\nlocation / { proxy_pass http://bakcend; } とした場合は proxy_pass の値に変数を入れてもダメでした\u0026hellip; balancer_by_lua というのが使えるようだ Nginx balancer_by_luaの話とupstream名前解決の話      ","date":"2016年3月17日","permalink":"/2016/03/nginx-resolving-proxy-upstream/","section":"Posts","summary":"自分の理解を整理できたのでメモ nginx を次のような設定で reverse proxy とした場合、起動時に名前解決した upstream.example.com の IP アドレスをずっと使い続けるため、IP アドレスが変","title":"nginx proxy の名前解決問題、ファイナルアンサー？"},{"content":"前回 の続きです。\n$ docker info Containers: 8 Running: 4 Paused: 0 Stopped: 4 Images: 8 Server Version: swarm/1.1.3 Role: primary Strategy: spread Filters: health, port, dependency, affinity, constraint Nodes: 2 agent1: 192.168.99.101:2376 └ Status: Healthy └ Containers: 3 └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs └ Error: (none) └ UpdatedAt: 2016-03-16T15:06:45Z agent2: 192.168.99.102:2376 └ Status: Healthy └ Containers: 5 └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs └ Error: (none) └ UpdatedAt: 2016-03-16T15:06:39Z Plugins: Volume: Network: Kernel Version: 4.1.19-boot2docker Operating System: linux Architecture: amd64 CPUs: 2 Total Memory: 2.043 GiB Name: f64f0f79da4d Swarm クラスタができたのでここに docker-compose でなにか起動してみます。 http://www.slideshare.net/zembutsu/introduction-to-docker-compose-and-swarm の zembutsu さん資料にある rocket.chat を実行してみましょう。 https://github.com/RocketChat/Rocket.Chat/blob/master/docker-compose.yml\nmongo:image:mongocommand:mongod --smallfiles --oplogSize 128rocketchat:image:rocketchat/rocket.chat:latestenvironment:- PORT=3000- ROOT_URL=http://localhost:3000- MONGO_URL=mongodb://mongo:27017/rocketchatlinks:- mongo:mongoports:- 3000:3000これで docker-compose up -d を実行すれば起動するはず\u0026hellip; が\n$ docker-compose up -d Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 3, in \u0026lt;module\u0026gt; File \u0026quot;/code/compose/cli/main.py\u0026quot;, line 54, in main File \u0026quot;/code/compose/cli/docopt_command.py\u0026quot;, line 23, in sys_dispatch File \u0026quot;/code/compose/cli/docopt_command.py\u0026quot;, line 26, in dispatch File \u0026quot;/code/compose/cli/main.py\u0026quot;, line 169, in perform_command File \u0026quot;/code/compose/cli/command.py\u0026quot;, line 53, in project_from_options File \u0026quot;/code/compose/cli/command.py\u0026quot;, line 89, in get_project File \u0026quot;/code/compose/cli/command.py\u0026quot;, line 70, in get_client File \u0026quot;/code/compose/cli/docker_client.py\u0026quot;, line 28, in docker_client File \u0026quot;/code/.tox/py27/lib/python2.7/site-packages/docker/client.py\u0026quot;, line 50, in __init__ docker.errors.TLSParameterError: If using TLS, the base_url argument must begin with \u0026quot;https://\u0026quot;.. TLS configurations should map the Docker CLI client configurations. See http://docs.docker.com/examples/https/ for API details. docker-compose returned -1 む\u0026hellip; DOCKER_HOST という環境変数を\n$ DOCKER_HOST=$(docker-machine ip manager):3376 $ echo $DOCKER_HOST 192.168.99.100:3376 と設定していましたが、docker-compose で使う場合は https:// をつける必要があるようです。(Problem when using the DOCKER_HOST variable in combination with docker-compose and https:// #894)\n$ DOCKER_HOST=https://$(docker-machine ip manager):3376 $ echo $DOCKER_HOST https://192.168.99.100:3376 $ docker-compose up -d Pulling mongo (mongo:latest)... agent1: Pulling mongo:latest... : downloaded agent2: Pulling mongo:latest... : downloaded Creating rocketchat_mongo_1 Pulling rocketchat (rocketchat/rocket.chat:latest)... agent1: Pulling rocketchat/rocket.chat:latest... : downloaded agent2: Pulling rocketchat/rocket.chat:latest... : downloaded Creating rocketchat_rocketchat_1 今度は起動したようです。が、\n$ docker ps Invalid bind address format: https://192.168.99.100:3376 docker コマンドでは https:// がついているとダメなようです\u0026hellip;\n$ DOCKER_HOST=$(docker-machine ip manager):3376 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7a057da5bd26 rocketchat/rocket.chat:latest \u0026quot;node main.js\u0026quot; 32 minutes ago Up 32 minutes 192.168.99.102:3000-\u0026gt;3000/tcp agent2/rocketchat_rocketchat_1 470f403c7c04 mongo \u0026quot;/entrypoint.sh mongo\u0026quot; 33 minutes ago Up 33 minutes 27017/tcp agent2/rocketchat_mongo_1,agent2/rocketchat_rocketchat_1/mongo,agent2/rocketchat_rocketchat_1/mongo_1,agent2/rocketchat_rocketchat_1/rocketchat_mongo_1 nodejs のアプリも mongo DB も agent2 側で起動してますね。docker-compose の塊は同じノードで実行される仕様なのか、たまたまなのかは要確認。 http://192.168.99.102:3000/ にアクセスすると Rocket.chat にアクセスできました。\n  $ docker-compose logs Attaching to rocketchat_rocketchat_1, rocketchat_mongo_1 rocketchat_1 | Updating process.env.MAIL_URL rocketchat_1 | ufs: store created at rocketchat_1 | ufs: temp directory created at /tmp/ufs rocketchat_1 | Updating process.env.MAIL_URL rocketchat_1 | configuring push rocketchat_1 | Using GridFS for Avatar storage rocketchat_1 | ➔ System ➔ startup rocketchat_1 | ➔ +---------------------------------------+ rocketchat_1 | ➔ | SERVER RUNNING | rocketchat_1 | ➔ +---------------------------------------+ rocketchat_1 | ➔ | | rocketchat_1 | ➔ | Version: 0.22.0 | rocketchat_1 | ➔ | Process Port: 3000 | rocketchat_1 | ➔ | Site URL: http://localhost:3000 | rocketchat_1 | ➔ | | rocketchat_1 | ➔ +---------------------------------------+ rocketchat_1 | {\u0026quot;line\u0026quot;:\u0026quot;71\u0026quot;,\u0026quot;file\u0026quot;:\u0026quot;percolate_synced-cron.js\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;SyncedCron: Scheduled \\\u0026quot;Generate and save statistics\\\u0026quot; next run @Wed Mar 16 2016 14:58:45 GMT+0000 (UTC)\u0026quot;,\u0026quot;time\u0026quot;:{\u0026quot;$date\u0026quot;:1458140325979},\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;} mongo_1 | 2016-03-16T14:57:06.297+0000 I CONTROL [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 64-bit host=470f403c7c04 mongo_1 | 2016-03-16T14:57:06.298+0000 I CONTROL [initandlisten] db version v3.2.4 mongo_1 | 2016-03-16T14:57:06.298+0000 I CONTROL [initandlisten] git version: e2ee9ffcf9f5a94fad76802e28cc978718bb7a30 mongo_1 | 2016-03-16T14:57:06.298+0000 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.0.1e 11 Feb 2013 mongo_1 | 2016-03-16T14:57:06.298+0000 I CONTROL [initandlisten] allocator: tcmalloc mongo_1 | 2016-03-16T14:57:06.298+0000 I CONTROL [initandlisten] modules: none mongo_1 | 2016-03-16T14:57:06.298+0000 I CONTROL [initandlisten] build environment: mongo_1 | 2016-03-16T14:57:06.298+0000 I CONTROL [initandlisten] distmod: debian71 mongo_1 | 2016-03-16T14:57:06.298+0000 I CONTROL [initandlisten] distarch: x86_64 mongo_1 | 2016-03-16T14:57:06.298+0000 I CONTROL [initandlisten] target_arch: x86_64 ... ","date":"2016年3月16日","permalink":"/2016/03/evaluate-docker-swarm-part2/","section":"Posts","summary":"前回 の続きです。 $ docker info Containers: 8 Running: 4 Paused: 0 Stopped: 4 Images: 8 Server Version: swarm/1.1.3 Role: primary Strategy: spread Filters: health, port, dependency, affinity, constraint Nodes: 2 agent1: 192.168.99.101:2376 └ Status: Healthy └ Containers: 3 └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e -","title":"Docker Swarm を試す – その2"},{"content":"Evaluate Swarm in a sandbox を参考に Docker Swarm を試してみます。\nSwarm クラスタを構築するための docker machine を作成する # docker-machine の現状確認 # $ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS default - virtualbox Stopped Unknown 今は不要なので削除しちゃう（残したままでも問題ない）\n$ docker-machine rm default About to remove default Are you sure? (y/n): y Successfully removed default マネージャー用サーバー (manager) 作成 # $ docker-machine create -d virtualbox manager Running pre-create checks... (manager) Default Boot2Docker ISO is out-of-date, downloading the latest release... (manager) Latest release for github.com/boot2docker/boot2docker is v1.10.3 (manager) Downloading /home/ytera/.docker/machine/cache/boot2docker.iso from https://github.com/boot2docker/boot2docker/releases/download/v1.10.3/boot2docker.iso... (manager) 0%....10%....20%....30%....40%....50%....60%....70%....80%....90%....100% Creating machine... (manager) Copying /home/ytera/.docker/machine/cache/boot2docker.iso to /home/ytera/.docker/machine/machines/manager/boot2docker.iso... (manager) Creating VirtualBox VM... (manager) Creating SSH key... (manager) Starting the VM... (manager) Check network to re-create if needed... (manager) Waiting for an IP... Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with boot2docker... Copying certs to the local machine directory... Copying certs to the remote machine... Setting Docker configuration on the remote daemon... Checking connection to Docker... Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env manager エージェント用サーバーを2台 (agent1, agent2) 作成する # $ docker-machine create -d virtualbox agent1 Running pre-create checks... Creating machine... (agent1) Copying /home/ytera/.docker/machine/cache/boot2docker.iso to /home/ytera/.docker/machine/machines/agent1/boot2docker.iso... (agent1) Creating VirtualBox VM... (agent1) Creating SSH key... (agent1) Starting the VM... (agent1) Check network to re-create if needed... (agent1) Waiting for an IP... Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with boot2docker... Copying certs to the local machine directory... Copying certs to the remote machine... Setting Docker configuration on the remote daemon... Checking connection to Docker... Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env agent1 $ docker-machine create -d virtualbox agent2 Running pre-create checks... Creating machine... (agent2) Copying /home/ytera/.docker/machine/cache/boot2docker.iso to /home/ytera/.docker/machine/machines/agent2/boot2docker.iso... (agent2) Creating VirtualBox VM... (agent2) Creating SSH key... (agent2) Starting the VM... (agent2) Check network to re-create if needed... (agent2) Waiting for an IP... Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with boot2docker... Copying certs to the local machine directory... Copying certs to the remote machine... Setting Docker configuration on the remote daemon... Checking connection to Docker... Docker is up and running! To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env agent2 作成した docker machine の確認 # $ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS agent1 - virtualbox Running tcp://192.168.99.101:2376 v1.10.3 agent2 - virtualbox Running tcp://192.168.99.102:2376 v1.10.3 manager - virtualbox Running tcp://192.168.99.100:2376 v1.10.3 Swarm ディスカバリトークンの作成 # docker コマンドで manager サーバーの操作をするように環境変数を設定する\n$ eval $(docker-machine env manager) $ printenv | grep DOCKER DOCKER\\_HOST=tcp://192.168.99.100:2376 DOCKER\\_MACHINE\\_NAME=manager DOCKER\\_TLS\\_VERIFY=1 DOCKER\\_CERT\\_PATH=/home/ytera/.docker/machine/machines/manager Swarm クラスタ用のユニークID（ディスカバリトークン）を生成する\n$ docker run --rm swarm create Unable to find image 'swarm:latest' locally latest: Pulling from library/swarm 25da0aa87182: Pull complete 45707a9f4c2b: Pull complete 7f0c09406c8f: Pull complete a3ed95caeb02: Pull complete Digest: sha256:5f2b4066b2f7e97a326a8bfcfa623be26ce45c26ffa18ea63f01de045d2238f3 Status: Downloaded newer image for swarm:latest 2aba3c5381a6783e37980a8ef90fa41a 2aba3c5381a6783e37980a8ef90fa41a がディスカバリトークンになります。どこか安全な場所にメモっておきます。 docker コマンドに --rm を指定して実行したので create コマンド実行後すぐに実行したイメージが削除されています。\n$ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Swarm マネージャとノードを作成する # docker-machine の状態確認\n$ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS agent1 - virtualbox Running tcp://192.168.99.101:2376 v1.10.3 agent2 - virtualbox Running tcp://192.168.99.102:2376 v1.10.3 manager * virtualbox Running tcp://192.168.99.100:2376 v1.10.3 次のようにして manager を起動します。manager マシンの 3376 ポートを container の 3376 ポートにマッピングしています。/var/lib/boot2docker を container の /certs にマウントしています。先ほど生成したディスカバリトークンを token:// で指定しています。\n$ docker run -d -p 3376:3376 -t -v /var/lib/boot2docker:/certs:ro \\ swarm manage -H 0.0.0.0:3376 --tlsverify \\ --tlscacert=/certs/ca.pem --tlscert=/certs/server.pem \\ --tlskey=/certs/server-key.pem \\ token://2aba3c5381a6783e37980a8ef90fa41a 17ba16d89bda270965e534474fd06d5698bcde0aa14397403fb2e970612cd763 /var/lib/boot2docker とは docker-machine で作成した boot2docker サーバーの /var/lib/boot2docker です。次のように manager マシンに ssh でログインすると確認できます。\n$ docker-machine ssh manager ## . ## ## ## == ## ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\\___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _ | |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __ | '_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ '__| | |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| \u0026lt; __/ | |_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_| Boot2Docker version 1.10.3, build master : 625117e - Thu Mar 10 22:09:02 UTC 2016 Docker version 1.10.3, build 20f81dd docker@manager:~$ sudo ls /var/lib/boot2docker/ ca.pem etc profile server.pem tls docker.log log server-key.pem ssh userdata.tar docker@manager:~$ manager コンテナが起動していることを確認してみます。\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 17ba16d89bda swarm \u0026quot;/swarm manage -H 0.0\u0026quot; About a minute ago Up About a minute 2375/tcp, 0.0.0.0:3376-\u0026gt;3376/tcp lonely_ritchie 続いて agent1 の操作に移ります docker コマンドの接続先を agent1 に切り替えます\n$ eval $(docker-machine env agent1) swarm コンテナを先ほどのディスカバリトークンを使って join させます\n$ docker run -d swarm join --addr=$(docker-machine ip agent1):2376 \\ token://2aba3c5381a6783e37980a8ef90fa41a Unable to find image 'swarm:latest' locally latest: Pulling from library/swarm 25da0aa87182: Pull complete 45707a9f4c2b: Pull complete 7f0c09406c8f: Pull complete a3ed95caeb02: Pull complete Digest: sha256:5f2b4066b2f7e97a326a8bfcfa623be26ce45c26ffa18ea63f01de045d2238f3 Status: Downloaded newer image for swarm:latest 171d5b1abb2c3840b831591a2c4fb231e68703bb2b3a05237cae84e6682e433f さらに agent2\n$ eval $(docker-machine env agent2) $ docker run -d swarm join --addr=$(docker-machine ip agent2):2376 \\ token://2aba3c5381a6783e37980a8ef90fa41a Unable to find image 'swarm:latest' locally latest: Pulling from library/swarm 25da0aa87182: Pull complete 45707a9f4c2b: Pull complete 7f0c09406c8f: Pull complete a3ed95caeb02: Pull complete Digest: sha256:5f2b4066b2f7e97a326a8bfcfa623be26ce45c26ffa18ea63f01de045d2238f3 Status: Downloaded newer image for swarm:latest 3f5c29f68c65334607622a89cafde0268d229ec365646326cd15ef60b5b06f42 agent は manager に対してオレはこの IP と Port で待ってるからよろしくって参加している感じだけどマルチキャストでも使ってるのかな？後で調べよう。 → Docker Hub as a hosted discovery service を使ってました\nSwarm マネージャの管理 # $ DOCKER_HOST=$(docker-machine ip manager):3376 $ docker info Containers: 2 Running: 2 Paused: 0 Stopped: 0 Images: 2 Server Version: swarm/1.1.3 Role: primary Strategy: spread Filters: health, port, dependency, affinity, constraint Nodes: 2 agent1: 192.168.99.101:2376 └ Status: Healthy └ Containers: 1 └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs └ Error: (none) └ UpdatedAt: 2016-03-15T15:05:38Z agent2: 192.168.99.102:2376 └ Status: Healthy └ Containers: 1 └ Reserved CPUs: 0 / 1 └ Reserved Memory: 0 B / 1.021 GiB └ Labels: executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs └ Error: (none) └ UpdatedAt: 2016-03-15T15:05:59Z Plugins: Volume: Network: Kernel Version: 4.1.19-boot2docker Operating System: linux Architecture: amd64 CPUs: 2 Total Memory: 2.043 GiB Name: 17ba16d89bda agent1, agent2 の Swarm クラスタが構成されているっぽいですね。\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 試しに hello-world コンテナを実行してみます\n$ docker run hello-world Hello from Docker. This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker Hub account: https://hub.docker.com For more examples and ideas, visit: https://docs.docker.com/userguide/ agent1 上で実行されたようです。\n$ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0c20dcb512c7 hello-world \u0026quot;/hello\u0026quot; 8 seconds ago Exited (0) 7 seconds ago agent1/naughty_jennings 3f5c29f68c65 swarm \u0026quot;/swarm join --addr=1\u0026quot; 9 minutes ago Up 9 minutes 2375/tcp agent2/thirsty_yonath 171d5b1abb2c swarm \u0026quot;/swarm join --addr=1\u0026quot; 11 minutes ago Up 11 minutes 2375/tcp agent1/amazing_ride まだまだ Swarm のことはわからないが Evaluate Swarm in a sandbox ページの内容はこれで終わり。 Docker Swarm を順に試していこう。\nDocker Swarm を試す – その2\n","date":"2016年3月15日","permalink":"/2016/03/evaluate-docker-swarm-part1/","section":"Posts","summary":"Evaluate Swarm in a sandbox を参考に Docker Swarm を試してみます。 Swarm クラスタを構築するための docker machine を作成する # docker-machine の現状確認 # $ docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS default - virtualbox Stopped Unknown 今は不要なので","title":"Docker Swarm を試す - その１"},{"content":"","date":"2016年3月8日","permalink":"/tags/ltsv/","section":"Tags","summary":"","title":"LTSV"},{"content":"","date":"2016年3月8日","permalink":"/tags/tomcat/","section":"Tags","summary":"","title":"Tomcat"},{"content":"Tomcat はたしか 7 から AccessLog Valve がデフォルトで有効になっていますがそのフォーマットは Apache httpd の common に近いものです。これを今時の LTSV にする方法をメモ。\nこれがデフォルト設定\n\u0026lt;!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern=\u0026quot;common\u0026quot; --\u0026gt; \u0026lt;Valve className=\u0026quot;org.apache.catalina.valves.AccessLogValve\u0026quot; directory=\u0026quot;logs\u0026quot; prefix=\u0026quot;localhost_access_log.\u0026quot; suffix=\u0026quot;.txt\u0026quot; pattern=\u0026quot;%h %l %u %t \u0026amp;quot;%r\u0026amp;quot; %s %b\u0026quot; /\u0026gt; pattern を書き換えれば良いわけですが、server.xml は XML なので TAB は \u0026amp;#9; とします。普通に TAB のコードを入れてもスペースになってしまいますし、\\t も使えません。Apache や nginx の用に改行を入れて見やすくすることもできなっぽい。\npattern=\u0026quot;host:%h\u0026amp;#9;time:%{yyyy-MM-dd hh:mm:ss}t\u0026amp;#9;ident:%l\u0026amp;#9;user:%u\u0026amp;#9;method:%m\u0026amp;#9;uri:%U%q\u0026amp;#9;protocol:%H\u0026amp;#9;status:%s\u0026amp;#9;size:%B\u0026amp;#9;referer:%{referer}i\u0026amp;#9;ua:%{user-agent}i\u0026amp;#9;msec:%D\u0026amp;#9;thread:%I\u0026quot; で次のように出力されます\nhost:127.0.0.1\ttime:2016-03-08 11:57:33\tident:-\tuser:-\tmethod:GET\turi:/favicon.ico\tprotocol:HTTP/1.1\tstatus:200\tsize:21630\treferer:http://localhost:8080/\tua:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.75 Safari/537.36\tmsec:3\tthread:http-bio-8080-exec-9 TAB を改行にして見やすくするとこんな内容\nhost:127.0.0.1 time:2016-03-08 11:57:33 ident:- user:- method:GET uri:/favicon.ico protocol:HTTP/1.1 status:200 size:21630 referer:http://localhost:8080/ ua:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.75 Safari/537.36 msec:3 thread:http-bio-8080-exec-9 リクエスト時刻は %{xxx}t で指定します。xxx 部分は SimpleDateFormat になります。time:%{yyyy-MM-dd'T'HH:mm:ss.SSSZ}t と指定すれば time:2016-03-09T00:13:25.991+0900 のように出力されます。\nhttp://tomcat.apache.org/tomcat-7.0-doc/config/valve.html#Access_Log_Valve\nUbuntu への Java のインストール方法はこちらを参照\nHow To Manually Install Oracle Java on a Debian or Ubuntu VPS\n","date":"2016年3月8日","permalink":"/2016/03/tomcat-access-log-in-ltsv/","section":"Posts","summary":"Tomcat はたしか 7 から AccessLog Valve がデフォルトで有効になっていますがそのフォーマットは Apache httpd の common に近いものです。これを今時の LTSV にする方法をメモ。 これがデフォ","title":"Tomcat のアクセスログを LTSV で出力する"},{"content":"","date":"2016年2月28日","permalink":"/tags/heroku/","section":"Tags","summary":"","title":"heroku"},{"content":"息子の社会化の勉強用にいくつかの都道府県の白地図を印刷しようと思い、「国土地理院」「白地図」でググってみたら http://maps.gsi.go.jp/development/ichiran.html がヒット。地理院タイルというものがあってこれを並べれば良さそうだったのでとりあえず並べて表示するHTMLを書いてプリントアウトした。しかし、これをいくつもやるのはつらそうだ、長方形に並べるだけなら簡単だということで並べた HTML 返すだけのサーバーを書いてみよう。ついでにまだ使ったことのない heroku で動かしてみようということで bottle (python) を使って書いてみた。(JavaScriptで実装すればサーバーなんて要らないけれども) 出来たものの、Googleマップの手軽さから比べるとだいぶ面倒だった。 なんかもっと便利なのありそうなのになぁと探してたら国土地理院の http://maps.gsi.go.jp/ がまさにそれだった\u0026hellip; orz 最初は使い方がわからなくて白地図にはできないのかと思っていたが、左下のアイコンから白地図を選択し、それでも写真などが表示されていたら、左上の「情報」ということこらから表示中の情報を選択して全部削除すれば良いだけだった。 マウス操作だけで好きな場所の地図が印刷できる。便利だ。 無駄な作業だったけど bottle と heroku の使い方を学びました。めでたしめでたし。 あ、あと。各都道府県が「へー、こんなところまで伸びてるんだ！こんな形だったんだ！」って再発見がありましたとさ。\n","date":"2016年2月28日","permalink":"/2016/02/blank-map/","section":"Posts","summary":"息子の社会化の勉強用にいくつかの都道府県の白地図を印刷しようと思い、「国土地理院」「白地図」でググってみたら http://maps.gsi.go.jp/development/ichiran.html がヒット。地理院タイルというも","title":"任意の場所の白地図を印刷したい"},{"content":"","date":"2016年2月17日","permalink":"/tags/http2/","section":"Tags","summary":"","title":"HTTP2"},{"content":"Tomcat 9 の server.xml を見ていたら次のような記述がありました。HTTP/2 をサポートしているようです。\nすぐに使う予定はないけれども動作確認してみたのでメモ。 環境は CentOS 7 Let\u0026rsquo;s Encrypt で証明書を用意してこの設定をアンコメントしてみました。が、これだけでは起動してくれませんでした。\n17-Feb-2016 23:51:15.022 SEVERE [main] org.apache.catalina.core.StandardService.initInternal Failed to initialize connector [Connector[HTTP/1.1-8443]] org.apache.catalina.LifecycleException: Failed to initialize component [Connector[HTTP/1.1-8443]] at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:112) at org.apache.catalina.core.StandardService.initInternal(StandardService.java:549) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:107) at org.apache.catalina.core.StandardServer.initInternal(StandardServer.java:855) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:107) at org.apache.catalina.startup.Catalina.load(Catalina.java:606) at org.apache.catalina.startup.Catalina.load(Catalina.java:629) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.catalina.startup.Bootstrap.load(Bootstrap.java:311) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:494) Caused by: org.apache.catalina.LifecycleException: The configured protocol [org.apache.coyote.http11.Http11AprProtocol] requires the APR/native library which is not available at org.apache.catalina.connector.Connector.initInternal(Connector.java:997) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:107) ... 12 more The configured protocol [org.apache.coyote.http11.Http11AprProtocol] requires the APR/native library which is not available ということで Tomcat Native Library とやらが必要なようです。\nsudo yum install apr-devel tar xvf tomcat-native-1.2.4-src.tar.gz cd tomcat-native-1.2.4-src/native ./configure --prefix=/usr --libdir=/usr/lib64 --with-java-home=/usr/java/latest --with-ssl make sudo make install これで libtcnative がインストールされました\n$ ls /usr/lib64/libtcnative-1.* /usr/lib64/libtcnative-1.a /usr/lib64/libtcnative-1.so /usr/lib64/libtcnative-1.so.0.2.4 /usr/lib64/libtcnative-1.la /usr/lib64/libtcnative-1.so.0 が、うまく行かない。\n18-Feb-2016 00:11:57.657 SEVERE [main] org.apache.catalina.core.AprLifecycleListener.lifecycleEvent Failed to initialize the SSLEngine. org.apache.tomcat.jni.Error: 70023: This function has not been implemented on this platform at org.apache.tomcat.jni.SSL.initialize(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.catalina.core.AprLifecycleListener.initializeSSL(AprLifecycleListener.java:283) at org.apache.catalina.core.AprLifecycleListener.lifecycleEvent(AprLifecycleListener.java:135) at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:94) at org.apache.catalina.util.LifecycleBase.setStateInternal(LifecycleBase.java:401) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:104) at org.apache.catalina.startup.Catalina.load(Catalina.java:606) at org.apache.catalina.startup.Catalina.load(Catalina.java:629) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.catalina.startup.Bootstrap.load(Bootstrap.java:311) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:494) 18-Feb-2016 00:11:58.253 WARNING [main] org.apache.tomcat.util.net.openssl.OpenSSLEngine. Failed getting cipher list java.lang.Exception: Not implemented at org.apache.tomcat.jni.SSL.newSSL(Native Method) at org.apache.tomcat.util.net.openssl.OpenSSLEngine.(OpenSSLEngine.java:81) at org.apache.tomcat.util.net.AprEndpoint.bind(AprEndpoint.java:365) at org.apache.tomcat.util.net.AbstractEndpoint.init(AbstractEndpoint.java:790) at org.apache.coyote.AbstractProtocol.init(AbstractProtocol.java:547) at org.apache.coyote.http11.AbstractHttp11Protocol.init(AbstractHttp11Protocol.java:66) at org.apache.catalina.connector.Connector.initInternal(Connector.java:1010) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:107) at org.apache.catalina.core.StandardService.initInternal(StandardService.java:549) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:107) at org.apache.catalina.core.StandardServer.initInternal(StandardServer.java:855) at org.apache.catalina.util.LifecycleBase.init(LifecycleBase.java:107) at org.apache.catalina.startup.Catalina.load(Catalina.java:606) at org.apache.catalina.startup.Catalina.load(Catalina.java:629) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.catalina.startup.Bootstrap.load(Bootstrap.java:311) at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:494) 18-Feb-2016 00:11:58.361 WARNING [main] org.apache.tomcat.util.net.AprEndpoint.bind Secure re-negotiation is not supported by the SSL library null 18-Feb-2016 00:11:58.380 WARNING [main] org.apache.tomcat.util.net.AprEndpoint.bind Honor cipher order option is not supported by the SSL library null 18-Feb-2016 00:11:58.386 WARNING [main] org.apache.tomcat.util.net.AprEndpoint.bind Disable compression option is not supported by the SSL library null 18-Feb-2016 00:11:58.386 WARNING [main] org.apache.tomcat.util.net.AprEndpoint.bind Disable TLS Session Tickets option is not supported by the SSL library null yum で入っている OpenSSL が 1.0.1e だからかな? ということで最新の OpenSSL をインストールしてやり直します。\ntar xvf openssl-1.0.2f.tar.gz cd openssl-1.0.2f ./config --prefix=/opt/openssl zlib shared make sudo make install cd tomcat-native-1.2.4-src/native ./configure --prefix=/usr --libdir=/usr/lib64 --with-java-home=/usr/java/latest --with-ssl=/opt/openssl make sudo make install リンク時に rpath が指定されていたので /opt/openssl の新しい OpenSSL が使われるはず。 ldd で確認してみます。\n$ ldd /usr/lib64/libtcnative-1.so.0.2.4 linux-vdso.so.1 =\u0026gt; (0x00007fff17fa5000) libssl.so.1.0.0 =\u0026gt; /opt/openssl/lib/libssl.so.1.0.0 (0x00007f826088e000) libcrypto.so.1.0.0 =\u0026gt; /opt/openssl/lib/libcrypto.so.1.0.0 (0x00007f8260440000) libapr-1.so.0 =\u0026gt; /lib64/libapr-1.so.0 (0x00007f826020b000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00007f825ffef000) libdl.so.2 =\u0026gt; /lib64/libdl.so.2 (0x00007f825fdea000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007f825fa29000) libz.so.1 =\u0026gt; /lib64/libz.so.1 (0x00007f825f813000) libuuid.so.1 =\u0026gt; /lib64/libuuid.so.1 (0x00007f825f60d000) libcrypt.so.1 =\u0026gt; /lib64/libcrypt.so.1 (0x00007f825f3d6000) /lib64/ld-linux-x86-64.so.2 (0x00007f8260d34000) libfreebl3.so =\u0026gt; /lib64/libfreebl3.so (0x00007f825f1d3000) イケました、イェイ！\n$ ~/bin/curl --http2 -sv https://cas.teraoka.me:8443/ -o /dev/null * Trying 127.0.0.1... * Connected to cas.teraoka.me (127.0.0.1) port 8443 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH * successfully set certificate verify locations: * CAfile: /etc/ssl/certs/ca-certificates.crt CApath: none * TLSv1.2 (OUT), TLS header, Certificate Status (22): } [5 bytes data] * TLSv1.2 (OUT), TLS handshake, Client hello (1): } [512 bytes data] * TLSv1.2 (IN), TLS handshake, Server hello (2): { [75 bytes data] * TLSv1.2 (IN), TLS handshake, Certificate (11): { [2493 bytes data] * TLSv1.2 (IN), TLS handshake, Server key exchange (12): { [333 bytes data] * TLSv1.2 (IN), TLS handshake, Server finished (14): { [4 bytes data] * TLSv1.2 (OUT), TLS handshake, Client key exchange (16): } [70 bytes data] * TLSv1.2 (OUT), TLS change cipher, Client hello (1): } [1 bytes data] * TLSv1.2 (OUT), TLS handshake, Finished (20): } [16 bytes data] * TLSv1.2 (IN), TLS change cipher, Client hello (1): { [1 bytes data] * TLSv1.2 (IN), TLS handshake, Finished (20): { [16 bytes data] * SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384 * ALPN, server accepted to use h2 * Server certificate: * subject: CN=cas.teraoka.me * start date: Feb 13 03:32:00 2016 GMT * expire date: May 13 03:32:00 2016 GMT * subjectAltName: cas.teraoka.me matched * issuer: C=US; O=Let's Encrypt; CN=Let's Encrypt Authority X1 * SSL certificate verify ok. * Using HTTP2, server supports multi-use * Connection state changed (HTTP/2 confirmed) * TCP_NODELAY set * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 } [5 bytes data] * Using Stream ID: 1 (easy handle 0x995dd0) } [5 bytes data] \u0026gt; GET / HTTP/1.1 \u0026gt; Host: cas.teraoka.me:8443 \u0026gt; User-Agent: curl/7.46.0 \u0026gt; Accept: */* \u0026gt; { [5 bytes data] \u0026lt; HTTP/2.0 200 \u0026lt; content-type:text/html;charset=UTF-8 \u0026lt; date:Wed, 17 Feb 2016 15:47:25 GMT \u0026lt; { [5 bytes data] * Connection #0 to host cas.teraoka.me left intact ","date":"2016年2月17日","permalink":"/2016/02/tomcat9-http2-server/","section":"Posts","summary":"Tomcat 9 の server.xml を見ていたら次のような記述がありました。HTTP/2 をサポートしているようです。 すぐに使う予定はないけれども動作確認してみたのでメモ","title":"Tomcat 9 を HTTP/2 サーバーにする"},{"content":"","date":"2016年2月8日","permalink":"/tags/acme/","section":"Tags","summary":"","title":"acme"},{"content":"過去2回 Acmesmith を filesystem, S3 を storage として試してきました。 Acmesmith で証明書発行を試す - その1 (filesystem) Acmesmith で証明書発行を試す - その2 (S3) 証明書の秘密鍵はセキュアな管理がが必要なので KMS (Key Management Service) を使って S3 に保存することを試してみます。 まずは AWS の IAM Console にある「Encryption Keys（暗号化キー）」にてキーを作成します（リージョンに注意しましょう）。 作成時の設定でキーの利用者に Acmesmith で使う IAM ユーザーを指定しておきます（後からでも設定可能です）。 後は key id （11111111-2222-3333-4444-555555555555 みたいなやつ）を acmesmith.yml に設定するだけです。\nendpoint:https://acme-v01.api.letsencrypt.org/storage:type:s3bucket:BUCKET-NAMEregion:ap-northeast-1use_kms:truekms_key_id:11111111-2222-3333-4444-555555555555challenge_responders:- route53:{}account_key_passphrase:certificate_key_passphrase:kms_key_id だけを設定しておくと account.pem （Let\u0026rsquo;s Encrypt のアカウント用秘密鍵）も証明書用の秘密鍵もこの KMS キーで暗号化されます。kms_key_id_account と kms_key_id_certificate_key をそれぞれ別のキーに設定すると別々のキーで暗号化されます。証明書の秘密鍵はダウンロードさせるけど Let\u0026rsquo;s Encrypt へのアクセスは権限を分けたいと行った場合に使えそうです。勝手に Revoke されないようにとか。 acmesmith.yml の準備ができたらこれまでと同様に register, authorize, request すれば証明書がゲットできます。 S3 Console で詳細を確認すると「サーバー側の暗号化: AWS KMS マスターキーの使用: acmetest」と表示されていました。暗号化されているようです。サーバーサイド暗号となっているのでキーへのアクセス権あれば透過的に扱えます。コンソールからダウロードしたらデコードされています。\naccount.pm  cert.pem, chain.pem, fullchain.pem, key.pem が作成されますが暗号の必要な key.pem だけが暗号化されています。\ncerts  使うだけじゃなくて revoke とか renew とかのコマンド追加の PR ができれば (2016/06/10 追記) 私なんかがやらなくてもどんどん改善されてました 0.4.0 での help はこんな出力 0.3.0 で autorenew や add-san が追加され、0.4.0 では save-pkcs12 が追加されてます\n$ bundle exec acmesmith help Commands: acmesmith add-san COMMON_NAME [ADDITIONAL_SANS] # request renewal of ... acmesmith authorize DOMAIN # Get authz for DOMAIN. acmesmith autorenew # request renewal of ... acmesmith current COMMON_NAME # show current versio... acmesmith help [COMMAND] # Describe available ... acmesmith list [COMMON_NAME] # list certificates o... acmesmith register CONTACT # Create account key ... acmesmith request COMMON_NAME [SAN] # request certificate... acmesmith save-certificate COMMON_NAME --output=PATH # Save certificate to... acmesmith save-pkcs12 COMMON_NAME --output=PATH # Save ceriticate and... acmesmith save-private-key COMMON_NAME --output=PATH # Save private key to... acmesmith show-certificate COMMON_NAME # show certificate acmesmith show-private-key COMMON_NAME # show private key Options: -c, [--config=CONFIG] # Default: ./acmesmith.yml -E, [--passphrase-from-env], [--no-passphrase-from-env] # Read $ACMESMITH_ACCOUNT_KEY_PASSPHRASE and $ACMESMITH_CERTIFICATE_KEY_PASSPHRASE for passphrases ","date":"2016年2月8日","permalink":"/2016/02/acmesmith-3/","section":"Posts","summary":"過去2回 Acmesmith を filesystem, S3 を storage として試してきました。 Acmesmith で証明書発行を試す - その1 (filesystem) Acmesmith で証明書発行を試す - その2 (S3) 証明書の秘密鍵はセキュアな管理がが必要","title":"Acmesmith で証明書発行を試す - その3"},{"content":"","date":"2016年2月8日","permalink":"/tags/kms/","section":"Tags","summary":"","title":"kms"},{"content":"","date":"2016年2月8日","permalink":"/tags/route53/","section":"Tags","summary":"","title":"route53"},{"content":"前回「Acmesmith で証明書発行を試す - その1」で で filesystem に保存する方法を試してみました。 今回は AWS S3 に保存するテストを行ってみます。KMS はまだ使いません。bucket 名は BUCKET-NAME として進めます。 aws s3 コマンドでも操作できるように IAM policy を設定します。README に書いてある policy には s3:GetBucketLocation がないために aws s3 コマンドではアクセスできませんでした。``` { \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Action\u0026rdquo;: [ \u0026ldquo;s3:ListBucket\u0026rdquo;, \u0026ldquo;s3:GetBucketLocation\u0026rdquo; ], \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;arn:aws:s3:::BUCKET-NAME\u0026rdquo; }, { \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Action\u0026rdquo;: [ \u0026ldquo;s3:PutObject\u0026rdquo;, \u0026ldquo;s3:GetObject\u0026rdquo;, \u0026ldquo;s3:DeleteObject\u0026rdquo; ], \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;arn:aws:s3:::BUCKET-NAME/*\u0026rdquo; } ] }\nacmesmith.sh の `storage` を S3 用に書き換えます。 認証情報は aws-cli の `~/.aws/credentials` を使うのでここには書きません。`region` は `~/.aws/config` の値は使われないので指定が必要です。 endpoint: https://acme-v01.api.letsencrypt.org/\nstorage: type: s3 bucket: BUCKET-NAME region: ap-northeast-1 use_kms: false\nchallenge_responders:\n route53: {}  account_key_passphrase: certificate_key_passphrase:\n前回作成したアカウントの `account.pem` が手元にあるのでこれを S3 にコピーしておきます。（新たに作成 (register) しても問題ありません） $ aws s3 cp account.pem s3://BUCKET-NAME/account.pem $ aws s3 ls s3://BUCKET-NAME/ 2016-02-07 17:28:05 1679 account.pem\n後は同じですね。 $ bundle exec acmesmith authorize www2.teraoka.me $ bundle exec acmesmith request www2.teraoka.me $ aws s3 ls s3://BUCKET-NAME/ PRE certs/ 2016-02-07 17:28:05 1679 account.pem $ aws s3 ls s3://BUCKET-NAME/certs/ PRE www2.teraoka.me/ $ aws s3 ls s3://BUCKET-NAME/certs/www2.teraoka.me/ PRE 20160207-032600_***********************************/ 2016-02-07 13:25:50 51 current $ aws s3 ls s3://BUCKET-NAME/certs/www2.teraoka.me/20160207-032600_***********************************/ 2016-02-07 13:25:49 1797 cert.pem 2016-02-07 13:25:50 1675 chain.pem 2016-02-07 13:25:50 3472 fullchain.pem 2016-02-07 13:25:50 1679 key.pem\n","date":"2016年2月7日","permalink":"/2016/02/acmesmith-2/","section":"Posts","summary":"前回「Acmesmith で証明書発行を試す - その1」で で filesystem に保存する方法を試してみました。 今回は AWS S3 に保存するテストを行ってみます。KMS は","title":"Acmesmith で証明書発行を試す - その2"},{"content":"無料でSSL証明書の発行ができる Let\u0026rsquo;s Encrypt が Public Beta となり、これからどんどん利用されていくと思われますが、公式(?)のツール https://github.com/letsencrypt/letsencrypt はちょっと使いにくいところがありました。 DigitalOcean にも解説記事 How To Secure Nginx with Let\u0026rsquo;s Encrypt on CentOS 7 がありましたが、あのツールでは HTTP でのドメイン認証となり、証明書を取得しようとしているドメイン(FQDN)が外部 (Let\u0026rsquo;s Encrypt 側のサーバー) からアクセス可能状態でなければなりません。 外部に公開していない、できないサーバーであったり、ロードバランサーの背後にあったり、Web じゃなくてメールサーバーとか LDAP サーバーで使いたいのにという場合に不便でした。 そんななか sorah さんが Acmesmith という便利ツールを公開されていたので早速試してみることにしました。 ACME Protocol では HTTP でのドメイン確認の他に DNS の TXT レコードを使う方法も規定されています。Acmesmith はこの DNS での処理を AWS Route53 を使うことによってレコードの追加削除まで自動で行ってくれるツールとなっています。さらに、鍵と証明書を S3 に保存することも、それを KMS によってセキュアに管理することにも対応しています。 今回はまずローカルファイルに証明書を書き出す方法でやってみます。 使い方は README に書いてありますね。 Ruby gems で公開されているのでまずは Gemfile を書いて bundle install します。``` $ cat Gemfile source \u0026lsquo;https://rubygems.org\u0026rsquo; gem \u0026lsquo;acmesmith\u0026rsquo; $ bundle install \u0026ndash;path vendor/bundle\n````bundle exec acmesmith help` を実行してみる``` $ bundle exec acmesmith help Commands: acmesmith authorize DOMAIN # Get authz for DOMAIN. acmesmith current COMMON_NAME # show current version for certificate acmesmith help [COMMAND] # Describe available commands or on\u0026hellip; acmesmith list [COMMON_NAME] # list certificates or its versions acmesmith register CONTACT # Create account key (contact e.g. \u0026hellip; acmesmith request COMMON_NAME [SAN] # request certificate for CN +COMMO\u0026hellip; acmesmith show-certificate COMMON_NAME # show certificate acmesmith show-private-key COMMON_NAME # show private key\nOptions: -c, [\u0026ndash;config=CONFIG] # Default: ./acmesmith.yml -E, [\u0026ndash;passphrase-from-env], [\u0026ndash;no-passphrase-from-env] # Read $ACMESMITH_ACCOUNT_KEY_PASSPHRASE and $ACMESMITH_CERT_KEY_PASSPHRASE for passphrases\n(revoke がまだ実装されていないのかな) コンフィグファイルが必要なので `acmesmith.yml` を作成します。 endpoint: https://acme-v01.api.letsencrypt.org/\nstorage: type: filesystem path: /home/ytera/acmesmish/certs\nchallenge_responders:\n route53: {}  account_key_passphrase: certificate_key_passphrase:\n````aws_access_keyの中にaccess_key_id, secret_access_keyを書くこともできますが aws-sdk を使っているので~/.aws/credentialsがあればそれを使ってくれます。 Route53 の操作のために IAM に必要な policy を設定しておく必要があります。これも README に全部書かれています。[https://github.com/sorah/acmesmith#all-access-s3--route53-setup](https://github.com/sorah/acmesmith#all-access-s3--route53-setup)acmesmith.ymlでstorageのpathに指定したディレクトリは予め作成しておく必要があります。 まずはregisterサブコマンドでアカウントを作成します。path配下にaccount.pemが作成されます。アカウントは公開鍵認証のようです。account_key_passphrase` が空であれば暗号化されずに保存されます。 次にドメインの認証です、ドメインは証明書のコモンネームに指定するものです。``` $ bundle exec acmesmith authorize www.teraoka.me =\u0026gt; Responding challenge dns-01 for www.teraoka.me in Acmesmith::ChallengeResponders::Route53 * UPSERT: TXT \u0026ldquo;_acme-challenge.www.teraoka.me\u0026rdquo;, \u0026ldquo;\\\u0026ldquo;ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ\\\u0026rdquo;\u0026rdquo; on /hostedzone/XXXXXXXXXXXXXX * requested change: /change/************** =\u0026gt; Waiting for change * change \u0026ldquo;/change/**************\u0026rdquo; is still \u0026ldquo;PENDING\u0026rdquo; \u0026hellip; * change \u0026ldquo;/change/**************\u0026rdquo; is still \u0026ldquo;PENDING\u0026rdquo; \u0026hellip; * change \u0026ldquo;/change/**************\u0026rdquo; is still \u0026ldquo;PENDING\u0026rdquo; \u0026hellip; * change \u0026ldquo;/change/**************\u0026rdquo; is still \u0026ldquo;PENDING\u0026rdquo; \u0026hellip; * change \u0026ldquo;/change/**************\u0026rdquo; is still \u0026ldquo;PENDING\u0026rdquo; \u0026hellip; * change \u0026ldquo;/change/**************\u0026rdquo; is still \u0026ldquo;PENDING\u0026rdquo; \u0026hellip; * synced! =\u0026gt; Requesting verification\u0026hellip; * verify_status: valid =\u0026gt; Cleaning up challenge dns-01 for www.teraoka.me in Acmesmith::ChallengeResponders::Route53 * DELETE: TXT \u0026ldquo;_acme-challenge.www.teraoka.me\u0026rdquo;, \u0026ldquo;\\\u0026ldquo;ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ\\\u0026rdquo;\u0026rdquo; on /hostedzone/XXXXXXXXXXXXXX * requested: /change/************** =\u0026gt; Done\nドメインの前に `_acme-challenge.` をつけた `TXT` レコードを作成し、validation のリクエストを出して、成功したら不要となった `TXT` を早速削除しています。ゴミが残らなくて良いですね。 このあと $ bundle exec acmesmith request www.teraoka.me\nと実行すれば完了です。証明書が `certs/certs/www.teraoka.me/` の配下に保存されています。 Let's Encrypt ではワイルドカード証明書の発行はできませんが SAN で複数ドメインの証明書は発行可能です。 $ bundle exec acmesmith request www.teraoka.me www.1q77.com\nと実行したら X509v3 Subject Alternative Name: DNS:www.1q77.com, DNS:www.teraoka.me\n","date":"2016年2月3日","permalink":"/2016/02/acmesmith-1/","section":"Posts","summary":"無料でSSL証明書の発行ができる Let\u0026rsquo;s Encrypt が Public Beta となり、これからどんどん利用されていくと思われますが、公式(?)のツール https://github.com/letsencrypt/letsencrypt はちょっと使いにくいとこ","title":"Acmesmith で証明書発行を試す - その1"},{"content":"","date":"2016年2月2日","permalink":"/tags/docker-gen/","section":"Tags","summary":"","title":"docker-gen"},{"content":"","date":"2016年2月2日","permalink":"/tags/forego/","section":"Tags","summary":"","title":"forego"},{"content":"先日ようやく Docker を触り始めたわけですが thumbor を使う場合、前段に nginx を置いてキャッシュさせるべきだから nginx の docker も必要だなっと思って調べてたら jwilder/nginx-proxy というものを見つけました。 大変便利そうな docker image だったのでこれについて書いてみます。 nginx-proxy は nginx の official image をベースに docker-gen というツールを使って template から nginx の設定ファイルを生成するように出来ています。 nginx.tmpl consul-template の docker API 版ですね。consul の変更ではなく docker の変更を監視してファイルを更新します。 前回作った thumbor を2つ動かしてその手前に nginx-proxy を置くとどうなるかというと EXPOSE されていれば -p で port forwarding する必要はありません。 thumbor の image は 8000 番を EXPOSE してあります。 VIRTUAL_HOST 環境変数を指定して起動します。\n$ docker run -d --name thumbor1 -e VIRTUAL_HOST=thumbor.example.com thumbor-centos 4930455aadf4a63055a29016dfde521c2dfa7c7fd34ee75498a4c3850adff56c $ docker run -d --name thumbor2 -e VIRTUAL_HOST=thumbor.example.com thumbor-centos 4318322fa2a6f66374474bc7b7a0ce1964a6aa828eae7852027b1f12faadd416 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4318322fa2a6 thumbor-centos \u0026quot;/bin/bash /bin/thumb\u0026quot; 4 seconds ago Up 3 seconds 8000/tcp thumbor2 4930455aadf4 thumbor-centos \u0026quot;/bin/bash /bin/thumb\u0026quot; 12 seconds ago Up 11 seconds 8000/tcp thumbor1 nginx-proxy は先に起動していても、後から起動しても良いですがホスト側の docker.sock にアクセスできるように volume 指定する必要があります。この socket を通じて Docker API で監視しています。\n$ docker run -d -p 80:80 --name nginx -v /var/run/docker.sock:/tmp/docker.sock:ro jwilder/nginx-proxy f259f6c444aadb5c9573f3ee598e6a101ac2598d2a018bb101b33fd7d6477d03 nginx-proxy を起動したところで /etc/nginx/conf.d/default.conf を確認してみます。\n$ docker exec nginx cat /etc/nginx/conf.d/default.conf # If we receive X-Forwarded-Proto, pass it through; otherwise, pass along the # scheme used to connect to this server map $http_x_forwarded_proto $proxy_x_forwarded_proto { default $http_x_forwarded_proto; \u0026#39;\u0026#39; $scheme; } # If we receive Upgrade, set Connection to \u0026#34;upgrade\u0026#34;; otherwise, delete any # Connection header that may have been passed to this server map $http_upgrade $proxy_connection { default upgrade; \u0026#39;\u0026#39; close; } gzip_types text/plain text/css application/javascript application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; log_format vhost \u0026#39;$host $remote_addr - $remote_user [$time_local] \u0026#39; \u0026#39;\u0026#34;$request\u0026#34; $status $body_bytes_sent \u0026#39; \u0026#39;\u0026#34;$http_referer\u0026#34; \u0026#34;$http_user_agent\u0026#34;\u0026#39;; access_log off; # HTTP 1.1 support proxy_http_version 1.1; proxy_buffering off; proxy_set_header Host $http_host; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $proxy_connection; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $proxy_x_forwarded_proto; server { server_name _; # This is just an invalid value which will never trigger on a real hostname. \tlisten 80; access_log /var/log/nginx/access.log vhost; return 503; } upstream thumbor.example.com { # thumbor2 \tserver 172.17.0.3:8000; # thumbor1 \tserver 172.17.0.2:8000; } server { server_name thumbor.example.com; listen 80 ; access_log /var/log/nginx/access.log vhost; location / { proxy_pass http://thumbor.example.com; } } thumbor.example.com にアクセスすると thumbor1, thumbor2 に振り分けられる設定になっています。 VIRTUAL_HOST という環境変数を設定するだけで。 thumbor1 を停止してみます。\n$ docker stop thumbor1 thumbor1 $ docker exec nginx cat /etc/nginx/conf.d/default.conf | grep -C 5 ^upstream server_name _; # This is just an invalid value which will never trigger on a real hostname. listen 80; access_log /var/log/nginx/access.log vhost; return 503; } upstream thumbor.example.com { # thumbor2 server 172.17.0.3:8000; } server { server_name thumbor.example.com; upstream thumbor.example.com のメンバーが減りました。また起動させれば復活します。 別の VIRTUAL_HOST を指定すれば別の virtual host 設定が追加されます。\n$ docker run -d --name thumbor3 -e VIRTUAL_HOST=foo.example.com thumbor-centos 6f933fdd9948a0793788c4d15088ba5c298b5d6606ee7c57a982e8de9b044b4e これで\nupstream foo.example.com { # thumbor3 \tserver 172.17.0.2:8000; } server { server_name foo.example.com; listen 80 ; access_log /var/log/nginx/access.log vhost; location / { proxy_pass http://foo.example.com; } } が default.conf に追加されました。 docker logs nginx でログを確認してみます\n$ docker logs nginx forego | starting nginx.1 on port 5000 forego | starting dockergen.1 on port 5100 dockergen.1 | 2016/02/01 15:02:40 Generated '/etc/nginx/conf.d/default.conf' from 3 containers dockergen.1 | 2016/02/01 15:02:40 Running 'nginx -s reload' dockergen.1 | 2016/02/01 15:02:40 Watching docker events dockergen.1 | 2016/02/01 15:16:46 Received event die for container 4930455aadf4 dockergen.1 | 2016/02/01 15:16:46 Generated '/etc/nginx/conf.d/default.conf' from 2 containers dockergen.1 | 2016/02/01 15:16:46 Running 'nginx -s reload' dockergen.1 | 2016/02/01 15:16:46 Received event stop for container 4930455aadf4 dockergen.1 | 2016/02/01 15:16:46 Contents of /etc/nginx/conf.d/default.conf did not change. Skipping notification 'nginx -s reload' dockergen.1 | 2016/02/01 15:19:35 Received event start for container 6f933fdd9948 dockergen.1 | 2016/02/01 15:19:35 Generated '/etc/nginx/conf.d/default.conf' from 3 containers dockergen.1 | 2016/02/01 15:19:35 Running 'nginx -s reload' forego ってのが登場しました。 Foreman in Go だそうです。 nginx-proxy には Procfile があって forego がこれを参照しているようです。 forego が nginx と docker-gen を起動させるようになっています。上の docker logs を見てもわかるように、どのプロセスが出力したメッセージかわかるように装飾してくれます。色までついています。docker のログを fluentd で処理したりするには不便ですが、目で見るには人に優しい感じです。 docker を動かすホストでログを fluentd で送る設定を動的に更新できる https://github.com/jwilder/docker-gen/blob/master/templates/fluentd.conf.tmpl も便利そうです。\n","date":"2016年2月2日","permalink":"/2016/02/nginx-proxy-and-docker-gen/","section":"Posts","summary":"先日ようやく Docker を触り始めたわけですが thumbor を使う場合、前段に nginx を置いてキャッシュさせるべきだから nginx の docker も必要だなっと思って調べてたら jwilder/nginx-proxy というもの","title":"nginx-proxy, docker-gen という便利ツール"},{"content":"","date":"2016年2月2日","permalink":"/tags/thumbor/","section":"Tags","summary":"","title":"thumbor"},{"content":"この手の問題は苦手なのでひととおりやってみよう。でも買ってしまうと期限がなくて放置してしまうから図書館で借りてみた。 最近本は電子版ばかり買ってるけど読み終わったものを売れなくてもったいなくなってきた、でも本を持ち歩きたくない今日このごろ。\n","date":"2016年2月1日","permalink":"/2016/02/math-puzzle/","section":"Posts","summary":"この手の問題は苦手なのでひととおりやってみよう。でも買ってしまうと期限がなくて放置してしまうから図書館で借りてみた。 最近本は電子版ばかり買っ","title":"プログラマ脳を鍛える数学パズル"},{"content":"Docker については Web や書籍で情報は追いつつも手を動かせていなかったがそろそろやらねばということで、まずは以前紹介した画像のサムネイル化やクロップ、フィルタ処理を行う thumbor を docker 化してみた。\nDockerfile などは github に置いてある。\n普段は CentOS を使うことが多いが公式 Docker image などでは ubuntu や debian が使われていることが多いようなので ubuntu での image も作ってみた。プライベートの Note PC はずっと Linux mint か Ubuntu だったけど未だに package 名とか apt に慣れない\u0026hellip; thumbor の .travis.yml を参考にして動くようになった。\nThumbor は画像処理エンジンを PIL (Python Image Library), GraphicsMagick (pgmagick), OpenCV の3つから選択できるようになっています。今回の docker image には全部含まれるようにしたので docker run の時の環境変数 (-e) で切り替えられます。（相変わらず pgmagick の build は重い\u0026hellip;）\nMakefile に書いておいたので make run-pil, make run-pgmagick, make run-opencv でそれぞれのエンジンで起動できます。3つ別ポートで起動させれば簡単にそれぞれの比較ができます。\n環境変数は ENGINE の切り替えにとどまらず、設定の動的書き換え全般が行えます。\nDockerfile の CMD で thumbor.sh を指定してあり、THUMBOR_ で始まる環境変数は THUMBOR_ を取り除いて設定ファイルに書き出して thumbor を起動するようになっています。設定可能な値は docker run -i --rm=true thumbor-centos /opt/thumbor/bin/thumbor-config で確認できます。（ENTRYPOINT を指定すると run 時に変更できないが CMD なら変更可能という学び） 同様に thumbor-url コマンドが実行できるので SECURITY_KEY でのハッシュ付き URL を作ることもできます。\nbase64.urlsafe_b64encode(hmac.new(security_key, unicode(url).encode(\u0026#39;utf-8\u0026#39;), hashlib.sha1).digest()) でもできますが。その他 crop とか filter 月の URL 生成のテストができます。すでに docker で動かしていれば docker run の代わりに docker exec でそのインスタンスを使うこともできます。 ENGINE の比較が簡単にできるので feature or facial detection の効果を見てみたいと思います。 こちらの画像を\nDETECTORS = [ \u0026#39;thumbor.detectors.face_detector\u0026#39;, \u0026#39;thumbor.detectors.feature_detector\u0026#39; ] とした OpenCV エンジンとそうでないものを比較すると次のようになりました。detection が有効であればサイズ調整時に人物が真ん中に来るようになります。その代わり処理が重い。それを回避するためにまずは detection なしの画像を返しておいて redis などに queue を登録し後のリクエストのために非同期で画像を準備しておく機能もあります。\n元画像  detection あり detection なし Thumbor は元画像は処理後の画像をキャッシュする機能がありますが、手前に nginx を置いてキャッシュさせたほうが効率的ですね。そこで nginx の docker を調査していたら docker-gen とそれを使った jwilder/nginx-proxy という便利ツールがありました。開発環境で便利に使えそうです。\nこれは別途記事にしたいと思います。\n","date":"2016年1月31日","permalink":"/2016/01/dockerize-thumbor/","section":"Posts","summary":"Docker については Web や書籍で情報は追いつつも手を動かせていなかったがそろそろやらねばということで、まずは以前紹介した画像のサムネイル化やクロップ、","title":"thumbor の docker 化"},{"content":"","date":"2016年1月12日","permalink":"/tags/dell/","section":"Tags","summary":"","title":"dell"},{"content":"Dell の PowerEdge サーバーを Linux で使う場合のお話です。\nDell の Linux 関連情報は http://linux.dell.com/ にあります。\nこれまで OpenManage Server Administrator については http://linux.dell.com/repo/hardware/latest/ の YUM repository を使ってインストールしていました。\nが、昨年 Dell System Update (DSU) http://linux.dell.com/repo/hardware/dsu/ が出現していました。 DSU を使うと Server Administrator だけでなく Firmware も Linux 上から簡単に更新できます。 わざわざ Download サイトにいって、必要なものを探してダウンロードしなくても良いのです、これは大変便利。 UEFI BIOS からも更新できましたが、OS の shutdown が必要ですし、何やってるのかわからない状態でずーーーっと待たされる遅さが困りものでした。 それでは使い方を見てみましょう リポジトリの登録 （CentOS 7 の最小インストールでは wget は入っていないので curl を使いたいところですが、スクリプト内でも wget が使われているので観念してインストールしましょう、さらに perl も使われているので yum -y install wget perl しましょう。）\n[root@server ~]# wget -q -O - http://linux.dell.com/repo/hardware/dsu/bootstrap.cgi | bash Downloading GPG key: http://linux.dell.com/repo/hardware/dsu/public.key Importing key into RPM. Write repository configuration Done! [root@server ~]# yum clean all Loaded plugins: fastestmirror Cleaning repos: base dell-system-update_dependent dell-system-update_independent : extras updates Cleaning up everything Cleaning up list of fastest mirrors DSU のインストール\n[root@server ~]# yum install dell-system-update Loaded plugins: fastestmirror base | 3.6 kB 00:00 dell-system-update_dependent | 2.3 kB 00:00 dell-system-update_independent | 2.3 kB 00:00 extras | 3.4 kB 00:00 updates | 3.4 kB 00:00 (1/6): base/7/x86_64/group_gz | 155 kB 00:00 (2/6): updates/7/x86_64/primary_db | 953 kB 00:00 (3/6): extras/7/x86_64/primary_db | 90 kB 00:00 (4/6): base/7/x86_64/primary_db | 5.3 MB 00:00 (5/6): dell-system-update_dependent/7/x86_64/primary_db | 32 kB 00:00 (6/6): dell-system-update_independent/primary_db | 111 kB 00:00 Determining fastest mirrors * base: ftp.tsukuba.wide.ad.jp * extras: ftp.tsukuba.wide.ad.jp * updates: ftp.tsukuba.wide.ad.jp Resolving Dependencies --\u0026gt; Running transaction check ---\u0026gt; Package dell-system-update.x86_64 0:1.1-15.12.00 will be installed --\u0026gt; Finished Dependency Resolution Dependencies Resolved ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: dell-system-update x86_64 1.1-15.12.00 dell-system-update_independent 2.0 M Transaction Summary ================================================================================ Install 1 Package Total download size: 2.0 M Installed size: 8.3 M Is this ok [y/d/N]: y Downloading packages: dell-system-update-1.1-15.12.00.x86_64.rpm | 2.0 MB 00:02 Running transaction check Running transaction test Transaction test succeeded Running transaction Installing : dell-system-update-1.1-15.12.00.x86_64 1/1 Verifying : dell-system-update-1.1-15.12.00.x86_64 1/1 Installed: dell-system-update.x86_64 0:1.1-15.12.00 Complete! Server Administrator のインストール\n[root@server ~]# yum install srvadmin-storageservices-cli srvadmin-storageservices-snmp Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile * base: ftp.tsukuba.wide.ad.jp * extras: ftp.tsukuba.wide.ad.jp * updates: ftp.tsukuba.wide.ad.jp Resolving Dependencies --\u0026gt; Running transaction check ---\u0026gt; Package srvadmin-storageservices-cli.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Processing Dependency: srvadmin-sysfsutils = 8.2.0 for package: srvadmin-storageservices-cli-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-storelib = 8.2.0 for package: srvadmin-storageservices-cli-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-storage-cli = 8.2.0 for package: srvadmin-storageservices-cli-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-storage = 8.2.0 for package: srvadmin-storageservices-cli-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-smcommon = 8.2.0 for package: srvadmin-storageservices-cli-8.2.0-1739.8348.el7.x86_64 ---\u0026gt; Package srvadmin-storageservices-snmp.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Processing Dependency: srvadmin-storage-snmp = 8.2.0 for package: srvadmin-storageservices-snmp-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-isvc-snmp = 8.2.0 for package: srvadmin-storageservices-snmp-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-idrac-snmp = 8.2.0 for package: srvadmin-storageservices-snmp-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-deng-snmp = 8.2.0 for package: srvadmin-storageservices-snmp-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Running transaction check ---\u0026gt; Package srvadmin-deng-snmp.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Processing Dependency: srvadmin-deng = 8.2.0-1739.8348.el7 for package: srvadmin-deng-snmp-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: libdcsupt.so.8()(64bit) for package: srvadmin-deng-snmp-8.2.0-1739.8348.el7.x86_64 ---\u0026gt; Package srvadmin-idrac-snmp.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Processing Dependency: srvadmin-omilcore for package: srvadmin-idrac-snmp-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: libdcsdrs.so.8()(64bit) for package: srvadmin-idrac-snmp-8.2.0-1739.8348.el7.x86_64 ---\u0026gt; Package srvadmin-isvc-snmp.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Processing Dependency: srvadmin-isvc = 8.2.0-1739.8348.el7 for package: srvadmin-isvc-snmp-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-hapi for package: srvadmin-isvc-snmp-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: libdcship.so.8()(64bit) for package: srvadmin-isvc-snmp-8.2.0-1739.8348.el7.x86_64 ---\u0026gt; Package srvadmin-smcommon.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-storage.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Processing Dependency: srvadmin-realssd for package: srvadmin-storage-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-nvme for package: srvadmin-storage-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: libxmlsup.so.2()(64bit) for package: srvadmin-storage-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: libsmbios.so.2()(64bit) for package: srvadmin-storage-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: libomacs.so.1()(64bit) for package: srvadmin-storage-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: libRealSSD-API.so()(64bit) for package: srvadmin-storage-8.2.0-1739.8348.el7.x86_64 ---\u0026gt; Package srvadmin-storage-cli.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Processing Dependency: libclpsup.so.4()(64bit) for package: srvadmin-storage-cli-8.2.0-1739.8348.el7.x86_64 ---\u0026gt; Package srvadmin-storage-snmp.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-storelib.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Processing Dependency: srvadmin-storelib-sysfs-x86_64 for package: srvadmin-storelib-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-storelib-sysfs for package: srvadmin-storelib-8.2.0-1739.8348.el7.x86_64 ---\u0026gt; Package srvadmin-sysfsutils.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Running transaction check ---\u0026gt; Package libsmbios.x86_64 0:2.2.27-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-deng.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-hapi.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-isvc.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-nvme.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-omacore.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Processing Dependency: srvadmin-ominst for package: srvadmin-omacore-8.2.0-1739.8348.el7.x86_64 --\u0026gt; Processing Dependency: srvadmin-omcommon for package: srvadmin-omacore-8.2.0-1739.8348.el7.x86_64 ---\u0026gt; Package srvadmin-omacs.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-omilcore.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Processing Dependency: smbios-utils-bin for package: srvadmin-omilcore-8.2.0-1739.8348.el7.x86_64 ---\u0026gt; Package srvadmin-rac-components.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-realssd.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-storelib-sysfs.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-xmlsup.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Running transaction check ---\u0026gt; Package smbios-utils-bin.x86_64 0:2.2.27-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-omcommon.x86_64 0:8.2.0-1739.8348.el7 will be installed ---\u0026gt; Package srvadmin-ominst.x86_64 0:8.2.0-1739.8348.el7 will be installed --\u0026gt; Finished Dependency Resolution Dependencies Resolved ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: srvadmin-storageservices-cli x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 2.7 k srvadmin-storageservices-snmp x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 2.8 k Installing for dependencies: libsmbios x86_64 2.2.27-1739.8348.el7 dell-system-update_dependent 1.6 M smbios-utils-bin x86_64 2.2.27-1739.8348.el7 dell-system-update_dependent 93 k srvadmin-deng x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 728 k srvadmin-deng-snmp x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 42 k srvadmin-hapi x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 941 k srvadmin-idrac-snmp x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 61 k srvadmin-isvc x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 3.6 M srvadmin-isvc-snmp x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 340 k srvadmin-nvme x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 12 k srvadmin-omacore x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 561 k srvadmin-omacs x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 2.6 M srvadmin-omcommon x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 1.6 M srvadmin-omilcore x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 30 k srvadmin-ominst x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 1.2 M srvadmin-rac-components x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 35 k srvadmin-realssd x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 94 k srvadmin-smcommon x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 668 k srvadmin-storage x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 3.1 M srvadmin-storage-cli x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 222 k srvadmin-storage-snmp x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 175 k srvadmin-storelib x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 320 k srvadmin-storelib-sysfs x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 44 k srvadmin-sysfsutils x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 54 k srvadmin-xmlsup x86_64 8.2.0-1739.8348.el7 dell-system-update_dependent 51 k Transaction Summary ================================================================================ Install 2 Packages (+24 Dependent packages) Total download size: 18 M Installed size: 95 M Is this ok [y/d/N]: y Downloading packages: (1/26): smbios-utils-bin-2.2.27-1739.8348.el7.x86_64.rpm | 93 kB 00:00 (2/26): libsmbios-2.2.27-1739.8348.el7.x86_64.rpm | 1.6 MB 00:02 (3/26): srvadmin-deng-8.2.0-1739.8348.el7.x86_64.rpm | 728 kB 00:02 (4/26): srvadmin-deng-snmp-8.2.0-1739.8348.el7.x86_64.rpm | 42 kB 00:00 (5/26): srvadmin-idrac-snmp-8.2.0-1739.8348.el7.x86_64.rpm | 61 kB 00:00 (6/26): srvadmin-hapi-8.2.0-1739.8348.el7.x86_64.rpm | 941 kB 00:02 (7/26): srvadmin-isvc-snmp-8.2.0-1739.8348.el7.x86_64.rpm | 340 kB 00:01 (8/26): srvadmin-isvc-8.2.0-1739.8348.el7.x86_64.rpm | 3.6 MB 00:03 (9/26): srvadmin-nvme-8.2.0-1739.8348.el7.x86_64.rpm | 12 kB 00:00 (10/26): srvadmin-omacore-8.2.0-1739.8348.el7.x86_64.rpm | 561 kB 00:01 (11/26): srvadmin-omacs-8.2.0-1739.8348.el7.x86_64.rpm | 2.6 MB 00:03 (12/26): srvadmin-omcommon-8.2.0-1739.8348.el7.x86_64.rpm | 1.6 MB 00:02 (13/26): srvadmin-omilcore-8.2.0-1739.8348.el7.x86_64.rpm | 30 kB 00:00 (14/26): srvadmin-rac-components-8.2.0-1739.8348.el7.x86_6 | 35 kB 00:00 (15/26): srvadmin-realssd-8.2.0-1739.8348.el7.x86_64.rpm | 94 kB 00:00 (16/26): srvadmin-ominst-8.2.0-1739.8348.el7.x86_64.rpm | 1.2 MB 00:01 (17/26): srvadmin-smcommon-8.2.0-1739.8348.el7.x86_64.rpm | 668 kB 00:01 (18/26): srvadmin-storage-cli-8.2.0-1739.8348.el7.x86_64.r | 222 kB 00:01 (19/26): srvadmin-storage-8.2.0-1739.8348.el7.x86_64.rpm | 3.1 MB 00:02 (20/26): srvadmin-storageservices-cli-8.2.0-1739.8348.el7. | 2.7 kB 00:00 (21/26): srvadmin-storageservices-snmp-8.2.0-1739.8348.el7 | 2.8 kB 00:00 (22/26): srvadmin-storage-snmp-8.2.0-1739.8348.el7.x86_64. | 175 kB 00:01 (23/26): srvadmin-storelib-sysfs-8.2.0-1739.8348.el7.x86_6 | 44 kB 00:00 (24/26): srvadmin-storelib-8.2.0-1739.8348.el7.x86_64.rpm | 320 kB 00:01 (25/26): srvadmin-xmlsup-8.2.0-1739.8348.el7.x86_64.rpm | 51 kB 00:00 (26/26): srvadmin-sysfsutils-8.2.0-1739.8348.el7.x86_64.rp | 54 kB 00:00 -------------------------------------------------------------------------------- Total 964 kB/s | 18 MB 00:19 Running transaction check Running transaction test Transaction test succeeded Running transaction Installing : srvadmin-xmlsup-8.2.0-1739.8348.el7.x86_64 1/26 Installing : srvadmin-smcommon-8.2.0-1739.8348.el7.x86_64 2/26 Installing : libsmbios-2.2.27-1739.8348.el7.x86_64 3/26 Installing : srvadmin-sysfsutils-8.2.0-1739.8348.el7.x86_64 4/26 Installing : srvadmin-hapi-8.2.0-1739.8348.el7.x86_64 5/26 Installing : smbios-utils-bin-2.2.27-1739.8348.el7.x86_64 6/26 Installing : srvadmin-omilcore-8.2.0-1739.8348.el7.x86_64 7/26 ********************************************************** After the install process completes, you may need to log out and then log in again to reset the PATH variable to access the Server Administrator CLI utilities ********************************************************** Installing : srvadmin-deng-8.2.0-1739.8348.el7.x86_64 8/26 Installing : srvadmin-omacs-8.2.0-1739.8348.el7.x86_64 9/26 Installing : srvadmin-isvc-8.2.0-1739.8348.el7.x86_64 10/26 Installing : srvadmin-deng-snmp-8.2.0-1739.8348.el7.x86_64 11/26 Installing : srvadmin-isvc-snmp-8.2.0-1739.8348.el7.x86_64 12/26 Installing : srvadmin-ominst-8.2.0-1739.8348.el7.x86_64 13/26 Installing : srvadmin-rac-components-8.2.0-1739.8348.el7.x86_64 14/26 Installing : srvadmin-idrac-snmp-8.2.0-1739.8348.el7.x86_64 15/26 Installing : srvadmin-omcommon-8.2.0-1739.8348.el7.x86_64 16/26 Installing : srvadmin-omacore-8.2.0-1739.8348.el7.x86_64 17/26 Installing : srvadmin-realssd-8.2.0-1739.8348.el7.x86_64 18/26 Installing : srvadmin-storelib-sysfs-8.2.0-1739.8348.el7.x86_64 19/26 Installing : srvadmin-storelib-8.2.0-1739.8348.el7.x86_64 20/26 Installing : srvadmin-nvme-8.2.0-1739.8348.el7.x86_64 21/26 Installing : srvadmin-storage-8.2.0-1739.8348.el7.x86_64 22/26 Installing : srvadmin-storage-cli-8.2.0-1739.8348.el7.x86_64 23/26 Installing : srvadmin-storage-snmp-8.2.0-1739.8348.el7.x86_64 24/26 Installing : srvadmin-storageservices-snmp-8.2.0-1739.8348.el7.x86_64 25/26 Installing : srvadmin-storageservices-cli-8.2.0-1739.8348.el7.x86_64 26/26 Verifying : srvadmin-deng-8.2.0-1739.8348.el7.x86_64 1/26 Verifying : srvadmin-hapi-8.2.0-1739.8348.el7.x86_64 2/26 Verifying : srvadmin-isvc-8.2.0-1739.8348.el7.x86_64 3/26 Verifying : srvadmin-nvme-8.2.0-1739.8348.el7.x86_64 4/26 Verifying : srvadmin-sysfsutils-8.2.0-1739.8348.el7.x86_64 5/26 Verifying : srvadmin-deng-snmp-8.2.0-1739.8348.el7.x86_64 6/26 Verifying : srvadmin-storage-cli-8.2.0-1739.8348.el7.x86_64 7/26 Verifying : srvadmin-smcommon-8.2.0-1739.8348.el7.x86_64 8/26 Verifying : srvadmin-storage-8.2.0-1739.8348.el7.x86_64 9/26 Verifying : srvadmin-rac-components-8.2.0-1739.8348.el7.x86_64 10/26 Verifying : srvadmin-omacs-8.2.0-1739.8348.el7.x86_64 11/26 Verifying : srvadmin-xmlsup-8.2.0-1739.8348.el7.x86_64 12/26 Verifying : srvadmin-omacore-8.2.0-1739.8348.el7.x86_64 13/26 Verifying : srvadmin-idrac-snmp-8.2.0-1739.8348.el7.x86_64 14/26 Verifying : srvadmin-storelib-8.2.0-1739.8348.el7.x86_64 15/26 Verifying : libsmbios-2.2.27-1739.8348.el7.x86_64 16/26 Verifying : srvadmin-storageservices-cli-8.2.0-1739.8348.el7.x86_64 17/26 Verifying : srvadmin-omcommon-8.2.0-1739.8348.el7.x86_64 18/26 Verifying : srvadmin-isvc-snmp-8.2.0-1739.8348.el7.x86_64 19/26 Verifying : srvadmin-storageservices-snmp-8.2.0-1739.8348.el7.x86_64 20/26 Verifying : srvadmin-omilcore-8.2.0-1739.8348.el7.x86_64 21/26 Verifying : smbios-utils-bin-2.2.27-1739.8348.el7.x86_64 22/26 Verifying : srvadmin-storelib-sysfs-8.2.0-1739.8348.el7.x86_64 23/26 Verifying : srvadmin-realssd-8.2.0-1739.8348.el7.x86_64 24/26 Verifying : srvadmin-ominst-8.2.0-1739.8348.el7.x86_64 25/26 Verifying : srvadmin-storage-snmp-8.2.0-1739.8348.el7.x86_64 26/26 Installed: srvadmin-storageservices-cli.x86_64 0:8.2.0-1739.8348.el7 srvadmin-storageservices-snmp.x86_64 0:8.2.0-1739.8348.el7 Dependency Installed: libsmbios.x86_64 0:2.2.27-1739.8348.el7 smbios-utils-bin.x86_64 0:2.2.27-1739.8348.el7 srvadmin-deng.x86_64 0:8.2.0-1739.8348.el7 srvadmin-deng-snmp.x86_64 0:8.2.0-1739.8348.el7 srvadmin-hapi.x86_64 0:8.2.0-1739.8348.el7 srvadmin-idrac-snmp.x86_64 0:8.2.0-1739.8348.el7 srvadmin-isvc.x86_64 0:8.2.0-1739.8348.el7 srvadmin-isvc-snmp.x86_64 0:8.2.0-1739.8348.el7 srvadmin-nvme.x86_64 0:8.2.0-1739.8348.el7 srvadmin-omacore.x86_64 0:8.2.0-1739.8348.el7 srvadmin-omacs.x86_64 0:8.2.0-1739.8348.el7 srvadmin-omcommon.x86_64 0:8.2.0-1739.8348.el7 srvadmin-omilcore.x86_64 0:8.2.0-1739.8348.el7 srvadmin-ominst.x86_64 0:8.2.0-1739.8348.el7 srvadmin-rac-components.x86_64 0:8.2.0-1739.8348.el7 srvadmin-realssd.x86_64 0:8.2.0-1739.8348.el7 srvadmin-smcommon.x86_64 0:8.2.0-1739.8348.el7 srvadmin-storage.x86_64 0:8.2.0-1739.8348.el7 srvadmin-storage-cli.x86_64 0:8.2.0-1739.8348.el7 srvadmin-storage-snmp.x86_64 0:8.2.0-1739.8348.el7 srvadmin-storelib.x86_64 0:8.2.0-1739.8348.el7 srvadmin-storelib-sysfs.x86_64 0:8.2.0-1739.8348.el7 srvadmin-sysfsutils.x86_64 0:8.2.0-1739.8348.el7 srvadmin-xmlsup.x86_64 0:8.2.0-1739.8348.el7 Complete! Server Administrator daemon の起動\n[root@server ~]# . /etc/profile.d/srvadmin-path.sh [root@server ~]# srvadmin-services.sh start Starting instsvcdrv (via systemctl): [ OK ] Starting dataeng (via systemctl): [ OK ] Starting dsm_om_shrsvc (via systemctl): [ OK ] それではいよいよ DSU による firmware 更新です --inventory をつけて実行すると現状のバージョンが確認できます\n[root@server ~]# dsu --inventory Getting System Inventory... 1. OpenManage Server Administrator ( Version : 8.2.0 ) 2. BIOS ( Version : 1.2.6 ) 3. Lifecycle Controller ( Version : 2.20.20.20 ) 4. Dell 32 Bit uEFI Diagnostics, version 4239, 4239A24, 4239.32 ( Version : 4239A24 ) 5. OS COLLECTOR 1.1, OSC_1.1, A00 ( Version : OSC_1.1 ) 6. Power Supply ( Version : 00.30.43 ) 7. PERC H730P Mini Controller 0 Firmware ( Version : 25.3.0.0016 ) 8. Firmware for - Disk 0 in Backplane 1 of PERC H730P Mini Controller 0 ( Version : TS04 ) 9. iDRAC ( Version : 2.20.20.20 ) 10. NetXtreme BCM5720 Gigabit Ethernet PCIe (em3) ( Version : 7.10.61 ) 11. NetXtreme BCM5720 Gigabit Ethernet PCIe (em4) ( Version : 7.10.61 ) 12. NetXtreme BCM5720 Gigabit Ethernet PCIe (em1) ( Version : 7.10.61 ) 13. NetXtreme BCM5720 Gigabit Ethernet PCIe (em2) ( Version : 7.10.61 ) 14. Intel(R) Ethernet 10G 2P X540-t Adapter ( Version : 16.5.20 ) 15. Intel(R) Ethernet 10G 2P X540-t Adapter ( Version : 16.5.20 ) 16. 13G SEP Firmware, BayID: 1 ( Version : 2.23 ) dsu コマンドを引数なしで実行するとどれを更新するか尋ねられます\n[root@server ~]# dsu Getting System Inventory... Determining Applicable Updates... |-----------Dell System Updates-----------| [ ] represents 'not selected' [*] represents 'selected' [-] represents 'Component already at repository version (cannot be selected)' Choose: q - Quit without update, c to Commit, - To Select/Deselect, a - Select All, n - Select None [-]1 OS COLLECTOR 1.1, OSC_1.1, A00 Current Version : OSC_1.1 same as : OSC_1.1 [-]2 13G SEP Firmware, BayID: 1 Current Version : 2.23 same as : 2.23 [ ]3 NetXtreme BCM5720 Gigabit Ethernet PCIe (em2) Current Version : 7.10.61 Upgrade to : 7.10.64 [ ]4 NetXtreme BCM5720 Gigabit Ethernet PCIe (em1) Current Version : 7.10.61 Upgrade to : 7.10.64 [ ]5 NetXtreme BCM5720 Gigabit Ethernet PCIe (em4) Current Version : 7.10.61 Upgrade to : 7.10.64 [ ]6 NetXtreme BCM5720 Gigabit Ethernet PCIe (em3) Current Version : 7.10.61 Upgrade to : 7.10.64 [-]7 Dell 32 Bit uEFI Diagnostics, version 4239, 4239A24, 4239.32 Current Version : 4239A24 same as : 4239A24 [-]8 PERC H730P Mini Controller 0 Firmware Current Version : 25.3.0.0016 same as : 25.3.0.0016 [-]9 Firmware for - Disk 0 in Backplane 1 of PERC H730P Mini Controller 0 Current Version : TS04 same as : TS04 [ ]10 iDRAC Current Version : 2.20.20.20 Upgrade to : 2.21.21.21 [-]11 OpenManage Server Administrator Current Version : 8.2.0 same as : 8.2 [ ]12 Intel(R) Ethernet 10G 2P X540-t Adapter Current Version : 16.5.20 Upgrade to : 17.0.12 [ ]13 Intel(R) Ethernet 10G 2P X540-t Adapter Current Version : 16.5.20 Upgrade to : 17.0.12 [ ]14 BIOS Current Version : 1.2.6 Upgrade to : 1.5.4 [ ]15 Power Supply Current Version : 00.30.43 Upgrade to : 00.30.44 Enter your choice : a a で更新可能なもの全てを選択します。\n|-----------Dell System Updates-----------| [ ] represents 'not selected' [*] represents 'selected' [-] represents 'Component already at repository version (cannot be selected)' Choose: q - Quit without update, c to Commit, - To Select/Deselect, a - Select All, n - Select None [-]1 OS COLLECTOR 1.1, OSC_1.1, A00 Current Version : OSC_1.1 same as : OSC_1.1 [-]2 13G SEP Firmware, BayID: 1 Current Version : 2.23 same as : 2.23 [*]3 NetXtreme BCM5720 Gigabit Ethernet PCIe (em2) Current Version : 7.10.61 Upgrade to : 7.10.64 [*]4 NetXtreme BCM5720 Gigabit Ethernet PCIe (em1) Current Version : 7.10.61 Upgrade to : 7.10.64 [*]5 NetXtreme BCM5720 Gigabit Ethernet PCIe (em4) Current Version : 7.10.61 Upgrade to : 7.10.64 [*]6 NetXtreme BCM5720 Gigabit Ethernet PCIe (em3) Current Version : 7.10.61 Upgrade to : 7.10.64 [-]7 Dell 32 Bit uEFI Diagnostics, version 4239, 4239A24, 4239.32 Current Version : 4239A24 same as : 4239A24 [-]8 PERC H730P Mini Controller 0 Firmware Current Version : 25.3.0.0016 same as : 25.3.0.0016 [-]9 Firmware for - Disk 0 in Backplane 1 of PERC H730P Mini Controller 0 Current Version : TS04 same as : TS04 [*]10 iDRAC Current Version : 2.20.20.20 Upgrade to : 2.21.21.21 [-]11 OpenManage Server Administrator Current Version : 8.2.0 same as : 8.2 [*]12 Intel(R) Ethernet 10G 2P X540-t Adapter Current Version : 16.5.20 Upgrade to : 17.0.12 [*]13 Intel(R) Ethernet 10G 2P X540-t Adapter Current Version : 16.5.20 Upgrade to : 17.0.12 [*]14 BIOS Current Version : 1.2.6 Upgrade to : 1.5.4 [*]15 Power Supply Current Version : 00.30.43 Upgrade to : 00.30.44 Enter your choice : c 選択できたら c で commit します。 するとインストールが始まります。\nインストールの過程で netstat コマンドが使われるので CentOS 7 の場合は net-tools package をインストールしておきます。\nInstalling Network_Firmware_0MT4K_LN_7.10.64... Collecting inventory... .. Running validation... NetXtreme BCM5720 Gigabit Ethernet PCIe (em3) The version of this Update Package is newer than the currently installed version. Software application name: NetXtreme BCM5720 Gigabit Ethernet PCIe (em3) Package version: 7.10.64 Installed version: 7.10.61 NetXtreme BCM5720 Gigabit Ethernet PCIe (em4) The version of this Update Package is newer than the currently installed version. Software application name: NetXtreme BCM5720 Gigabit Ethernet PCIe (em4) Package version: 7.10.64 Installed version: 7.10.61 NetXtreme BCM5720 Gigabit Ethernet PCIe (em1) The version of this Update Package is newer than the currently installed version. Software application name: NetXtreme BCM5720 Gigabit Ethernet PCIe (em1) Package version: 7.10.64 Installed version: 7.10.61 NetXtreme BCM5720 Gigabit Ethernet PCIe (em2) The version of this Update Package is newer than the currently installed version. Software application name: NetXtreme BCM5720 Gigabit Ethernet PCIe (em2) Package version: 7.10.64 Installed version: 7.10.61 Executing update... WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER DELL PRODUCTS WHILE UPDATE IS IN PROGRESS. THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE! ....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... The system should be restarted for the update to take effect. Installing iDRAC-with-Lifecycle-Controller_Firmware_1X82C_LN_2.21.21.21_A00... Collecting inventory... . Running validation... iDRAC The version of this Update Package is newer than the currently installed version. Software application name: iDRAC Package version: 2.21.21.21 Installed version: 2.20.20.20 Executing update... WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER DELL PRODUCTS WHILE UPDATE IS IN PROGRESS. THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE! ................................................................................................................................................................................................................................................................................................................................................................................................................... The update completed successfully. Installing Network_Firmware_F8H29_LN_17.0.12_A00... Collecting inventory... ........ Running validation... Intel(R) Ethernet 10G 2P X540-t Adapter The version of this Update Package is newer than the currently installed version. Software application name: Intel(R) Ethernet 10G 2P X540-t Adapter Package version: 17.0.12 Installed version: 16.5.20 Intel(R) Ethernet 10G 2P X540-t Adapter The version of this Update Package is newer than the currently installed version. Software application name: Intel(R) Ethernet 10G 2P X540-t Adapter Package version: 17.0.12 Installed version: 16.5.20 Executing update... WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER DELL PRODUCTS WHILE UPDATE IS IN PROGRESS. THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE! ............................................................................................ The system should be restarted for the update to take effect. Installing Power_Firmware_Y181V_LN_00.30.44... Collecting inventory... .. Running validation... Power Supply The version of this Update Package is newer than the currently installed version. Software application name: Power Supply Package version: 00.30.44 Installed version: 00.30.43 Executing update... WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER DELL PRODUCTS WHILE UPDATE IS IN PROGRESS. THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE! .......................................................................... The system should be restarted for the update to take effect. Installing BIOS_1VCPR_LN_1.5.4... Collecting inventory... ......... Running validation... PowerEdge R530/R430/T430 BIOS The version of this Update Package is newer than the currently installed version. Software application name: BIOS Package version: 1.5.4 Installed version: 1.2.6 Executing update... WARNING: DO NOT STOP THIS PROCESS OR INSTALL OTHER DELL PRODUCTS WHILE UPDATE IS IN PROGRESS. THESE ACTIONS MAY CAUSE YOUR SYSTEM TO BECOME UNSTABLE! ...................................................................................... The system should be restarted for the update to take effect. Done! Please run 'dsu --inventory' to check the inventory Please reboot the system for update(s) to take effect 更新完了です。dsu --inventory で本当に更新されているか確認します。 後は reboot で完了です。 らっくちーーーん！！ HPE (HP) には https://downloads.linux.hpe.com/ というサイトがありますね。 Fujitsu さんなど日本のメーカーさんもやってくれないかなぁ (Java Applet とか嫌だよぅ)\n追記 (2017/6/8) # 追加で i686 の package が必要な場合があった\nFetching ESM_Firmware_J7YYK_LN32_2.85_A00 ... Installing ESM_Firmware_J7YYK_LN32_2.85_A00 The following packages are required for update package to run: compat-libstdc++-33.i686 libstdc++.i686 libxml2.i686 Please check Update package User guide for instructions for installing the dependencies ESM_Firmware_J7YYK_LN32_2.85_A00 could not be installed ","date":"2016年1月12日","permalink":"/2016/01/update-dell-poweredge-firmware-with-dsu/","section":"Posts","summary":"Dell の PowerEdge サーバーを Linux で使う場合のお話です。 Dell の Linux 関連情報は http://linux.dell.com/ にあります。 これまで OpenManage Server Administrator については http://linux.dell.com/repo/hardware/latest/ の YUM repository を使ってインストールしていました。 が、","title":"DELL PowerEdge の Firmware は DSU で簡単更新"},{"content":"","date":"2016年1月12日","permalink":"/tags/firmware/","section":"Tags","summary":"","title":"firmware"},{"content":"前からやろうやろうと思ってなかなか手をつけられずにいたのですが、やっと実装しました。 （エラーハンドリングとかまだだけど） Postfix に限らず SMTP サーバーのログは from と to などが別の行として出力されます。 そして、各行ごとに出力される項目が異なります。from の行には size と nrcpt など。to には relay 先、delay, delays, status など。接続元や message-id はまた別の行です。 そこで IOS　ビッグデータ技術ブログ: Postfixのログをfluentdを使ってTreasureDataに送る を見つけてとりあえず fluent-plugin-multi-format-parser を使って elasticsearch に送って配信エラー監視したり kibana で確認したりしてましたが、to の行で bounced になってるのを見て from は何かなと思ったら queue id から from を検索して確認するのがかなり面倒なのでやっぱりこれは1行にまとめなければと。 大雑把には to 以外のデータを redis なり memcached に登録しておいて to でそれを引っ張りだして返せば良いわけだけども、どうやって実装するのかなと。 最初は Filter プラグインで実装しようかと考えていましたが redis なり memcached なりを使うとするとそこでエラーになる可能性があるため、エラーの場合にリトライできるように BufferedOutput で実装を始めました。が、処理した結果をまた fluentd に戻す方法がわからなくて、困ってしまいました。どうしようかなって fluentd のリポジトリを眺めているうちに out_exec_filter というものがあることに気づいてこれだ！！ということで実装しました。 エラーハンドリングがないけど\n#!/opt/td-agent/embedded/bin/ruby require \u0026#39;redis\u0026#39; require \u0026#39;json\u0026#39; redis = Redis.new while line = STDIN.gets record = JSON.load(line) if record.has_key?(\u0026#39;from\u0026#39;) # time は to のもので上書きされてしまうので from の時の値を別名で保存しておく record[\u0026#39;received_at\u0026#39;] = Time.at(record[\u0026#39;time\u0026#39;].to_i).strftime(\u0026#39;%Y-%m-%dT%H:%M:%S%:z\u0026#39;) end if record.has_key?(\u0026#39;qid\u0026#39;) key = record[\u0026#39;host\u0026#39;] + \u0026#39;:\u0026#39; + record[\u0026#39;qid\u0026#39;] stored = redis.hgetall(key) || {} if record.has_key?(\u0026#39;to\u0026#39;) print JSON.generate(stored.merge(record)) + \u0026#34;\\n\u0026#34; elsif record[\u0026#39;message\u0026#39;] == \u0026#39;removed\u0026#39; redis.del(key) else redis.mapped_hmset(key, stored.merge(record)) # postfix の bounce_queue_lifetime の値に合わせて expire を設定する redis.expire(key, 86400) end end end Postfix のログの parse を fluent-plugin-multi-format-parser ではなく Parser プラグインを書きました。gem 化してないので td-agent では /etc/td-agent/plugin/ ディレクトリに置いて使います。\nmodule Fluent class TextParser class PostfixLogParser \u0026lt; Parser Plugin.register_parser(\u0026#39;postfix_log_parser\u0026#39;, self) config_param :time_format, :string, :default =\u0026gt; nil def configure(conf) super @time_parser = TimeParser.new(@time_format) @addr_keys = [\u0026#39;to\u0026#39;, \u0026#39;from\u0026#39;, \u0026#39;orig_to\u0026#39;] end def parse_message(message) log = {} if m = message.match(/^(?\u0026lt;qid\u0026gt;[A-F0-9]+):\\s+/) log[\u0026#39;qid\u0026#39;] = m[\u0026#39;qid\u0026#39;] message.gsub!(/^[A-F0-9]+:\\s+/, \u0026#39;\u0026#39;) else log[\u0026#39;message\u0026#39;] = message return log end if m = message.match(/, status=(?\u0026lt;status\u0026gt;\\S+)\\s+(?\u0026lt;message\u0026gt;.*)$/) log[\u0026#39;status\u0026#39;] = m[\u0026#39;status\u0026#39;] log[\u0026#39;message\u0026#39;] = m[\u0026#39;message\u0026#39;] message.gsub!(/, status=.*$/, \u0026#39;\u0026#39;) end if ! message.match(/^[a-z0-9\\-]+=/) log[\u0026#39;message\u0026#39;] = message return log end message.split(/, /).each do |kv| (key, value) = kv.split(\u0026#39;=\u0026#39;) if @addr_keys.include?(key) log[key] = value.gsub!(/^\\\u0026lt;(.*)\\\u0026gt;$/, \u0026#39;\\1\u0026#39;) else log[key] = value end end log end # to や from のドメインで集計したいこともあるのでドメイン抽出 def domain(addr) local, domain = addr.split(\u0026#39;@\u0026#39;, 2) parts = domain.split(\u0026#39;.\u0026#39;).reverse # 属性型JPドメインでは後ろから3つ分 (ISPとかのサブドメインを無視する) if parts[0] == \u0026#39;jp\u0026#39; and parts[1].length == 2 return [parts[2], parts[1], parts[0]].join(\u0026#39;.\u0026#39;) elsif parts.length \u0026gt;= 2 return [parts[1], parts[0]].join(\u0026#39;.\u0026#39;) else return domain end end def parse(text) m = text.match(/^(?\u0026lt;time\u0026gt;\\S+\\s+\\S+\\s+\\S+)\\s+(?\u0026lt;host\u0026gt;\\S+)\\s+(?\u0026lt;process\u0026gt;[^\\[]+)\\[(?\u0026lt;pid\u0026gt;\\d+)\\]:\\s+(?\u0026lt;message\u0026gt;.*)/) time = @time_parser.parse(m[\u0026#39;time\u0026#39;]) record = parse_message(m[\u0026#39;message\u0026#39;]) record[\u0026#39;host\u0026#39;] = m[\u0026#39;host\u0026#39;] record[\u0026#39;process\u0026#39;] = m[\u0026#39;process\u0026#39;] record[\u0026#39;pid\u0026#39;] = m[\u0026#39;pid\u0026#39;] # nrcpt は後ろに \u0026#34;(queue active)\u0026#34; とかついてるけど削って数値として扱えるようにしておく if record.has_key?(\u0026#39;nrcpt\u0026#39;) record[\u0026#39;nrcpt\u0026#39;].gsub!(/\\s.*$/, \u0026#39;\u0026#39;) end if record.has_key?(\u0026#39;to\u0026#39;) record[\u0026#39;to_domain\u0026#39;] = domain(record[\u0026#39;to\u0026#39;].downcase) end if record.has_key?(\u0026#39;from\u0026#39;) \u0026amp;\u0026amp; record[\u0026#39;from\u0026#39;] != \u0026#39;\u0026#39; record[\u0026#39;from_domain\u0026#39;] = domain(record[\u0026#39;from\u0026#39;].downcase) end yield time, record end end end end td-agent.conf はこんな感じ(必要に応じて Buffer まわりの調整を)\n\u0026lt;source\u0026gt; type tail format postfix_log_parser time_format %b %e %T path /var/log/maillog pos_file /var/lib/td-agent/mail.pos tag mail.syslog \u0026lt;/source\u0026gt; \u0026lt;match mail.syslog\u0026gt; type exec_filter command /etc/td-agent/exec_filter/postfix_log_binder.rb in_format json out_format json tag mail.filtered time_key time \u0026lt;/match\u0026gt; \u0026lt;match mail.filtered\u0026gt; type elasticsearch hosts kibana2:9200,kibana3:9200 type_name postfix logstash_format true logstash_prefix postfix flush_interval 5s \u0026lt;/match\u0026gt; /var/log/maillog はそのままでは td-agent ユーザーでは読めないので rsyslog の設定を変更します。\n$DirCreateMode 0750 $FileCreateMode 0640 $DirGroup sys $FileGroup sys td-agent ユーザーを sys グループに所属させます。\nsudo usermod -a -G sys td-agent プラグインの書き方は公式ドキュメントを参照しました http://docs.fluentd.org/articles/plugin-development 昔 fluent-plugin-fortigate-log-parser ってのを書いたけど Parser なのに Output プラグインとして実装してしまったので書き直したいな。\n","date":"2015年10月29日","permalink":"/2015/10/fluentd-postfix-plugin/","section":"Posts","summary":"前からやろうやろうと思ってなかなか手をつけられずにいたのですが、やっと実装しました。 （エラーハンドリングとかまだだけど） Postfix に限らず SMTP サーバー","title":"FluentdでPostfixのログをつなぎ合わせる"},{"content":"","date":"2015年10月29日","permalink":"/tags/postfix/","section":"Tags","summary":"","title":"Postfix"},{"content":"","date":"2015年10月29日","permalink":"/tags/ruby/","section":"Tags","summary":"","title":"ruby"},{"content":"何が入ってるかわからないイメージじゃなくて自分で作りたいと思ったので作ってみたので自分用にメモ。 ほぼ「CentOS7.1にてVagrant Base Boxを作成する - とあるエンジニアの技術メモ」に書いてあるままですけど。 まずはインストール用に ISO ファイルをダウンロード https://www.centos.org/download/ Oracle VM VirtualBox マネージャーで新しい仮想サーバーを作成。ここでは名前を「centos7」とする。 仮想ハードディスクは「VDI」で「可変サイズ」サイズは適当に「16GB」としておいた。 設定を開いてオーディオ、USBを無効化する。フロッピーもチェックを外す。 ダウンロードした ISO ファイルをマウントして起動し、CentOS をインストール 「vagrant」ユーザーを作成しておく。 インストール後``` yum -y update yum -y groupinstall \u0026ldquo;Development Tools\u0026rdquo; yum -y install curl sed -i \u0026rsquo;s/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config systemctl stop firewalld systemctl disable firewalld rm /etc/udev/rules.d/70-persistent-ipoib.rules yum clean alll\nvagrant ユーザーがパスワードなしでrootでなんでも出来るようにする requiretty もコメントアウト visudo\n「デバイス」→「Guest Additions CD イメージの挿入...」を選択後 メニューが表示されない場合は「右Ctrl」+「Home」 mount -r /dev/cdrom /mnt cd /mnt sh VBoxLinuxAdditions.run cd umount /mnt\numount したらデバイスからCDを削除しておく(eject) mkdir ~vagrant/.ssh chmod 755 ~vagrant/.ssh curl -o ~vagrant/.ssh/authorized_keys https://raw.githubusercontent.com/mitchellh/vagrant/master/keys/vagrant.pub chmod 644 ~vagrant/.ssh/authorized_keys chown -R vagrant:vagrant ~vagrant\n後は sshd\\_config いじって公開鍵認証だけにするとか、root での ssh ログインを許可しないとか不要なログを削除しておくとか。 shutdown -h now\nvagrant package --base centos7 package.box vagrant box add --name centos7 package.box ```box ファイルとして追加されていることを確認``` vagrant box list ```package.box はもう不要なので削除``` rm package.box ```vagrant で起動してみる``` vagrant init centos7 vagrant up ```","date":"2015年10月29日","permalink":"/2015/10/create-original-centos7-vagrant-box/","section":"Posts","summary":"何が入ってるかわからないイメージじゃなくて自分で作りたいと思ったので作ってみたので自分用にメモ。 ほぼ「CentOS7.1にてVagrant Base","title":"CentOS7の自前Vagrant boxを作成する"},{"content":"PostgreSQL には他のDBのテーブルを参照（更新もできる）できるようにする Foreign Data Wrapper (FDW) という機能が搭載されています。 これよりも前に dblink がありましたが、仕組み上パフォーマンス的に厳しく使いにくさがありました。FDW ではかなり良くなっています。PostgreSQL 以外の DB と接続することも可能です。 PostgreSQL9.3 開発と運用を支える新機能 http://enterprisezine.jp/dbonline/detail/5620 PostgreSQL 標準の replication は DB 全体での同期しかなく、レプリカは参照しかできませんから分析用のサーバーとして使おうにも書き込むことができません。将来的には論理 replication ができるようになりそうです。特定のテーブルだけとか。 http://www.postgresql.org/docs/9.4/static/logicaldecoding.html WAL を decode して必要なものだけ取り出す感じ。Oracle の LogMiner 的な。 DB まるごと FDW でアクセスしたいと言われた場合、テーブル一個ずつ手作業で作成するのは非常にしんどいので便利なツール転がってないかなと調べてみたらやっぱりありました。感謝感謝。 http://www.postgresonline.com/journal/archives/322-Generating-Create-Foreign-Table-Statements-for-postgres_fdw.html それではこの便利ツールを使った場合の流れをメモ\n概要 # 例として apple と orange というデータベースを作成し、それぞれDBと同じ名前のユーザーを作成。 apple には pgbench を使ってテーブルを作成（どんなテーブルを作るか考えるの面倒なので）。 orange には apple のテーブルを参照する FOREIGN TABLE を作成する 環境は CentOS 7 と PostgreSQL 9.4\nPostgreSQL と pgbench (contrib) をインストール＆起動 # $ sudo yum install -y http://yum.postgresql.org/9.4/redhat/rhel-7-x86_64/pgdg-centos94-9.4-1.noarch.rpm $ sudo yum install -y postgresql94-server postgresql94-contrib $ sudo -u postgres /usr/pgsql-9.4/bin/initdb -E utf8 --no-locale -D /var/lib/pgsql/9.4/data $ sudo systemctl enable postgresql-9.4 $ sudo systemctl start postgresql-9.4 ユーザーとデータベースの作成 # $ sudo -u postgres /usr/pgsql-9.4/bin/psql -c \u0026quot;create user apple password 'ringo'\u0026quot; $ sudo -u postgres /usr/pgsql-9.4/bin/psql -c \u0026quot;create user orange password 'mikan'\u0026quot; $ sudo -u postgres /usr/pgsql-9.4/bin/createdb -E utf8 -O apple apple $ sudo -u postgres /usr/pgsql-9.4/bin/createdb -E utf8 -O orange orange /var/lib/pgsql/9.4/data/pg_hba.conf の書き換えと反映。 初期設定ではローカルからのアクセスにパフワードが不要となっているので postgres ユーザ以外は必要にしておく。\n# TYPE DATABASE USER ADDRESS METHOD # \u0026quot;local\u0026quot; is for Unix domain socket connections only local all postgres trust local all all md5 # IPv4 local connections: host all postgres 127.0.0.1/32 trust host all all 127.0.0.1/32 md5 # IPv6 local connections: host all postgres ::1/128 trust host all all ::1/128 md5 $ sudo systemctl reload postgresql-9.4 apple DB にテーブルを作成する # pgbench の初期化モードで。\n$ /usr/pgsql-9.4/bin/pgbench -i -U apple apple $ /usr/pgsql-9.4/bin/psql -U apple apple=\u0026gt; \\d List of relations Schema | Name | Type | Owner --------+------------------+-------+------- public | pgbench_accounts | table | apple public | pgbench_branches | table | apple public | pgbench_history | table | apple public | pgbench_tellers | table | apple (4 rows) orange ユーザーが apple のテーブルを SELECT できるようにする\n$ /usr/pgsql-9.4/bin/psql -U apple apple=\u0026gt; GRANT SELECT ON pgbench_accounts, pgbench_branches, pgbench_history, pgbench_tellers TO orange; GRANT orange DB にて FOREIGN TABLE を使う準備をする # CREATE EXTENSION は postgres (特権) ユーザーで実行する必要がある\n$ sudo -u postgres /usr/pgsql-9.4/bin/psql orange orange=# CREATE EXTENSION postgres_fdw; CREATE EXTENSION orange=# \\dew List of foreign-data wrappers Name | Owner | Handler | Validator --------------+----------+----------------------+------------------------ postgres_fdw | postgres | postgres_fdw_handler | postgres_fdw_validator (1 row) CREATE SERVER と CREATE USER MAPPING で apple DB への接続情報を定義する。これも postgres ユーザーで行う。 ここでは誰でも FOREIGN TABLE を参照できるように public で USER MAPPING を指定しました。 USER MAPPING という名前からもわかるように、これは orange DB へのログインユーザーに対して apple DB へどのログインIDでアクセスするかを設定します。権限の異なるアカウントを使い分けることであるアカウントでは更新可能だが、別のアカウントでは更新不可などと制限できる。 orange ユーザーでなく apple ユーザーとしてアクセスさせることも可能で、そうすると今回の例の場合 FOREIGN TABLE 側から更新することも可能となる。（FOREIGN TABLE に対する GRANT も必要）\n$ sudo -u postgres /usr/pgsql-9.4/bin/psql orange orange=# CREATE SERVER apple FOREIGN DATA WRAPPER postgres_fdw OPTIONS ( orange(# dbname 'apple', orange(# host 'localhost', orange(# port '5432' orange(# ); CREATE SERVER orange=# CREATE USER MAPPING FOR public SERVER apple OPTIONS (user 'orange', password 'mikan'); CREATE USER MAPPING apple DB にて便利 function を定義する # # /usr/pgsql-9.4/bin/psql -U apple apple=\u0026gt; CREATE OR REPLACE FUNCTION script_foreign_tables(param_server text apple(\u0026gt; , param_schema_search text apple(\u0026gt; , param_table_search text, param_ft_prefix text) RETURNS SETOF text apple-\u0026gt; AS apple-\u0026gt; $$ apple$\u0026gt; -- params: param_server: name of foreign data server apple$\u0026gt; -- param_schema_search: wildcard search on schema use % for non-exact apple$\u0026gt; -- param_ft_prefix: prefix to give new table in target database apple$\u0026gt; -- include schema name if not default schema apple$\u0026gt; -- example usage: SELECT script_foreign_tables('prod_server', 'ch01', '%', 'ch01.ft_'); apple$\u0026gt; WITH cols AS apple$\u0026gt; ( SELECT cl.relname As table_name, na.nspname As table_schema, att.attname As column_name apple$\u0026gt; , format_type(ty.oid,att.atttypmod) AS column_type apple$\u0026gt; , attnum As ordinal_position apple$\u0026gt; FROM pg_attribute att apple$\u0026gt; JOIN pg_type ty ON ty.oid=atttypid apple$\u0026gt; JOIN pg_namespace tn ON tn.oid=ty.typnamespace apple$\u0026gt; JOIN pg_class cl ON cl.oid=att.attrelid apple$\u0026gt; JOIN pg_namespace na ON na.oid=cl.relnamespace apple$\u0026gt; LEFT OUTER JOIN pg_type et ON et.oid=ty.typelem apple$\u0026gt; LEFT OUTER JOIN pg_attrdef def ON adrelid=att.attrelid AND adnum=att.attnum apple$\u0026gt; WHERE apple$\u0026gt; -- only consider non-materialized views and concrete tables (relations) apple$\u0026gt; cl.relkind IN('v','r') apple$\u0026gt; AND na.nspname LIKE $2 AND cl.relname LIKE $3 apple$\u0026gt; AND cl.relname NOT IN('spatial_ref_sys', 'geometry_columns' apple$\u0026gt; , 'geography_columns', 'raster_columns') apple$\u0026gt; AND att.attnum \u0026gt; 0 apple$\u0026gt; AND NOT att.attisdropped apple$\u0026gt; ORDER BY att.attnum ) apple$\u0026gt; SELECT 'CREATE FOREIGN TABLE ' || $4 || table_name || ' (' apple$\u0026gt; || string_agg(quote_ident(column_name) || ' ' || column_type apple$\u0026gt; , ', ' ORDER BY ordinal_position) apple$\u0026gt; || ') apple$\u0026gt; SERVER ' || quote_ident($1) || ' OPTIONS (schema_name ''' || quote_ident(table_schema) apple$\u0026gt; || ''', table_name ''' || quote_ident(table_name) || '''); ' As result apple$\u0026gt; FROM cols apple$\u0026gt; GROUP BY table_schema, table_name apple$\u0026gt; $$ language 'sql'; CREATE FUNCTION CREATE FOREIGN TABLE の生成 # コピペしやすいように \\t, \\a で psql の出力を調整している。\n$ /usr/pgsql-9.4/bin/psql -U apple apple=\u0026gt; \\t Tuples only is on. apple=\u0026gt; \\a Output format is unaligned. apple=\u0026gt; SELECT script_foreign_tables('apple', 'public', '%', 'public.ft_'); CREATE FOREIGN TABLE public.ft_pgbench_accounts (aid integer, bid integer, abalance integer, filler character(84)) SERVER apple OPTIONS (schema_name 'public', table_name 'pgbench_accounts'); CREATE FOREIGN TABLE public.ft_pgbench_branches (bid integer, bbalance integer, filler character(88)) SERVER apple OPTIONS (schema_name 'public', table_name 'pgbench_branches'); CREATE FOREIGN TABLE public.ft_pgbench_history (tid integer, bid integer, aid integer, delta integer, mtime timestamp without time zone, filler character(22)) SERVER apple OPTIONS (schema_name 'public', table_name 'pgbench_history'); CREATE FOREIGN TABLE public.ft_pgbench_tellers (tid integer, bid integer, tbalance integer, filler character(84)) SERVER apple OPTIONS (schema_name 'public', table_name 'pgbench_tellers'); apple=\u0026gt; 実際には次のようにファイルに書きだしたほうが便利 (psql 内での \\o create_foreign_table.sql でも可)。\npsql -At -c \u0026quot;SELECT script_foreign_tables('apple', 'public', '%', 'public.ft_')\u0026quot; \u0026gt; create_foreign_table.sql script_foreign_tables の引数は SELECT script_foreign_tables(\u0026lsquo;apple\u0026rsquo;, \u0026lsquo;public\u0026rsquo;, \u0026lsquo;%\u0026rsquo;, \u0026lsquo;public.ft_');\n# 実行例の値\n説明\n1\napple\nCREATE FOREIGN TABLE で指定する SERVER\n2\npublic\napple DB での対象テーブルの schema\n3\n%\napple DB での対象テーブルを絞るための LIKE 条件 (% の場合は全てとなる)\n4\npublic.ft_\nCREATE TABLE 時の prefix (public.ft_ であれば public schema に ft_{original_table_name} という TABLE が作成される)。 public. であれば同じ名前の TABLE が作成される\nCREATE FOREIGN TABLE # postgres ユーザーで orange DB に貼り付けて (もしくはファイルを読み込ませて) FOREIGN TABLE を作成する\n$ sudo -u postgres /usr/pgsql-9.4/bin/psql orange orange=# CREATE USER MAPPING FOR public SERVER apple OPTIONS (user 'orange', password 'mikan'); CREATE USER MAPPING orange=# CREATE FOREIGN TABLE public.ft_pgbench_accounts (aid integer, bid integer, abalance integer, filler character(84)) orange-# SERVER apple OPTIONS (schema_name 'public', table_name 'pgbench_accounts'); CREATE FOREIGN TABLE orange=# CREATE FOREIGN TABLE public.ft_pgbench_branches (bid integer, bbalance integer, filler character(88)) orange-# SERVER apple OPTIONS (schema_name 'public', table_name 'pgbench_branches'); CREATE FOREIGN TABLE orange=# CREATE FOREIGN TABLE public.ft_pgbench_history (tid integer, bid integer, aid integer, delta integer, mtime timestamp without time zone, filler character(22)) orange-# SERVER apple OPTIONS (schema_name 'public', table_name 'pgbench_history'); CREATE FOREIGN TABLE orange=# CREATE FOREIGN TABLE public.ft_pgbench_tellers (tid integer, bid integer, tbalance integer, filler character(84)) orange-# SERVER apple OPTIONS (schema_name 'public', table_name 'pgbench_tellers'); CREATE FOREIGN TABLE orange=# postgres ユーザーで作成しているため、必要なユーザーがアクセス可能なように GRANT して上げる必要がある。これも大量にあるとつらいので次のようなクエリで対応する (FOREIGN TABLE は pg_stat_user_tables から取得できなかった)\nSELECT\u0026#39;GRANT SELECT ON \u0026#39;||relname||\u0026#39; TO orange;\u0026#39;FROMpg_catalog.pg_classWHERErelkind=\u0026#39;f\u0026#39;;PostgreSQL 9.5 から FDW がもっと便利に # IMPORT FOREIGN SCHEMA を使えば簡単に source database の schema をコピーして foreign table が作れるようになるみたいです。 http://www.postgresql.org/docs/9.5/static/sql-importforeignschema.html\nPostgreSQL 9.5 新機能紹介 from NTT DATA OSS Professional Services\n以上\n","date":"2015年9月19日","permalink":"/2015/09/postgresql-rakuchin-fdw-foreign-table/","section":"Posts","summary":"PostgreSQL には他のDBのテーブルを参照（更新もできる）できるようにする Foreign Data Wrapper (FDW) という機能が搭載されています。 これよりも前に dblink がありましたが、仕組み上","title":"PostgreSQL 楽ちん FDW Foreign Table 作成"},{"content":"私のスマホは Xperia Z3 (SO-01G) に乗り換えたので息子のおもちゃと化していた LG G2 mini ですが、あの変な場所（背面）についている電源とボリュームの機械スイッチが効かなくなってしまったので特殊ドライバー（アネックス(ANEX) 特殊精密差替ドライバーY型 No.3607 を購入して分解してみました。（水没歴あり）\nちょいと掃除してあげたら（？）スイッチは復活しました。めでたしめでたし。\n ","date":"2015年8月8日","permalink":"/2015/08/take-the-lg-g2-mini-to-pieces/","section":"Posts","summary":"私のスマホは Xperia Z3 (SO-01G) に乗り換えたので息子のおもちゃと化していた LG G2 mini ですが、あの変な場所（背面）についている電源とボリュームの機械スイッチが効","title":"LG G2 mini を分解してみた"},{"content":"","date":"2015年8月8日","permalink":"/tags/%E6%A0%BC%E5%AE%89%E3%82%B9%E3%83%9E%E3%83%9B/","section":"Tags","summary":"","title":"格安スマホ"},{"content":"前回は Wordpress のチュートリアルを試しました、次はもう少し複雑な Guestbook Tutorial - Container Engine — Google Cloud Platform をなぞります。\nCreate a Container Engine cluster # GKE クラスタを作成します\n$ gcloud beta container clusters create guestbook Creating cluster guestbook...done. Created [https://container.googleapis.com/v1/projects/PROJECTID/zones/asia-east1-c/clusters/guestbook]. kubeconfig entry generated for guestbook. To switch context to the cluster, run $ kubectl config use-context gke_PROJECTID_asia-east1-c_guestbook NAME ZONE MASTER_VERSION MASTER_IP MACHINE_TYPE STATUS guestbook asia-east1-c 0.21.1 203.0.113.167 n1-standard-1 RUNNING Wordpress の時はノード数と GCE インスタンスの g1-small を指定しましたが、今回は指定がありません。 どうなってるのか確認してみましょう。\n$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS gke-guestbook-f6a9e1a2-node-bjb5 asia-east1-c n1-standard-1 10.240.145.243 203.0.113.104 RUNNING gke-guestbook-f6a9e1a2-node-woin asia-east1-c n1-standard-1 10.240.158.209 203.0.113.237 RUNNING gke-guestbook-f6a9e1a2-node-zti0 asia-east1-c n1-standard-1 10.240.209.195 203.0.113.101 RUNNING n1-standard-1 が3つ起動してますね。 n1-standard-1 は今日(2015/7/12)では1インスタンス $0.069/時間 のようです。https://cloud.google.com/compute/?hl=ja 3ノードそれぞれで docker ps してみました。 fluentd (ログコレクション), etcd (分散KVS), SkyDNS (etcd をバックエンドとした DNS サーバー), kube2sky (Kubernetes と SkyDNS のブリッジ), heapster (リソースモニタリング) などが動いています。\u0026ldquo;pause\u0026rdquo; ってなんだろう？\n$ sudo docker ps -a --no-trunc=true CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7d406295a24dd57eac1af3e0023b943d613ed6f7424b937412d90d7374760205 gcr.io/google_containers/fluentd-gcp:1.8 \u0026quot;\\\u0026quot;/bin/sh -c '/usr/sbin/google-fluentd \\\u0026quot;$FLUENTD_ARGS\\\u0026quot; \u0026gt; /var/log/google-fluentd.log'\\\u0026quot;\u0026quot; 4 minutes ago Up 4 minutes k8s_fluentd-cloud-logging.7721935b_fluentd-cloud-logging-gke-guestbook-f6a9e1a2-node-bjb5_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_85a07d24 014716b09d6de5c5546cd7fac5fe7c420071dd513cdee552d82dda5106de7399 gcr.io/google_containers/heapster:v0.15.0 \u0026quot;/heapster --source=kubernetes:''\u0026quot; 4 minutes ago Up 4 minutes k8s_heapster.5ec26f85_monitoring-heapster-v5-emb1z_kube-system_f9c0a5a4-289e-11e5-9359-42010af03fdb_18db2cfe 7c85c94394ce25d5092e41bf2f1cba1bf67984b3c1cc27ebb775ffab75bbc76f gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; 5 minutes ago Up 5 minutes k8s_POD.e4cc795_monitoring-heapster-v5-emb1z_kube-system_f9c0a5a4-289e-11e5-9359-42010af03fdb_70f26e8e be3069e309c9bdf6bacab0b0a83b641de1b24232d2438bd7db0d755697038c29 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; 5 minutes ago Up 5 minutes k8s_POD.e4cc795_fluentd-cloud-logging-gke-guestbook-f6a9e1a2-node-bjb5_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_8f560911 $ sudo docker ps -a --no-trunc=true CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d46086762093a7ae2b6885695afe3e6829e53f0546d1f8153c1c26d62038a5c3 gcr.io/google_containers/fluentd-gcp:1.8 \u0026quot;\\\u0026quot;/bin/sh -c '/usr/sbin/google-fluentd \\\u0026quot;$FLUENTD_ARGS\\\u0026quot; \u0026gt; /var/log/google-fluentd.log'\\\u0026quot;\u0026quot; 16 minutes ago Up 16 minutes k8s_fluentd-cloud-logging.7721935b_fluentd-cloud-logging-gke-guestbook-f6a9e1a2-node-woin_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_334baa58 78c2c76ddb6af757752ebab0494da0470c810bc9d4dd6dd4c76615b4b04540bd gcr.io/google_containers/kube-ui:v1 \u0026quot;/kube-ui\u0026quot; 16 minutes ago Up 16 minutes k8s_kube-ui.e47d83a6_kube-ui-v1-gx17k_kube-system_f9c09a3f-289e-11e5-9359-42010af03fdb_8e9d5d77 d8460a693d20c6d4e2b2bf18700c1e7e589ea1c36e2c9d6c94c871227480c583 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; 16 minutes ago Up 16 minutes k8s_POD.3b46e8b9_kube-ui-v1-gx17k_kube-system_f9c09a3f-289e-11e5-9359-42010af03fdb_cc87173f e908771293b93ee52639fdf6400ff64338769c306252a6daa70f63fee3824703 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; 16 minutes ago Up 16 minutes k8s_POD.e4cc795_fluentd-cloud-logging-gke-guestbook-f6a9e1a2-node-woin_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_818f6b11 $ sudo docker ps -a --no-trunc=true CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c688081b385db0e2008f10705d412501983c8a52a938692d64e6dee14a1bd74e gcr.io/google_containers/skydns:2015-03-11-001 \u0026quot;/skydns -machines=http://localhost:4001 -addr=0.0.0.0:53 -domain=cluster.local.\u0026quot; 17 minutes ago Up 17 minutes k8s_skydns.58dac13a_kube-dns-v6-qyyz6_kube-system_f9c0ad25-289e-11e5-9359-42010af03fdb_b864c5a2 ee5987aa158d726b376b327b12eb84457340a3d1c5e9fa93dc0d38f00131e2a5 gcr.io/google_containers/fluentd-gcp:1.8 \u0026quot;\\\u0026quot;/bin/sh -c '/usr/sbin/google-fluentd \\\u0026quot;$FLUENTD_ARGS\\\u0026quot; \u0026gt; /var/log/google-fluentd.log'\\\u0026quot;\u0026quot; 17 minutes ago Up 17 minutes k8s_fluentd-cloud-logging.7721935b_fluentd-cloud-logging-gke-guestbook-f6a9e1a2-node-zti0_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_5a94cca9 08c23fc0567836698bd44eec9bf07298657094ad1ef3a3fa79dc886ff5c46e68 gcr.io/google_containers/kube2sky:1.11 \u0026quot;/kube2sky -domain=cluster.local\u0026quot; 17 minutes ago Up 17 minutes k8s_kube2sky.a17e6ab0_kube-dns-v6-qyyz6_kube-system_f9c0ad25-289e-11e5-9359-42010af03fdb_4f7667c3 42dcd9cafab7def216f24f859615a3f90166e8971cfad6eb1ec88d8a8eaf8bba gcr.io/google_containers/etcd:2.0.9 \u0026quot;/usr/local/bin/etcd -listen-client-urls http://127.0.0.1:2379,http://127.0.0.1:4001 -advertise-client-urls http://127.0.0.1:2379,http://127.0.0.1:4001 -initial-cluster-token skydns-etcd\u0026quot; 17 minutes ago Up 17 minutes k8s_etcd.962798f6_kube-dns-v6-qyyz6_kube-system_f9c0ad25-289e-11e5-9359-42010af03fdb_30d92bc0 5bbad3fdb141277f4cff48851901bbf8c4e05821bf6aea9f2fb57b7914fd1fcd gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; 17 minutes ago Up 17 minutes k8s_POD.8fdb0e41_kube-dns-v6-qyyz6_kube-system_f9c0ad25-289e-11e5-9359-42010af03fdb_59ac2004 9fbc741b98754aed17778101fd0ed5746f1a3acfb21191ca4686776dbc7ae0f6 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; 17 minutes ago Up 17 minutes k8s_POD.e4cc795_fluentd-cloud-logging-gke-guestbook-f6a9e1a2-node-zti0_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_4053a8c5 Linux distribution は Debian でした。\n$ cat /etc/os-release PRETTY_NAME=\u0026quot;Debian GNU/Linux 7 (wheezy)\u0026quot; NAME=\u0026quot;Debian GNU/Linux\u0026quot; VERSION_ID=\u0026quot;7\u0026quot; VERSION=\u0026quot;7 (wheezy)\u0026quot; ID=debian ANSI_COLOR=\u0026quot;1;31\u0026quot; HOME_URL=\u0026quot;http://www.debian.org/\u0026quot; SUPPORT_URL=\u0026quot;http://www.debian.org/support/\u0026quot; BUG_REPORT_URL=\u0026quot;http://bugs.debian.org/\u0026quot; Step one: Start the Redis master # チュートリアルのサイトからダウンロードした redis-master-controller.json を使って pod を作成します。 この pod はシングルコンテナです。\n$ kubectl create -f redis-master-controller.json replicationcontrollers/redis-master $ kubectl get pods -l name=redis-master POD IP CONTAINER(S) IMAGE(S) HOST LABELS STATUS CREATED MESSAGE redis-master-4qexb 10.16.1.4 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=redis-master Running 25 seconds master redis Running 10 seconds gke-guestbook-f6a9e1a2-node-bjb5 に pod が追加されたようなので、またログインして docker ps してみます。\n$ sudo docker ps -a --no-trunc=true CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 623c6f0e1312d0f3e40eb924d105f6ac4164a4e4bad5d60b52333c570e82c45a gcr.io/google_containers/heapster:v0.15.0 \u0026quot;/heapster --source=kubernetes:''\u0026quot; 4 minutes ago Up 4 minutes k8s_heapster.5ec26f85_monitoring-heapster-v5-emb1z_kube-system_f9c0a5a4-289e-11e5-9359-42010af03fdb_5fe2012f 5be9079c528100f6b342ddd1bccb8d987f32acbc3cdf95adc393ce2c3d412553 redis:latest \u0026quot;/entrypoint.sh redis-server\u0026quot; 13 minutes ago Up 13 minutes k8s_master.3173469e_redis-master-4qexb_default_a613affd-28a3-11e5-9359-42010af03fdb_6129e534 14087860fcf4e6648493e4895dca603b3b014c865a7eb69f828cc0d9d2bb0120 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; 13 minutes ago Up 13 minutes k8s_POD.49eee8c2_redis-master-4qexb_default_a613affd-28a3-11e5-9359-42010af03fdb_e8510c23 8a2b8a55b671ea003aee66ef7b048140fd7de393c77667e2c9bdda0565128588 gcr.io/google_containers/heapster:v0.15.0 \u0026quot;/heapster --source=kubernetes:''\u0026quot; 14 minutes ago Exited (137) 4 minutes ago k8s_heapster.5ec26f85_monitoring-heapster-v5-emb1z_kube-system_f9c0a5a4-289e-11e5-9359-42010af03fdb_c28b7121 a17c35601da8dd07abce20b554537f814ba4cf1355d72804574efce81b62c4d6 gcr.io/google_containers/heapster:v0.15.0 \u0026quot;/heapster --source=kubernetes:''\u0026quot; 24 minutes ago Exited (137) 14 minutes ago k8s_heapster.5ec26f85_monitoring-heapster-v5-emb1z_kube-system_f9c0a5a4-289e-11e5-9359-42010af03fdb_c3470740 7d406295a24dd57eac1af3e0023b943d613ed6f7424b937412d90d7374760205 gcr.io/google_containers/fluentd-gcp:1.8 \u0026quot;\\\u0026quot;/bin/sh -c '/usr/sbin/google-fluentd \\\u0026quot;$FLUENTD_ARGS\\\u0026quot; \u0026gt; /var/log/google-fluentd.log'\\\u0026quot;\u0026quot; 46 minutes ago Up 46 minutes k8s_fluentd-cloud-logging.7721935b_fluentd-cloud-logging-gke-guestbook-f6a9e1a2-node-bjb5_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_85a07d24 7c85c94394ce25d5092e41bf2f1cba1bf67984b3c1cc27ebb775ffab75bbc76f gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; 46 minutes ago Up 46 minutes k8s_POD.e4cc795_monitoring-heapster-v5-emb1z_kube-system_f9c0a5a4-289e-11e5-9359-42010af03fdb_70f26e8e be3069e309c9bdf6bacab0b0a83b641de1b24232d2438bd7db0d755697038c29 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; 46 minutes ago Up 46 minutes k8s_POD.e4cc795_fluentd-cloud-logging-gke-guestbook-f6a9e1a2-node-bjb5_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_8f560911 これで 10.16.1.4 の 6379 port にアクセスすると Redis が起動していることが確認できます。 どれかの node に ssh してアクセスしてみます。\n$ gcloud compute ssh gke-guestbook-f6a9e1a2-node-zti0 telnet は入っていませんでしたが、nc コマンドがあったので試してみます\n$ echo info | nc 10.16.1.4 6379 $1886 # Server redis_version:3.0.2 redis_git_sha1:00000000 redis_git_dirty:0 redis_build_id:4795df119e2d77fe redis_mode:standalone os:Linux 3.16.0-0.bpo.4-amd64 x86_64 arch_bits:64 multiplexing_api:epoll gcc_version:4.7.2 process_id:1 run_id:24fbeb83eb004d5d6e3828f37bea703974a87606 tcp_port:6379 uptime_in_seconds:326 uptime_in_days:0 hz:10 lru_clock:10648884 config_file: # Clients connected_clients:1 client_longest_output_list:0 client_biggest_input_buf:0 blocked_clients:0 # Memory used_memory:815912 used_memory_human:796.79K used_memory_rss:3551232 used_memory_peak:815912 used_memory_peak_human:796.79K used_memory_lua:36864 mem_fragmentation_ratio:4.35 mem_allocator:jemalloc-3.6.0 # Persistence loading:0 rdb_changes_since_last_save:0 rdb_bgsave_in_progress:0 rdb_last_save_time:1436711918 rdb_last_bgsave_status:ok rdb_last_bgsave_time_sec:-1 rdb_current_bgsave_time_sec:-1 aof_enabled:0 aof_rewrite_in_progress:0 aof_rewrite_scheduled:0 aof_last_rewrite_time_sec:-1 aof_current_rewrite_time_sec:-1 aof_last_bgrewrite_status:ok aof_last_write_status:ok # Stats total_connections_received:2 total_commands_processed:1 instantaneous_ops_per_sec:0 total_net_input_bytes:20 total_net_output_bytes:1951 instantaneous_input_kbps:0.00 instantaneous_output_kbps:0.00 rejected_connections:0 sync_full:0 sync_partial_ok:0 sync_partial_err:0 expired_keys:0 evicted_keys:0 keyspace_hits:0 keyspace_misses:0 pubsub_channels:0 pubsub_patterns:0 latest_fork_usec:0 migrate_cached_sockets:0 # Replication role:master connected_slaves:0 master_repl_offset:0 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 # CPU used_cpu_sys:0.07 used_cpu_user:0.08 used_cpu_sys_children:0.00 used_cpu_user_children:0.00 # Cluster cluster_enabled:0 # Keyspace docker logs コマンドで出力を確認できます。\n$ sudo docker logs 5be9079c528100f6b342ddd1bccb8d987f32acbc3cdf95adc393ce2c3d412553 1:C 12 Jul 14:38:38.271 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 3.0.2 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 1 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 1:M 12 Jul 14:38:38.272 # Server started, Redis version 3.0.2 1:M 12 Jul 14:38:38.272 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 1:M 12 Jul 14:38:38.272 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 1:M 12 Jul 14:38:38.272 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 12 Jul 14:38:38.272 * The server is now ready to accept connections on port 6379 Step two: Start the Redis master\u0026rsquo;s service # 次にサービスを設定します\n$ kubectl create -f redis-master-service.json services/redis-master $ kubectl get services -l name=redis-master NAME LABELS SELECTOR IP(S) PORT(S) redis-master name=redis-master name=redis-master 10.19.253.109 6379/TCP redis-master は1つだけなのでサービスを作っても代わり映えがしませんが、このアドレスを使ってアクセスします。pod が入れ替わってもこのアドレスは変わりません。次の redis slave は複数 pod のロードバランサー的に動作します。\nStep three: Start the replicated Redis worker pods # 今度は Replication Controller で同じ pod を複数起動させ、その数をキープさせます。\n$ kubectl create -f redis-worker-controller.json replicationcontrollers/redis-slave redis-slave は REPLICAS が2になっています\n$ kubectl get rc CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS redis-master master redis name=redis-master 1 redis-slave slave kubernetes/redis-slave:v2 name=redis-slave 2 kubectl get pods も2つの pod が確認できます\n$ kubectl get pods -l name=redis-slave POD IP CONTAINER(S) IMAGE(S) HOST LABELS STATUS CREATED MESSAGE redis-slave-a90om gke-guestbook-f6a9e1a2-node-woin/ name=redis-slave Pending About a minute slave kubernetes/redis-slave:v2 redis-slave-xkeft gke-guestbook-f6a9e1a2-node-bjb5/ name=redis-slave Pending About a minute slave kubernetes/redis-slave:v2 すぐだとまだ Pending でしたが、しばらくすると Running になりました。\n$ kubectl get pods -l name=redis-slave POD IP CONTAINER(S) IMAGE(S) HOST LABELS STATUS CREATED MESSAGE redis-slave-a90om 10.16.2.4 gke-guestbook-f6a9e1a2-node-woin/10.240.158.209 name=redis-slave Running 7 minutes slave kubernetes/redis-slave:v2 Running 5 minutes redis-slave-xkeft 10.16.1.5 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=redis-slave Running 7 minutes slave kubernetes/redis-slave:v2 Running 5 minutes Step four: Create the Redis worker service # redis-master 同様にサービスを作成します\n$ kubectl create -f redis-worker-service.json services/redis-slave $ kubectl get services NAME LABELS SELECTOR IP(S) PORT(S) kubernetes component=apiserver,provider=kubernetes 10.19.240.1 443/TCP redis-master name=redis-master name=redis-master 10.19.253.109 6379/TCP redis-slave name=redis-slave name=redis-slave 10.19.244.0 6379/TCP redis-slave という名前ですが、これはほんとに replication されてるのか確認してみましょう master に問い合わせてみると2つの slave が接続されてることが確認できました。 slave0, slave1 のアドレスに見知らぬものが表示されてますね、これは NAPT でアドレス変換されているのでしょうか\n$ echo info replication | nc 10.16.1.4 6379 $309 # Replication role:master connected_slaves:2 slave0:ip=10.240.158.209,port=6379,state=online,offset=617,lag=0 slave1:ip=10.16.1.1,port=6379,state=online,offset=617,lag=1 master_repl_offset:617 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:2 repl_backlog_histlen:616 今度は slave に問い合わせてみました。 master_host には redis-master とあります。これは redis-master というサービスのアドレスが etcd によって名前解決できるようになっているわけですね（きっと）\n$ echo info replication | nc 10.16.2.4 6379 $363 # Replication role:slave master_host:redis-master master_port:6379 master_link_status:up master_last_io_seconds_ago:5 master_sync_in_progress:0 slave_repl_offset:743 slave_priority:100 slave_read_only:1 connected_slaves:0 master_repl_offset:0 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 $ echo info replication | nc 10.16.1.5 6379 $363 # Replication role:slave master_host:redis-master master_port:6379 master_link_status:up master_last_io_seconds_ago:6 master_sync_in_progress:0 slave_repl_offset:785 slave_priority:100 slave_read_only:1 connected_slaves:0 master_repl_offset:0 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 Step five: Create the guestbook web server pods # 次は frontend という名の web serveer をセットアップします。\n$ kubectl create -f frontend-controller.json replicationcontrollers/frontend $ kubectl get rc CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS frontend php-redis kubernetes/example-guestbook-php-redis:v2 name=frontend 3 redis-master master redis name=redis-master 1 redis-slave slave kubernetes/redis-slave:v2 name=redis-slave 2 まだ起動中でしょうか Pending ですね。\n$ kubectl get pods POD IP CONTAINER(S) IMAGE(S) HOST LABELS STATUS CREATED MESSAGE frontend-ltoyt gke-guestbook-f6a9e1a2-node-woin/ name=frontend Pending 47 seconds php-redis kubernetes/example-guestbook-php-redis:v2 frontend-o6dla gke-guestbook-f6a9e1a2-node-bjb5/ name=frontend Pending 47 seconds php-redis kubernetes/example-guestbook-php-redis:v2 frontend-rurh0 gke-guestbook-f6a9e1a2-node-zti0/ name=frontend Pending 47 seconds php-redis kubernetes/example-guestbook-php-redis:v2 redis-master-4qexb 10.16.1.4 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=redis-master Running 50 minutes master redis Running 50 minutes redis-slave-a90om 10.16.2.4 gke-guestbook-f6a9e1a2-node-woin/10.240.158.209 name=redis-slave Running 17 minutes slave kubernetes/redis-slave:v2 Running 15 minutes redis-slave-xkeft 10.16.1.5 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=redis-slave Running 17 minutes slave kubernetes/redis-slave:v2 Running 15 minutes きました\n$ kubectl get pods POD IP CONTAINER(S) IMAGE(S) HOST LABELS STATUS CREATED MESSAGE frontend-ltoyt 10.16.2.5 gke-guestbook-f6a9e1a2-node-woin/10.240.158.209 name=frontend Running 3 minutes php-redis kubernetes/example-guestbook-php-redis:v2 Running About a minute frontend-o6dla 10.16.1.6 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=frontend Running 3 minutes php-redis kubernetes/example-guestbook-php-redis:v2 Running About a minute frontend-rurh0 10.16.0.4 gke-guestbook-f6a9e1a2-node-zti0/10.240.209.195 name=frontend Running 3 minutes php-redis kubernetes/example-guestbook-php-redis:v2 Running About a minute redis-master-4qexb 10.16.1.4 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=redis-master Running 53 minutes master redis Running 52 minutes redis-slave-a90om 10.16.2.4 gke-guestbook-f6a9e1a2-node-woin/10.240.158.209 name=redis-slave Running 19 minutes slave kubernetes/redis-slave:v2 Running 18 minutes redis-slave-xkeft 10.16.1.5 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=redis-slave Running 19 minutes slave kubernetes/redis-slave:v2 Running 18 minutes これまでの流れでだいたいわかりました、次はサービスの設定ですね。\nStep six: Create a guestbook web service with an external IP, and open up the firewall # redis の時とは違って今回は type: LoadBalancer という指定が入っています。ということは redis-slave は LoadBalancer ではないということなのかな。\n$ kubectl create -f frontend-service.json services/frontend frontend というサービスができました\n$ kubectl get services NAME LABELS SELECTOR IP(S) PORT(S) frontend name=frontend name=frontend 10.19.248.123 80/TCP kubernetes component=apiserver,provider=kubernetes 10.19.240.1 443/TCP redis-master name=redis-master name=redis-master 10.19.253.109 6379/TCP redis-slave name=redis-slave name=redis-slave 10.19.244.0 6379/TCP 次は frontend に外部からアクセスできるようにします\n$ kubectl get nodes NAME LABELS STATUS gke-guestbook-f6a9e1a2-node-bjb5 kubernetes.io/hostname=gke-guestbook-f6a9e1a2-node-bjb5 Ready gke-guestbook-f6a9e1a2-node-woin kubernetes.io/hostname=gke-guestbook-f6a9e1a2-node-woin Ready gke-guestbook-f6a9e1a2-node-zti0 kubernetes.io/hostname=gke-guestbook-f6a9e1a2-node-zti0 Ready Wordpress のと同じように node の NAME の -name までを使います。この場合 gke-guestbook-f6a9e1a2-node ですね\n$ gcloud compute firewall-rules create --allow=tcp:80 \\ --target-tags=gke-guestbook-f6a9e1a2-node guestbook Created [https://www.googleapis.com/compute/v1/projects/PROJECTID/global/firewalls/guestbook]. NAME NETWORK SRC_RANGES RULES SRC_TAGS TARGET_TAGS guestbook default 0.0.0.0/0 tcp:80 gke-guestbook-f6a9e1a2-node Gobal IP address を確認してアクセスしてみます (API の version 違いみたいな warning が表示されたけど)\n$ kubectl describe services frontend Name:\tfrontend Labels:\tname=frontend Selector:\tname=frontend Type:\tLoadBalancer IP:\t10.19.248.123 LoadBalancer Ingress:\t203.0.113.110 Port:\t80/TCP NodePort:\t30698/TCP Endpoints:\t10.16.0.4:80,10.16.1.6:80,10.16.2.5:80 Session Affinity:\tNone No events. LoadBalancer Ingress の IP アドレスの port 80 にアクセスしてみましょう\nGKE Tutorial Guestbook  わーい\nResizing a replication controller: Changing the number of web servers # frontend の数を3から5に増やしてみましょう。Docker なので増やすのは簡単そうです、kubernetes では次の1行です\n$ kubectl scale --replicas=5 rc frontend 増えましたね\n$ kubectl get pods POD IP CONTAINER(S) IMAGE(S) HOST LABELS STATUS CREATED MESSAGE frontend-ltoyt 10.16.2.5 gke-guestbook-f6a9e1a2-node-woin/10.240.158.209 name=frontend Running 31 minutes php-redis kubernetes/example-guestbook-php-redis:v2 Running 28 minutes frontend-o6dla 10.16.1.6 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=frontend Running 31 minutes php-redis kubernetes/example-guestbook-php-redis:v2 Running 28 minutes frontend-rurh0 10.16.0.4 gke-guestbook-f6a9e1a2-node-zti0/10.240.209.195 name=frontend Running 31 minutes php-redis kubernetes/example-guestbook-php-redis:v2 Running 28 minutes frontend-t67ho 10.16.2.6 gke-guestbook-f6a9e1a2-node-woin/10.240.158.209 name=frontend Running 18 seconds php-redis kubernetes/example-guestbook-php-redis:v2 Running 18 seconds frontend-xn9hx 10.16.0.5 gke-guestbook-f6a9e1a2-node-zti0/10.240.209.195 name=frontend Running 18 seconds php-redis kubernetes/example-guestbook-php-redis:v2 Running 18 seconds redis-master-4qexb 10.16.1.4 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=redis-master Running About an hour master redis Running About an hour redis-slave-a90om 10.16.2.4 gke-guestbook-f6a9e1a2-node-woin/10.240.158.209 name=redis-slave Running 47 minutes slave kubernetes/redis-slave:v2 Running 45 minutes redis-slave-xkeft 10.16.1.5 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=redis-slave Running 47 minutes slave kubernetes/redis-slave:v2 Running 45 minutes 減らすのも同様です\n$ kubectl scale --replicas=1 rc frontend scaled 減った\n$ kubectl get pods POD IP CONTAINER(S) IMAGE(S) HOST LABELS STATUS CREATED MESSAGE frontend-o6dla 10.16.1.6 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=frontend Running 32 minutes php-redis kubernetes/example-guestbook-php-redis:v2 Running 30 minutes redis-master-4qexb 10.16.1.4 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=redis-master Running About an hour master redis Running About an hour redis-slave-a90om 10.16.2.4 gke-guestbook-f6a9e1a2-node-woin/10.240.158.209 name=redis-slave Running 48 minutes slave kubernetes/redis-slave:v2 Running 47 minutes redis-slave-xkeft 10.16.1.5 gke-guestbook-f6a9e1a2-node-bjb5/10.240.145.243 name=redis-slave Running 48 minutes slave kubernetes/redis-slave:v2 Running 47 minutes Cleanup # 破産しないように忘れずにお掃除を。\n$ kubectl delete services frontend services/frontend $ gcloud beta container clusters delete guestbook The following clusters will be deleted. - [guestbook] in [asia-east1-c] Do you want to continue (Y/n)? Y Deleting cluster guestbook...done. Deleted [https://container.googleapis.com/v1/projects/PROJECTID/zones/asia-east1-c/clusters/guestbook]. $ gcloud compute firewall-rules delete guestbook The following firewalls will be deleted: - [guestbook] Do you want to continue (Y/n)? y Deleted [https://www.googleapis.com/compute/v1/projects/PROJECTID/global/firewalls/guestbook]. 念の為 https://console.developers.google.com/ で消えてることを確認してみましょうか Pod, Replication Controller, Service を設定するための YAML / JSON について調べるといろいろわかるのかな\n","date":"2015年7月12日","permalink":"/2015/07/gke-guestbook-tutorial/","section":"Posts","summary":"前回は Wordpress のチュートリアルを試しました、次はもう少し複雑な Guestbook Tutorial - Container Engine — Google Cloud Platform をなぞります。 Create a Container Engine cluster # GKE クラスタを作成します $ gcloud beta container clusters create guestbook Creating cluster","title":"GKE Guestbook チュートリアルを試す"},{"content":"","date":"2015年7月12日","permalink":"/tags/google-compute-engine/","section":"Tags","summary":"","title":"Google Compute Engine"},{"content":"","date":"2015年7月12日","permalink":"/tags/tutorial/","section":"Tags","summary":"","title":"tutorial"},{"content":"Hello WordPress - Container Engine — Google Cloud Platform をなぞってみます。 Before You Begin で kubectl コマンドが使えるようにします。\nStep 1: Create your cluster # まずはクラスタの作成。クラスタはひとつのマスターインスタンスといくつかのワーカーノードで構成されます。これらは Compute Engine の仮想マシンです。いくつのワーカーノードを作成するかはクラスタ生成時に指定できます。 Hello Wordpress のチュートリアルではあまりリソースを必要としないので g1-small 1つで作成します。\n$ gcloud beta container clusters create hello-world --num-nodes 1 --machine-type g1-small Creating cluster hello-world...done. Created [https://container.googleapis.com/v1/projects/PROJECTID/zones/asia-east1-c/clusters/hello-world]. kubeconfig entry generated for hello-world. To switch context to the cluster, run $ kubectl config use-context gke_PROJECTID_asia-east1-c_hello-world NAME ZONE MASTER_VERSION MASTER_IP MACHINE_TYPE STATUS hello-world asia-east1-c 0.21.1 203.0.113.245 g1-small RUNNING Compure Engine のインスタンスを確認してみる\n$ gcloud compute instances list NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS gke-hello-world-9a51d286-node-5b6i asia-east1-c g1-small 10.240.147.88 203.0.113.237 RUNNING SSH で仮想マシンにログインできます\n$ gcloud compute ssh gke-hello-world-9a51d286-node-5b6i こんな motd が設定されてました\n$ cat /etc/motd === GCE Kubernetes node setup complete === $ docker ps を実行してみる。/pause ってなにしてるんだろ？\n$ sudo docker ps -a --no-trunc=true CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b8deba1de24333df5e2f2cd1fb445827ce676d475f479d450be341593818fb27 gcr.io/google_containers/skydns:2015-03-11-001 \u0026quot;/skydns -machines=http://localhost:4001 -addr=0.0.0.0:53 -domain=cluster.local.\u0026quot; About an hour ago Up About an hour k8s_skydns.bc99c0c0_kube-dns-v6-d2chz_kube-system_370cef57-2876-11e5-afa0-42010af0b13a_d6fe6fff 54727acc2b03937792e867a7f3988ae898ec6814216925a8763f34acddc63f45 gcr.io/google_containers/heapster:v0.15.0 \u0026quot;/heapster --source=kubernetes:''\u0026quot; About an hour ago Up About an hour k8s_heapster.c2816f0b_monitoring-heapster-v5-hi3cc_kube-system_370f7e40-2876-11e5-afa0-42010af0b13a_0bafdc86 aa5facac3aca1444fc37bb945621a06cf90c39940c6cbf16f291f84dca6304fb gcr.io/google_containers/kube2sky:1.11 \u0026quot;/kube2sky -domain=cluster.local\u0026quot; About an hour ago Up About an hour k8s_kube2sky.54c6a36_kube-dns-v6-d2chz_kube-system_370cef57-2876-11e5-afa0-42010af0b13a_690dff48 1756481deaab0382f6c3d0b83560d23e5590443a30c8814f5ba324072965ba48 gcr.io/google_containers/kube-ui:v1 \u0026quot;/kube-ui\u0026quot; About an hour ago Up About an hour k8s_kube-ui.484b832c_kube-ui-v1-hzr55_kube-system_371210d2-2876-11e5-afa0-42010af0b13a_5e76bbeb c433d0e2fadde42262161e06861c27b3bc7b2f18612f42c3484d0665ba0a7893 gcr.io/google_containers/etcd:2.0.9 \u0026quot;/usr/local/bin/etcd -listen-client-urls http://127.0.0.1:2379,http://127.0.0.1:4001 -advertise-client-urls http://127.0.0.1:2379,http://127.0.0.1:4001 -initial-cluster-token skydns-etcd\u0026quot; About an hour ago Up About an hour k8s_etcd.f9e6987c_kube-dns-v6-d2chz_kube-system_370cef57-2876-11e5-afa0-42010af0b13a_9ab851ad f9e9fc96d1b514605f048c332a90addd4d2250a36f8c5278a17345867368652a gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; About an hour ago Up About an hour k8s_POD.3b46e8b9_kube-ui-v1-hzr55_kube-system_371210d2-2876-11e5-afa0-42010af0b13a_1fd41fda 8dd19c0c51ffc9269d8d3b91af4c661e0cdfbc5f54dfdd4afcbce1c8093d20ed gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; About an hour ago Up About an hour k8s_POD.e4cc795_monitoring-heapster-v5-hi3cc_kube-system_370f7e40-2876-11e5-afa0-42010af0b13a_fbf88044 56561a56f421f92d98c96ae70178f54171d66466728d6a2811a5d2930f40b7e0 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; About an hour ago Up About an hour k8s_POD.8fdb0e41_kube-dns-v6-d2chz_kube-system_370cef57-2876-11e5-afa0-42010af0b13a_47ffc351 037e26826835806068f4714b62338ea41c4c3a36aa02a3c7813c1f281eabe068 gcr.io/google_containers/fluentd-gcp:1.8 \u0026quot;\\\\\u0026quot;/bin/sh -c '/usr/sbin/google-fluentd \\\\\u0026quot;$FLUENTD_ARGS\\\\\u0026quot; \u0026gt; /var/log/google-fluentd.log'\\\\\u0026quot;\u0026quot; About an hour ago Up About an hour k8s_fluentd-cloud-logging.7721935b_fluentd-cloud-logging-gke-hello-world-9a51d286-node-5b6i_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_17ce15ff 259b13a26f0ca23dfc30cd39db674e62da4578351948f50ab8ac8ff8440dc439 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; About an hour ago Up About an hour k8s_POD.e4cc795_fluentd-cloud-logging-gke-hello-world-9a51d286-node-5b6i_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_dc38b285 Step 2: Create your pod # tutum/wordpress イメージを使って Pod を作成する。 この wordpress のチュートリアルでは単一 container の Pod ですが、複数の container をまとめた pod を作成することもできます。実際の運用では複数の container をまとめたもののほうが主流となりそうです。\n$ kubectl run wordpress --image=tutum/wordpress --port=80 CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS wordpress wordpress tutum/wordpress run=wordpress 1 $ kubectl get rc wordpress CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS wordpress wordpress tutum/wordpress run=wordpress 1 $ sudo docker ps -a --no-trunc=true CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 364f8dc700b305473bedf1099ab85acbc714d3f19debb28b56703937e5029dbc tutum/wordpress:latest \u0026quot;/run.sh\u0026quot; 4 minutes ago Up 4 minutes k8s_wordpress.b91f4b7e_wordpress-jalj8_default_9ddba397-2883-11e5-afa0-42010af0b13a_c9481e56 477e90ae29c4dd75162b857f91829dc714fe224853e1cc24dd9bc45028a4ef72 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; 4 minutes ago Up 4 minutes k8s_POD.ef28e851_wordpress-jalj8_default_9ddba397-2883-11e5-afa0-42010af0b13a_529d3a4d b8deba1de24333df5e2f2cd1fb445827ce676d475f479d450be341593818fb27 gcr.io/google_containers/skydns:2015-03-11-001 \u0026quot;/skydns -machines=http://localhost:4001 -addr=0.0.0.0:53 -domain=cluster.local.\u0026quot; About an hour ago Up About an hour k8s_skydns.bc99c0c0_kube-dns-v6-d2chz_kube-system_370cef57-2876-11e5-afa0-42010af0b13a_d6fe6fff 54727acc2b03937792e867a7f3988ae898ec6814216925a8763f34acddc63f45 gcr.io/google_containers/heapster:v0.15.0 \u0026quot;/heapster --source=kubernetes:''\u0026quot; About an hour ago Up About an hour k8s_heapster.c2816f0b_monitoring-heapster-v5-hi3cc_kube-system_370f7e40-2876-11e5-afa0-42010af0b13a_0bafdc86 aa5facac3aca1444fc37bb945621a06cf90c39940c6cbf16f291f84dca6304fb gcr.io/google_containers/kube2sky:1.11 \u0026quot;/kube2sky -domain=cluster.local\u0026quot; About an hour ago Up About an hour k8s_kube2sky.54c6a36_kube-dns-v6-d2chz_kube-system_370cef57-2876-11e5-afa0-42010af0b13a_690dff48 1756481deaab0382f6c3d0b83560d23e5590443a30c8814f5ba324072965ba48 gcr.io/google_containers/kube-ui:v1 \u0026quot;/kube-ui\u0026quot; About an hour ago Up About an hour k8s_kube-ui.484b832c_kube-ui-v1-hzr55_kube-system_371210d2-2876-11e5-afa0-42010af0b13a_5e76bbeb c433d0e2fadde42262161e06861c27b3bc7b2f18612f42c3484d0665ba0a7893 gcr.io/google_containers/etcd:2.0.9 \u0026quot;/usr/local/bin/etcd -listen-client-urls http://127.0.0.1:2379,http://127.0.0.1:4001 -advertise-client-urls http://127.0.0.1:2379,http://127.0.0.1:4001 -initial-cluster-token skydns-etcd\u0026quot; About an hour ago Up About an hour k8s_etcd.f9e6987c_kube-dns-v6-d2chz_kube-system_370cef57-2876-11e5-afa0-42010af0b13a_9ab851ad f9e9fc96d1b514605f048c332a90addd4d2250a36f8c5278a17345867368652a gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; About an hour ago Up About an hour k8s_POD.3b46e8b9_kube-ui-v1-hzr55_kube-system_371210d2-2876-11e5-afa0-42010af0b13a_1fd41fda 8dd19c0c51ffc9269d8d3b91af4c661e0cdfbc5f54dfdd4afcbce1c8093d20ed gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; About an hour ago Up About an hour k8s_POD.e4cc795_monitoring-heapster-v5-hi3cc_kube-system_370f7e40-2876-11e5-afa0-42010af0b13a_fbf88044 56561a56f421f92d98c96ae70178f54171d66466728d6a2811a5d2930f40b7e0 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; About an hour ago Up About an hour k8s_POD.8fdb0e41_kube-dns-v6-d2chz_kube-system_370cef57-2876-11e5-afa0-42010af0b13a_47ffc351 037e26826835806068f4714b62338ea41c4c3a36aa02a3c7813c1f281eabe068 gcr.io/google_containers/fluentd-gcp:1.8 \u0026quot;\\\\\u0026quot;/bin/sh -c '/usr/sbin/google-fluentd \\\\\u0026quot;$FLUENTD_ARGS\\\\\u0026quot; \u0026gt; /var/log/google-fluentd.log'\\\\\u0026quot;\u0026quot; About an hour ago Up About an hour k8s_fluentd-cloud-logging.7721935b_fluentd-cloud-logging-gke-hello-world-9a51d286-node-5b6i_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_17ce15ff 259b13a26f0ca23dfc30cd39db674e62da4578351948f50ab8ac8ff8440dc439 gcr.io/google_containers/pause:0.8.0 \u0026quot;/pause\u0026quot; About an hour ago Up About an hour k8s_POD.e4cc795_fluentd-cloud-logging-gke-hello-world-9a51d286-node-5b6i_kube-system_d0feac1ad02da9e97c4bf67970ece7a1_dc38b285 Step 3: Allow external traffic # 外部からアクセスできるようにする。 Pod へのアクセスは標準では内部ネットワークからのアクセスしかできないので、外部からアクセスできるようにする。 kubectl の expose (さらす、露出する) コマンドで公開します。\n$ kubectl expose rc wordpress --create-external-load-balancer=true NAME LABELS SELECTOR IP(S) PORT(S) wordpress run=wordpress run=wordpress 80/TCP $ kubectl get services wordpress NAME LABELS SELECTOR IP(S) PORT(S) wordpress run=wordpress run=wordpress 10.191.254.84 80/TCP 203.0.113.9 $ kubectl get nodes NAME LABELS STATUS gke-hello-world-9a51d286-node-5b6i kubernetes.io/hostname=gke-hello-world-9a51d286-node-5b6i Ready この NAME の -node までが node-name-prefix で、firewall-rule の設定にこれを使います。\n$ gcloud compute firewall-rules create hello-world-80 --allow tcp:80 \\\\ --target-tags gke-hello-world-9a51d286-node Created \\[https://www.googleapis.com/compute/v1/projects/PROJECTID/global/firewalls/hello-world-80\\]. NAME NETWORK SRC_RANGES RULES SRC_TAGS TARGET_TAGS hello-world-80 default 0.0.0.0/0 tcp:80 gke-hello-world-9a51d286-node Wordpressにアクセス可能なことを確認 # $ kubectl get services wordpress NAME LABELS SELECTOR IP(S) PORT(S) wordpress run=wordpress run=wordpress 10.191.254.84 80/TCP http://203.0.113.9/ にアクセスして Wordpress が動いてることを確認します。\nクラスタを削除する # 使い終わったクラスタは削除しないとお金がかかるので忘れずに削除しましょう\n$ kubectl delete services wordpress services/wordpress $ kubectl stop rc wordpress replicationcontrollers/wordpress $ gcloud beta container clusters delete hello-world The following clusters will be deleted. - \\[hello-world\\] in \\[asia-east1-c\\] Do you want to continue (Y/n)? Y Deleting cluster hello-world...done. Deleted \\[https://container.googleapis.com/v1/projects/PROJECTID/zones/asia-east1-c/clusters/hello-world\\]. $ gcloud compute firewall-rules delete hello-world-80 The following firewalls will be deleted: - \\[hello-world-80\\] Do you want to continue (Y/n)? Y Deleted \\[https://www.googleapis.com/compute/v1/projects/PROJECTID/global/firewalls/hello-world-80\\]. 次は Guestbook のチュートリアルを試してみます\n","date":"2015年7月12日","permalink":"/2015/07/gke-hello-wordpress-tutorial/","section":"Posts","summary":"Hello WordPress - Container Engine — Google Cloud Platform をなぞってみます。 Before You Begin で kubectl コマンドが使えるようにします。 Step 1: Create your cluster # まずはクラスタの作成。クラスタはひとつのマスターイン","title":"GKE Hello Wordpress チュートリアルを試す"},{"content":"","date":"2015年5月22日","permalink":"/tags/sophos/","section":"Tags","summary":"","title":"sophos"},{"content":"","date":"2015年5月22日","permalink":"/tags/virus/","section":"Tags","summary":"","title":"virus"},{"content":"「ソフォス、無料版 Linux アンチウイルス製品を個人向けに提供開始」ということなので早速手元の Linux Mint にインストールしてみました。 環境はこれ\n$ cat /etc/lsb-release DISTRIB_ID=LinuxMint DISTRIB_RELEASE=17 DISTRIB_CODENAME=qiana DISTRIB_DESCRIPTION=\u0026quot;Linux Mint 17 Qiana\u0026quot; $ cat /etc/os-release NAME=\u0026quot;Ubuntu\u0026quot; VERSION=\u0026quot;14.04.1 LTS, Trusty Tahr\u0026quot; ID=ubuntu ID_LIKE=debian PRETTY_NAME=\u0026quot;Ubuntu 14.04.1 LTS\u0026quot; VERSION_ID=\u0026quot;14.04\u0026quot; HOME_URL=\u0026quot;http://www.ubuntu.com/\u0026quot; SUPPORT_URL=\u0026quot;http://help.ubuntu.com/\u0026quot; BUG_REPORT_URL=\u0026quot;http://bugs.launchpad.net/ubuntu/\u0026quot; $ uname -a Linux vaio 3.13.0-24-generic #47-Ubuntu SMP Fri May 2 23:30:00 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux Sophos のサイトからインストーラ「sav-linux-free-9.9.tgz」をダウンロードします。 中身はこんな感じ\n$ tar ztf sav-linux-free-9.9.tgz sophos-av/ sophos-av/sav.tar sophos-av/talpa.tar sophos-av/uncdownload.tar sophos-av/install.sh それではインスールします\n$ cd sophos-av $ sudo ./install.sh Sophos Anti-Virus ================= Copyright (c) 1989-2015 Sophos Limited. All rights reserved. Sophos Anti-Virus インストーラへようこそ。Sophos Anti-Virus には、オンアクセススキャナ、 オンデマンドコマンドラインスキャナ、Sophos Anti-Virus デーモン、および Sophos Anti-Virus GUI があります。 オンアクセススキャナ ファイルがアクセスされると検索し、未感染の場合のみアクセスを許可 オンデマンドスキャナ コンピュータの全体または一部を直ちに検索 Sophos Anti-Virus デーモン Sophos Anti-Virus にコントロール、ログ、メール警告機能を 提供するバックグラウンドプロセス Sophos Anti-Virus GUI Web ブラウザ経由でアクセスするユーザーインターフェース 「Enter」キーを押して、使用許諾契約書を表示してください。そして、 を押してスクロールダウン してください。 (使用許諾契約書は省略) ライセンス内容に同意しますか？ はい(Y)/いいえ(N) [N] \u0026gt; Y Sophos Anti-Virus のインストール先を指定してください。 [/opt/sophos-av] \u0026gt; オンアクセス検索を有効にしますか？ はい(Y)/いいえ(N) [Y] \u0026gt; パスワードが入力されなかったため、Sophos Anti-Virus GUI を無効にしています。 有効にするには、/opt/sophos-av/bin/savsetup を実行してください。 ソフォスは、Sophos Anti-Virus での自動アップデートの設定をお勧めします。 ソフォスから直接アップデートしたり（要アカウント情報）、自社サーバー（ディレクトリや Web サイト（アカウント情報が必要な場合もあります））からアップデートすることができます。 オートアップデートの種類を選択してください: ソフォス(s)/自社サーバー(o)/なし(n) [s] \u0026gt; ソフォスから直接アップデートしています。 SAV for Linux の無償バージョン (f) と サポート対応付きバージョン (s) のどちらをインストールしますか？ [s] \u0026gt; f Sophos Anti-Virus for Linux の無償バージョンに対して、サポート対応は提供されていません。 無償ツールのフォーラムは次のサイトを参照してください。http://openforum.sophos.com/ ソフォスからアップデートを行うためにプロキシが必要ですか？ はい(Y)/いいえ(N) [N] \u0026gt; Sophos Anti-Virus をインストールしています.... 適切なカーネルサポートを選択しています... Sophos Anti-Virus が起動すると、カーネルサポートを見つけるためアップデートします。 これによって大幅な遅れが発生することがあります。 Sophos Anti-Virus は、インストール終了後、開始されました。 Adding system startup for /etc/init.d/sav-protect ... /etc/rc0.d/K20sav-protect -\u0026gt; ../init.d/sav-protect /etc/rc1.d/K20sav-protect -\u0026gt; ../init.d/sav-protect /etc/rc6.d/K20sav-protect -\u0026gt; ../init.d/sav-protect /etc/rc2.d/S20sav-protect -\u0026gt; ../init.d/sav-protect /etc/rc3.d/S20sav-protect -\u0026gt; ../init.d/sav-protect /etc/rc4.d/S20sav-protect -\u0026gt; ../init.d/sav-protect /etc/rc5.d/S20sav-protect -\u0026gt; ../init.d/sav-protect Removing any system startup links for /etc/init.d/sav-rms ... Removing any system startup links for /etc/init.d/sav-web ... インストールが完了しました。 ご使用のコンピュータは Sophos Anti-Virus で保護されるようになりました。 警告: ソフォスがバイナリカーネルモジュールを提供していないカーネル環境で Sophos Anti-Virus を稼動しています。したがって、カーネルモジュールはローカルでコンパイル されました。対応するプラットフォームとカーネルについてはサポートデータベースの文章 14377 を参照してください。 ```インストールできました、プロセスをみるとこんな感じ``` $ pstree -Apa 3710 savd,3710 etc/savd.cfg |-savscand,5948 --incident=unix://tmp/incident... | |-{savscand},5949 | |-{savscand},5950 | |-{savscand},5951 | |-{savscand},5952 | |-{savscand},5953 | |-{savscand},5954 | |-{savscand},5960 | |-{savscand},5961 | |-{savscand},5962 | |-{savscand},5963 | |-{savscand},5964 | |-{savscand},5965 | \\`-{savscand},5966 |-savscand,5959 --incident=unix://tmp/incident socketpair://45/46 ... | |-{savscand},5967 | |-{savscand},5968 | |-{savscand},5969 | |-{savscand},5970 | |-{savscand},5971 | |-{savscand},5980 | |-{savscand},5981 | |-{savscand},5982 | |-{savscand},5983 | |-{savscand},5984 | |-{savscand},5985 | \\`-{savscand},5986 |-{savd},3713 |-{savd},3714 |-{savd},3716 \\`-{savd},5947 ではテスト用の Virus である Eicar を試してみます。 次の文字列を vi で eicar というファイルとして保存してみます。\nX5O!P%@AP[4\\PZX54(P^)7CC)7}$EICAR-STANDARD-ANTIVIRUS-TEST-FILE!$H+H* すると次のようなウインドウが出てきました。\n Linux Mint (Cinnamon) のファイルマネージャである Nemo で eicar ファイルのあるフォルダを開いても表示されます。 インストール時に GUI が無効になっていると表示されていたので、これを有効にしてみます。\nパスワードが入力されなかったため、Sophos Anti-Virus GUI を無効にしています。 有効にするには、/opt/sophos-av/bin/savsetup を実行してください。 $ sudo /opt/sophos-av/bin/savsetup Sophos Anti-Virus 対話型 環境設定へようこそ [1] 自動アップデートの環境設定 [2] Sophos Anti-Virus GUI 環境設定 [q] 終了 実行する操作を選択してください。 [1] \u0026gt; 2 Sophos Anti-Virus GUI デーモンを有効にしますか？ はい(Y)/いいえ(N) [N] \u0026gt; Y Sophos Anti-Virus GUI デーモンを設置するポート番号を入力してください。 [8081] \u0026gt; Sophos Anti-Virus GUI デーモンへのアクセスに使用するユーザー名を入力してください。 [admin] \u0026gt; Sophos Anti-Virus GUI デーモンへのアクセスに使用するパスワードを入力してください。 \u0026gt; 同じパスワードを再入力してください。 \u0026gt; Adding system startup for /etc/init.d/sav-web ... /etc/rc0.d/K20sav-web -\u0026gt; ../init.d/sav-web /etc/rc1.d/K20sav-web -\u0026gt; ../init.d/sav-web /etc/rc6.d/K20sav-web -\u0026gt; ../init.d/sav-web /etc/rc2.d/S20sav-web -\u0026gt; ../init.d/sav-web /etc/rc3.d/S20sav-web -\u0026gt; ../init.d/sav-web /etc/rc4.d/S20sav-web -\u0026gt; ../init.d/sav-web /etc/rc5.d/S20sav-web -\u0026gt; ../init.d/sav-web 有効になりました。 http://localhost:8081/ にアクセスします。次のような管理ツールでした。\n      自動駆除、自動削除を有効にしてみる GUI の「検索」タブから「感染アイテムを自動駆除する」、「感染アイテムを自動削除する」を有効にした後、再度 eicar にアクセスしてみると次のように表示され削除されました。\n ESET も Linux のデスクトップ版を提供予定のようで現在先行評価版（無料）が公開されています。法人向けなので正規版は当然有償でしょう。 ESET NOD32アンチウイルス for Linux Desktop V4.0 先行評価版プログラム公開のご案内\n","date":"2015年5月22日","permalink":"/2015/05/using-sophos-anti-virus-for-linux/","section":"Posts","summary":"「ソフォス、無料版 Linux アンチウイルス製品を個人向けに提供開始」ということなので早速手元の Linux Mint にインストールしてみました。 環境はこれ $ cat /etc/lsb-release DISTRIB_ID=LinuxMint DISTRIB_RELEASE=17 DISTRIB_CODENAME=qiana","title":"無料版の公開された Sophos Anti-Virus for Linux を Linux Mint で試す"},{"content":"画像を表示場所に合わせていろいろなサイズで表示したいという要件は良くあるようで「画像変換Night」というイベントが開催されるほどのようです。\n ngx_small_light (nginx) mod_small_light (Apache) Graid (golang) go-thumber (golang)  今回は、こういった用途でもつかえる thumbor を紹介します。\n https://github.com/thumbor/thumbor https://github.com/thumbor/thumbor/wiki  thumbor の概要 # thumbor は python で書かれた画像のオンデマンド変換 HTTP サーバーです (Python 2.7, 3.2 をサポート)\nTornado という非同期処理のフレームワークが使われています。画像変換は Proxy サーバーのような動作をするので非同期処理が良いようです。\n 画像のアップロードにも対応しています 画像変換ライブラリ（エンジン）を次の3つから選べます  PIL (Python Image Library) GraphicsMagick (pgmagick) OpenCV   多くのフィルターがかけられます OpenCV を使えば顔認識などでうまいことクロップできます セキュリティ対策できています  クライアントライブラリなども各言語用がそろっています ChangeLog 的なものがないのがちと辛いですが、結構更新されています。 今回 thumbor-url のバグを報告しましたがすぐに対応してもらえてました。\nthumbor は誰が使っている？ # Who\u0026rsquo;s using it に書かれていますが私のよく知らないサイトですね\u0026hellip;\n実行例 # オリジナル画像として これ (2592x1936) を使います。 まずは縦横比を維持したままで、縦と横どちらかを指定のサイズに合わせる方法です。\n縦横比を維持して横幅を 300px にする # http://thumbor/unsafe/300x0/https://blog.1q77.com/wp-content/uploads/2015/04/IMG_0516.jpg 縦横比を維持して高さを 300px にする # http://thumbor/unsafe/0x300/https://blog.1q77.com/wp-content/uploads/2015/04/IMG_0516.jpg\n縦横比を維持する場合には後で出てくる fit-in が便利です。\n縦横を指定のサイズに切り詰める 300x100 # http://thumbor/unsafe/300x100/https://blog.1q77.com/wp-content/uploads/2015/04/IMG_0516.jpg\nサイズ変更に加え、グレイスケールに変換する # http://thumbor/unsafe/400x200/filters:grayscale()/https://blog.1q77.com/wp-content/uploads/2015/04/IMG_0516.jpg\n輝度を変える # http://thumbor/unsafe/200x200/filters:brightness(-20)/https://blog.1q77.com/wp-content/uploads/2015/04/IMG_0516.jpg\nhttp://thumbor/unsafe/200x200/filters:brightness(40)/https://blog.1q77.com/wp-content/uploads/2015/04/IMG_0516.jpg\n指定の場所を切り取る # http://thumbor/unsafe/1520x330:1750x650/https://blog.1q77.com/wp-content/uploads/2015/04/IMG_0516.jpg\nこんな感じでオンデマンドに画像のリサイズやクロップ、フィルタ適用が可能です。 URL 体系はこちらにあります。 https://github.com/thumbor/thumbor/wiki/Usage\nURLの生成 # アプリで動的にリンクを作成する場合であれば、それぞれの言語のライブラリのドキュメントを参照してください。 thumbor をインストールするとURL生成のためのコマンドラインツールもインストールされています。 /opt/thumbor に virtualenv でインストールしたとすると /opt/thumbor/bin/thumbor-url にあります。 4.9 以降、構成変更があって thumbor-url が期待通りに動作しなくなっていました。リリースされていたら 5.0.2 以降を使ってください。 URL は体系は https://github.com/thumbor/thumbor/wiki/Usage にあります。\n$ /opt/thumbor/bin/thumbor-url -h Usage: thumbor-url [options] imageurl or type thumbor-url -h (--help) for help Options: --version show program's version number and exit -h, --help show this help message and exit -l KEY_FILE, --key_file=KEY_FILE The file to read the security key from [default: none]. -k KEY, --key=KEY The security key to encrypt the url with [default: none]. -w WIDTH, --width=WIDTH The target width for the image [default: 0]. -e HEIGHT, --height=HEIGHT The target height for the image [default: 0]. -n, --fitin Indicates that fit-in resizing should be performed. -m, --meta Indicates that meta information should be retrieved. --adaptive Indicates that adaptive fit-in cropping should be used. --full Indicates that fit-full cropping should be used. -s, --smart Indicates that smart cropping should be used. -t, --trim Indicate that surrounding whitespace should be trimmed. -f, --horizontal-flip Indicates that the image should be horizontally flipped. -v, --vertical-flip Indicates that the image should be vertically flipped. -a HALIGN, --halign=HALIGN The horizontal alignment to use for cropping [default: center]. -i VALIGN, --valign=VALIGN The vertical alignment to use for cropping [default: middle]. --filters=FILTERS Filters to be applied to the image, e.g. brightness(10) [default: ]. -o, --old-format Indicates that thumbor should generate old-format urls [default: False]. -c CROP, --crop=CROP The coordinates of the points to manual cropping in the format leftxtop:rightxbottom (100x200:400x500) [default: none]. resize # -w WIDTH と -e HEIGHT で縦横のピクセル数を指定します。 これだけでだと縦と横の大きい方のサイズに拡大縮小され、小さい方に合わせてはみ出した部分が削れれます。\n$ thumbor-url -w 100 -e 100 https://blog.1q77.com/wp-content/uploads/2015/04/IMG_0516.jpg fit-in # --fitin をつけると指定した縦横からはみ出ないように縦横比を維持したままリサイズされます。 800x600 の画像を fit-in で 300x300 にすると 300x225 にリサイズされます。 ただし、fill フィルターを追加すると 300x300 になり、上下の空きスペースは指定の色で埋められます。 filters:fill(red) で上下に赤いエリアがつきます。\nfull-fit-in # --fitin に --full を追加すると、指定のサイズの小さい方に合わせてリサイズし、縦横比を維持するため大きい方は指定のサイズを超えます。 URL の fit-in/ 部分が full-fit-in/ になります。サーバー側は機能します。 800x600 の画像を full-fit-in で 300x300 にすると 400x300 になります。\ncrop # -c W1xH1:W2xH2 画像内の2点を指定して四角く切り抜きます。\ncrop and resize # -c と -w, -e を組み合わせると、crop で切り抜いた部分をさらにリサイズします。\ntrim # 画像の縁の空白を削ります。\nfilters # --filters=FILTERS でグレースケールや輝度、コントラストの変更、ぼかし、回転などが利用できます。 Available Filters --filters='blur(6)' この --filters オプションも 4.9 以降では正しく動作しません。（5.0.0現在） filters と複数形になっているように URL では filters:grayscale():blur(6):fill(red) と複数組み合わせることができます。\n-a HALIGN, \u0026ndash;halign=HALIGN # サイズを切り詰める際に左右、中央のどこに寄せるかを指定します\n-i VALIGN, \u0026ndash;valign=VALIGN # サイズを切り詰める際に上下、中央のどこに寄せるかを指定します\nOpenCV を使った顔検出 # opencv-engine を使った場合、focal-point detection が使えます。「フォーカルポイント (wikipedia)」 これは画像のリサイズ、切り取りの際に顔や注目ポイントを見つけ出して、いい感じに切り出してくれる機能です。 「Facial Detection」と「Feature Detection」があり、まず顔検出を試み、見つからなければ特徴から良さげなところを切り取ってくれます。 使い方は簡単、URLに smart を入れるだけです。詳細は Detection algorithms で。 こちらも確認してください\n Enabling detectors Available detectors  顔検出は非常に重い処理であるため、Lazy detection が推奨されています。処理を Redis や AWS SQS の Queue に入れてひとまず顔検出なしで結果を返した後に処理します。この場合、画像のキャッシュに気をつける必要があります。\nSecurity # thumbor-url を試した方は気になったかもしれませんが、このコマンドで生成される path の先頭に謎のハッシュ値が含まれています。\n# /opt/thumbor/bin/thumbor-url -w 100 -e 50 http://example.com/example.jpg URL: /wFTrGO1ExNalW1qwgnQC5HZijv4=/100x50/http://example.com/example.jpg /wFTrGO1ExNalW1qwgnQC5HZijv4=/100x50/http://example.com/example.jpg これが thumbor のセキュリティ機能です。thumbor の設定ファイルに SECURITY_KEY という設定があり、これと /100x50/http://example.com/example.jpg を使って SHA1-HMAC を計算した値です。\nbase64.urlsafe_b64encode(hmac.new(SECURITY_KEY, unicode(URL).encode(\u0026#39;utf-8\u0026#39;), hashlib.sha1).digest()) サーバーはこの値を確認して不正なリクエストを処理しないようにします。 これまで unsafe という特殊な値を使っていましたが、本番サーバーでは ALLOW_UNSAFE_URL を False にして無効にするべきです。\nALLOW_UNSAFE_URL = False SECURITY_KEY = 'd4dHBbcGDJf6GetnHNRc5MBNntAEgGC4' thumbor-urlでは-lか-k` で指定します。\n -l KEY_FILE, --key_file=KEY_FILE The file to read the security key from [default: none]. -k KEY, --key=KEY The security key to encrypt the url with [default: none]. https://github.com/thumbor/thumbor/wiki/Security 設定ファイルの ALLOWED_SOURCES に元画像サーバーのドメインの許可リストを設定し、特定のサーバーの画像のみを対象とすることもできます。\n続きは後日 # pgmagick の compile で異常にメモリを使われ OOM Killer で異常終了していることになかなか気づかずハマってしまったり、thumbor-url のバグを調べたり、Python 2.6 のサポートが終わってたりしてここまで異常に時間がかかってしまい、疲れたので続きはまた後日書くことにします。 画像のアップロードとかキャッシュや結果の保存、セットアップや構成などについて調べて書ければと。\n試し方 # Ansible の Playbook を用意したので CentOS / RHEL なかたはこれで構築できます。 debian や ubuntu なら apt で入れられるらしい。 ansible でセットアップすると supervisor 管理下の thumbor へ nginx から proxy する環境が構築されます。 unsafe でアクセスできます。必要であれば thumbor.yml の thumbor_allow_unsafe_url を \u0026quot;False\u0026quot; にしてください。 thumbor_secure_key も変更してください。 次はいつ書けるかな\n","date":"2015年5月21日","permalink":"/2015/05/using-thumbor-part1/","section":"Posts","summary":"画像を表示場所に合わせていろいろなサイズで表示したいという要件は良くあるようで「画像変換Night」というイベントが開催されるほどのようです","title":"オンデマンド画像変換サーバー thumbor を使う - その1"},{"content":"","date":"2015年5月21日","permalink":"/tags/%E7%94%BB%E5%83%8F%E5%A4%89%E6%8F%9B/","section":"Tags","summary":"","title":"画像変換"},{"content":"au iPhone5 から MVNO に切り替える際にゲームしないし、LG G2 mini 程度で十分だろうと思って切り替えた訳ですが、実際に使ってみると電源と音量ボタンがおかしなところについてるし、もっさりしてるし、メモリが1Gしかなくて常駐アプリはどうしても必要なものに制限しないとアプリが裏で再起動しまくる状態でした。Kindle も Facebook アプリもアンインストールしました。 もっとも苦痛だったのが地下鉄駅構内を歩いている時、地下鉄で移動中に音楽を聞いているとそれが頻繁に停止してしまうことでした。電波を探す処理と音楽再生との問題だとか。 そうそう、G2 mini はクリップボードの拡張がしてあって、文字列をコピーするとそれが右側からひょこっとメモ用紙みたいなのが出てきて表示されるのです。Keepass とかでパスワードをコピーしてもそれが出てきて、「ふざけんな」ってなるんです。そしてその機能を無効にする方法がない\u0026hellip; これはサテライトオフィスのセキュリティブラウザってのを使う場合にも邪魔になります。あのアプリはコピペ禁止のためにかなり頻繁にクリップボードにアクセスするっぽいのですが、そのせいでずっと右側にピコピコメモ用紙が出たり引っ込んだりするんです。ちょーうざい。LG の端末ってどれもこうなのかな？\nLG G2 mini の気に入らないところまとめ #  動作がもっさり メモリが足りない 内臓ストレージが小さい 地下鉄で音楽再生が止まる クリップボードの拡張が気に入らない カメラの性能がヒドイ  どの機種に乗り換えるか # それでやっぱり買い換えようかなぁと候補探しになりました。\n Nexus 5 Nexus 6 iPhone 6 iPhone 5S Xperia Z3 Xperia Z3 Compact VAIO Phone Xperia J1 Compact ZenFone2 Ascend Mate7  あたりが候補。 G2 mini を買ってしまった後に iPhone, Nexus 6 はちょっと高すぎるので除外。 VAIO Phone は発表をまだかまだかと待っていましたが、大変残念な仕様だったので却下。 Xperia J1 Compact はテザリングも可能であるし迷いましたが、Xperia Z3 でもテザリングを可能にする方法が見つかり、その上安いということで Xperia は J1 Compact よりは Z3, Z3 Compact に軍配。 ZenFone2 も期待ほど安くなくって見送り。 後は Ascend Mate7, Nexus 5 との比較ですが、結局は docomo の施策によって一気に中古価格の下がった Xperia Z3 にしました。Compact にしなかったのは画面は大きいほうが Nexus 7 も不要になるかなというのと、今後しばらく使うことを考えればメモリは大いに越したことはないかなと。そうそう、Walkman アプリもあることだし、さすがに地下鉄で音楽再生が止まってしまうことも無いだろうというのもありました。そして、ソニーのカメラにも期待していました。 そして4月7日に Xperia Z3 を Amazon 経由で購入。37,970円でした。でもこのお店、たしか楽天にもあったからそっちで買えば楽天ポイントがもらえたのに\u0026hellip;とすこし後悔。 BIC SIM (IIJmio) の Micro SIM を使っていたので Nano SIM への交換が必要で、これは店頭で交換ということができないので1日ほど SIM の使えない期間が発生しました。 事前にいついつから止まりますって連絡が来るのかと思いきや突然使えなくなって「おやっ？」って感じでした。その後にお届け予定の通知メールが来た。\n使ってみて # 使ってみてどうかというと、最初は docomo との契約が無いため docomo ID なんてもらえないんだろうと思ってました。そして、そのせいでずっと docomo アプリの更新に失敗しましたという通知が定期的にでるのは止めようがないんだとあきらめていました。が、ググってみたら docomo との契約がなくても docomo ID は普通にゲット出来ました。これで無事 docomo アプリの更新も行えるようになりました。 docomo ID の取得はこちらから https://id.smt.docomo.ne.jp/cgi8/id/register docomo や Sony 製の不要なアプリがガンガン削除、無効化しようとしていましたが、ちょいちょいアプリの異常終了が出るので諦めました。メモリは足りてるし放置です。 G2 mini と比べてどうかというと、G2 mini で不満だったもっさり感はなくなりましたし、地下鉄でも音楽再生は止まりません。ただし、アプリが固まったり、異常終了する回数は Xperia の方が多いですね。docomo や Sony の変な機能・アプリのせいでしょうか？ Lollipop が降ってくるのはずっと先のことでしょうね。Android M の話が聞こえてきているのに。 Android もだいぶましになったろうと思っていましたがやっぱり iPhone がいいかなぁ。\n期待のカメラ画質 # 期待していたカメラですが、これは好みの問題なのかもしれませんが私は iPhone の方が好きですね。iPhone も Sony のイメージセンサーらしいのにね。\n通話アプリ # 以前、楽天でんわを使っていましたが、IIJmio なので「みおふぉんダイアル」に切り替えてみました\n    アクセサリ # 薄い透明なケースと、USB端子の蓋を充電の度に開け閉めしなくてすむようにマグネット式の充電端子を使うためのケーブルを買って使ってます。\n","date":"2015年4月30日","permalink":"/2015/04/switch-from-lg-g2-mini-to-so-01g/","section":"Posts","summary":"au iPhone5 から MVNO に切り替える際にゲームしないし、LG G2 mini 程度で十分だろうと思って切り替えた訳ですが、実際に使ってみると電源と音量ボタンがおかしなと","title":"LG G2 mini をやめて Xperia Z3 SO-01G にした (MVNO)"},{"content":"","date":"2015年4月4日","permalink":"/tags/cassandra/","section":"Tags","summary":"","title":"Cassandra"},{"content":"お手軽だということで Redis をストレージエンジンとした Zipkin サーバーを使っていましたが、Redis ではメモリに収める必要があり、データ容量的に足りなくなったため Cassandra を使うことにしました。 ここでは CentOS 6.6 へ Cassandra と Zipkin サーバーをセットアップする手順を記しておきます。Zipkin をアプリに組み込む方法には触れません。 Zipkin とは Twitter が開発している分散トレーシングツールで、今ちょっとバズってるマイクロサービスを進めていくとどのサービスのどこが遅いのかを知る必要がでてきて、リクエスト毎にアプリをまたいだトレースができるツールです。「LINEのマイクロサービス環境における分散トレーシング « LINE Engineers' Blog」を読むとわかりやすいかもしれません。（お金があって対応言語などがマッチしていれば AppDynamics が良いと思いますけどね）\n構成はサーバー4台それぞれに Zipkin と Cassandra をインストール。Zipkin Collector へのアクセスを HAProxy で4台に振り分ける。keepalived でうち1台に VIP をもたせる。(Ansible の Playbook 貼れば終わりなんだけど\u0026hellip;)\nZipkin+Cassandra  Redis の時に Zipkin Collector が過負荷でどうしようもなかったので、Zipkin サーバーも複数並べたくて、どうせならということで Cassandra サーバーに相乗りさせてみましたが、Cassandra Cluster とは別にしても問題ありません。特に既に Cassandra Cluster が存在する場合など。また、Zipkin Query や Zipkin Web はブラウザからアクセスするサーバーなので沢山並べる必要もありません。今回はどれが故障しても慌てなくて済むように同じ構成のサーバーを並べています。\nCassandra のセットアップ # Cassandra のインストールは DataStax のパッケージを用います。このパッケージには /etc/security/limits.d/cassandra.conf も含まれていて limit まわりも適切に設定してくれます。こういうの重要です。\n[datastax]  name = DataStax Repo for Apache Cassandra baseurl = http://rpm.datastax.com/community enabled = 0 gpgcheck = 0 $ sudo yum install --enablelrepo=datastax dsc20 cassandra20 vm.max_map_count を変更した方が良いようです。\n$ echo 'vm.max_map_count = 131072' | sudo tee -a /etc/sysctl.conf $ sudo sysctl -p Java の Heap Size は /etc/cassandra/default.conf/cassandra-env.sh で設定します。 MAX_HEAP_SIZE と HEAP_NEWSIZE ですが、未設定の場合は搭載メモリから自動で計算されます。 /etc/cassandra/default.conf/cassandra.yaml で Cluster Name と seed サーバーを設定します。4台のうち2台を seed としています。\ncluster_name:\u0026#39;Zipkin Cluster\u0026#39;seed_provider:- class_name:org.apache.cassandra.locator.SimpleSeedProviderparameters:- seeds:\u0026#34;192.168.1.101,192.168.1.103\u0026#34;listen_address:192.168.1.xrpc_address:0.0.0.0これで4台で sudo service cassandra start すれば cluster が構成されます。\nZipkin 用スキーマの作成 # https://github.com/twitter/zipkin/blob/master/zipkin-cassandra/src/schema/cassandra-schema.txt にスキーマの定義ファイルがあるのでダウンロードし、\n$ cassandra-cli -h localhost -f cassandra-schema.txt とすることで作成できます。 が、cassandra-cli は deprecated とありますので cqlsh で作ったほうが良いのかも。 cqlsh で DESCRIBE KEYSPACE \u0026quot;Zipkin\u0026quot;; と打てば定義が確認できます。 cassandra-schema.txt を流し込んだだけだと Replication Factor が 1 で4サーバーあっても1台停止すると使えなくなってしまうので 2 に変更します。ついでに class も SimpleStrategy にしてしまいます。複数 DC じゃないので SimpleStrategy。\ncqlsh\u0026gt; ALTER KEYSPACE \u0026quot;Zipkin\u0026quot; WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 2 }; ALTER KEYSPACE したら nodetool repair コマンドを実行する必要があります。 cassandra-cli ではなく cqlsh を使ってスキーマを作成する場合は DESCRIBE KEYSPACE で表示される文を使えばできそうな感じです。\ncqlsh\u0026gt; DESCRIBE KEYSPACE \u0026quot;Zipkin\u0026quot;; CREATE KEYSPACE \u0026quot;Zipkin\u0026quot; WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': '2' }; USE \u0026quot;Zipkin\u0026quot;; CREATE TABLE \u0026quot;AnnotationsIndex\u0026quot; ( key blob, column1 bigint, value blob, PRIMARY KEY ((key), column1) ) WITH COMPACT STORAGE AND bloom_filter_fp_chance=0.010000 AND caching='KEYS_ONLY' AND comment='' AND dclocal_read_repair_chance=0.100000 AND gc_grace_seconds=864000 AND index_interval=128 AND read_repair_chance=0.000000 AND replicate_on_write='true' AND populate_io_cache_on_flush='false' AND default_time_to_live=0 AND speculative_retry='NONE' AND memtable_flush_period_in_ms=0 AND compaction={'class': 'SizeTieredCompactionStrategy'} AND compression={'sstable_compression': 'LZ4Compressor'}; CREATE TABLE \u0026quot;Dependencies\u0026quot; ( key blob, column1 bigint, value blob, PRIMARY KEY ((key), column1) ) WITH COMPACT STORAGE AND bloom_filter_fp_chance=0.010000 AND caching='KEYS_ONLY' AND comment='' AND dclocal_read_repair_chance=0.100000 AND gc_grace_seconds=864000 AND index_interval=128 AND read_repair_chance=0.000000 AND replicate_on_write='true' AND populate_io_cache_on_flush='false' AND default_time_to_live=0 AND speculative_retry='NONE' AND memtable_flush_period_in_ms=0 AND compaction={'class': 'SizeTieredCompactionStrategy'} AND compression={'sstable_compression': 'LZ4Compressor'}; CREATE TABLE \u0026quot;DurationIndex\u0026quot; ( key blob, column1 bigint, value blob, PRIMARY KEY ((key), column1) ) WITH COMPACT STORAGE AND bloom_filter_fp_chance=0.010000 AND caching='KEYS_ONLY' AND comment='' AND dclocal_read_repair_chance=0.100000 AND gc_grace_seconds=864000 AND index_interval=128 AND read_repair_chance=0.000000 AND replicate_on_write='true' AND populate_io_cache_on_flush='false' AND default_time_to_live=0 AND speculative_retry='NONE' AND memtable_flush_period_in_ms=0 AND compaction={'class': 'SizeTieredCompactionStrategy'} AND compression={'sstable_compression': 'LZ4Compressor'}; CREATE TABLE \u0026quot;ServiceNameIndex\u0026quot; ( key blob, column1 bigint, value blob, PRIMARY KEY ((key), column1) ) WITH COMPACT STORAGE AND bloom_filter_fp_chance=0.010000 AND caching='KEYS_ONLY' AND comment='' AND dclocal_read_repair_chance=0.100000 AND gc_grace_seconds=864000 AND index_interval=128 AND read_repair_chance=0.000000 AND replicate_on_write='true' AND populate_io_cache_on_flush='false' AND default_time_to_live=0 AND speculative_retry='NONE' AND memtable_flush_period_in_ms=0 AND compaction={'class': 'SizeTieredCompactionStrategy'} AND compression={'sstable_compression': 'LZ4Compressor'}; CREATE TABLE \u0026quot;ServiceNames\u0026quot; ( key blob, column1 blob, value blob, PRIMARY KEY ((key), column1) ) WITH COMPACT STORAGE AND bloom_filter_fp_chance=0.010000 AND caching='KEYS_ONLY' AND comment='' AND dclocal_read_repair_chance=0.100000 AND gc_grace_seconds=864000 AND index_interval=128 AND read_repair_chance=0.000000 AND replicate_on_write='true' AND populate_io_cache_on_flush='false' AND default_time_to_live=0 AND speculative_retry='NONE' AND memtable_flush_period_in_ms=0 AND compaction={'min_sstable_size': '52428800', 'class': 'SizeTieredCompactionStrategy'} AND compression={'chunk_length_kb': '64', 'sstable_compression': 'LZ4Compressor'}; CREATE TABLE \u0026quot;ServiceSpanNameIndex\u0026quot; ( key blob, column1 bigint, value blob, PRIMARY KEY ((key), column1) ) WITH COMPACT STORAGE AND bloom_filter_fp_chance=0.010000 AND caching='NONE' AND comment='' AND dclocal_read_repair_chance=0.100000 AND gc_grace_seconds=864000 AND index_interval=128 AND read_repair_chance=0.000000 AND replicate_on_write='true' AND populate_io_cache_on_flush='false' AND default_time_to_live=0 AND speculative_retry='NONE' AND memtable_flush_period_in_ms=0 AND compaction={'min_sstable_size': '52428800', 'class': 'SizeTieredCompactionStrategy'} AND compression={'chunk_length_kb': '64', 'sstable_compression': 'LZ4Compressor'}; CREATE TABLE \u0026quot;SpanNames\u0026quot; ( key blob, column1 blob, value blob, PRIMARY KEY ((key), column1) ) WITH COMPACT STORAGE AND bloom_filter_fp_chance=0.010000 AND caching='NONE' AND comment='' AND dclocal_read_repair_chance=0.100000 AND gc_grace_seconds=864000 AND index_interval=128 AND read_repair_chance=0.000000 AND replicate_on_write='true' AND populate_io_cache_on_flush='false' AND default_time_to_live=0 AND speculative_retry='NONE' AND memtable_flush_period_in_ms=0 AND compaction={'min_sstable_size': '52428800', 'class': 'SizeTieredCompactionStrategy'} AND compression={'chunk_length_kb': '64', 'sstable_compression': 'LZ4Compressor'}; CREATE TABLE \u0026quot;TopAnnotations\u0026quot; ( key blob, column1 bigint, value blob, PRIMARY KEY ((key), column1) ) WITH COMPACT STORAGE AND bloom_filter_fp_chance=0.010000 AND caching='KEYS_ONLY' AND comment='' AND dclocal_read_repair_chance=0.100000 AND gc_grace_seconds=864000 AND index_interval=128 AND read_repair_chance=0.000000 AND replicate_on_write='true' AND populate_io_cache_on_flush='false' AND default_time_to_live=0 AND speculative_retry='NONE' AND memtable_flush_period_in_ms=0 AND compaction={'class': 'SizeTieredCompactionStrategy'} AND compression={'sstable_compression': 'LZ4Compressor'}; CREATE TABLE \u0026quot;Traces\u0026quot; ( key blob, column1 blob, value blob, PRIMARY KEY ((key), column1) ) WITH COMPACT STORAGE AND bloom_filter_fp_chance=0.010000 AND caching='KEYS_ONLY' AND comment='' AND dclocal_read_repair_chance=0.100000 AND gc_grace_seconds=864000 AND index_interval=128 AND read_repair_chance=0.000000 AND replicate_on_write='true' AND populate_io_cache_on_flush='false' AND default_time_to_live=0 AND speculative_retry='NONE' AND memtable_flush_period_in_ms=0 AND compaction={'class': 'SizeTieredCompactionStrategy'} AND compression={'sstable_compression': 'LZ4Compressor'}; cqlsh\u0026gt; 古いデータの掃除 # 放っておくとデータは溜まりっぱなしですが、Zipkin + Cassandra ではデフォルトで7日で TTL が設定されているので Cassandra の compaction を実行することで TTL を過ぎたデータを削除することができます。 nodetool compast を定期的に実行しましょう。\nOpsCenter # OpsCenter (http://www.datastax.com/what-we-offer/products-services/datastax-opscenter) を使うとかっちょいい画面でクラスターのモニタリングができます。\n$ sudo yum install --enablerepo=datastax opscenter 今回はここのセットアップ方法は省略\nZipkin のセットアップ # /opt/zipkin に collector, query, web をインストールします。 collector, query, web をそれぞれダウンロードします。 https://github.com/twitter/zipkin/releases/\n https://github.com/twitter/zipkin/archive/1.1.0.zip https://github.com/twitter/zipkin/releases/download/1.1.0/zipkin-collector-service.zip https://github.com/twitter/zipkin/releases/download/1.1.0/zipkin-query-service.zip https://github.com/twitter/zipkin/releases/download/1.1.0/zipkin-web.zip  zipkin-collector-seervice.zip, zipkin-query-service.zip, zipkin-web.zip はそのまま /opt/zipkin に展開します。 /opt/zipkin/zipkin-collector-service-1.1.0 /opt/zipkin/zipkin-query-service-1.1.0 /opt/zipkin/zipkin-web-1.1.0 1.1.0.zip は中の zipkin-web というディレクトリだけを取り出して /opt/zipkin/zipkin-web に置きます。 Zipkin の各サービスは Supervisord を使って管理します。 EPEL リポジトリにあります\n$ sudo yum install supervisor --enablerepo=epel Zipkin 用ユーザーを作成します\n$ sudo groupadd zipkin $ sudo useradd -g zipkin zipkin /etc/supervisord.conf を書いて sudo service supervisord start で 8080/tcp で zipkin-web にアクセスできるはずです。 Zipkin へのデータ登録は 9410/tcp です。\n[program:zipkin-collector] command = /usr/bin/java -Xmn1000m -Xms2000m -Xmx2000m -cp /opt/zipkin/zipkin-collector-service-1.1.0/libs -jar /opt/zipkin/zipkin-collector-service-1.1.0/zipkin-collector-service-1.1.0.jar -f /opt/zipkin/zipkin-collector-service-1.1.0/config/collector-cassandra.scala user = zipkin autostart = true stopwaitsecs = 10 log_stdout = true log_stderr = true logfile = /var/log/supervisor/zipkin-collector.log logfile_maxbytes = 10MB logfile_backups = 10 [program:zipkin-query] command = /usr/bin/java -cp /opt/zipkin/zipkin-query-service-1.1.0/libs -jar /opt/zipkin/zipkin-query-service-1.1.0/zipkin-query-service-1.1.0.jar -f /opt/zipkin/zipkin-query-service-1.1.0/config/query-cassandra.scala user = zipkin autostart = true stopwaitsecs = 10 log_stdout = true log_stderr = true logfile = /var/log/supervisor/zipkin-query.log logfile_maxbytes = 10MB logfile_backups = 10 [program:zipkin-web] command = /usr/bin/java -cp /opt/zipkin/zipkin-web-1.1.0/libs -jar /opt/zipkin/zipkin-web-1.1.0/zipkin-web-1.1.0.jar -f /opt/zipkin/zipkin-web-1.1.0/config/web-dev.scala -D local_docroot=/opt/zipkin/zipkin-web/src/main/resources user = zipkin autostart = true stopwaitsecs = 10 log_stdout = true log_stderr = true logfile = /var/log/supervisor/zipkin-web.log logfile_maxbytes = 10MB logfile_backups = 10 [supervisord] http_port = /tmp/supervisor.sock pidfile = /var/run/supervisord.pid minfds = 1024 minprocs = 200 nodaemon = false loglevel = info logfile = /var/log/supervisor/supervisord.log logfile_maxbytes = 10MB logfile_backups = 10 [supervisorctl] serverurl = unix:///tmp/supervisor.sock 接続先の Cassandra については /opt/zipkin/zipkin-collector-service-1.1.0/config/collector-cassandra.scala, /opt/zipkin/zipkin-query-service-1.1.0/config/query-cassandra.scala の中で指定します。デフォルトで localhost になっています。 もしもストレージに Redis を使いたい場合は config ディレクトリにある collector-redis.scala, query-redis.scala を使います。 HBase 用のファイルもあります。 Redis ではアクセス（登録）が多いと collector プロセスが全然処理しきれなかったのであまり使われていないのかもしれません。Cassandra に変更したらサクサクになりました。 これで動作はするのですが、デフォルトのままでは collector が DEBUG ログを出力してログの量が多すぎるので次のように書き換えました。 私 Scala はまったくわからないので https://groups.google.com/forum/#!topic/zipkin-user/NwZFPzYeo9I を参考に同僚にやってもらったわけですが。\n--- collector-cassandra.scala.orig\t2013-08-27 14:55:06.000000000 +0900 +++ collector-cassandra.scala\t2015-03-11 09:51:28.629353999 +0900 @@ -13,10 +13,12 @@  * See the License for the specific language governing permissions and * limitations under the License. */ +import com.twitter.zipkin.builder.{ZipkinServerBuilder, Scribe}  import com.twitter.zipkin.builder.Scribe import com.twitter.zipkin.cassandra import com.twitter.zipkin.collector.builder.CollectorServiceBuilder import com.twitter.zipkin.storage.Store +import com.twitter.logging._  val keyspaceBuilder = cassandra.Keyspace.static(nodes = Set(\u0026#34;localhost\u0026#34;)) val cassandraBuilder = Store.Builder( @@ -25,5 +27,13 @@  cassandra.AggregatesBuilder(keyspaceBuilder) ) +val loggers = List( + LoggerFactory( + level = Some(Level.INFO), + handlers = List(ConsoleHandler()) + ) +) +  CollectorServiceBuilder(Scribe.Interface(categories = Set(\u0026#34;zipkin\u0026#34;))) .writeTo(cassandraBuilder) + .copy(serverBuilder = ZipkinServerBuilder(9410, 9990).loggers(loggers)) HAProxyで負荷分散 # お好みでどうぞ\nkeepalived で VIP を持たせる # こちらもお好みで\n","date":"2015年4月4日","permalink":"/2015/04/setup-cassandra-zipkin-server/","section":"Posts","summary":"お手軽だということで Redis をストレージエンジンとした Zipkin サーバーを使っていましたが、Redis ではメモリに収める必要があり、データ容量的に足りなく","title":"Cassandraを使ったZipkinサーバーの構築"},{"content":"","date":"2015年4月4日","permalink":"/tags/zipkin/","section":"Tags","summary":"","title":"Zipkin"},{"content":"","date":"2015年3月20日","permalink":"/tags/fortigate/","section":"Tags","summary":"","title":"FortiGate"},{"content":"その昔 「続オレオレFortiAnalyzer」という記事を書きました。 何故か FortiGate のログの時刻フォーマットに余計なスペース(0x20)が入ってて困るという話。 ががが！！今日見てみたらスペースが入ってなかったのです。あれ？ FortiOS の更新してないのになぜ？あっ、rsyslog から syslog-ng に変えたせいかっ！ ということで、再度 rsyslog で確認してみるとやっぱり date=2015-03-20,time=12: 34:56 となる。 どうやら rsyslog は tag が必須なのかログメッセージの最初のコロン(:)までをタグとして扱い、 tag: msg というフォーマットで書きだすようです。 確かに tcpdump で見ても送られてくるデータにこのスペースは含まれていませんでした。 そう、FortiGate さんのバグではなかったのです。ごめんなさい。 つーことで gem を更新しました。 https://rubygems.org/gems/fluent-plugin-fortigate-log-parser\n","date":"2015年3月20日","permalink":"/2015/03/rsyslog-insert-unexpected-white-space/","section":"Posts","summary":"その昔 「続オレオレFortiAnalyzer」という記事を書きました。 何故か FortiGate のログの時刻フォーマットに余計なスペース(0x20)が入ってて","title":"FortiGate さんごめんなさい、悪いのは rsyslog でした"},{"content":"仮想サーバーとして KVM を使用していますが、同居 VM のトラフィックが無視できなくなったのでなんとかできないかと調査したところ nwfilter というものがあったのでこれで制御することにしました。 libvirt: Network Filters\nnwfilter の一覧を確認 # nwfilter-list サブコマンドで定義済みフィルターの一覧が確認できます。\n$ sudo virsh nwfilter-list UUID Name ---------------------------------------------------------------- 85626774-2856-297d-983f-fecb05dc8b0d allow-arp b84e8f59-4fd8-add9-def1-894340211cb4 allow-dhcp a316dd94-c7c1-6aa2-2aa1-e2177ae08639 allow-dhcp-server 643613e5-3c4e-2380-8d6a-20550faf0234 allow-incoming-ipv4 40ed48cb-aaf3-bdc0-3638-b7e081820984 allow-ipv4 eb6b8d68-92a9-7d1c-7344-aaa8c3ca9284 clean-traffic c2165185-ad1f-0831-8ecc-03e6a3a458d6 no-arp-ip-spoofing 4a3faac2-fb68-be19-2d47-279428ddd1dd no-arp-mac-spoofing bfd9a7ab-8890-2aed-54ae-af4ea6316d20 no-arp-spoofing 28fc5edb-e04d-3aaa-66d9-a39d4557dde7 no-ip-multicast 5fb20664-40ca-e724-fd66-9f67c312270f no-ip-spoofing acd88779-283a-f7f0-f821-7552baa49690 no-mac-broadcast 48e0b0dc-6b2e-56ac-034f-da0a4b1d110e no-mac-spoofing ee18e17e-b539-e365-71e2-4d34613556c1 no-other-l2-traffic ae70065b-b6b7-841e-5af9-5c9c9ad1ccf4 no-other-rarp-traffic 724421c2-add7-09b2-de72-c6e171579769 qemu-announce-self 5ac97619-ab7f-15b1-c05a-1c18d4d669c5 qemu-announce-self-rarp フィルターの定義内容を確認する # このフィルターの定義内容は nwfilter-dumpxml で確認できます\n$ sudo virsh nwfilter-dumpxml clean-traffic eb6b8d68-92a9-7d1c-7344-aaa8c3ca9284 \u0026lt;filterref\u0026gt; は filter 名の別のフィルターがそこに読み込まれます。 \u0026lt;filterref filter='no-mac-spoofing'/\u0026gt; には次の内容が入ります。\n$ sudo virsh nwfilter-dumpxml no-mac-spoofing 48e0b0dc-6b2e-56ac-034f-da0a4b1d110e フィルターをゲストに適用する # virsh edit domain で対象ゲストの設定を編集します。 ネットワークインターフェース (\u0026lt;interface\u0026gt;) の中に \u0026lt;filterref\u0026gt; を書きます。\nパラメータの不要なフィルターの場合は \u0026lt;filterref filter='filtername'/\u0026gt; だけとなります。 \u0026ldquo;IP\u0026rdquo; は を  に書くことでも指定できるようです。ない場合は最初のパケットから抽出されるようです。\nclean-taffic は source MAC アドレスや IP アドレスを偽装したパケットを通さないようにするフィルターです。\n新しくフィルターを定義する # 今回やりたかったのはそのゲストと関係ないとトラフィックが流れてくるのを止めたかったので clean-taffic を参考に自作してみます。\nmulticast は通すけど IPv4 のその他は自分の IP アドレス宛でなかったら drop します。\nsudo virs nwfilter-define my-allow-ipv4.xml sudo virs nwfilter-define my-clean-traffic.xml 後は各 Guest の interface 設定に追記します。\n","date":"2015年3月4日","permalink":"/2015/03/kvm-nwfilter/","section":"Posts","summary":"仮想サーバーとして KVM を使用していますが、同居 VM のトラフィックが無視できなくなったのでなんとかできないかと調査したところ nwfilter というものがあったの","title":"KVMでnwfilterを使ってトラッフィクを制御する"},{"content":"","date":"2015年3月4日","permalink":"/tags/nwfilter/","section":"Tags","summary":"","title":"nwfilter"},{"content":"","date":"2015年2月14日","permalink":"/tags/keepalived/","section":"Tags","summary":"","title":"keepalived"},{"content":"Redis を冗長化したいなって思って探していたらこんなのを見つけたので試してみます。\n「Highly Available Redis Cluster | Simplicity is the keynote of all true elegance」\n最初は Pacemaker で構築しようと考えていましたが Redis の作者様が Sentinel をおすすめしていたのでこの構成を試してみます。\nそれぞれのバージョンは\n OS: CentOS 6.6 Redis: 2.8.19 HAProxy: 1.5.11 Keepalived: 1.2.13  図にするとこんな感じ\nRedis の Master / Slave は Sentinel でコントロールします。 Sentinel については「Redis Sentinelで自動フェイルオーバー」が詳しいです（ちょっと古いかもしれないけど）。\nRedis の Master / Slave 管理は Sentinel がうまいことやってくれますが、Master に VIP を持たせて常に同じ IP アドレスに接続すれば良いということにはできません。この部分をこの構成では HAProxy に委ねています。\nHAProxy は Redis サーバー全て（今回の例では Master と Slave の2台）を LB のメンバーとし、ヘルスチェックで Master であることを確認します。 これにより Slave のサーバーは Down している扱いとなり、アクセスを振り分けません。 クライアントが Master の IP アドレスを Sentinel に問い合わせたり、Sentinel が Master の変更をクライアントに通知したりする必要がなくて楽ちんですね。\nただし、気をつけるべきこともあります。それは、うっかり Master として Redis を起動させてしまわないこと。\nMaster だったサーバーが突然再起動し、Slave が Master に昇格した後、旧 Master がそのまま起動してくると HAProxy がアクセスを振り分けてしまいます。Sentinel が見つければすぐに Slave に変更してくれますが、一時的にであってもアクセスさせたくありません。よって、自動起動を無効にしておくと良いでしょう。（ネットワークの分断とか不安定なネットワークとかを考えるとなかなか簡単ではないかも）\n最後に HAProxy の冗長化です。これは keepalived で VRRP を使って Virtual IP を使ってアクセスするようにします。HAProxy はどちらも同じように稼働させておけば良いです。Master だから、Backup だからという役割による変更は不要です。\nそれではそれぞれの設定です。\nRedis Sentinel # port 2379 logfile /var/log/redis/sentinel.log dir /tmp sentinel monitor mymaster 192.168.1.20 6379 2 sentinel auth-pass mymaster my-redis-password sentinel down-after-milliseconds mymaster 30000 sentinel parallel-syncs mymaster 1 sentinel failover-timeout mymaster 180000 # sentinel notification-script mymaster /var/redis/notify.sh # sentinel client-reconfig-script mymaster /var/redis/reconfig.sh mymaster の名称は任意の文字列で指定できます。また、複数設定することもできます。mymaster1とmymaster2` など。ということは1組の Sentinel で複数の Redis Cluster を管理できるということです。 Sentinel は Master から Slave の情報を取得するため、Master を指定するだけです。\nHAProxy # global log 127.0.0.1 local2 notice maxconn 4096 chroot /var/lib/haproxy user nobody group nobody daemon defaults log global mode tcp retries 3 option redispatch maxconn 2000 timeout connect 2s timeout client 120s timeout server 120s frontend redis bind :6379 default_backend redis_backend backend redis_backend option tcp-check tcp-check send AUTH\\ my-redis-password\\r\\n tcp-check expect string +OK tcp-check send PING\\r\\n tcp-check expect string +PONG tcp-check send INFO\\ REPLICATION\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK server redis1 192.168.1.21:6379 check inter 1s server redis2 192.168.1.22:6379 check inter 1s tcp-check の send で送る文字列を expect で期待する応答にマッチする文字列を指定しています。書いてある順に1つの TCP セッションで実行されます（スペースの前にはバックスラッシュが必要です）。AUTH パスワード でパスワードを送り、応答に +OK が含まれていたら PING を送り、応答に +PONG が含まれていたら INFO REPLICATION を送り role:master が含まれていたら OK。生きた Master であると確認できます。認証が不要ならその部分は不要で PING から始めれば良いです。\nkeepalived # multicast で packet 垂れ流すのもよくないので unicast で設定しています。\nglobal_defs { notification_email { admin@example.com } notification_email_from keepalived@example.com smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ホスト名 } vrrp_script check_haproxy { script \u0026quot;pkill -0 -x haproxy\u0026quot; # haproxy というプロセスの存在確認 interval 1 # 1秒おきに実行 fall 2 # 2回続けてのの失敗でダウンと判断 raise 2 # 2回成功で正常と判断 } vrrp_instance REDIS { state BACKUP # nopreempt のため2台とも BACKUP とする interface eth0 smtp_alert virtualrouter_id 51 # 同一サブネット内で一意な数字 priority 101 # 数字の大きい方が Master advert_int 1 # VRRP パケットの送信間隔 nopreermpt # 自動 fail back しない unicast_peer { # multicast でなく unicast で ok 192.168.1.32 # 相方のアドレスを指定 } authentication { auth_type PASS auth_pass hogehoge # 8文字までの文字列 } virtual_ipaddress { 192.168.1.30 # VIP } track_script { check_haproxy } # Master への昇格時に実行するスクリプト (ただし、今回の構成ではなにもする必要がない) # notify_master /some/where/notify_master.sh } でもちょっと大げさ？ # ここまでの構成を1発で構築できる Ansible playbook を書いたものの、これはちょっと大げさじゃないか？と思えてきました。 今回欲しいのはただの Active / Standby 構成でした。\nSentinel を有効活用し、Redis の Replica を増やして負荷を分散すると（効果的かどうかは謎）か、1つの HAProxy の対で複数の Redis Cluster を管理する（Sentinel も1組で良い）とかやりたい場合は良い構成であるきがするものの、単純な Active / Standby としては大げさです。\nまた、せっかく高速な Redis を使うのに間に HAProxy を入れると少なからずオーバーヘッドがあります。\nredis-benchmark で簡単に試してみたところ半分くらいのパフォーマンスになることもありました（元が速いのでそれでも速いとは思います）。 で、HAProxy 外しちゃばいいんじゃないかと考えたら、そうすると keepalived で VIP を持った方が必ず Redis の Master になる必要があり、Redis Sentinel に管理させるわけにはいかなくなりました。\n結局 keepalived が master に昇格した際にそのホストの Redis を Master に切り替えるという構成になります。\n「RedisをKeepalivedでフェイルオーバーする構成案 - 酒日記 はてな支店」になるってことです。 fujiwara さんが書かれた頃はまだ keepalived で unicast の VRRP がサポートされてなかったようですが今では unicast が使えるので EC2 などのクラウドサービスでも使えそうです。 keepalived の notify_master で Redis を Master に昇格 redis-cli slaveof no one させ、変更を redis.conf に反映 redis-cli config rewrite させます。 CONFIG REWRITE は 2.8 で追加されたコマンドなのでこれも fujiwara さんの記事の時点ではなかった機能ですね。 notify_backup を使えば BACKUP に切り替わった際に Redis を slave にするということもできます。\nglobal_defs { notification_email { admin@example.com } notification_email_from keepalived@example.com smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ホスト名 } vrrp_script check_redis { script \u0026quot;/some/where/check_redis.sh\u0026quot; # redis のどうさ確認スクリプト interval 2 # 2秒おきに実行 fall 2 # 2回続けてのの失敗でダウンと判断 raise 2 # 2回成功で正常と判断 } vrrp_instance REDIS { state BACKUP # nopreempt のため2台とも BACKUP とする interface eth0 smtp_alert virtualrouter_id 51 # 同一サブネット内で一意な数字 priority 101 # 数字の大きい方が Master advert_int 1 # VRRP パケットの送信間隔 nopreermpt # 自動 fail back しない unicast_peer { # multicast でなく unicast で ok 192.168.1.32 # 相方のアドレスを指定 } authentication { auth_type PASS auth_pass hogehoge # 8文字までの文字列 } virtual_ipaddress { 192.168.1.30 # VIP } track_script { check_redis } # Master への昇格時に実行するスクリプト notify_master /some/where/notify_master.sh # notify_backup /some/where/notify_backup.sh } シンプルだし、Active / Standby 構成ならこっちですね。\n","date":"2015年2月14日","permalink":"/2015/02/redis-ha/","section":"Posts","summary":"Redis を冗長化したいなって思って探していたらこんなのを見つけたので試してみます。 「Highly Available Redis Cluster | Simplicity is the keynote of all true elegance」 最初は Pacemaker で","title":"Redis の冗長化を考える (keepalived, HAProxy, Redis Sentinel)"},{"content":"","date":"2015年2月14日","permalink":"/tags/sentinel/","section":"Tags","summary":"","title":"sentinel"},{"content":"SSL の有効な HTTP サーバーを nginx で構築する機会があったので OCSP Stapling (と SPDY) を有効にしてみた話。(SPDY は listen に spdy って追加するだけなので特に書くことなし) 環境は CentOS 6 で nginx は nginx.org の mainline RPM (1.7.10)\nOCSP Stapling とは # SSL は信頼できる認証局によって発行された証明書であることを確認することでそのサーバーも信頼できるもの（少なくともそのドメインの所有者の正当なサーバー）とします。 が、その証明書はその後、失効されているかもしれません。鍵が漏れたとかその疑いがある場合などに失効させて再発行してもらいます。OpenSSL の脆弱性さわぎ (Heartbleed) の時には多くの証明書が失効されているはずです。 そのため、失効されてるかどうかを OCSP や CRL によって別途確認する必要があり、クライアントはそのための通信をする必要があるのです。 OCSP Stapling はその通信を不要にしようというものです。Staple はステープラー（ホッチキス）のそれで、束ねて一緒に渡しちゃえというものです。実印を押した書類に印鑑証明を添えて提出するのに似てる？ GlobalSign にもドキュメントがありました NGINX - Enable OCSP Stapling\nnginx での設定 # 簡単です。SSL の server { } の中に\nssl_stapling on; ssl_stapling_verify on; ssl_trusted_certificate /some/where/trusted.crt; を書くだけです。キモは ssl_trusted_certigicate で指定する証明書ファイルです。 RapidSSL は GeoTrust なので Root 証明書はこちら GeoTrust Root Certificates RapidSSL の中間証明書は RapidSSL Intermediate CAs から。今回は SHA2 の証明書なので Intermediate CA Certificates: RapidSSL with SHA-2 (under SHA-1 Root) の2つ。Root証明書と中間証明書の合計3つを1つのファイルにまとめます。 順番にも気をつける必要があり、先の GlobalSign のドキュメントにも \u0026ldquo;This must contain the intermediate \u0026amp; root certificates (in that order from top to bottom)\u0026rdquo; とあります。\n$ echo QUIT | openssl s_client -connect localhost:443 -status 2\u0026gt; /dev/null | head -n 20 これで OCSP Response Status: successful という文字列がみつかればうまくいっていそうです。初回やしばらくアクセスのなかった後は OCSP response: no response sent が返ってきますが、その次のアクセスでは OCSP Response が返ってきます。 OCSP のデータが付加される分、ネゴシエーション時のトラッフィクが増えるので keep-alive は有効にしましょう。そうでなくても SSL のハンドシェイクは重いようなので。 Qualys SSL Labs - Projects / SSL Server Test で確認してみましょう。 HTTPS サーバーの設定例\nserver { listen 443 ssl spdy; ssl_certificate /some/where/server.crt; ssl_certificate_key /some/where/server.key; ssl_session_cache shared:SSL:10m; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # https://wiki.mozilla.org/Security/Server_Side_TLS  ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA; ssl_prefer_server_ciphers on; add_header Alternate-Protocol 443:npn-spdy/3; add_header Strict-Transport-Security \u0026#34;max-age=31536000;\u0026#34;; # enable ocsp stapling  ssl_stapling on; ssl_stapling_verify on; ssl_trusted_certificate /some/where/trusted.crt; } ","date":"2015年2月12日","permalink":"/2015/02/nginx-ocsp-stapling-rapidssl/","section":"Posts","summary":"SSL の有効な HTTP サーバーを nginx で構築する機会があったので OCSP Stapling (と SPDY) を有効にしてみた話。(SPDY は listen に spdy って追加するだけなので特に書くことなし) 環","title":"nginx で OCSP Stapling (RapidSSL)"},{"content":"","date":"2015年2月12日","permalink":"/tags/ocsp/","section":"Tags","summary":"","title":"ocsp"},{"content":"","date":"2015年2月12日","permalink":"/tags/spdy/","section":"Tags","summary":"","title":"spdy"},{"content":"","date":"2014年10月18日","permalink":"/tags/android/","section":"Tags","summary":"","title":"android"},{"content":"私はスマホで通話をすることが少ないので docomo, au, softbank に毎月高額の支払いをするのがバカバカしくなったので2年縛りが9月末で終わるのに合わせ、MVNO への切り替えを検討していました。 本当は iPhone5 の電池交換を行って使い続けられればそれが一番良かったのですが、au の iPhone5 だったせいで docomo の MVNO サービスでは使えないため、新たにスマホを調達する必要がありました。（KDDI の MVNO 的なサービスもあったようですが、iPhone5 では LTE でしか接続できず、まともに使えない） かつては Android なんて使えねーと思っていましたが、タブレットは Nexus 7 (2013) を使っていてもう Android でも大丈夫そうだし、iPhone へのこだわりもなくなったので安い Android 端末にしようということで ZTE Blade Vec 4G や Nexus 5 と悩んだ末に LG G2 mini (LG-D620J) を購入しました。楽天のモバックスというお店で 29,800円でした。 ミドルクラスのG2 mini LG-D620J、4.7型ながら比較的コンパクト 選定のポイントは\n 安い (MVNO で縛られたくないので一括で購入したい) ゲームはしないので性能はそこそこで良い 大きいのは使いづらいので嫌。できれば iPhone5, 5S サイズ 電池の交換が可能  で、MVNO サービスで BIC SIM を選択したのは評判の良い IIJ のみおふぉんと等のサービスでさらに Wi2 300 という無線LANサービスが追加費用無しで使えるという特典があったからです。 ちょうど IIJ が最安プランの高速通信上限を 1G から 2G に上げてくれて月額1,600円のミニマムスタートプランで快適です。 MNPの転入手続きのため、10月1日にビックカメラ有楽町店に開店直後(10:00過ぎ)に行きました。ここでのお店の対応が非常にまずくて長蛇の列ができ尋常じゃない待ち時間となっていました。もう改善されていることを願います。この手続き中に気づいたのですが MNP すると1年縛りなんですね。 Wi2 300 は実は私の生活圏（都内ですけど）ではまったく不要でした。不要というか邪魔なくらい。というのも 「プレミアムエリアお試しキャンペーン」というのが終了しており、接続アプリを入れた状態だとその使えないアクセスポイントにも勝手に接続してくれるのです。その度に WiFi をオフにして、オンにするのを忘れるという\u0026hellip; でででででー、LG G2 mini の具合ですがこれがどうにもイケてません\u0026hellip;\n iPhone5 からの乗り換えなのでカメラ性能の落差がハンパない 背面ボタンはとても使いづらい 1GBメモリは少ない、起動するアプリを最小限にしておかないとつらい 8GBストレージは少ない、SDカード使えるけど移せないものが沢山ある 無線LANの感度(?)が悪い、家の無線LANがまともに使える範囲が iPhone, Nexus 7 に比べて極端に狭い かっこいい着信音が無い\u0026hellip;  やっぱり Nexus5 にしておけば良かったかなぁとちょっと公開している。でも4.7インチでさえ横幅が広すぎるのに5インチは辛い。XPERIA Z3 Compact をお安く手に入れたい\u0026hellip; (SIMフリーの海外モデル XPERIA Z3 Compactレビュー) そして、Android のプッシュ通知がよくわからんです\u0026hellip; これが一番困ってる。 「LG G2 mini用 ソフトジャケット」はいい感じです。\n","date":"2014年10月18日","permalink":"/2014/10/lg-g2-mini-with-bic-sim/","section":"Posts","summary":"私はスマホで通話をすることが少ないので docomo, au, softbank に毎月高額の支払いをするのがバカバカしくなったので2年縛りが9月末で終わるのに合わせ、MVNO へ","title":"au の iPhone5 から LG G2 mini + BIC SIM に乗り換えた話"},{"content":"","date":"2014年9月6日","permalink":"/tags/linux-mint/","section":"Tags","summary":"","title":"Linux Mint"},{"content":"","date":"2014年9月6日","permalink":"/tags/wine/","section":"Tags","summary":"","title":"wine"},{"content":"昔は LINE にも Web 版ってのがあったらしいのですが、いまはもう無くって、PC 用には Windows のアプリしかありません。でも私の PC は Linux (Linux Mint 17 Qiana) なのです。 前にも wine で動かそうとしたことがあるのですがその時はうまく動作しませんでした。でもまた試してみようとやってみたら動きました。 ググると wine 1.6 ではダメだけど 1.7 では動いたとあったので 1.7 を ubuntu にインストールする方法を調べて http://ubuntuhandbook.org/index.php/2014/06/install-wine-1-7-20-ubuntu-linux/ を参考に\n$ sudo add-apt-repository ppa:ubuntu-wine/ppa $ sudo apt-get update $ sudo apt-get install wine1.7 これで wine 1.7 はインストールできました。でも LINE のインストーラを実行したら mono と gecko が無い、wine でインストールもできるけど distribution の package を使ったほうが良いよと表示されるので\n$ sudo apt-get install wine-mono4.5.2 $ sudo apt-get install wine-gecko2.24 これで一応動いてます。\n","date":"2014年9月6日","permalink":"/2014/09/line-on-linux-with-wine/","section":"Posts","summary":"昔は LINE にも Web 版ってのがあったらしいのですが、いまはもう無くって、PC 用には Windows のアプリしかありません。でも私の PC は Linux (Linux Mint 17 Qiana) なのです。 前にも wine","title":"wine を使って Linux で LINE アプリを使う"},{"content":"Google Apps でみんなが使えるアドレス帳を操作しようと思ったら存在するのにウェブの管理画面から操作できないんですよね。 これまた API 使って操作するスクリプトを書こうかと思ったけどこれはめんどくさそうだったから既存のものを探したら google-shared-contacts-client というものが見つかったのでこれを使うことにする。 使い方は簡単 まずは依存ライブラリのインストール\n$ pip install gdata それから google-shared-contacts-client-1.1.3.tar.gz を展開して shared_contacts_profiles.py を実行するだけ\n$ tar xvf google-shared-contacts-client-1.1.3.tar.gz $ cd google-shared-contacts-client $ python shared_contacts_profiles.py Usage: shared_contacts_profiles.py --admin=EMAIL [--clear] [--import=FILE [--output=FILE]] [--export=FILE] If you specify several commands at the same time, they are executed in in the following order: --clear, --import, --export regardless of the order of the parameters in the command line. Options: -h, --help show this help message and exit -a EMAIL, --admin=EMAIL email address of an admin of the domain -p PASSWORD, --password=PASSWORD password of the --admin account -i FILE, --import=FILE imports an MS Outlook CSV file, before export if --export is specified, after clearing if --clear is specified -o FILE, --output=FILE output file for --import; will contain the added and updated contacts/profiles in the same format as --export --dry_run does not authenticate and import contacts/profiles for real -e FILE, --export=FILE exports all shared contacts/profiles of the domain as CSV, after clearing or import took place if --clear or --import is specified --clear deletes all contacts; executed before --import and --export if any of these flags is specified too Nothing to do: specify --import, --export, or --clear とりあえず既存のデータを export して中身をのぞいてみる。 UTF-8 な CSV ファイルなので Excel で開く場合はテキストで読み込ませる。(バージョンによってはそのまま開けるのかな？) 開いてびっくり!! なんと Email address の項目が69個もある!! 更新(import)する時は必要な(使う)列だけあれば良いです。文字コードは UTF-8 じゃないと化けます。エクセルで編集、保存する場合は注意。 ただし、Action は必須です。add, update, delete が入ります。add 以外の場合は ID も必須です。どれを更新、削除するのか指定しないといけませんから。 項目のリストは https://code.google.com/p/google-shared-contacts-client/wiki/SupportedContactFields にあります。使いそうにないものばっかりです。 そして、このコマンドで import したデータはすぐに export 出来ますが、実際にブラウザで Google Apps にアクセスしてみてもすぐには反映されません。かなーり長いことかかるようです。それを知らずに何度も試すと時間を無駄にします\u0026hellip;しました。\n","date":"2014年9月6日","permalink":"/2014/09/google-shared-contacts-client/","section":"Posts","summary":"Google Apps でみんなが使えるアドレス帳を操作しようと思ったら存在するのにウェブの管理画面から操作できないんですよね。 これまた API 使って操作するスクリプ","title":"Google Apps の共有アドレス帳の操作"},{"content":"","date":"2014年9月6日","permalink":"/tags/googleapps/","section":"Tags","summary":"","title":"googleapps"},{"content":"","date":"2014年8月14日","permalink":"/tags/api/","section":"Tags","summary":"","title":"API"},{"content":"GoogleApps のアカウントやグループ操作をブラウザからポチポチやってたら日が暮れそうだったのでコマンドラインツールを書きました。 https://github.com/yteraoka/googleapps-directory-tools 旧版の API は廃止予定とのことで OAuth 2.0 版の API を使ってます（ぜんぜん理解していないけれど）。 google-api-python-client を使っていて、samples/groupssettings/groupsettings.py を参考にしています(Python は素人です)。 OOP っぽく使えるライブラリにすれば良かったなと思いつつ、取り急ぎ動くものが必要だったということもありとりあえずは今のかたちで。 対応している操作は\n Orgunits (組織、部署) Users (ユーザー) Users.aliases (ユーザーエイリアス) Groups (グループ) Group.aliases (グループエイリアス) Members (グループのメンバー) Groups Settings (Groups for business 用設定)  それぞれの確認、一覧(検索)、作成、削除、更新です。あと、一部は JSON ファイルからの一括登録。それぞれの API でサポートされていることはほぼできるはず。\nセットアップ # Python のバージョンは 2.7.6, 2.7.8 でしかテストしてません。CentOS 6 ですが、source から xbuild で入れました。後は\n$ pip install google-api-python-client simplejson するだけ。simplejson は json ファイルから一括で登録したりするときにしか使わないけど常に load してるので必要。\nOAuth 認証 # まず、Google Developers Console (このアカウントは Google Apps とは関係ない個人のアカウントでも問題ありません) にて既存のプロジェクトを選択するか、新規作成し、左のメニューにある「API と認証」の「認証情報」を選択する。「新しいクライアントIDを作成する」をクリックし、「アプリケーションの種類」を「インストールされているアプリケーション」、「インストールされているアプリケーションの種類」を「その他」としてクライアントIDを作成する。すると「ネイティブ アプリケーションのクライアント ID」が新たに追加されるので「JSONをダウンロード」をクリックして clone した中の private ディレクトリに client_secret.json という名前で保存する。またそのプロジェクトで使う API を有効にする必要があるため、左のメニューから「API と認証」→「API」で「Admin SDK」と「Groups Settings API」を有効にします。 その後、次のようにコマンドを実行すると\n$ ./user.py --noauth_local_webserver list -d your.domain.name 次のように表示されるのでブラウザに URL を貼り付けて、Google Apps の管理者アカウントでログインした状態で承認します。そこで表示されるコードをコピペでターミナルに貼り付けます。\nGo to the following link in your browser: https://accounts.google.com/o/oauth2/auth?... Enter verification code: Authentication successful. と表示されれば次から使えるようになります。 今のバージョンではここで unknown command '--noauth_local_webserver' と表示されてしまいますが、無視してください。\nユーザーを操作する # 一覧 # $ ./user.py list -d example.com これだけの場合はメールアドレス、姓、名だけを表示しますが、--json を追加すると JSON フォーマットで他の詳細情報も出力されます。--json の代わりに --jsonPretty を使えば読みやすく出力します。list API は件数が多いと paging されて結果が帰りますが、list コマンドはすべてのページを取り出して返します。--query を使えば検索できます。クエリの書き方 各サブコマンドに --help をつければ簡単な使い方が表示されます。\n$ ./user.py --help usage: user.py [-h] [--auth_host_name AUTH_HOST_NAME] [--noauth_local_webserver] [--auth_host_port [AUTH_HOST_PORT [AUTH_HOST_PORT ...]]] [--logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL}] {list,get,insert,patch,delete,setadmin,unsetadmin,bulkinsert} ... positional arguments: {list,get,insert,patch,delete,setadmin,unsetadmin,bulkinsert} sub command list Retrieves a paginated list of either deleted users or all users in a domain get Retrieves a user insert Creates a user patch Updates a user delete Deletes a user setadmin Makes a user a super administrator unsetadmin Makes a user a normal user bulkinsert bulk insert optional arguments: -h, --help show this help message and exit $ ./user.py list --help usage: user.py list [-h] [-v] [--json] [--jsonPretty] [--orderBy {email,familyName,givenName}] [--maxResults MAXRESULTS] [-q QUERY] [-r] [--showDeleted] domain positional arguments: domain search domain optional arguments: -h, --help show this help message and exit -v, --verbose show all user data --json output in JSON --jsonPretty output in pretty JSON --orderBy {email,familyName,givenName} show all user data --maxResults MAXRESULTS Acceptable values are 1 to 500 -q QUERY, --query QUERY search query -r, --reverse DESCENDING sort --showDeleted show deleted user only ユーザーの追加 # 次のようにしてユーザーを追加できます。\n$ ./user.py insert t.yamada@example.com password 山田 太郎 --orgUnitPath で組織グループを指定可能 (--orgUnitPath /営業部 など) ですが、事前に組織を作成しておく必要があります。orgunit.py で作成可能ですが、このコマンドには customerId を渡す必要があります。ユーザーの情報に含まれているので get や list に --json や --jsonPretty をつけて実行すれば確認することができます。\nグループを操作する # グループの作成、削除は group.py で行い、その参加者の操作は member.py で行います。 Groups for Businness では Web でメールの管理(スレッド表示や投稿、返信など)ができます。この設定については group-settings.py で行います。こちらは項目が大変多いです。ドキュメント (Google Apps Groups Settings API) を参照してください。\n$ ./group-settings.py patch --help usage: group-settings.py patch [-h] [--whoCanInvite {ALL_MANAGERS_CAN_INVITE,ALL_MEMBERS_CAN_INVITE}] [--whoCanJoin {ANYONE_CAN_JOIN,ALL_IN_DOMAIN_CAN_JOIN,INVITED_CAN_JOIN,CAN_REQUEST_TO_JOIN}] [--whoCanPostMessage {ALL_IN_DOMAIN_CAN_POST,ALL_MANAGERS_CAN_POST,ALL_MEMBERS_CAN_POST,ANYONE_CAN_POST,NONE_CAN_POST}] [--whoCanViewGroup {ALL_IN_DOMAIN_CAN_VIEW,ALL_MANAGERS_CAN_VIEW,ALL_MEMBERS_CAN_VIEW,ANYONE_CAN_VIEW}] [--whoCanViewMembership {ALL_IN_DOMAIN_CAN_VIEW,ALL_MANAGERS_CAN_VIEW,ALL_MEMBERS_CAN_VIEW}] [--messageModerationLevel {MODERATE_ALL_MESSAGES,MODERATE_NON_MEMBERS,MODERATE_NEW_MEMBERS,MODERATE_NONE}] [--spamModerationLevel {ALLOW,MODERATE,SILENTLY_MODERATE,REJECT}] [--whoCanLeaveGroup {ALL_MANAGERS_CAN_LEAVE,ALL_MEMBERS_CAN_LEAVE}] [--whoCanContactOwner {ALL_IN_DOMAIN_CAN_CONTACT,ALL_MANAGERS_CAN_CONTACT,ALL_MEMBERS_CAN_CONTACT,ANYONE_CAN_CONTACT}] [--messageDisplayFont {DEFAULT_FONT,FIXED_WIDTH_FONT}] [--replyTo {REPLY_TO_CUSTOM,REPLY_TO_SENDER,REPLY_TO_LIST,REPLY_TO_OWNER,REPLY_TO_IGNORE,REPLY_TO_MANAGERS}] [--membersCanPostAsTheGroup {true,false}] [--includeInGlobalAddressList {true,false}] [--customReplyTo CUSTOMREPLYTO] [--sendMessageDenyNotification {true,false}] [--defaultMessageDenyNotificationText DEFAULTMESSAGEDENYNOTIFICATIONTEXT] [--showInGroupDirectory {true,false}] [--allowGoogleCommunication {true,false}] [--allowExternalMembers {true,false}] [--allowWebPosting {true,false}] [--primaryLanguage {ja,en-US}] [--maxMessageBytes MAXMESSAGEBYTES] [--isArchived {true,false}] [--archiveOnly {true,false}] [--json] [--jsonPretty] groupUniqueId positional arguments: groupUniqueId group email address optional arguments: -h, --help show this help message and exit --whoCanInvite {ALL_MANAGERS_CAN_INVITE,ALL_MEMBERS_CAN_INVITE} --whoCanJoin {ANYONE_CAN_JOIN,ALL_IN_DOMAIN_CAN_JOIN,INVITED_CAN_JOIN,CAN_REQUEST_TO_JOIN} --whoCanPostMessage {ALL_IN_DOMAIN_CAN_POST,ALL_MANAGERS_CAN_POST,ALL_MEMBERS_CAN_POST,ANYONE_CAN_POST,NONE_CAN_POST} --whoCanViewGroup {ALL_IN_DOMAIN_CAN_VIEW,ALL_MANAGERS_CAN_VIEW,ALL_MEMBERS_CAN_VIEW,ANYONE_CAN_VIEW} --whoCanViewMembership {ALL_IN_DOMAIN_CAN_VIEW,ALL_MANAGERS_CAN_VIEW,ALL_MEMBERS_CAN_VIEW} --messageModerationLevel {MODERATE_ALL_MESSAGES,MODERATE_NON_MEMBERS,MODERATE_NEW_MEMBERS,MODERATE_NONE} --spamModerationLevel {ALLOW,MODERATE,SILENTLY_MODERATE,REJECT} --whoCanLeaveGroup {ALL_MANAGERS_CAN_LEAVE,ALL_MEMBERS_CAN_LEAVE} --whoCanContactOwner {ALL_IN_DOMAIN_CAN_CONTACT,ALL_MANAGERS_CAN_CONTACT,ALL_MEMBERS_CAN_CONTACT,ANYONE_CAN_CONTACT} --messageDisplayFont {DEFAULT_FONT,FIXED_WIDTH_FONT} --replyTo {REPLY_TO_CUSTOM,REPLY_TO_SENDER,REPLY_TO_LIST,REPLY_TO_OWNER,REPLY_TO_IGNORE,REPLY_TO_MANAGERS} --membersCanPostAsTheGroup {true,false} --includeInGlobalAddressList {true,false} --customReplyTo CUSTOMREPLYTO Reply-To header (REPLY_TO_CUSTOM) --sendMessageDenyNotification {true,false} --defaultMessageDenyNotificationText DEFAULTMESSAGEDENYNOTIFICATIONTEXT --showInGroupDirectory {true,false} --allowGoogleCommunication {true,false} --allowExternalMembers {true,false} --allowWebPosting {true,false} --primaryLanguage {ja,en-US} --maxMessageBytes MAXMESSAGEBYTES --isArchived {true,false} --archiveOnly {true,false} --json output in JSON --jsonPretty output in pretty JSON エイリアスの操作 # ユーザーとグループにはエイリアスが設定可能です。これはそれぞれ user-alias.py, group-alias.py で行います。\n今後 # 必要になるか気が向いたら機能追加とかやっていきます。もしも「こんなツールが欲しかったんだよ」というかたがいらっしゃいましたら使っていただいてフィードバックをいただければと。Python はもっとこう書けとかいうアドバイスもお待ちしております。\n","date":"2014年8月14日","permalink":"/2014/08/googleapps-directory-tools/","section":"Posts","summary":"GoogleApps のアカウントやグループ操作をブラウザからポチポチやってたら日が暮れそうだったのでコマンドラインツールを書きました。 https://github.com/yteraoka/googleapps-directory-tools 旧版の API は廃止予定との","title":"GoogleAppsのアカウント操作用コマンドラインツールを書いた"},{"content":"","date":"2014年7月27日","permalink":"/tags/keepass/","section":"Tags","summary":"","title":"keepass"},{"content":"KeePass でブラウザへのID、パスワード入力を管理しましょう。\nWindows # KeePass 本体のインストール方法は省略 ブラウザとの連携は WebAutoType プラグインでできます。この方法だと IE でも Firefox でも Chrome でもこれだけで同じように使えます。ブラウザ側にプラグインは不要です。 http://keepass.info/plugins.html#webautotype http://sourceforge.net/p/keepass/discussion/329220/thread/ecff27ab/ インストールは http://sourceforge.net/projects/webautotype/files/latest/download ここから zip ファイルをダウンロードして、KeePass フォルダに WebAutoType.plgx をコピーする。 KeePass を起動する。 これだけで OK 後は、KeePass の URL 欄をちゃんと埋めておいてそれにマッチするサイトに アクセスし、ログフィンフォームのユーザーID欄にカーソルがある状態で\nCtrl + Alt + A をタイプすると勝手に入力してくれます。 入力シーケンスはデフォルトでは次のようになっていますが KeePass のそれぞれのエントリでカスタマイズ可能です。\n{USERNAME}{TAB}{PASSWORD}{ENTER} マッチするアカウントが複数あったら選択ウインドウが表示されます。 OpenSource です。 http://sourceforge.net/p/webautotype/code/HEAD/tree/ WebAutoType の弱点は Basic 認証には対応しないところです。Linux 編で紹介する KeePassHttp は Basic 認証にも対応しているので、ブラウザごとにプラグインを入れるのが気にならなければそちらが良いかも。\nLinux (ubuntu / mint) # Linux では WebAutoType は使えないので KeePassHttp を使います。 How to Integrate KeePass With Chrome and Firefox in Ubuntu に書いてあります。 私は Linux Mint ですが、ubuntu ベースなので同じです。 https://github.com/pfn/keepasshttp\n$ sudo apt-get install keepass2 mono-complete KeePassHttp から KeePassHttp.plgx を Download し、/usr/lib/keepass2/plugins/ ディレクトリに置きます。このディレクトリが存在しない場合は作成します。その後 keepass を起動します。 ブラウザそれぞれにもプラグインをインストールします。Firefox には PassIFox を、Google Chrome には chromeIPass をインストールします。 後は KeePass のエントリに URL を設定しておけばブラウザでマッチする URL にアクセスしたら何も入力しなくても自動でフォームに入力された状態になります。アクセス許可の確認が出たりはしますが。\nMac # 持ってないから知らない。Keychain でできるんですかね？\n","date":"2014年7月27日","permalink":"/2014/07/keepass/","section":"Posts","summary":"KeePass でブラウザへのID、パスワード入力を管理しましょう。 Windows # KeePass 本体のインストール方法は省略 ブラウザとの連携は WebAutoType プラグインでできます。この方法だ","title":"KeePassでパスワード管理"},{"content":"","date":"2014年7月27日","permalink":"/tags/password/","section":"Tags","summary":"","title":"password"},{"content":"","date":"2014年7月27日","permalink":"/tags/security/","section":"Tags","summary":"","title":"security"},{"content":"しばらくぶりに fluentd / td-agent 案件の対応をしたら、いろいろ機能追加されてたりして便利になっていたのでメモ。（2014/7/19 出力フォーマット指定のところに追記しました）\n The td-agent ChangeLog fluentd ChangeLog  td-agent 1.1.18 で rewrite-tag-filter が同梱されるようになった # これまで追加で\n$ sudo fluent-gem install fluent-plugin-rewrite-tag-filter してたけど不要になりました。install しようと思って実行したらもう入ってるよって言われた\u0026hellip;\ntd-agent 1.1.19 で tail-ex が不要になった # td-agent 1.1.19 で fluentd v0.10.45 となったので tail-ex の merge が反映されました。\n Release 0.10.45 - 2014/03/28\n in_tail: Merge in_tail_ex and in_tail_multiline features   in_tail の path におもむろに strftime の%記号を入れてあげれば動作します。 read_from_head を true にすると初回はファイルの先頭から読んでくれます。\ntd-agent 1.1.20 で out_file が format 指定可能になった # fluentd v0.10.49 で out_file が TextFormatter を使って出力フォーマットを指定できるようになりました。\n  out_file: Add format option to support various output data format   format で指定可能なのはいまのところ\nout_file\nこれまで通りのフォーマットです。time, tag の出力を停止することも可能で、json に含めることも可能。デフォルトでは TIME\u0026lt;TAB\u0026gt;TAG\u0026lt;TAB\u0026gt;JSON となりますが output_time, output_tag を false にすることで出力させないこともできます。include_time_key, include_tag_key を true にすることで time と tag を JSON に含められます\njson\nJSON 出力ですが out_file で代用できそうな気もする。でも json って明示してある方がわかりやすいですね。out_file と違って1行がまるっと JSON なので Parse がより楽ちんですね。time や tag を出力するには include_time_key, include_tag_key を true にする必要があります\nltsv\nLTSV で書き出せます。include_tag_key, tag_key, include_time_key, time_key オプションで tag と time を含めるようにも指定できます。time は出力のフォーマットを指定可能です。\n type tail path /path/to/ltsv_log.%Y%m%d time_format %Y-%m-%d %H:%M:%S tag ltsv.test format ltsv time_key time type file path /apps/tmp/ltsv_test_file format ltsv include_time_key true time_format %Y-%m-%dT%H:%M:%S%z このようにすることで、LTSV を LTSV のまま書き出すことができます。次の single_value と違い、各項目でフィルタリングやルーティングが行いやすいですね\nsingle_value\nfluent-plugin-file-alternative の代わりに使えるようです。in_tail で format none として読みだしたデータを書き出すのに適しています。ログファイルの1行1行をそのまま集めたい場合ですね\n type tail path /path/to/log.%Y%m%d format none #message_key message tag test.access type file file /path/to/out_file format single_value #message_key message key の名前はどちらもデフォルトが message で message_key パラメータで指定可能です。\nout_file の format はまだ GitHub の code を眺めただけで試してない。 out_file 関連では daemon モードでの umask 0 をなんとかしてほしいなぁ。 Fluentd も Ansible みたいにドキュメントが code に入ってると嬉しいなぁと思いました。 「このオプションはバージョンxxで追加されました」とか入っているし、ansible-doc コマンドも便利ですよね。それでも Ansible も code 読まないとわからないことも沢山あるけど。\n","date":"2014年7月15日","permalink":"/2014/07/fluentd-recent-changes/","section":"Posts","summary":"しばらくぶりに fluentd / td-agent 案件の対応をしたら、いろいろ機能追加されてたりして便利になっていたのでメモ。（2014/7/19 出力フォーマット指定のと","title":"fluentd / td-agent がいろいろ便利になってた"},{"content":"","date":"2014年7月15日","permalink":"/tags/td-agent/","section":"Tags","summary":"","title":"td-agent"},{"content":" A quick demo of Ansible and GCE http://t.co/6rqxSt41NL\n— Ansible (@ansible) 2014, 6月 27\n という tweet を見て、Docker Meetup で GCE の $500 分クーポンをもらっていたのを思い出したので試してみました。\nブラザから操作してみる # まずは、Ansible を使わないでブラウザからインスタンスを立ち上げてみます。https://console.developers.google.com/ Web Console から手動でインスタンスを立ち上げるには、プロジェクトを作成・選択し、画面左にあるメニューから「COMPUTE」→「COMPUTE ENGINE」→「VMインスタンス」→「Spin up VMs fast」で「Create an instance」 次のような入力・選択項目がありました。\n 名前 メタデータ （任意の名前と値） HTTPトラフィックを許可する HTTPSトラフィックを許可する ゾーン  asia-east1-a asia-east1-b europe-west1-a europe-west1-b us-centra11-a us-central1-b   マシンタイプ  f1-micro (vCPU 1 個、メモリ 0.6 GB) n1-standard-1 (vCPU 1 個、メモリ 3.8 GB) g1-small (vCPU 1 個、メモリ 1.7 GB) n1-highcpu-2 (vCPU 2 個、メモリ 1.8 GB) n1-highcpu-4 (vCPU 4 個、メモリ 3.6 GB) n1-highcpu-8 (vCPU 8 個、メモリ 7.2 GB) n1-highcpu-16 (vCPU 16 個、メモリ 14.4 GB) n1-highmem-2 (vCPU 2 個、メモリ 13 GB) n1-highmem-4 (vCPU 4 個、メモリ 26 GB) n1-highmem-8 (vCPU 8 個、メモリ 52 GB) n1-standard-1 (vCPU 1 個、メモリ 3.8 GB) n1-standard-2 (vCPU 2 個、メモリ 7.5 GB) n1-standard-4 (vCPU 4 個、メモリ 15 GB) n1-standard-8 (vCPU 8 個、メモリ 30 GB) n1-standard-16 (vCU 16 個、メモリ 60 GB)   ブートソース  イメージからディスクを新規作成 スナップショットからディスクを新規作成 既存のディスク   イメージ  backports-debian-7-wheezy-v20140619 debian-7-wheezy-v20140619 centos-6-v20140619 sles-11-sp3-v20140609 rhel-6-v20140619   ディスクタイプ  Standard Persistent Disk SSD Persistent Disk   ネットワーク  default （最初から存在するネットワーク 10.240.0.0/16 だった） 新規作成   外部IP  エフェメラル 新しい静的IPアドレス    SSH でログインしてみる # インスタンス起動後のログインには gcutil が必要でした。 インストールは簡単``` $ curl https://sdk.cloud.google.com | bash\nインストールしたらログインする必要があります。 $ gcloud auth login\nログインしたらプロジェクトを指定します $ gcloud config set project\nインスタンスを確認してみます $ gcutil listinstances +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | name | zone | status | network-ip | external-ip | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | instance-1 | asia-east1-b | RUNNING | 10.240.249.235 | 107.167.xxx.yyy | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\n次のコマンドで ssh 接続できます $ gcutil ssh instance-1\nSSH 接続に先だって `~/.ssh/google_compute_engine` の公開鍵がコピーされます。この鍵がまだ作成されていない場合はここで作成されます。 zone や project も指定できます $ gcutil \u0026ndash;service_version=\u0026ldquo;v1\u0026rdquo; \u0026ndash;project=\u0026ldquo;プロジェクトID\u0026rdquo; ssh \u0026ndash;zone=\u0026ldquo;asia-east1-b\u0026rdquo; \u0026ldquo;インスタンス名\u0026rdquo;\n* [Google Compute Engine を使ってみる(1) プロジェクト作成から Google Cloud SDK インストールまで #gcloud #gce](http://jitsu102.hatenablog.com/entry/2014/05/18/183133) * [Google Compute Engine を使ってみる(2) インスタンスの起動と削除 #gcloud #gce](http://jitsu102.hatenablog.com/entry/2014/07/01/072422) Ansible でやってみる -------------- それでは Ansible でセットアップしてみましょう。[GoogleCloudPlatform/compute-video-demo-ansible](https://github.com/GoogleCloudPlatform/compute-video-demo-ansible) リポジトリを使わせていただきます。``` $ git clone https://github.com/GoogleCloudPlatform/compute-video-demo-ansible.git ```Ansible の他に apache-libcloud が必要となるので pip でインストールします。``` $ sudo pip install apache-libcloud ```[Google Deveopers Console](https://console.developers.google.com/project/appyt/apiui/credential) の 「APIS \u0026amp; AUTH」→「認証情報」で「新しいクライアントIDを作成」からサービスアカウントを作成します。この過程でキーが生成されます。パスフレーズは `notasecret` になっています。 これを `~/gce-privatekey.p12` として保存したとします。pkcs12 フォーマットでは Ansible (libcloud) で扱えないため PEM に変換します。``` $ openssl pkcs12 -in ~/gce-privatekey.p12 \\\\ -passin pass:notasecret -nodes -nocerts \\\\ | openssl rsa -out ~/gce-privatekey.pem ```Ansible の Playbook で使う変数を設定します。 GCE の inventory で必要になる設定は `gce.ini` に書きます。すでに libcloud を使っていれば `secrets.py` に設定済みかもしれませんが、私はお初なので `gce.ini` の `gce_service_account_email_address`, `gce_service_account_pem_file_path`, `gce_project_id` を設定しました。`gce_service_account_email_address` は web console で作成したサービスアカウントのものです。`gce_service_account_pem_file_path` は先ほど PEM に変換したもの。 次にケチってインスタンスタイプを f1-micro に変えます。``` $ sed -i 's/n1-standard-1/f1-micro/g' gce\\_vars/machines ```zone (リージョン) も us から asia に変更します。``` $ sed -i 's/us-central1/asia-east1/g' \\\\ gce\\_vars/lb gce\\_vars/zonea gce\\_vars/zoneb cleanup.yml ```さて、ここからがハマりポイントでした。インベントリファイルであろう `ansible_hosts` に書いてあるのはこれだけです。``` \\[local\\] 127.0.0.1 \\[gce\\_instances\\] myinstance\\[1:4\\] ```んんん？ myinstance1,2,3,4 のIPアドレスとかはどうやって知るの？ instance の作成までは local へ接続して行うので可能なのですが web.yml にある Web サーバーセットアップができません。 これはどうやら gce.py というインベントリスクリプトを使うらしい。 が、[plugins/inventory/gce.py](https://github.com/ansible/ansible/blob/devel/plugins/inventory/gce.py) は Ansible の GitHub リポジトリにはあるのに pip でインストールされたファイルの中にはありません... ムムム。ないものはしかたないので GitHub から `gce.py` だけダウンロードして使いました。 これはこういうものらしい。 \u0026gt; [@yteraoka](https://twitter.com/yteraoka) inventoryのファイルはインストールされないはずです。githubからダウンロードする必要があるかと思います。 \u0026gt; \u0026gt; — shirou - しろう (@r\\_rudi) [2014, 7月 1](https://twitter.com/r_rudi/statuses/483986662322475008) それでは -i で gce.py を指定して実行してみよう。Dynamic Inventory ってやつですね。``` $ export GCE\\_INI\\_PATH=./gce.ini $ chmod a+x gce.py $ ansible-playbook -i gce.py site.yml ```あれ... ダメじゃん gce.py で取得できる情報には `local` というグループも `gce_instances` というグループもないんですね。 なんと、Ansible は `-i` でディレクトリを指定すると複数のインベントリファイル、スクリプトの内容をマージしてくれるんですね。[Using Multiple Inventory Sources](http://docs.ansible.com/intro_dynamic_inventory.html#using-multiple-inventory-sources) ということで``` $ mkdir inventory $ mv ansible\\_hosts inventory/ $ mv gce.py inventory/ そんでもってついでに `ansible_hosts` はちょいといじっておきます [local] 127.0.0.1 ansible_connection=local\n[gce_instances] myinstance[1:4] ansible_ssh_private_key_file=/home/xxx/.ssh/google_compute_engine\n 何を変えたかというと、localhost への接続に SSH を使う必要はないので `ansible_connection` を `local` に変更。GCE インスタンスには `~/.ssh/google_compute_engine` の公開鍵がコピーされるので ssh 接続時にはこの秘密鍵を使うように `ansible_ssh_private_key_file` を設定。 新しいインスタンスで毎回 host key の確認をするのは面倒なので環境変数 `ANSIBLE_HOST_KEY_CHECKING` を `False` にします。 $ export GCE_INI_PATH=./gce.ini $ export ANSIBLE_HOST_KEY_CHECKING=False $ ansible-playbook -i inventory site.yml\n これでやっと動きます。4つのインスタンスが Web サーバーとしてセットアップされ、1つのロードバランサーの下にセットされます。ロードバランサーのIPアドレスを調べてアクセスしてみましょう。 終わったら `cleanup.yml` を使って消しておきましょう。 $ export GCE_INI_PATH=./gce.ini $ export ANSIBLE_HOST_KEY_CHECKING=False $ ansible-playbook -i inventory cleanup.yml\n `gcutil` ってコマンドラインでいろいろできて便利ですね 気になった方はお試しあれ。 でも今回試した Playbook は全部のインスタンスに Global IP アドレスがふられちゃいます。VPN 接続して private address だけでやるか SSH の踏み台サーバーを立ててそこを経由させるようなことも必要かな。 ","date":"2014年7月2日","permalink":"/2014/07/ansible-gce/","section":"Posts","summary":"A quick demo of Ansible and GCE http://t.co/6rqxSt41NL — Ansible (@ansible) 2014, 6月 27 という tweet を見て、Docker Meetup で GCE の $500 分クーポンをもらっていたのを思い出したので試してみました。 ブラザから操作","title":"AnsibleでGCEサーバーをセットアップする"},{"content":"","date":"2014年7月2日","permalink":"/tags/gce/","section":"Tags","summary":"","title":"gce"},{"content":"GC まわりのチューニングが難しいということで JVM Operation Casual Talks などでも運用担当者からの評判がよろしくない Java ですが、遠い昔のように遅くない、むしろ速い実行環境ですよね。 Java のコードが書ける運用担当は数が少ない（私もまったく書けない）というのも理由かなとは思いますが、最近のトレンドである構成管理とかイミュータブル、Blue Green Deployment などの面では Java ってとても良いと思うのですよね。 war にしてしまえば、単にそれを置き換えれば deploy できるし、サーバーまで組み込んであれば jar を直接実行するだけかもしれません。JDK も入れ替えて再起動するだけだし、はぁ、Java ってこういうところ楽だわぁと複雑な構成のくそめんどくさい Ansible Playbook を書いた後に思ったのでした。 Java 8 でさらに便利になったみたいだし Java がんばれ！！ でも Applet はイカンですよ。PRIMERGY の管理ツール何とかしてください F 社さん。 あ、あとコマンドラインツールが Java ってのもいただけない。昔々の AWS のツールとか OpenDJ （おや？サイトデザイン変わってる。新しいバージョン試さなきゃ） のコマンドとか、毎度起動に時間がかかりすぎます。 p.s. maven わけわかんないし、XML もキライです\n","date":"2014年4月23日","permalink":"/2014/04/java/","section":"Posts","summary":"GC まわりのチューニングが難しいということで JVM Operation Casual Talks などでも運用担当者からの評判がよろしくない Java ですが、遠い昔のように遅くない、むしろ速い実行","title":"Java がんばれ"},{"content":"","date":"2014年4月12日","permalink":"/tags/asterisk/","section":"Tags","summary":"","title":"Asterisk"},{"content":"「クラウドでIP電話サーバを動かそう！ハンズオン」に参加してきました。 講師の皆様お疲れ様でした。 参加者にはさくらのクラウド20,000円分のクーポンが配られました。さくらインターネットさん、横田さんありがとうございました。 そのクーポンを使ってさくらのクラウドで Asterisk を動かして、IP電話を使ってみようという内容でした。 Asterisk (Wikipedia) の存在はかなり前から知ってはいましたが、実際に試してみたことはありませんでした。触ってみたいなとは思っていたので『おぉ、これはちょうど良い機会』と思って参加してみました。 VPS やクラウドがなかった時代にはこういうハンズオンは難しかったですよねぇ。 IP電話もあたりまえの時代で、ハードの所有なしに外線の発着もできます。スマホがIP電話端末になりますし。 今回の内容\n FUSION IP Phone SMART に signup (外線発着用SIPアカウントの取得)  このサービスは月額基本料金が不要なので ISPの提供しているIP電話を契約済みでそのSIP情報がわかっていればそれを使うことも可能なのかも (Wikipediaにはひかり電話でもできそうなことが書いてある)   さくらのクラウドアカウントの取得 さくらのクラウドで CentOS 6.5 サーバーをセットアップ  さくらさん CentOS 6.5 のイメージの OpenSSL が古かったよ〜   Asterisk のインストール   インストール手順はほぼAsterisk 11に書いてあるとおり\n  サンプルのコンフィグファイルが多すぎてビビりました!!\n $ ls /etc/asterisk/*.conf | wc -l 98 え\u0026hellip;\n— yteraoka (@yteraoka) April 12, 2014\n でも実際に使ったのはファイル2つだけで内容もシンプルなものでした\n   PC のソフトフォンとスマホのIP電話アプリで内線通話を行う  私の PC は　Linux Mint で 配られた資料に Linux 用のソフトフォン情報はなかったので VirtualBox に Windows を入れて X-Lite を使ってみたものの、発着信音は出るのに通話の音がでない\u0026hellip; zoiper が Linux 対応してたのでこれでイケました iPhone でも zoiper を使いました おぉ!!とっても簡単にスマホでの無料通話が!!   FUSIONのSIPアカウントで外線発信、外線の着信を指定の端末に転送  FUSIONの050番号に着信した電話を指定のSIPアカウントに転送する ソフトフォンからFUSIONの050番号で一般の電話に発信する ただ、スマホから050で発信したいだけならスマホアプリだけでできますけどね    資料が公開されたら誰でも簡単に試せそう。 PBX とはなにか、Asterisk で何ができるのかの説明もありました。 参考資料「入門ガイド コールセンターのシステムを基礎から解説」 FreePBX という Asterisk のウェブインターフェースがあるそうです。Schmoozeとパートナー契約をしている日本の株式会社クルーグが正式な日本語版をリリースしたそうです。 講師をされた方の会社の紹介 http://www.youwire.jp/ 懇親会には参加できなかったのですが、参加できてたら @nouphet さんに OTRS の話を聞いてみたかった。\n","date":"2014年4月12日","permalink":"/2014/04/asterisk-handson/","section":"Posts","summary":"「クラウドでIP電話サーバを動かそう！ハンズオン」に参加してきました。 講師の皆様お疲れ様でした。 参加者にはさくらのクラウド20,000円分の","title":"Asterisk のハンズオンに参加してきた"},{"content":"","date":"2014年4月12日","permalink":"/tags/ip%E9%9B%BB%E8%A9%B1/","section":"Tags","summary":"","title":"IP電話"},{"content":"","date":"2014年4月12日","permalink":"/tags/sip/","section":"Tags","summary":"","title":"SIP"},{"content":"Linux の rdesktop コマンドで Windows 7 にリモートデスクトップ接続する際の font smoothing を有効にする方法です。\nググったら rdesktop: Connect to Windows 7 and Vista with ClearType font smoothing enabled っていうそのまんまのブログがヒットするわけですけど\u0026hellip;\nrdesktop -x 0x8F mywinserver # equals the modem default + font smoothing rdesktop -x 0x81 mywinserver # equals the broadband default + font smoothing rdesktop -x 0x80 mywinserver # equals the LAN default + font smoothing って書いてあるので -x 0x80 オプションを追加してあげればイケます。 これだとネットワーク的にちょっと重いよという場合は 0x8F とか 0x81 で。 このフラグはこんなことになってるらしいので好きな組み合わせでどうぞ。 私は 0x80 でしか試してない。でも壁紙なんていらないし、ドラッグのやつもメニューのアニメーションもカーソルの影も不要だから 0xAF で良かったかな。\n#define RDP5_DISABLE_NOTHING\t0x00 #define RDP5_NO_WALLPAPER\t0x01 #define RDP5_NO_FULLWINDOWDRAG\t0x02 #define RDP5_NO_MENUANIMATIONS\t0x04 #define RDP5_NO_THEMING\t0x08 #define RDP5_NO_CURSOR_SHADOW\t0x20 #define RDP5_NO_CURSORSETTINGS\t0x40\t/* disables cursor blinking */ #define RDP5_ENABLE_FONT_SMOOTHING 0x80 ","date":"2014年4月2日","permalink":"/2014/04/rdesktop-font-smoothing/","section":"Posts","summary":"Linux の rdesktop コマンドで Windows 7 にリモートデスクトップ接続する際の font smoothing を有効にする方法です。 ググったら rdesktop: Connect to Windows 7 and Vista with ClearType font smoothing enabled っていうそのまんまのブログ","title":"rdesktop コマンドで font smoothing を有効にする"},{"content":"Ansible Advent Calendar 2013 のネタとして書こうとした残骸があったので、書いてみる。 Ansible は task の実行結果を register という設定で変数に保存できます。 rc でコマンドの exit code に、stdout / stderr で標準出力/標準エラーにアクセスできます。``` # mod_passenger.so のインストールされているべきパスを取得する\n name: get mod_passenger.so path shell: /opt/ruby-{{ ruby_version }}/bin/passenger-install-apache2-module \u0026ndash;snippet | grep passenger_module | awk \u0026lsquo;{print $3}\u0026rsquo; register: mod_passenger_path changed_when: False  先の結果を利用して mod_passenger.so の存在確認を行う #  name: check mod_passenger.so installed command: test -f {{ mod_passenger_path.stdout }} register: mod_passenger_installed failed_when: mod_passenger_installed.rc not in [0, 1] changed_when: False  mod_passenger.so がまだ存在しなかったら build する #  name: build mod_passenger.so environment: PATH: \u0026ldquo;/opt/apache_{{ httpd_version }}/bin/:/bin:/usr/bin:/usr/local/bin\u0026rdquo; command: \u0026gt; /opt/ruby-{{ ruby_version }}/bin/passenger-install-apache2-module \u0026ndash;apxs2-path /opt/apache_{{ httpd_version }}/bin/apxs \u0026ndash;auto creates={{ mod_passenger_path.stdout }} when: mod_passenger_installed.rc == 1  httpd.conf に書く passenger の設定を取得する #  name: get passenger snippet command: \u0026gt; /opt/ruby-{{ ruby_version }}/bin/passenger-install-apache2-module \u0026ndash;snippet changed_when: False register: passenger_snippet  template で先の snippet を使う #  name: httpd.conf template: \u0026gt; src=httpd.conf.j2 dest=/opt/apache_{{ httpd_version }}/conf/httpd.conf owner=root group=root mode=0644 notify: restart httpd  httpd.conf.j2 の中で次の様に register で保存した値を使うことができます。 {{ passenger_snippet.stdout }} {% if passenger_snippet.stdout %} PassengerPoolIdleTime 1200 PassengerMaxPoolSize 5 #PassengerPreStart http://localhost/ {% endif %}\n````changed_when: Falseはコマンドが exit code = 0 で終了すると changed として扱われて、「えっなんか変更された？」ってちょっとビビるのを回避するためです。failed_when: mod_passenger_installed.rc not in [0, 1] は test コマンドでファイルが存在しなかった場合に failed として扱われて「ドキッ」としないためです。ignore_errors: True` としても赤字で表示されちゃうので。\n","date":"2014年2月12日","permalink":"/2014/02/ansible-register/","section":"Posts","summary":"Ansible Advent Calendar 2013 のネタとして書こうとした残骸があったので、書いてみる。 Ansible は task の実行結果を register という設定で変数に保存できます。 rc でコマンドの exit code に、st","title":"Ansible でコマンドの出力を後の task で使う"},{"content":"","date":"2014年2月11日","permalink":"/tags/elasticsearch/","section":"Tags","summary":"","title":"elasticsearch"},{"content":"","date":"2014年2月11日","permalink":"/tags/kibana3/","section":"Tags","summary":"","title":"kibana3"},{"content":" Fluentd + Kibana3 で FortiAnalyzer いらず (更新あり) 続オレオレFortiAnalyzer  のさらに続きです。 FortiOS を 4 から 5 にあげたらログのフォーマットというかカラム名(?)が変わったので fluentd plugin もそれに合わせて変更しました。 ついでに GitHub に上げました。 https://github.com/yteraoka/fluent-plugin-fortigate-log-parser gem にする予定はないので td-agent だったら /etc/td-agent/plugin/ にコピーして使ってください。 FortiOS 4 で使う場合は\nfortios_version 4 を指定してください。GeoIP (World Map) を使わないなら関係ないですけど。 country_map_file オプションを使う場合は http://dev.maxmind.com/geoip/legacy/geolite/ から GeoIPCountryCSV.zip をダウンロードして\n$ ruby mkCountryMap.rb GeoIPCountryWhois.csv \u0026gt; country.map で Japan =\u0026gt; JP, United States =\u0026gt; US などの変換（フィールドの追加）ができるので Kibana で World Map を使って地図表示できます。 FortiOS 5 からは Web インターフェースからは syslog 設定ができなくなったみたいですね。 そして、CSV 出力時の time のフォーマットがバグってる問題も修正されてませんでした。将来的に syslog 出力なくなったら悲しいな。 Kibana 便利ですよ、誰か FortiAnalyzer と比べてみてください。 # FortiAnalyzer 使ったことないから比べられない\u0026hellip;\n","date":"2014年2月11日","permalink":"/2014/02/fluentd-kibana3-fortianalyzer-3/","section":"Posts","summary":"Fluentd + Kibana3 で FortiAnalyzer いらず (更新あり) 続オレオレFortiAnalyzer のさらに続きです。 FortiOS を 4 から 5 にあげたらログのフォーマットというかカラム名(","title":"オレオレFortiAnalyzerその3"},{"content":"DNS を AWS Route53 で管理するにあたり、ブラウザでポチポチやるのはやっぱり誤操作が怖いし、履歴の管理ができないよなぁということで Roadworker を試してみました。\nインストール # インストールは gem install だけ\n$ gem install roadworker --no-ri --no-rdoc Bundler を使う場合は\n$ cat \u0026lt;\u0026lt;_EOD_ \u0026gt; Gemfile source 'https://rubygems.org' gem 'roadworker' _EOD_ $ bundle install --path bundle $ bundle exec roadwork な感じで。 次に Route53 の Action を許可した IAM ユーザーを用意し、環境変数 (AWS_SECRET_ACCESS_KEY, AWS_ACCESS_KEY_ID) をセットします。\nExport # 既に Route53 を利用中なら次のように -e を指定することで設定を export することができます。Roadworker の定義ファイルはデフォルトが Routefile なので -o Routefile と指定することでこのファイルに書き出せます。 -o を指定しなかった場合は標準出力に出力されます。\n$ roadwork -e -o Routefile 複数 zone を管理する場合は zone ごとにファイルを分けると管理しやすいかもしれません。これも既に利用中の場合、-e に加え、--split を指定すると zone 毎にファイルを分けて export してくれます。\n$ roadwork -e --split この場合、-o を指定せずとも Routefile に書き出されます。（-o で別ファイルを指定することもできます） 内容は次のようになります。\nrequire 'example.com.route' require 'example.net.route' hosted_zone \u0026quot;example.com.\u0026quot; do rrset \u0026quot;www.example.com.\u0026quot;, \u0026quot;A\u0026quot; do ttl 300 resource_records( \u0026quot;93.184.216.119\u0026quot; ) end rrset \u0026quot;example.com.\u0026quot;, \u0026quot;TXT\u0026quot; do ttl 300 resource_records( \u0026quot;\\\u0026quot;v=spf1 redirect=_spf.google.com\\\u0026quot;\u0026quot; ) end end hosted_zone \u0026quot;example.net.\u0026quot; do rrset \u0026quot;www.example.net.\u0026quot;, \u0026quot;A\u0026quot; do ttl 300 resource_records( \u0026quot;93.184.216.119\u0026quot; ) end rrset \u0026quot;example.net.\u0026quot;, \u0026quot;SPF\u0026quot; do ttl 300 resource_records( \u0026quot;\\\u0026quot;v=spf1 redirect=_spf.google.com\\\u0026quot;\u0026quot; ) end end DNS のルックアップ結果と Routefile を比較する # -t オプションを指定することで、実際に DNS サーバーに問い合わせた結果と比較します。\n$ roadwork -t zone 毎にファイルを分けた場合は -f でファイルを指定することで当該 zone だけを対象にテストできます。 気になる点\nTXT や SPF レコードの先頭、末尾の「\u0026quot;」を取り除いた上に連続するスペースを一つにまとめてから比較しているのがちょっと気になりました。quoted string なんだからそのまま比較するべきじゃないかな。「\u0026quot;」も取り除く必要はなさそう。\nDRY RUN # AWS に設定を反映させるには -a (apply) オプションを指定しますが、まずは --dry-run をつけて何が変更されるのかを確認しましょう。\n$ roadwork -a --dry-run Apply `Routefile` to Route53 (dry-run) No change ここの No change は差分がないという意味ではなく設定を変更しなかったという意味です。ここでは --dry-run なので当然 No change ですね。\n反映 # -a で反映させます。 更新内容も表示されてとっても便利です。例は A レコードの TTL だけ変更してみたもの。\n$ roadwork -a Apply `Routefile` to Route53 Update ResourceRecordSet: www.example.com. A set ttl=600 zone の追加・削除 # roadwork コマンドだけで zone の追加・削除も行えます。Routefile に zone を追加する（別ファイルならファイル作って Routefile で require）だけで apply (-a) すれば zone が追加されます。 www.example.com の A レコードを含む example.com zone を追加した例\n$ roadwork -a --dry-run Apply `Routefile` to Route53 (dry-run) Create HostedZone: example.com (dry-run) Create ResourceRecordSet: www.example.com A (dry-run) No change $ roadwork -a Apply `Routefile` to Route53 Create HostedZone: example.com Create ResourceRecordSet: www.example.com A とっても簡単に追加できて便利です。削除も同様にファイルから消して apply すれば zone が削除されます。これはちょっと怖いです。そのため --force を付けないと zone ごと消す処理は実行されません。他の変更は反映されます。\n$ roadwork -a --dry-run Apply `Routefile` to Route53 (dry-run) Undefined HostedZone (pass `--force` if you want to remove): example.com. (dry-run) No change $ roadwork -a Apply `Routefile` to Route53 Undefined HostedZone (pass `--force` if you want to remove): example.com. No change そこで、 route53:DeleteHostedZone Action を IAM に付与しないようにしてみたのですが、zone を削除する処理ではまず zone 内の全てのレコードを消した後に zone を消すようになっているので次のようなエラーになるものの空っぽの zone が残るだけで結局全部消えてしまいます。\n$ roadwork -a --force Apply `Routefile` to Route53 Delete HostedZone: example.com. Delete ResourceRecordSet: www.example.com. A User: arn:aws:iam::********:user/roadworker is not authorized to perform: route53:DeleteHostedZone on resource: arn:aws:route53:::hostedzone/******** でもまあ --force をあえて付けない限り消えないから大丈夫かな。 それより、Git や Subversion で管理するものの git pull や svn up を忘れて古いレコードを反映してしまうのが怖いかも。--dry-run と同様の処理後に Y/n のプロンプトが出て Y って入力しないと反映しないようにしたほうが安心かなぁ。 もしくは、リビジョン番号みたいな TXT レコードを作って、それがより大きな値に更新されてないと反映しないとかかな。\nハマった・・・ # 自前の BIND 管理から Route53 への移行で、移行中に -t のテストを試していてハマってしまいました。（全部私が悪いのですが） テストのために Roadworker で AWS に TXT と SPF を追加してみてテストしたら、ちょうどこの2つだけエラーになるんです。Ruby の net/dns がこの2つに対応してないということがわかり、dnsruby を使って書きなおしてみました。だがしかし、その後 Roadworker に含まれる net-dns-ext.rb が net/dns を拡張して対応済みなことに気づく。原因は試していたサーバーが当該ドメインを持っているコンテンツサーバーに問い合わせるようになっていたから Route53 に問い合わせていなかったというオチでした orz\u0026hellip; 移行中ということでコンテンツサーバーにも zone が残った状態でした。 （はい、キャッシュサーバーとコンテンツサーバーを分けろよという話ですね）\nSOA と NS レコードは？ # Amazon が自動で設定してくれるので管理する必要がないということで、標準では Export されたりしませんし、Routefile にも書きません。でも NS レコードはレジストラでネームサーバー指定するとき必要ですよね。そんな時は --with-soa-ns を付けて export すれば出てきます。もちろん AWS Console に行けば確認できます。\n$ roadwork -e --with-soa-ns zone 毎に違う NS レコードなので要注意。\nまとめ # Roadworker 大変便利なので Route53 使うなら是非使いましょう。菅原さんありがとう。\n菅原さんのスライド # AWSをコードで定義する from Sugawara Genki\n","date":"2014年1月15日","permalink":"/2014/01/managing-aws-route53-with-roadworker/","section":"Posts","summary":"DNS を AWS Route53 で管理するにあたり、ブラウザでポチポチやるのはやっぱり誤操作が怖いし、履歴の管理ができないよなぁということで Roadworker を試してみました。 イン","title":"AWS Route53 の管理に Roadworker を試した"},{"content":"","date":"2014年1月15日","permalink":"/tags/roadworker/","section":"Tags","summary":"","title":"roadworker"},{"content":"","date":"2013年12月22日","permalink":"/tags/advent-calendar/","section":"Tags","summary":"","title":"Advent Calendar"},{"content":"この投稿は Ansible Advent Calendar 2013 の23日目の記事です。 22日目: Ansibleを導入したい人の為のくどきポイント 前回の Advent Calendar ポストの翌日が3736回目の誕生日だったのですが、胃腸炎でゲロゲロしながら過ごした @yteraoka です。息子もゲロゲロしてて、ゲロゲロがゲロゲロを介護するという老老介護ならぬゲロゲロ介護でした。素晴らしい誕生日ですね。。。 最近の Ansible の話題と言えば ANSIBLEWORKS GALAXY ですが、こっちは明日、明後日できっと誰か書いてくれますよね。Ansible 開発者の書いた Playbook とか参考になりますね。へ〜、はぁ〜って見てました。っていうほどまだ見れてないけど今後、どんどん参考にしていきたい。 今日の話題は Ansible Tutorial をリニューアルしましたよという告知。まぁ、内容的にはほとんど変わってないわけですけど\u0026hellip; ググると上位に出てくるので Ansible の変化にもついていかないとなという事で。 一応変更点を上げると\n 1.4 でテストした かつて {{ foo }} が使えなかったモジュールで ${ foo } としていたところを {{ foo }} が使えるようになったので変更。${ fooo } は廃止予定のようです。 WordPress の Secret Key 設定に https://api.wordpress.org/secret-key/1.1/salt/ を使って register, stdout を活用 (php で require した方が良かった？) Ansible in detail は全然更新できなかったので Wiki に変更 その Wiki に failed_when, changed_when を追加。この機能便利です。 あと typo の修正とか細かい所は忘れました  もっと書きたいことはあったりしますが Wiki に追加して行こうと思います。 まだ、入門には使えるんじゃないかなと我ながら思ってます。Ansible 使ってみようかなって思ってる人は試してみてください。ではでは〜 明日は @r_rudi さんです。\n","date":"2013年12月22日","permalink":"/2013/12/ansible-tutorial-renewal/","section":"Posts","summary":"この投稿は Ansible Advent Calendar 2013 の23日目の記事です。 22日目: Ansibleを導入したい人の為のくどきポイント 前回の Advent Calendar ポストの翌日が3736回目の誕生","title":"Ansible Tutorial リニューアルしました"},{"content":"この投稿は Ansible Advent Calendar 2013 の13日目の記事です。 前日はこちら 2打席目です。 息子二人が連続で胃腸炎にかかりゲロとの戦いを強いられていて書こうと思っていたネタの準備ができなかったので、モジュール一覧ページを見て、「おーっ、こんな機能あったんだぁ！」って思ったものを書いてみます。\nfetch モジュールの validate_md5 # validate\\_md5={yes|no} ```というパラメータが 1.4 で追加されていました。fetch 元と fetch してきたファイルの md5sum を比較して正しくコピーできたことを確認できます。 しかし copy モジュールにはこの機能ないんですね。 代わりにといっては何ですが、 copy と template モジュールには``` validate=任意のコマンド ```というパラメータがあります。 `%s` というマクロが使えて、作成した一時ファイル（rename前）の path に置換されて実行されます。例えば Apache のコンフィグファイルであれば``` validate=\u0026quot;/usr/sbin/httpd -t -f %s\u0026quot; ```とすることで syntax check に失敗したら rename 前にエラーで終了させることができます。起動しない設定ファイルを反映してしまうことを防げます（もちろん syntax check では見つけられないミスもありますが）。 おっと、1.4 で lineinfile モジュールにもこの機能が追加されていました。 ### unarchive モジュール 1.4 で新規に追加されたモジュールです。 command モジュールで `tar xf ... chdir=/some/where` 使えば tar.gz でも tar.bz2 でも tar.xz も自動判別して展開はしてくれますが、このモジュールを使うと `tar` の `--diff` 機能で差分が見つかった場合のみ実際に展開されます。これはちょっと便利。changed かどうかは重要ですからね。 zip ファイルの場合は常に展開されるようです。 ### file モジュールの state=touch これまでは `file`, `directory`, `link`, `hard`, `absent` だけでしたが、`touch` が 1.4 で追加されていました。 Passenger アプリの deploy 時に tmp/restart.txt の timestamp を更新することでアプリだけリロードさせることができますが、こんな場合に使えますね。 もちろん `command: touch /some/where/tmp/restart.txt` でできるんですけど。 ### postgresql\\_privs 思ってたより便利そう。 ヤバい時間がない... ### postgresql\\_user 1.4 からパスワードが必須ではなくなってました。LDAP 認証時にはパスワード設定する必要ないから助かる。 [頓挫した技術系アドベントカレンダーの一覧(2013年)](http://dic.nicovideo.jp/a/%E9%A0%93%E6%8C%AB%E3%81%97%E3%81%9F%E6%8A%80%E8%A1%93%E7%B3%BB%E3%82%A2%E3%83%89%E3%83%99%E3%83%B3%E3%83%88%E3%82%AB%E3%83%AC%E3%83%B3%E3%83%80%E3%83%BC%E3%81%AE%E4%B8%80%E8%A6%A7(2013%E5%B9%B4)) に乗ってしまうと困るから今日はこれで終了。 3打席目やります。 明日は私の誕生日です（さらにおじさんに...） じゃなくって、明日は [@r\\_rudi](https://twitter.com/r_rudi) さんの2打席目です。","date":"2013年12月13日","permalink":"/2013/12/ansible-advent-calendar-13/","section":"Posts","summary":"この投稿は Ansible Advent Calendar 2013 の13日目の記事です。 前日はこちら 2打席目です。 息子二人が連続で胃腸炎にかかりゲロとの戦いを強いられていて書こうと思ってい","title":"Ansible 最近の発見"},{"content":"この投稿は Apache httpd Advent Calendar 2013 ではありません\u0026hellip; 2013-11-25 に Apache httpd 2.4.7 がリリースされました。 CHANGES_2.4.7 この中に\n *) mod_headers: Add \u0026lsquo;Header note header-name note-name\u0026rsquo; for copying a response headers value into a note. [Eric Covener]\n という変更を見つけました。これは!! 「Apache で Response Header を消しつつその値をログに書き出す」で書いた機能が Apache に追加されたようです。 当該部分のコードはこれ\ncase hdr_note: apr_table_setn(r-\u0026gt;notes, process_tags(hdr, r), apr_table_get(headers, hdr-\u0026gt;header)); break; 私は subprocess_env にコピーしたのですが、これは notes にコピーしていますね。 notes っていうのは module 間でデータを受け渡しできるメモ用テーブルです。 mod_log_config で %{VARNAME}n として書き出すことができます。「モニカジ#3に参加してきた」で触れた @kazeburo さんの mod_copy_header もこの notes で実装されてます。（あ、Example の LogFormat のところにミスが\u0026hellip;） mod_headers の note は次のように使います。CGI で出力するヘッダーを扱う場合は always が必要です（これでしばらくハマりました）。Proxy だったら不要。\nHeader [always] note X-Foo foo Header [always] unset X-Foo LogFormat \u0026quot;... %{foo}n\u0026quot; xxx Client に返したくないデータのはずなので unset をお忘れなく。返して良いヘッダーをログに書き出すだけなら %{X-Foo}o で出来ますし。\nおまけ # 2.4.7 の mod_headers には setifempty という機能も追加されていました。「Set If Empty」で名前の通りの機能ですね。\n *) mod_headers: Add \u0026lsquo;setifempty\u0026rsquo; command to Header and RequestHeader. [Eric Covener]\n ","date":"2013年12月9日","permalink":"/2013/12/mod_headers-note/","section":"Posts","summary":"この投稿は Apache httpd Advent Calendar 2013 ではありません\u0026hellip; 2013-11-25 に Apache httpd 2.4.7 がリリースされました。 CHANGES_2.4.7 この中に *) mod_headers: Add \u0026lsquo;Header note header-name note-name\u0026rsquo; for copying a response headers value into a note. [Eric Covener] という変更を","title":"mod_headers に note 機能が追加されました"},{"content":"ワイワイ！ この投稿は Fluentd Advent Calendar 2013 の7日目の記事です。 前日 10ヶ月ほど前、私が @tagomoris 氏のブログをコピペしながら人生初 rubygem として fluent-plugin-tail-asis を書いた時、http://fluentd.org/plugin/ の plugin はまだ2桁だったんじゃないかと思うのですが、今覗いてみるとなんと 209 です。もう全部を調べるわけにはいかない量です。Fluentd プラグインは簡単に書けることから調べるの面倒だから自分で書いちゃえってなりがちな気もします。実際、私も他では利用価値の無さそうな plugin はちょろっと書いて /etc/td-agent/plugin/ に放り込むだけだったりします。private な Git リポジトリには入っていますけども。 そこで、今回はよく使われてそうなプラグインを「こんなことをしたい場合にはこれ」っていうかたちで紹介したいと思います。とは言うものの、そんなにいろんな使い方してないので自分が使うかもなあってのをいくつかピックアップして紹介してみます。 4日目の @yoshi_ken さんの「Fluentdが流行る理由がいま分かる、10の実践逆引きユースケース集」っていうタイトルを見て「やべっ！逆引きだっ！」って思って焦りました。\nMongoDB に保存したい # fluent-plugin-mongo MongoDB 一番人気みたいです。私、全然知りませんけど。 データサイズを物理メモリより小さくしておけばイケてるらしいです。 Store Apache Logs into MongoDB\nログの内容によって振り分ける、フィルターする # fluent-plugin-rewrite-tag-filter 正規表現でマッチさせて tag を書き換えることによって送り先などを振り分けることができます。 不要なログは type null に送って捨てることができます。大変便利です。 レコードの内容によらず、ルーティングするには fluent-plugin-route が使えるようです。\nElasticsearch に保存して Kibana で可視化したい # fluent-plugin-elasticsearch Kibana 便利ですね。 Map 機能があって2桁の国コードを使って地図上に表現できます。fluent-plugin-geoip を使うことでIPアドレスから国コードを得ることができます。組み合わせて使うと便利です。 Free Alternative to Splunk Using Fluentd\nログの各行をそのまま送りたい・集めたい # format /^(.\\*)$/ とすればずっと昔からできたのですが、今は format none と指定することで正規表現を使わず処理できて CPU にやさしくなります。0.10.39 よりも前の fluentd を使ってる場合は拙作の fluent-plugin-tail-asis が使えます。これで JSON のひとつのキーに行がまるごと入ります。しかし、これを type file で書き出すと。\ntimestamp tag {\u0026quot;key\u0026quot;:\u0026quot;line\u0026quot;} となってしまうので fluent-plugin-file-alternative を使ってファイルに書き出します。指定のキーの値だけを1行として書きだしてくれます。\nログを AWS S3 に保存したい # fluent-plugin-s3 を使います。前項の行をそのまま送るという用途では fluent-plugin-s3-alternative が使えます。AWS のアクセスキーなどを設定ファイルに書きたくない、リポジトリに登録したくないという場合は fluent-plugin-config_pit が使えます。 Store Apache Logs into Amazon S3\nファイル名に日付を含むログを tail したい # fluent-plugin-tail-ex を使うことで glob(*) や strftime 記号を使ったファイルの指定を行うことができます。 ただし、 pos_file が使えません。また refresh_interval を短めに設定しておかないと refresh 後のログしか拾ってくれないので、最大でこの秒数のログを読み込むことができません。 ログを取りこぼしたくない場合は自前で symbolic link を張り直す処理を組んだほうが良いと思います。\n2014-02-07更新\n作者の方に大変申し訳無い。再度確認したら read_all っていう設定があって、新しいファイルは先頭から読んでくれるし、pos_file も有効ですね。私はなんでこんな大きな勘違いしたんだろうか？\nサーバー間の通信を暗号化したい # インターネット越しにログを forward で送るときに気になるポイントですね。fluent-plugin-secure-forward で通信を暗号化できるようです。\n指定の条件にマッチしたら通知を行いたい # ログから危険な兆候を見つけたりしたらメールやIRC、電話で通知したいという場合には fluent-plugin-filter を使って、条件を設定し、通知に使いたいプラグインに渡すと良いみたいです。ikachan を使って IRC に通知する fluent-plugin-ikachan とか、メールで通知する fluent-plugin-mail とかが使えそうです。 Twillio で電話をかけるっていうのもできそうです。fluent-plugin-twilio これがそのまま使えるかどうかはわからない。 Splunk-like Grep-and-Alert-Email System Using Fluentd\n肥大化しすぎた config ファイルをなんとかしたい # きまったパターンの設定がつらつらと並んでいる場合は fluent-plugin-forest を使うことでシンプルにできるかもしれません。\nTwitter を Input / Output に使いたい # fluent-plugin-twitter 今回、プラグインをつらつら眺めていて、これは面白そうだなと思いました。\nHTTPのステータスコードとか指定のパターン毎に行数をカウントしたい # fluent-plugin-numeric-counter がHTTP の Status 毎とか、200番台、300番台、400番台、500番台をそれぞれ数えるとかで使えるようです。この結果を fluent-plugin-growthforecast で GrowthForcast に渡せばグラフ化も簡単。でも、この用途だと今時は Kibana の方が良いかも。\nレスポンスタイムのタイル値を計測したい # fluent-plugin-numeric-monitor を使うことでサービスのレスポンスタイムのタイル値を計算することができます。98%タイル値とか重要ですよね。これはまだ Kibana ではできないので fluent-plugin-growthforecast と組み合わせるのが良さそうです。 min / max / avg / sum も出せます。\nFortiGate のログを FortiAnalyzer を買わずに可視化したい # そんな奇特な方はこちらのブログをどうぞ。「Fluentd + Kibana3 で FortiAnalyzer いらず」、「続オレオレFortiAnalyzer」\n 監視や可視化は @sonots さんのブログに大変わかりやすくまとめられています。\n sonots:blog : FluentdとGrowthForecastを使った可視化 〜 Haikanko OSS化への道(4) sonots:blog : fluentdを使ったログ監視 〜 Haikanko OSS化への道(3)   今回のネタのきっかけは↓コレです。おぉ、こんな便利なプラグインがあったの知らなかったよってことで。\n@repeatedly おぉ、rewrite-tag-filter は tag を書き換えるだけかと思ってスルーしてました。イケそうです。ありがとうございます。\n\u0026mdash; yteraoka (@yteraoka) October 30, 2013  明日はこの @repeatedly さんんで〜す。\n ところで、 fluentd の config parser はコメントとともに行末の空白を取り除いてしまうため、LTSV で\nlabel_delimiter \u0026quot;: \u0026quot; てなことができずに困ってしまいました。こういうのどうですかね？手を抜きすぎですかね？\n# diff -u /usr/lib64/fluent/ruby/lib/ruby/gems/1.9.1/gems/fluentd-0.10.39/lib/fluent/config.rb /tmp/config.rb --- /usr/lib64/fluent/ruby/lib/ruby/gems/1.9.1/gems/fluentd-0.10.39/lib/fluent/config.rb\t2013-09-25 05:23:14.000000000 +0900 +++ /tmp/config.rb\t2013-12-05 23:10:22.933934714 +0900 @@ -181,6 +181,7 @@  elsif m = /^([a-zA-Z0-9_]+)\\s*(.*)$/.match(line) key = m[1] value = m[2] + value.gsub!(/^\u0026#34;(.*)\u0026#34;$/, \u0026#39;\\1\u0026#39;) or value.gsub!(/^\u0026#39;(.*)\u0026#39;$/, \u0026#39;\\1\u0026#39;)  if allow_include \u0026amp;\u0026amp; key == \u0026#39;include\u0026#39; process_include(attrs, elems, value) else ","date":"2013年12月6日","permalink":"/2013/12/fluentd-plugins/","section":"Posts","summary":"ワイワイ！ この投稿は Fluentd Advent Calendar 2013 の7日目の記事です。 前日 10ヶ月ほど前、私が @tagomoris 氏のブログをコピペしながら人生初 rubygem として fluent-plugin-tail-asis を書いた時、http:/","title":"逆引き Fluentd plugins （更新あり）"},{"content":"この投稿は Ansible Advent Calendar 2013 の5日目の記事です。 前日はこちら こんにちは、Ansible Tutorial をそろそろリニューアルしないといけないなと思いつつ、更新すら全然できていない yteraoka です。あの Playbook は 1.2 時代に書いて 1.3 でも 1.4 でもテストしてないので動かないかもしれない\u0026hellip; WordPress だってバージョンの指定がもう古いし。 このブログの前回のポスト「ansible の copy/template で例外対応」で次は fileglob を試してみると書いたので、今日はそれをやってみます。 が、その前に\u0026hellip; 前回、with_first_found を試して copy モジュールは期待通りに動作したのに template はちょっと期待と違う動作となりました。なんでかなぁ？とコードをチラ見してたところこんな発見がありました。\n with_first_found と似て非なる first_available_file というものがあったのか #ansible\n— yteraoka (@yteraoka) 2013, 11月 27\n first_available_file を使うと、期待通りに role の templates ディレクトリの中を順番に探してくれました。 ヤッタネ！ では今日の本題 fileglob を使ったファイルのコピーです。『えっ！そんなの copy モジュールでディレクトリ毎コピーすればいいじゃん！』っていう Ansible マスターなお方はちょいとお待ちを。 copy モジュールの再帰コピーは 1.4 からの新機能 Ansible 1.4 Released! なんです。私、まだ 1.3 ユーザーなのです。 EPEL さんなかなか 1.4 にならないので只今 Python コンパイル中でござる。 では順番に。 テキトーにファイルを準備``` $ ls roles/test/files/conf.d aaa bbb ccc ddd.conf eee.conf\nfiles ディレクトリの conf.d/\\*.conf を対象にしてみる。 debug モジュール便利ですね。 $ cat roles/test/tasks/fileglob.yml #  debug: msg={{ item }} with_fileglob: conf.d/*.conf tags: test  実行 $ ansible-playbook -i hosts site.yml -l ansibletest1 -t test -v\nPLAY [ansibletest1] ***********************************************************\nGATHERING FACTS *************************************************************** ok: [ansibletest1]\nTASK: [debug msg=] ************************************************************ ok: [ansibletest1] =\u0026gt; (item=/home/ytera/ansible/roles/test/files/conf.d/ddd.conf) =\u0026gt; {\u0026ldquo;item\u0026rdquo;: \u0026ldquo;/home/ytera/ansible/roles/test/files/conf.d/ddd.conf\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;/home/ytera/ansible/roles/test/files/conf.d/ddd.conf\u0026rdquo;} ok: [ansibletest1] =\u0026gt; (item=/home/ytera/ansible/roles/test/files/conf.d/eee.conf) =\u0026gt; {\u0026ldquo;item\u0026rdquo;: \u0026ldquo;/home/ytera/ansible/roles/test/files/conf.d/eee.conf\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;/home/ytera/ansible/roles/test/files/conf.d/eee.conf\u0026rdquo;}\nPLAY RECAP ******************************************************************** ansibletest1 : ok=2 changed=0 unreachable=0 failed=0\n","date":"2013年11月6日","permalink":"/2013/11/ansible-fileglob/","section":"Posts","summary":"この投稿は Ansible Advent Calendar 2013 の5日目の記事です。 前日はこちら こんにちは、Ansible Tutorial をそろそろリニューアルしないといけないなと思いつつ、更新すら全","title":"ansible の copy でファイルを glob 指定する"},{"content":"例外対応って言っても、stacktrace 出すとかじゃないです。ほとんどのサーバーでは共通なんだけど、一部のサーバーでだけ変えたい設定への対応方法です。 変数の値だけで対応できる範囲なら、グループやデフォルトで設定する値をホストなどの変数で上書きしてあげればOKデスね。 こんな感じで``` test_src_filename: \u0026ldquo;default\u0026rdquo;\ntest\\_src\\_filename: \u0026quot;host01\u0026quot; - copy: src={{test_src_filename}}.conf dest=/etc/aaa/bbb.conf\nでも、それでは難しいって場合は with\\_first\\_found ってのが使えるんです。 [Selecting Files And Templates Based On Variables](http://www.ansibleworks.com/docs/playbooks_conditionals.html#selecting-files-and-templates-based-on-variables) で見つけました。 でも試したら、ここに書いてある通りでは動かなかったので [source code](https://github.com/ansible/ansible/blob/devel/lib/ansible/runner/lookup_plugins/first_found.py) を見て試してみました。 copy 元ファイルや template ファイルをリストで指定して、最初に見つかったファイルが使われるんです。 きっと使いたい場面が出てきますよね。 例 - copy: src={{item}} dest=/some/where/test.conf with_first_found: - files: - \u0026ldquo;{{ ansible_hostname }}.conf\u0026rdquo; - default.conf skip: true\nこの例のように files に変数を使う場合はクオートする必要がありました。シングルクオートでもダブルクオートでも可。 `skip: true` を付けるとマッチするファイルが見つけられなかった場合もエラーとはならず skip されます。この例の場合は default.conf にはマッチするように用意するはずですが、特定のホストでしか必要の無いファイルだったりする場合に使えますね。 `ignore_errors: true` でもエラー時にそこで停止せずに先に進めますが、この場合は register で登録した結果は failed です（ここを書き換える方法もありますが）。 source code のコメントにあるように探すディレクトリも paths: で複数指定できますし、files にディレクトリを含めることも可能です。 次はこれを template で試してみます。 - template: src={{item}} dest=/some/where/test.conf with_first_found: - files: - \u0026lsquo;{{ansible_hostname}}.conf.j2\u0026rsquo; - default.conf.j2\nこれでイケそうなものですが、なぜかファイル名が None で、role の template directory と ansible の root directory を探しにいってしまい、そんなのねーよとなってしまいます。 - template: src={{item}} dest=/some/where/test.conf with_first_found: - files: - \u0026lsquo;/home/ytera/ansible/roles/test/templates/{{ansible_hostname}}.conf.j2\u0026rsquo; - /home/ytera/ansible/roles/test/templates/default.conf.j2\nもしくは - template: src={{item}} dest=/some/where/test.conf with_first_found: - files: - \u0026lsquo;{{ansible_hostname}}.conf.j2\u0026rsquo; - default.conf.j2 paths: - /home/ytera/ansible/roles/test/templates/\n","date":"2013年11月6日","permalink":"/2013/11/ansible-with-first-found/","section":"Posts","summary":"例外対応って言っても、stacktrace 出すとかじゃないです。ほとんどのサーバーでは共通なんだけど、一部のサーバーでだけ変えたい設定への対","title":"#ansible の copy/template で例外対応"},{"content":"急遽 Mobile Router / Portable WiFi が必要になったので、当日その場で借りられるところを探してみた。 店頭ですぐ借りられるところがなかなか見つからなくって、東新宿にあるという「WiFi レンタル.com」というところまで行かなきゃダメなのかなぁと諦めかけていたところ、 オフィスのすぐ近くに PuPuRu というところがあったので、そこに行ってみました。 すぐ近くというだけで即決です。行ってみるとビルの一室で怪しげな感じもありましたし、そこで紙にクレジットカードの番号を書くのもちょいとためらわれましたが、借りました。 11月末まではキャンペーン期間でちょっと安いみたいです。 WiFiルータプラン 事務手数料が1,575円で、送料が返却分込で1,000円（直接取りに行って、直接返しに行けば0円） 壊したり、紛失したり、汚した時のための保険は任意で1,000円 後は利用日数に合わせてプランを選択です。1〜4日の利用であれば2,100円ですが、同月内という条件が曲者で、月末だと割高になったりします。私は、SSタイプ(2,100円)x2でそれを超えて必要だったら延長を申し込むという事にしました。延長は延長分の日数に合わせたタイプの料金 + 1,000円なのでSタイプにするより安くすむ場合はこれがお得です。 借りた機器は Pocket WiFi LTE（GL06P） です。 このブログ書こうと思ったら TRE MOBILE RENTAL ってところも近くにあったことを発見しました。 Nexus7 + GL06P ん？ Nexus 7 も一緒に貸してくれるのか！！ PuPuRu も iPad やスマホ貸してくれますけど。この Nexus 7 は安いですね。 料金は1日2日だけの利用であれば TRE がずっとお得ですね。 あぁ、そういえば Nexus 7 の無線LANはマジで糞だわぁ、先代も2013年版も… 2013は良くなってるかと思ったのに。売って iPad のどれかにするかなぁ、でも高いなぁ…\n","date":"2013年10月29日","permalink":"/2013/10/rent-a-mobile-router-from-pupuru/","section":"Posts","summary":"急遽 Mobile Router / Portable WiFi が必要になったので、当日その場で借りられるところを探してみた。 店頭ですぐ借りられるところがなかなか見つからなくって、東新宿にあ","title":"PuPuRu で Portable WiFi をレンタルしてみた"},{"content":"","date":"2013年10月29日","permalink":"/tags/rental/","section":"Tags","summary":"","title":"rental"},{"content":"前回、1.2 系と 1.3 系で変数の優先順位が変わったという記事を書きましたが、template 周りもちょっと変わってるみたいです。 1.2 では次のような loop 処理で、リストが未定義だった場合もエラーになることなく動作していたのですが、1.3.2 で動かしてみたらエラーになってしまいました。``` {% for filter in iptables_input_filters %} -A INPUT {{filter}} {% endfor %}\nということで次のように対応しました。 {% if iptables_input_filters is defined %} {% for filter in iptables_input_filters %} -A INPUT {{filter}} {% endfor %} {% endif %}\n### サーバーの情報集めて利用したい Ansible は Server とか Agent が不要なのが売りなわけで、そこに惹かれて選んでいるのですが Chef Server とかにもちょっと憧れるわけです。情報を一元管理できたらいいなぁって。 そうしたら、なんだか setup モジュールが使えそうだったんですね。 [https://github.com/ansible/ansible/blob/devel/library/system/setup](https://github.com/ansible/ansible/blob/devel/library/system/setup) EXAMPLES の中に次のような例がありました。``` \\# Display facts from all hosts and store them indexed by I(hostname) at C(/tmp/facts). ansible all -m setup --tree /tmp/facts ```これで /tmp/facts にホスト名をファイル名にした JSON ファイルが収集されるんです。よし、じゃあこれを DB に入れちゃおう。（例では使われていませんが ansible に -s オプションをつけてあげると sudo を使うっていう意味で、root じゃないと取得できないデータがあるのでつけた方が良いです） Ansible の Fact って何？って場合はこちらをどうぞ [http://yteraoka.github.io/ansible-tutorial/ansible-in-detail.html#gathering-facts](http://yteraoka.github.io/ansible-tutorial/ansible-in-detail.html#gathering-facts) そして、ohai とか [factor](https://github.com/puppetlabs/facter) の情報も取得できるみたいです。 [Ansibleを支えるfact: プラットフォームの情報を取得](http://tdoc.info/blog/2013/08/23/ansible_fact.html) ### DB っていってもどれにいれようかな？ ってことで最初に試したのは PostgreSQL の JSON 型です。PostgreSQL には慣れてるし、9.3 で JSON 機能が強化されたみたいだしと、早速試したら... ん？エラーで入らない... どうやら 4kB 制限があるっぽい。 4kB 超えたらエラーで INSERT できない。 勘違いでした、別の環境でやり直したら INSERT できました なんだったんだろう？再現テストしてみよう。 \u0026gt; ダメだったのは Linux Mint (Cinnamon) の gnome-terminal で JSON を psql でコピペした場合。Windows から PuTTY で同じ事をしたら問題なかった。gnome-terminal でもファイルにコピペして SQL ファイルとして psql -f したら問題なかった。 他に JSON 突っ込める DB といえば MongoDB!! （今日初めて触る） 今日（もう昨日か）もまた Twitter で dis られてた MongoDB、ガチで使うなら渋谷で人さらいしてこないとダメらしいけど、今回のようなゆるふわ利用なら問題ないんじゃないかなと。 EPEL から yum で入るので簡単に起動までも簡単。``` $ sudo yum -y install mongodb mongodb-server $ sudo /sbin/service mongod start ```なんか [mongoimport](http://docs.mongodb.org/manual/reference/program/mongoimport/#mongoimport) なるコマンドがあるのでこれ使えるのかな？``` $ mongoimport -d ansible -c facts20131022 --type json --file /tmp/facts/hostname exception:BSON representation of supplied JSON is too large: code FailedToParse: FailedToParse: Field name expected: offset:1 ```ダメポ... 1ドキュメントは1行にしないとダメらしい。 つーことで、こんな感じでまるっとインポート。``` ansible -s -m setup -i hosts -t /tmp/facts all for file in \\`ls /tmp/facts/\\*\\` do cat $file | tr -d \u0026quot;\\\\n\u0026quot; echo done | mongoimport -d ansible -c facts$(date +%Y%m%d) --type json --file - ```（最後の「-」が抜けてることに気づいたので追記 2013-10-24）``` $ mongo MongoDB shell version: 2.4.6 connecting to: test \u0026gt; show dbs ansible\t0.203125GB local\t0.078125GB \u0026gt; use ansible switched to db ansible \u0026gt; show collections facts facts20131022 system.indexes \u0026gt; db.facts20131022.find().count() 21 ```インポートできたっぽい。コレクション名に日付を入れておいたので過去のも見れるし、不要ならまるっと削除しちゃおう。（update とか面倒） コレクションの削除はこんな感じ。``` $ mongo MongoDB shell version: 2.4.6 connecting to: test \u0026gt; show dbs ansible\t0.203125GB local\t0.078125GB \u0026gt; use ansible switched to db ansible \u0026gt; show collections facts facts20131022 system.indexes \u0026gt; db.facts20131022.drop() true \u0026gt; show collections facts system.indexes \u0026gt; ```検索してみる。Xeon のサーバー一覧とOpteron のサーバー一覧``` \\\u0026gt; db.facts20131022.find({\u0026quot;ansible\\_facts.ansible\\_processor\u0026quot;: /Xeon/},{\u0026quot;\\_id\u0026quot;:0,\u0026quot;ansible\\_facts.ansible\\_hostname\u0026quot;:1}) { \u0026quot;ansible\\_facts\u0026quot; : { \u0026quot;ansible\\_hostname\u0026quot; : \u0026quot;vm03\u0026quot; } } { \u0026quot;ansible\\_facts\u0026quot; : { \u0026quot;ansible\\_hostname\u0026quot; : \u0026quot;vm04\u0026quot; } } { \u0026quot;ansible\\_facts\u0026quot; : { \u0026quot;ansible\\_hostname\u0026quot; : \u0026quot;vm05\u0026quot; } } \u0026gt; db.facts20131022.find({\u0026quot;ansible\\_facts.ansible\\_processor\u0026quot;: /Opteron/},{\u0026quot;\\_id\u0026quot;:0,\u0026quot;ansible\\_facts.ansible\\_hostname\u0026quot;:1}) { \u0026quot;ansible\\_facts\u0026quot; : { \u0026quot;ansible\\_hostname\u0026quot; : \u0026quot;vm01\u0026quot; } } { \u0026quot;ansible\\_facts\u0026quot; : { \u0026quot;ansible\\_hostname\u0026quot; : \u0026quot;vm02\u0026quot; } } ```超やっつけですが、一応出来ました。 MAC アドレスからサーバーを探したり、「あの壊れたサーバーのシリアル/サービスタグってなんだっけ？」という場合にも使える。 ### カスタム Facts 「あの仮想ゲストはどのホストに載ってたっけなぁ？」を解決するために、VM ホストで /etc/ansible/facts.d/guests.fact を次のようなスクリプトにして作成し、実行権限をつけておけば setup module が実行して収集してくれます。 動的でないものは JSON 書いたテキストファイルで OK。拡張子を .fact にする必要があるのと、ファイル名が JSON のキーになるという仕様。``` $ cat /etc/ansible/facts.d/guests.fact #!/bin/sh guests=( $(virsh list --name) ) list=\u0026quot;\u0026quot; for name in ${guests\\[@\\]} do let i=i+1 list=\u0026quot;${list}\\\\\u0026quot;${name}\\\\\u0026quot;\u0026quot; if \\[ $i -ne ${#guests\\[@\\]} \\] ; then list=\u0026quot;${list}, \u0026quot; fi done echo \u0026quot;\\[ $list \\]\u0026quot; ```こんな感じで収集されます。``` { \u0026quot;ansible\\_facts\u0026quot;: { \u0026quot;ansible\\_local\u0026quot;: { \u0026quot;guests\u0026quot;: \\[ \u0026quot;guest1\u0026quot;, \u0026quot;guest2\u0026quot;, \u0026quot;guest3\u0026quot; \\] } } } ```ホスト名に kibana を含む Guest の Host を探す``` \\\u0026gt; db.facts20131022.find({\u0026quot;ansible\\_facts.ansible\\_local.guests\u0026quot;: /kibana/},{\u0026quot;\\_id\u0026quot;:0,\u0026quot;ansible\\_facts.ansible\\_hostname\u0026quot;:1}) { \u0026quot;ansible\\_facts\u0026quot; : { \u0026quot;ansible\\_hostname\u0026quot; : \u0026quot;vm05\u0026quot; } } setup module の filter 機能 # setup module は filter 機能もあるので、必要な情報だけに絞って取り出すこともできます。``` $ ansible all -s -i hosts -m setup -a \u0026lsquo;filter=ansible_product_*\u0026rsquo;\ntestdb1 | success \u0026raquo; { \u0026ldquo;ansible_facts\u0026rdquo;: { \u0026ldquo;ansible_product_name\u0026rdquo;: \u0026ldquo;KVM\u0026rdquo;, \u0026ldquo;ansible_product_serial\u0026rdquo;: \u0026ldquo;NA\u0026rdquo;, \u0026ldquo;ansible_product_uuid\u0026rdquo;: \u0026ldquo;CFB8DD36-FD8A-3FE5-D8AA-57B60878AD0A\u0026rdquo;, \u0026ldquo;ansible_product_version\u0026rdquo;: \u0026ldquo;RHEL 6.3.0 PC\u0026rdquo; }, \u0026ldquo;changed\u0026rdquo;: false }\n ### MongoDB 以外の選択肢？ ElasticSearch って使えるのかな？ CouchDB ってのも JSON らしいな。 Ansible のインベントリファイル(-i で指定するやつ)はテキストファイルでなくて、スクリプトなどを、指定してDBから引っ張って来させたりできるので、いろいろ発展の余地はありますね。","date":"2013年10月21日","permalink":"/2013/10/ansible-fatcs/","section":"Posts","summary":"前回、1.2 系と 1.3 系で変数の優先順位が変わったという記事を書きましたが、template 周りもちょっと変わってるみたいです。 1.2 では次のような","title":"Ansible の fatcs (インベントリ情報) を MongoDB に突っ込む"},{"content":"","date":"2013年10月21日","permalink":"/tags/mongodb/","section":"Tags","summary":"","title":"mongodb"},{"content":"Changes in Ansible Variable Precedence Between v1.2 and v1.3 を見たら Ansible の変数の優先順が version 1.2 と 1.3 で変わってるって書いてあるじゃないですか！！ まだ本格導入してないから大丈夫だけど、この手のツールのバージョンアップは慎重に行う必要がありますね。 やらなきゃなぁと思っていた変数の優先度整理をこれを機にやってみました。 https://github.com/cookrn/ansible_variable_precedence に変数の優先度確認用の Playbook があったので、これを参考にテストしてみました。 上記の README.md には順序が次のように書かれていましたが、あれ？ちょっと違うんじゃね？というのと、もうちょい詳しく知りたいと思ってテスト用 Playbook を書いてテストしてみました。\n  Register Variables Ansible assigned fact vars Role Dependency Parameters Vars file vars Command line extra var Playbook vars Playbook Role parameter Role var Inventory Host variable Inventory Group variable Role default variable   setup モジュールで設定される facts については ansible_ prefix が付くし、被らないかなということでテストに含めませんでした。 ちなみに、オレオレ fact を /etc/ansible/facts.d/test.fact に書くとこんな感じで読み込まれました。``` { \u0026ldquo;ansible_facts\u0026rdquo;: { \u0026ldquo;ansible_local\u0026rdquo;: { \u0026ldquo;test\u0026rdquo;: { \u0026ldquo;test_var\u0026rdquo;: \u0026ldquo;a fact var\u0026rdquo; } } } }\n ### 1.3 でのテスト結果 ansible のバージョン 1.3.2 でのテスト結果です。``` $ ansible --version ansible 1.3.2  当該 task 中で register を使って登録した値 コマンドラインオプションで指定した値``` \u0026ndash;extra-vars \u0026ldquo;name=value\u0026rdquo;  (依存先 role においては meta/main.yml の dependencies 内で設定した値)``` dependencies:  role: dep_role test_var: \u0026ldquo;set in dependencies\u0026rdquo;   依存 role の task の中で register を使って登録した値 playbook ファイルの role 指定するところで設定した値``` roles: - { role: role_name, var_name: 123}  playbook の vars_file で指定したファイル内で設定した値``` vars_file: - vars/test.yml  playbook の vars で指定した値``` vars: test_var: defined in playbook vars  当該 role の vars/main.yml で設定した値 依存 role の vars/main.yml で設定した値 inventroy ファイルのホスト変数\\[local\\] localhost test\\_var=\u0026quot;defined in inventory:host\u0026quot;複数のグループに所属するために、同一ホストが複数回登場する場合は、最後に設定された変数が有効となる inventory ファイルのグループ変数\\[local:vars\\] test\\_var=\u0026quot;defined in inventory:group\u0026quot; host_vars ディレクトリのホスト名ファイルで設定した値 group_vars ディレクトリのグループ名ファイルで設定した値\n(複数グループに入っている場合にどれが適用されるかは source code を読まないとわからない。後日読んでみる) group_vars/all ファイルで設定した値 当該 role の defaults/main.yml で設定した値 依存 role の defaults/main.yml で設定した値  変数優先度のテスト結果は次の通りです。変数定義する箇所多すぎ。\n1.2 でのテスト結果 # 1.2.2 でもテストしてみました。``` $ ansible \u0026ndash;version ansible 1.2.2\n1. 当該 task の register で登録した値 2. コマンドラインオプションで設定した値 3. playbook ファイルの role 指定するところで設定した値 4. playbook の vars\\_file で指定したファイル内で設定した値 5. 当該 role の vars/main.yml で設定した値 6. playbook の vars で指定した値 7. inventroy ファイルのホスト変数 8. inventory ファイルのグループ変数 9. host\\_vars ディレクトリのホスト名ファイルで設定した値 10. group\\_vars ディレクトリのグループ名ファイルで設定した値 11. group\\_vars/all ファイルで設定した値","date":"2013年10月19日","permalink":"/2013/10/ansible-precedence-rules/","section":"Posts","summary":"Changes in Ansible Variable Precedence Between v1.2 and v1.3 を見たら Ansible の変数の優先順が version 1.2 と 1.3 で変わってるって書いてあるじゃないですか！！ まだ本格導入してないから大丈夫だけど、この手","title":"Ansible の変数の優先順"},{"content":"","date":"2013年10月19日","permalink":"/tags/pagespeed/","section":"Tags","summary":"","title":"pagespeed"},{"content":"","date":"2013年10月19日","permalink":"/tags/png/","section":"Tags","summary":"","title":"PNG"},{"content":"Google PageSpeed Insights で画像ファイルはもっと小さくなるよって注意されるので OptiPNG を試してみた。 CentOS で EPEL にパッケージがあったのでインストールはこれだけ。\n$ sudo yum -y install optipng とりあえず実行してみる\n$ optipng OptiPNG 0.6.4: Advanced PNG optimizer. Copyright (C) 2001-2010 Cosmin Truta. Synopsis: optipng [options] files ... Files: Image files of type: PNG, BMP, GIF, PNM or TIFF Basic options: -?, -h, -help\tshow the extended help -o optimization level (0-7)\tdefault 2 -v\tverbose mode / show copyright and version info Examples: optipng file.png\t(default speed) optipng -o5 file.png\t(moderately slow) optipng -o7 file.png\t(very slow) Type \u0026quot;optipng -h\u0026quot; for extended help. Help を確認\n$ optipng -h OptiPNG 0.6.4: Advanced PNG optimizer. Copyright (C) 2001-2010 Cosmin Truta. Synopsis: optipng [options] files ... Files: Image files of type: PNG, BMP, GIF, PNM or TIFF Basic options: -?, -h, -help\tshow this help -o optimization level (0-7)\tdefault 2 -v\tverbose mode / show copyright and version info General options: -fix\tenable error recovery -force\tenforce writing of a new output file -keep\tkeep a backup of the modified files -preserve\tpreserve file attributes if possible -quiet\tquiet mode -simulate\tsimulation mode -snip\tcut one image out of multi-image or animation files -out write output file to -dir write output file(s) to -log log messages to --\tstop option switch parsing Optimization options: -f PNG delta filters (0-5)\tdefault 0,5 -i PNG interlace type (0-1)\tdefault -zc zlib compression levels (1-9)\tdefault 9 -zm zlib memory levels (1-9)\tdefault 8 -zs zlib compression strategies (0-3)\tdefault 0-3 -zw zlib window size (32k,16k,8k,4k,2k,1k,512,256) -full\tproduce a full report on IDAT (might reduce speed) -nb\tno bit depth reduction -nc\tno color type reduction -np\tno palette reduction -nx\tno reductions -nz\tno IDAT recoding Optimization details: The optimization level presets -o0 \u0026lt;=\u0026gt; -o1 -nx -nz -o1 \u0026lt;=\u0026gt; [use the libpng heuristics]\t(1 trial) -o2 \u0026lt;=\u0026gt; -zc9 -zm8 -zs0-3 -f0,5\t(8 trials) -o3 \u0026lt;=\u0026gt; -zc9 -zm8-9 -zs0-3 -f0,5\t(16 trials) -o4 \u0026lt;=\u0026gt; -zc9 -zm8 -zs0-3 -f0-5\t(24 trials) -o5 \u0026lt;=\u0026gt; -zc9 -zm8-9 -zs0-3 -f0-5\t(48 trials) -o6 \u0026lt;=\u0026gt; -zc1-9 -zm8 -zs0-3 -f0-5\t(120 trials) -o7 \u0026lt;=\u0026gt; -zc1-9 -zm8-9 -zs0-3 -f0-5\t(240 trials) The libpng heuristics -o1 \u0026lt;=\u0026gt; -zc9 -zm8 -zs0 -f0\t(if PLTE is present) -o1 \u0026lt;=\u0026gt; -zc9 -zm8 -zs1 -f5\t(if PLTE is not present) The most exhaustive search (not generally recommended) [no preset] -zc1-9 -zm1-9 -zs0-3 -f0-5\t(1080 trials) Examples: optipng file.png\t(default speed) optipng -o5 file.png\t(moderately slow) optipng -o7 file.png\t(very slow) optipng -i1 -o7 -v -full -sim experiment.png なんかいろいろオプションはあるみたいだけど、とりあえず optimization level は max で全部（数は少ない）最適化してみる\n$ find . -type f -name '*.png' -print0 | xargs -0 optipng -o7 very slow って書いてあるだけあったかなり時間がかかりますね。対した画像じゃないのに。 試したのはさくらのVPS 2Gプランのサーバーです。 /proc/cpuinfo 見ると Xeon E5645 とありますね。そんなに悪くない。どれだけ Over commit されてるかわかりませんけど。 うちの KVM は Guest からだと QEMU Virtual CPU って表示されるんだけどなぁって思ってググったら -cpu host なんていうオプションがあったんですね。試してみよう。 サーバ屋日記: kvmによる仮想マシン\u0026quot;-cpu host\u0026quot;オプションで性能向上する場合がある /etc/libvirt/qemu/xxx.xml に次の1行を追加すれば良いようだ（2行に分割されるけど）。\n","date":"2013年10月19日","permalink":"/2013/10/optipng/","section":"Posts","summary":"Google PageSpeed Insights で画像ファイルはもっと小さくなるよって注意されるので OptiPNG を試してみた。 CentOS で EPEL にパッケージがあったのでインストールはこれだけ。 $ sudo yum -y install optipng","title":"PNG を最適化"},{"content":"","date":"2013年10月19日","permalink":"/tags/qemu/","section":"Tags","summary":"","title":"QEMU"},{"content":"Fluentd + Kibana3 で FortiAnalyzer いらず の続きです。 前回、CSVフォーマットに対応しようと書いたので対応させてみました。Parser はまたベンチマークとってみたところ、CSVじゃないバージョンよりも軽くなってました。 これも Gist に置いてあります。 https://gist.github.com/yteraoka/7043113 ただ、 CSV フォーマットにも罠がありました！ strptime になぜかコケる\u0026hellip;\nOct 19 00:00:00 fortigate date=2013-10-18,time=23: 59:59,... ん？んんん？時と分の間に謎の空白が！！\nFortiGate の Syslog 設定について # Webインターフェースでは Syslog に traffic ログや webfilter ログを出力する設定が無いのですが、CLI で操作することで出力することができます。 Syslog も FortiAnalyzer 用出力もそれぞれ3つの出力設定が可能です。 fortigate-cli-40-mr3.pdf\n syslogd syslogd2 syslogd3 fortianalyzer fortianalyzer2 fortianalyzer3  2,3 は GUI からアクセスできません。GUI からアクセスできる syslogd も traffic ログなどを設定する項目は GUI に無いので、これは触らないことにしておいて syslogd2 を使ってこれらのログを出力することにします。\n$ show log syslogd2 setting config log syslogd2 setting set status enable set server \u0026quot;10.20.30.40\u0026quot; set csv enable set facility local6 end syslog (2じゃない) の出力項目の確認をしてみます。\n$ show log syslogd filter config log syslogd filter set email disable set traffic disable set web disable set infected disable end email, traffic, web, infected が disable にされています。だから syslog2 では filter は空っぽにしておけば全部出せます。\nstrptime の cache # strptime って実は結構重い処理だったということで、最近、Fluentd の TextParser には strptime の結果を cache する仕組みが入りました。FortiGate のログも traffic ログや webfilter ログを出してると秒間そこそこの量のログが吐き出されます。こういう場合は cache が欲しいです。追加してみよう。\nパフォーマンス面 # 30,000〜50,000 lines / min くらいでは特に問題は発生していないですね。 不要な項目は ElasticSearch に送らないようにしているので 5GB のログファイルから生成される ElasticSearch のインデックスファイルは 1GB 程度。 ElasticSearch のサーバーは KVM Guest で 2GB RAM、 JVM の heap は 1GB. KVM Host の Storage は RAID5 でそんなに速くない。CPU は Xeon 5570 で 2 vCPU 割り当てている。でも CPU はあまり使われていない。 特に困ったこともないのでチューニングもしていない。 syslog サーバーでの Fluentd の CPU 使用率は高い。\n","date":"2013年10月19日","permalink":"/2013/10/fluentd-kibana3-fortianalyzer-2/","section":"Posts","summary":"Fluentd + Kibana3 で FortiAnalyzer いらず の続きです。 前回、CSVフォーマットに対応しようと書いたので対応させてみました。Parser はまたベンチマークとってみたとこ","title":"続オレオレFortiAnalyzer"},{"content":"2013-10-15 ちょいと更新 いや、FortiAnalyzer 使ったこと無いんでホントに代わりになるかどうか知らないんですけど、お高いんですよ（ね？）今はクラウド版っつーのもあるみたいですね。 で、FortiAnalyzer が無いとログをマトモに見れないのかなぁなんて思ってたんですけど、どうやって Analyzer にログ送ってるんだろう？って試したら Syslog (UDP) だったんですね。TCP の 514 にも謎のパケットが飛んでるけどそっちは放置。 Syslog だったらファイルに書き出したものを Fluentd でなんとでもなるぞと。 ログのフォーマットは次のような感じで key=value がスペース区切りで並んでいます。value にスペースが含まれる可能性のある場合はクオートされます key=\u0026ldquo;aaa bbb\u0026rdquo; のように。ただし、 service という項目だけはスペースを含む可能性があるのにクオートされません。でも幸い、この項目にスペースが入るのはカスタム設定で自分でスペースを入れて設定した場合のみなので回避可能です。\nOct 10 00:00:00 192.168.1.1 date=2013-10-09 time=23:59:59 devname=FG..... device_id=FG... \\ type=traffic subtype=other pri=warning status=deny vd=\u0026quot;root\u0026quot; \\ src=192.168.1.2 srcname=192.168.1.2 src_port=1234 \\ dst=192.168.1.255 dstname=192.168.1.255 \\ dst_country=\u0026quot;Reserved\u0026quot; src_country=\u0026quot;Reserved\u0026quot; .... これの parser を Fluentd の plugin として書きました。 https://gist.github.com/yteraoka/6911252 FortiGate のログはすごく項目が多くて、Kibana3 で見る必要の無い項目は ElasticSearch に入れたくなかったので必要な項目だけに絞るか、不要な項目を削るという機能を入れました。需要なさそうだから gem にしたりしない。/etc/td-agent/plugins/ に置けば使えるので。\n ※ 2013-10-15追記 (1) 書いてる時からヒドいなぁと思いながらいた Loop での gsub! はやっぱりとっても重いので書き直しました。Gist に Bench も載っけておきました。\n ここまで来れば、後は fluent-plugin-elasticsearch で ElasticSearch に突っ込んで Kibana3 で自由自在です。Kibana3 については前回の 「Kibana3 を使ってみよう」 をどうぞ。 FortiGate のログには src_country, dst_country という項目に国名を出力してくれます。でも、2文字の国コードじゃないので http://dev.maxmind.com/geoip/legacy/geolite/ あたりからリストをダウンロードして置換してあげれば Kibana3 の World Map 機能が使えそうです。 Syslog は rsyslog の Template 機能で出力先を日別に変わるようにしているのですが、この場合、Fluentd 付属の in_tail ではそのままでは日をまたげません。でも fluent-plugin-tail-ex ってのがあるよって教えてもらいました。\n @yteraoka 遅レスすみません。。これ使ってます。 http://t.co/hNJveaOJMZ\n— 名前 (@majesta0110) October 3, 2013\n これは便利そうだという事で試してみたものの、pos_file が効かないのと、停止時になんかエラーが出るのが気になったので見送りました。詳細は調べてない。またオレオレ plugin を書こうかとも思いましたが、 fluent-plugin-tail-asis ほど簡単にも書けなさそうだったから symlink を張り替えるスクリプトを書くことで対応しました。ところで、今回は parse 処理は out_fortigate_syslog_parser.rb で行うので tail で parse する必要がないため、使うなら fluent-plugin-tail-ex-asis の方がより合っていそうです。（tail-asis 相当の機能は none parser として本家に merge されたので asis の役割は終わりました） rsyslog の Template を使った例\n$template HostSeparatedDailyLogFile,\u0026quot;/some/where/%syslogfacility-text%/%$YEAR%%$MONTH%%$DAY%/%HOSTNAME%\u0026quot; *.* ?HostSeparatedDailyLogFile 「Kibana3 を使ってみよう」 の中で触れた ElasticSearch の Template 機能ですがちょうど良い例を見つけました。 https://gist.github.com/deverton/2970285 これを参考にすると良さげです。ElasticSearch には ip っていうタイプもあったので IP アドレスのところはこれを使ってみました。 FortiGate ユーザーのみなさん、FortiAnalyzer 買ってなかったら試してみてください。ではでは。\n ※ 2013-10-15追記 (2) @ipv6labs さんから「区切り文字カンマでロギングすると楽ですよ」と助言をいただき、「え？ CSV 出力って syslog の設定にしかないんじゃないの？そして syslog だと traffic ログって出せないんじゃ？？」って思ってたん出すけどやり取りの中で config log syslogd filter で設定できそうなことがわかりました。ありがとうございます。 log : {disk | fortianalyzer | fortianalyzer2 | fortianalyzer3 | memory | syslogd | syslogd2 | syslogd3 | webtrends | fortiguard} filter でもまぁ、カンマ区切りにしても\n \u0026gt; dns_name=\u0026quot;www.googleadservices.com\u0026quot;,dns_ip=\u0026quot;173.194.38.90,173.194.38.89,173.194.38.77\u0026quot; \u0026gt; \u0026gt; とか、値にもカンマが入るし、 CSV って言っても値が key=\u0026ldquo;value\u0026rdquo; だし\u0026hellip; まぁ、書きなおさなくてもいっかな。 あ、そもそもの問題は key に空白が含まれる可能性があるってやつだからやっぱりカンマ区切りにしておこう。\n","date":"2013年10月10日","permalink":"/2013/10/fluentd-kibana3-fortianalyzer/","section":"Posts","summary":"2013-10-15 ちょいと更新 いや、FortiAnalyzer 使ったこと無いんでホントに代わりになるかどうか知らないんですけど、お高いんですよ（ね？）今はク","title":"Fluentd + Kibana3 で FortiAnalyzer いらず (更新あり)"},{"content":"","date":"2013年10月10日","permalink":"/tags/syslog/","section":"Tags","summary":"","title":"syslog"},{"content":"","date":"2013年10月7日","permalink":"/tags/lua/","section":"Tags","summary":"","title":"lua"},{"content":"画像サーバーなどでログインチェックを Apache + mod_perl で実装していましたが、古代のクライアントへの対応はもう不要だろうということで、mod_perl やめたいし、もっとシンプルな実装にできそうだから nginx + mod_lua を試してみようとやってみました。\nキャッシュとして memcached を使いたかったので https://github.com/agentzh/lua-resty-memcached も込で全部入れてくれる OpenResty をインストールしました。\nOpenResty のインストール # $ tar xvf ngx\\_openresty-1.4.2.9.tar.gz $ cd ngx\\_openresty-1.4.2.9 $ ./configure --prefix=/opt/ngx\\_openresty-1.4.2.9 --with-luagit $ make $ sudo make install Lua モジュールについては HttpLuaModule ( http://wiki.nginx.org/HttpLuaModule ) にドキュメントがあります。Lua でコンテンツを返す場合は content_by_lua, content_by_lua_file を使いますが、認証処理を行うには access_by_lua, access_by_lua_file を使います。他にも header_filter_by\u0026hellip; や rewrite_by\u0026hellip;, body_filter_by\u0026hellip; があります。 *_file の方は Lua を別ファイルとして読み込みます、compile 済みのファイルを指定することも可能です。 *_file でない方は Nginx の設定ファイルに直接コードを書きます。\n今回は認証フィルタの話なので access_by_lua* についてのみ。\n簡単な例 # 例えば、IE からのアクセスを拒否したい（403 Forbidden を返したい）という場合は次のように書けます（これだけなら Lua 不要ですよね、たぶん。Nginxはまだ詳しく知らない）。途中で ngx.exit せずに最後までいくか、途中で ngx.exit(ngx.OK) で終了すればアクセスは許可されます。\nlocation / { access\\_by\\_lua ' ngx.log(ngx.DEBUG, \u0026quot;User-Agent: \u0026quot;, ngx.var.http\\_user\\_agent) if string.match(ngx.var.http\\_user\\_agent, \u0026quot;MSIE\u0026quot;) then ngx.exit(ngx.HTTP\\_FORBIDDEN) end '; } 別サーバーへ問い合わせる # ログイン済みかどうかを確認するためには cookie を確認すると思います。LuaModule で Foo という名前の cookie の値を取得するには ngx.var.cookie_Foo とします。変数に入れるには\nlocal cookie\\_value = ngx.var.cookie\\_Foo とします。Cookie 名が変数に入っている場合は次のようにすることで取り出せます。\nlocal cookie\\_name = \u0026quot;Foo\u0026quot; local cookie\\_value = ngx.var\\[\u0026quot;cookie\\_\u0026quot; .. cookie\\_name\\] 取り出した cookie の値からそのリクエストが有効かどうかを判定するために、別のサーバーに問い合わせる必要があります。memcached に入ってるなら直接そのサーバーに問い合わせるという方法もありますね。でもアプリで Consistent Hashing とかしてると困りますね。twemproxy 使ってれば大丈夫ですかね。 でも今回は Web サーバーに GET で問い合わせる方法を説明します。\nngx.location.capture() を使うことで、別の URI へリクエストを出すことができます。が、ngx.location.capture(\u0026ldquo;http://example.com/auth?session=XXXX\u0026quot;) などと直接別のサーバーを指定することはできません。 これをどうするかというと lua-nginx-module の紹介 ならびに Nginx+Lua+Redisによる動的なリバースプロキシの実装案 - hibomaのはてなダイアリー で紹介されているように\nupstream \\_session\\_server { server session.example.com:80; } server { listen 80; server\\_name localhost; location / { access\\_by\\_lua ' local sessid = ngx.var.cookir\\_SESSION\\_COOKIE if sessid then local res = ngx.location.capture(\u0026quot;/\\_auth/session?sessid=\u0026quot; .. sessid) if res.status == 200 and string.match(res.body, \u0026quot;OK\u0026quot;) then ngx.exit(ngx.OK) end end ngx.exit(ngx.HTTP\\_FORBIDDEN) '; } location /\\_auth { internal; rewrite ^/\\[^/\\]\\*/(.\\*) /$1 break; proxy\\_pass http://\\_session\\_server; } } てな感じで、/_auth などを経由して proxy させることができます。internal 指定しておくことで直接ブラウザからアクセスされないようにできます。ngx.location.capture() は subrequest として /_auth へアクセスするため、これへは access_by_lua が適用されません。\n/ へアクセスしたら2回の問い合わせが発生してなんだろう？って悩んだが / でチェックした後に、内部的に /index.html へアクセスし直すのでした。\nNginx ほぼ初めてだから location / で access_by_lua 設定したら、他の location で認証が効かなくてハマった。全体に適用するなら location の外に設定すべきですね。\nmemcached を使う # 毎回別のサーバーに問い合わせるのはよろしくないので、ローカルの memcached にキャッシュさせたいと思います。/aaa へのアクセスに memcached からの GET aaa の結果をそのまま返すという利用法であれば MemcachdModule でできるようですが、今回の用途では https://github.com/agentzh/lua-resty-memcached の方がマッチしそうでした。\n使い方はドキュメントに書いてあるとおりで、\nlocal memcached = require \u0026quot;resty.memcached\u0026quot; local memc, err = memcached:new() memc:set\\_timeout(1000) -- 1 sec memc:connect(\u0026quot;127.0.0.1\u0026quot;, 11211) memc:set(\u0026quot;dog\u0026quot;, 32) local res, flags, err = memc:get(\u0026quot;dog\u0026quot;) memc:set\\_keepalive(10000, 100) -- memc:close() な感じです、エラー処理を端折ってますが、使うときにはちゃんと書きましょう。 set() は3番目の引数に expire (秒) を、4番目に flag を指定できます。 close() の代わりに set_keepalive() を使うことでその connection を connection pool に入れることができます。毎度毎度接続・切断を繰り返すのはよろしくないので pool しましょう。1番目の引数が idle timeout (ミリ秒) で2番目の引数が pool の最大値です。\nコンパイル # Lua のコンパイルは OpenResty で一緒にインストールされた luajit を使います。\n$ luajit/bin/luajit -b auth-filter.lua auth-filter.luac この .luac を access_by_lua_file で指定することが可能です。\nLua の文法 # ちょっとだけ。今のところ Nginx でサポートされているのは Lua 5.1 のようです。 http://www.lua.org/manual/5.1/manual.html\nコメントは SQL と同じかな？「\u0026ndash;」から行末までがコメントとなります。\n配列は PHP と似ていて、連想配列だけみたいです。table と呼ぶみたいです。添字は0ではなく1から始まります。#table で要素数が得られます、ただし、欠番や値がnullの要素があると、その前まででの数が返ります。ipairs(), pairs() というイテレータがあります。ipairs() が配列用のようです。1から#table まで loop させられます。pairs() は key と value が返ります。ipairs では欠番以降を処理できませんが、pairs() であれば全部処理できます。\n文字列連結は「..」です。\n変数のスコープはデフォルトが Global なので、Perl の「my」のように「local」で宣言しましょう。\nif, for, while, function の終了は end です。loop から抜けるのは break で、function が値を戻すのは return.\nlocal t = { \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot; } table.insert(t, \u0026quot;C\u0026quot;) table.remove(t, 2) table.concat(t, \u0026quot;,\u0026quot;) for i, v in ipairs(t) do ngx.log(ngx.DEBUG, string.format(\u0026quot;\\[%d\\]: %s\u0026quot;, i, v)) end local i = 1 while i \u0026lt;= #t do ngx.log(ngx.DEBUG, string.format(\u0026quot;\\[%d\\]: %s\u0026quot;, i, t\\[i\\])) i = i + 1 -- += や ++ は無い end local t = { a=\u0026quot;A\u0026quot;, b=\u0026quot;B\u0026quot;, c=\u0026quot;C\u0026quot; } for k, v in pairs(t) do -- 順序は保証されない ngx.log(ngx.DEBUG, k, \u0026quot;: \u0026quot;, v) if k == \u0026quot;A\u0026quot; then ngx.log(ngx.DEBUG, \u0026quot;not A\u0026quot;) else ngx.log(ngx.DEBUG, \u0026quot;not A\u0026quot;) end end それでは良い Lua 生活を〜〜\n","date":"2013年10月7日","permalink":"/2013/10/nginx-mod_lua/","section":"Posts","summary":"画像サーバーなどでログインチェックを Apache + mod_perl で実装していましたが、古代のクライアントへの対応はもう不要だろうということで、mod_perl やめ","title":"Nginx + mod_lua で認証フィルタを作ってみる"},{"content":"話題の kibana3 を使ってみたメモです。\nfluentd-plugin-elasticsearch で ElasticSearch に突っ込めば簡単に Kibana3 を試せるよっていうブログはちょいちょい見るのですが、Kibana3 をどうやって使うのか書かれてるものが少ないようなので書いてみます。そういうことなので、ElasticSearch に取り込むところまでは省略。\nでも一点だけ、ElasticSearch の template 機能を使ったほうが良いですよ（と会社の同僚に教えてもらいました）。\nfluentd-plugin-elasticsearch はすべてのフィールドを文字列として送ってしまうので、ElasticSearch 側でこのフィールドは Integer だよとか、日付型だよって教えてあげないと行けませんが、Kibana では日毎のインデックスを使うのでインデックス毎に設定するのは厳しいです。template を設定しておくと、指定したルールにマッチするインデックスに自動でスキーマが適用されます。全部のフィールドを指定する必要がありません。文字列で良いフィールドは放置でOK。複数サービスでも使えるようにインデックスの名前と template のマッチ条件を合わせてあげるのが良いです。Integer 指定は重要で、Kibana3 で合計や平均値をグラフにするのに必要です。文字列よりもインデックスサイズが小さくなりますしね。\n これが、Kibana3 の初期ページです。\n右下の\n Sample Dashboard Unconfigured Dashboard Blank Dashboard  から始めても良いですし、左上の Introduction と書いてある右の歯車アイコンで、このページを直接いじっても良いです。\nそれでは、歯車から設定していきましょう。\nIndex 指定  まずは、インデックス名の指定です。 Timestamping で none 以外を選択すると左のフォームが表示されるので、[logstash-]YYYY.MM.DD の logstash- 部分を自分で指定したものに変更しましょう。\n次はパーツを配置するための行 (rows) を作成します。\nKibana3 row 設定  行のタイトルと高さ（ピクセル）を指定して作成します。順番は後から変更できます。行を作成するとページの左端に行のタイトルが表示されます。それぞれの行にどのパーツを配置するかはタイトルの上にある小さな歯車アイコンから行います。\nKibana3 panels  Twieer Bootstrap なので（ですよね？）各パーツの幅を12段階から選択します。\nパネルの種類には次のものがあります。\n bettermap column dashcontrol derivequeries fields filtering histgram hits map pie query table terms text timepicker trends  が、ここでは私の使ったものだけを簡単に紹介します。\n    全体像はこんな感じ。\n まずは TImepicker から。これで対象範囲を期間で絞ります。Relative, Absolute, Since の3タイプがあります。ワンクリックで相互切り替えできます。\nTimepicker relative  Relative: ワンクリックで過去5分とか過去6時間とか切り替えられます。並べるリストもカスタマイズ可能です。\nTimepicker absolute  Absolute: 開始、終了日時を直接指定します\nTimepicker since  Since: 指定の日時以降を対象にします\nSearch  期間が絞れたところで、次は query で対象を絞ります。複数の検索条件を保存でき、それぞれの条件を使ってグラフなどを表示することができます。検索クエリの Syntax は Lucene のページを確認しましょう。 LuceneTutorial.com とか？最後の欄にある「＋」アイコンをクリックすることでクエリを追加できます。\nQuery Alias  色のついて丸いところをクリックすると別の色を選択することができます、また、クエリが長いとわかりづらいので名前をつけることができます。日本語も可。グラフの方にはその名前で表示されます。左上のピンアイコンをクリックすると pinned 状態にできます。グラフのパネルで queries 選択で pinned, unpinned を選択することで pinned したものを対象にしたり、その逆にしたりをして簡単に表示対象を切り替えることができます。\nKibana3 Access Count Panel  続いてヒストグラムを使って、アクセス数を表示します。この例では Lines を選択しています。線の太さも選べます。複数の線グラフを並べるので Stack のチェックを外してあります。\nKibana3 Access Count  こんなグラフになります。Countだじゃなくて、Max, Mean, Min というのもあるのでレスポンスタイムの最大値や平均値のグラフも簡単にできます。\nKibana3 Response Time  これもヒストグラムですが、今度はレスポンスタイム毎に Stack する棒グラフにしてみました。「\u0026lt; 200ms」などクエリのAliasが表示されていますね。このクエリは「usec:[0-199999]」てな感じです。\nPie Chart  お次はステータスコードのパイチャート\nあと、全体像の下につらつらと表示されているのは「table」というパネルで、検索条件にマッチしたログがリスト表示されます。ページングサイズも指定可能。クリックするとそのログについてインデックスされているすべての情報が確認できます。User-Agent とか Remote Host とか。\nKibana3 Save  苦労して設定したパネルは右上のフロッピーディスクアイコン（若者にはわからない？）をクリックし、名前をつけて保存しておきましょう。次からフォルダアイコンから呼び出せます。デフォルトに指定することもできます。\nはあ〜長かった（画像が多いだけか）。 どうでしょう？試してみたくなりました？ もっとこうやったら良いとか便利情報お待ちしてます。 あ、そうそう、日別に作られてるインデックスは貯めこまないで削除しましょう。APIを駆使すれば自動化可能です。Vagrant で 1GB heap の環境で試してたけど、このブログのアクセスログ（少ない）を数ヶ月分突っ込んでみたら、ちょいちょい応答がなくなって ElasticSearch を再起動すると復活するような感じでした。\n","date":"2013年10月2日","permalink":"/2013/10/lets-try-kibana3/","section":"Posts","summary":"話題の kibana3 を使ってみたメモです。 fluentd-plugin-elasticsearch で ElasticSearch に突っ込めば簡単に Kibana3 を試せるよっていうブログはちょいちょい見るのですが、Kibana3 をどうやって使うの","title":"Kibana3 を使ってみよう #kibana3"},{"content":"8月31日（土）「セキュリティ初級ペネトレーションハンズオン＠東京」に参加してきました。 ブログを書くまでが勉強会だっちゅーの！（ぷにゅっ）ということで、以下参加レポート 会場は渋谷。（渋谷駅周辺の迷路ではいつも道に迷う…） 参加者が持参のノートPC上で VMware なり、 VirtualBox で複数のサーバーを起動させて、そのサーバー間で penetration を実際に行なってみるという形式で行われました。 内容はこちらにあります。\n 自己紹介 実践(1) 脆弱なCGIを足がかりに侵入する 実践(2) 推測可能な SNMP Community を足がかりにブルートフォース＆msfを使って侵入する グループ毎に例題についての対策の検討と発表  どこぞで見たことのある内容ではあるらしいのですが、私は知らなかったので、なるほどぉ CGI のコマンドインジェクションに nc を投入して listen してやれば shell が取れるのかぁ、と勉強になりました。 初心者向けというのもありますし、主催者もセキュリティの専門家ではないということで解説がないという点もまあ仕方ないですよねと。でもセキュリティへの意識はまた上がりましたので参加して良かったです。 事前に Backtrack5 R3 の仮想サーバーを準備してきてね、ツールをダウンロードしてきてねとサイトに書いてあったのに準備してこなかった人が沢山いて待ち時間が長いとか、無線LAN環境の準備がうまくいっていないとか最初はちょっと何だこりゃって感じでしたが、終わってみれば初級だし良かったんじゃない？という感じでした。 初級なのにガチな人が初心者を装って混じってたみたいですね。 Metasploit Framework をもうちょっと調べてみようと思いました。悪用はしませんよ。 主催のみなさんお疲れ様でした。 石川さんにお会いして「ごめんなさい」することができました。SELinux有効にしようかな…\n 「石川さんって、石川さんごめんなさいの石川さんですか？」と聞かれる事案 #今日のアレ\n— ishikawa84g＠ಜೀವಂತವಾ (@ishikawa84g) August 31, 2013\n 石川さんってさくらの横田さん @Wslash になんとなく似てるなと思うのは私だけ？\n","date":"2013年8月31日","permalink":"/2013/08/penetration-hands-on/","section":"Posts","summary":"8月31日（土）「セキュリティ初級ペネトレーションハンズオン＠東京」に参加してきました。 ブログを書くまでが勉強会だっちゅーの！（ぷにゅっ）と","title":"「セキュリティ初級ペネトレーションハンズオン＠東京」に参加してきた"},{"content":"Ansible でユーザーアカウントを一括で作成する方法です。 user モジュール では次のようにして OS アカウントを作成することができますが、沢山のユーザーを作成したい場合どう書くのが良いのでしょうか。``` - user: name=john comment=\u0026ldquo;John Doe\u0026rdquo; uid=1040 group=admin\n次のように `with_items` を使うことで loop 処理ができますが、これでは uid や group、コメントが指定できません。 - user: name={{item}} with_items: - john - bob - alice\n````with_items` にはハッシュのリストを指定することもできるので``` - user: name={{item.name}} uid={{item.uid}} comment=\u0026quot;{{item.comment}}\u0026quot; group={{item.group}} with_items: - { name: \u0026lsquo;john\u0026rsquo;, uid: 1001, group: \u0026lsquo;users\u0026rsquo;, comment: \u0026lsquo;John Doe\u0026rsquo; } - { name: \u0026lsquo;bob\u0026rsquo;, uid: 1002, group: \u0026lsquo;users\u0026rsquo;, comment: \u0026lsquo;スポンジ Bob\u0026rsquo; } - { name: \u0026lsquo;alice\u0026rsquo;, uid: 1003, group: \u0026lsquo;users\u0026rsquo;, comment: \u0026lsquo;不思議の国の Alice\u0026rsquo; }\nさらに、変数は group\\_vars などのディレクトリに別ファイルで作成することで users:\n { name: \u0026lsquo;john\u0026rsquo;, uid: 1001, group: \u0026lsquo;users\u0026rsquo;, state: \u0026lsquo;present\u0026rsquo;, comment: \u0026lsquo;John Doe\u0026rsquo; } { name: \u0026lsquo;bob\u0026rsquo;, uid: 1002, group: \u0026lsquo;users\u0026rsquo;, state: \u0026lsquo;present\u0026rsquo;, comment: \u0026lsquo;スポンジ Bob\u0026rsquo; } { name: \u0026lsquo;alice\u0026rsquo;, uid: 1003, group: \u0026lsquo;users\u0026rsquo;, state: \u0026lsquo;present\u0026rsquo;, comment: \u0026lsquo;不思議の国の Alice\u0026rsquo; }  \\- user: name={{item.name}} uid={{item.uid}} comment=\u0026quot;{{item.comment}}\u0026quot; group={{item.group}} state={{item.state}} with\\_items: users ```こんな感じでどうでしょう？ [External Inventory Scripts](http://www.ansibleworks.com/docs/api.html#external-inventory-scripts) を使うことでもっとうまく管理することができるかもしれない。","date":"2013年8月19日","permalink":"/2013/08/create-user-using-ansible/","section":"Posts","summary":"Ansible でユーザーアカウントを一括で作成する方法です。 user モジュール では次のようにして OS アカウントを作成することができますが、沢山のユーザーを作成し","title":"Ansible でユーザーを一括作成する"},{"content":"最近 Ansible ブームな私としては Docker で Growthforecast をインストールする方法 を見ると、Ansible でもやりたくなってしまうのです。ということで、サクッと GrowthForecast をインストールする Playbook をちゃちゃっと書いてみました。 まだブラッシュアップの余地はありますし、CentOS 向けのところしか書いてない。 Debian / Ubuntu とか他の Linux Distribution 用のところは誰かよろしく。Gentoo だとオレオレ ebuild 書けばこんなのいらない？ 今回の Playbook は AWX のインストーラーを参考にしたので Ansible AWX を試す その2 #ansible が参考になります。 Playbook は GitHub ( https://github.com/yteraoka/growthforecast-playbook ) にあげておいたので次のようにすれば GrowthForecast が CentOS 6 で使えるようになります。\n$ git clone https://github.com/yteraoka/growthforecast-playbook.git $ cd growthforecast-playbook $ ansible-playbook -i hosts site.yml Ansible のインストール方法は Ansible Tutorial あたりを読んでいただければ良いのではないかと。 ACアダプタ無しの Ultrabook で試してると perl の compile や cpanm GrowthForecast に時間がかかりすぎるわぁ\u0026hellip; あら、なんか graph のフォントの問題があるかな。\n","date":"2013年8月14日","permalink":"/2013/08/install-growthforeast-with-ansible/","section":"Posts","summary":"最近 Ansible ブームな私としては Docker で Growthforecast をインストールする方法 を見ると、Ansible でもやりたくなってしまうのです。ということで、サクッと GrowthForecast をインス","title":"Ansible で Growthforecast をインストールする方法"},{"content":"前回の続きです。\nAnsibleWorks AWX User Guide では projects/helloworld/helloworld.yml というテスト用の Playbook が載っているのですが、この内容が次のようになっており user: root が指定してあるため Credentials の SSH Username が helloworld.yml で上書きされて Authentication failed となるという現象で少しハマりました。\n---- name:Hello World!hosts:alluser:roottasks:- name:Hello World!shell:echo \u0026#34;Hi! AWX is working\u0026#34;AWX の良い所は実行のログがずっと残って簡単に参照や再実行ができるところですね。Playbook の作成自体は手動です。ユーザーや権限管理ができるので権限分離の上でぽちぽちボタンをクリックするだけでアプリのデプロイができたりするのは便利なのかもしれない。\njob の実行履歴  失敗した job の詳細画面  成功した job の詳細画面  さて、前回 AWX のインストーラーが Playbook でできているから参考になりそうなので読んでみようと書きました。さっそく確認してみます。 まずはインベントリファイルの myhosts と、メインの site.yml です。\n[all] 127.0.0.1 ---# This playbook deploys the AWX application (database, web and worker) to a# single server.- hosts:alltasks:- name:group hosts by distributiongroup_by:key=\u0026#34;{{ ansible_distribution }}-{{ ansible_distribution_version }}\u0026#34;- hosts:RedHat-6*:CentOS-6*:SL-6*user:rootroles:- {role:packages_el6 }- {role: postgres, pg_hba_location:\u0026#34;/var/lib/pgsql/data/pg_hba.conf\u0026#34;}- {role:awx_install }- {role: supervisor, sup_init_name:\u0026#34;supervisord\u0026#34;, sup_conf_location:\u0026#34;/etc/supervisord.conf\u0026#34;}- {role: httpd, httpd_init_name:\u0026#34;httpd\u0026#34;}- {role:iptables }- {role:misc }- hosts:Fedora-*user:rootroles:- {role:packages_fedora }- {role: postgres, pg_hba_location:\u0026#34;/var/lib/pgsql/data/pg_hba.conf\u0026#34;}- {role:awx_install }- {role: supervisor, sup_init_name:\u0026#34;supervisord\u0026#34;, sup_conf_location:\u0026#34;/etc/supervisord.conf\u0026#34;}- {role: httpd, httpd_init_name:\u0026#34;httpd\u0026#34;}- {role:iptables }- {role:misc }- hosts:Ubuntu-12*:Ubuntu-13*user:rootroles:- {role:packages_ubuntu }- {role: postgres, pg_hba_location:\u0026#34;/etc/postgresql/9.1/main/pg_hba.conf\u0026#34;}- {role:awx_install }- {role: supervisor, sup_init_name:\u0026#34;supervisor\u0026#34;, sup_conf_location:\u0026#34;/etc/supervisor/conf.d/awx.conf\u0026#34;}- {role: httpd, httpd_init_name:\u0026#34;apache2\u0026#34;}- {role:misc }なるほどぉ。group_by でダイナミックにグルーピングを行い、Linux の distribution 毎の role へと振り分けることが可能なわけですね。 そして、roles を使って role を割り付ける際に変数を設定できるようです。Distribution 間での差がファイルの path だけとか、簡単なものについてはこうやって変数を使うことで role (task) をまとめられるんですね。 今回は CentOS 6 へインストールしたので以降は CentOS 用の role を見ていきます。\npackages_el6\n- name:install EL6 awx yum repositorytemplate:src=yum_repo.j2 dest=/etc/yum.repos.d/awx.repowhen:tarball is not definedregister:yum_repo- name:yum clean cached awx repository informationcommand:yum clean all --disablerepo=* --enablerepo=ansibleworks-awxwhen:tarball is not defined and yum_repo.changed- name:install required packages for EL6yum:name=$item state=installedwith_items:- Django14- httpd- mod_wsgi- libselinux-python- policycoreutils-python- postgresql-server- python-psycopg2- python-setuptools- setools-libs-python- supervisortags:packages注目すべき点を含む task を抜粋しました。\n\u0026quot;install EL6 awx yum repository\u0026quot; では when: tarball is not defined と register: yum_repo がポイントです。when では ansible-playbook の -e, --extra-vars で指定する EXTRA_VARS (key=value) で tarball という変数が指定されているかどうかでこの task を実行するかどうか判断しています。AWX のインストールでは setup.sh に -e オプションで渡した引数が ansible-playbook コマンドに渡されます。 register: yum_repo はこの task の結果情報を yum_repo という変数(Object?)に登録します。以降の task でこの情報を参照することができます。この機能の詳細は Advanced Playbooks (register variables) が参考になります。 次の \u0026quot;yum clean cached awx repository information\u0026quot; task の when で yum_repo の情報が参照されています。when: tarball is not defined and yum_repo.changed 先ほどの tarball 変数が未定義という条件に and で yum_repo.changed とあります。あの task で変更があった場合ということになります。yum_repo の task で変更の必要がなかった場合にはこの task も実行されないということになります。 そして、次の task には with_items と $item があります。with_items のリストの要素をループで $item 変数に入れて処理します。\npostgres\n- name:init postgresqlcommand:service postgresql initdb creates=/var/lib/pgsql/data/PG_VERSIONwhen:ansible_distribution != \u0026#34;Ubuntu\u0026#34;tags:postgresql- name:update postgresql authentication settingstemplate:src=pg_hba.conf.j2 dest={{pg_hba_location}} owner=postgresnotify:restart postgresqltags:postgresql- name:restart postgresql and configure to startup automaticallyservice:name=postgresql state=restarted enabled=yestags:postgresql- name:wait for postgresql restartcommand:sleep 10tags:postgresqlPostgreSQL 関連 task の抜粋です（次以降も抜粋です）。1つ目の task では when: ansible_distribution != \u0026quot;Ubuntu\u0026quot; という条件が指定されています。Ubuntu ではインストーラーが initdb を実行してくれるんでしょう(locale ってどう指定されてるんだろう?)。command で creates=/var/lib/pgsql/PG_VERSION が指定されているのでこのファイルが存在していればこの task (initdb) は実行されません。 次の task では template モジュールを使って pg_hba.conf を書き換えています。notify: restart postgresql により、pg_hba.conf が書き換えられたら PostgreSQL をリスタートします。この \u0026quot;restart postgresql\u0026quot; 処理は roles/postgres/handlers/main.yml で定義されています。でも pg_hba.conf の更新で restart って良くないな、reload で十分なのに。 最後の task は sleep です。PostgreSQL の起動には時間がかかるので 10 秒待っています。でもこれは pause モジュールを使う方が良いのかな？ それとも中断の選択肢を与えないための command モジュールでの sleep なのかな？\nawx_install\n- name:configure awx useruser:name={{aw_user}} system=yes home={{aw_home}} shell=/bin/bash comment=\u0026#34;AWX\u0026#34;when:tarball is definedtags:awx,user- name:create django super usershell:echo \u0026#34;from django.contrib.auth.models import User; User.objects.filter(username=\u0026#39;{{admin_username}}\u0026#39;).count() or User.objects.create_superuser(\u0026#39;{{admin_username}}\u0026#39;, \u0026#39;{{admin_email}}\u0026#39;, \u0026#39;{{admin_password}}\u0026#39;)\u0026#34; | awx-manage shellsudo_user:awxtags:awxここは特に特殊な使い方は無いですね。tags ってカンマ区切りで複数指定できるんですね。まあ tags って s がついてるくらいだから。 2つ目のは「Python ってワンライナー書けるんだ！？」と\u0026hellip;\nsupervisor\n- name:update supervisor configini_file:dest={{sup_conf_location}} section=\u0026#34;program:awx-celeryd\u0026#34; option=\u0026#34;{{item.option}}\u0026#34; value=\u0026#34;{{item.value}}\u0026#34;with_items:- option:commandvalue:\u0026#34;/usr/bin/awx-manage celeryd -B -l info --autoscale=20,2\u0026#34;- option:directoryvalue:\u0026#34;{{ aw_home }}\u0026#34;- option:uservalue:\u0026#34;{{ aw_user }}\u0026#34;- option:autostartvalue:\u0026#34;true\u0026#34;- option:autorestartvalue:\u0026#34;true\u0026#34;- option:stopwaitsecsvalue:600- option:log_stdoutvalue:\u0026#34;true\u0026#34;- option:log_stderrvalue:\u0026#34;true\u0026#34;- option:logfilevalue:\u0026#34;/var/log/supervisor/awx-celeryd.log\u0026#34;- option:logfile_maxbytesvalue:50MB- option:logfile_backupsvalue:999notify:restart supervisortags:supervisorへー、with_items って list の要素が hash っていうのもありなんですね。\nhttpd\n- name:determine if selinux is installedshell:which getenforce || exit 0register:selinux_installed- name:determine if selinux is enabledshell:getenforce | grep -q Disabled || echo yes \u0026amp;\u0026amp; echo noregister:selinux_enabledwhen:selinux_installed.stdout != \u0026#34;\u0026#34;ignore_errors:true- name:allow Apache network connectionsseboolean:name=httpd_can_network_connect state=true persistent=yeswhen:selinux_installed.stdout != \u0026#34;\u0026#34; and selinux_enabled.stdout == \u0026#34;yes\u0026#34;tags:httpdSELinux の有効・無効をチェックして必要な場合に seboolean モジュールで SELinux の設定を行なっています。register, when, ignore_errors が活用されています。\niptables\n- name:determine if iptables is installedcommand:iptables --versionregister:iptables_installedignore_errors:True- name:insert iptables rule for httpdlineinfile:dest=/etc/sysconfig/iptables state=present regexp=\u0026#34;^.*INPUT.*tcp.*$httpd_port.*ACCEPT\u0026#34; insertafter=\u0026#34;^:OUTPUT \u0026#34; line=\u0026#34;-A INPUT -p tcp --dport {{httpd_port}} -j ACCEPT\u0026#34;when:iptables_installed.rc == 0notify:restart iptablestags:iptables httpdiptables の設定はできれば restart なしで行えれば良いのですが、やっぱりちょっと難しいですかね。\nmisc\n- name:deploy config file for rsyslogdcopy:src=51-awx.conf dest=/etc/rsyslog.d/51-awx.confnotify:restart rsyslogdtags:- misc- rsyslogおや？tags が今度はカンマ区切りじゃなくて list になってますね。こうも書けるようです。\n以上、インストーラーの Playbook を調査してみました。\nAnsible を使ったインストーラーも悪くないですね。\nAnsible については Ansible Tutorial, Ansible in detail もよろしく〜。\n","date":"2013年8月10日","permalink":"/2013/08/ansible-awx-part2/","section":"Posts","summary":"前回の続きです。 AnsibleWorks AWX User Guide では projects/helloworld/helloworld.yml というテスト用の Playbook が載っているのですが、この内容が次のようになっており user: root が指定してあるため Credentials の SSH Username が helloworld.yml で上書","title":"Ansible AWX を試す その2 #ansible"},{"content":"","date":"2013年8月10日","permalink":"/tags/awx/","section":"Tags","summary":"","title":"awx"},{"content":"このところ Ansible Tutorial を書いたりして Ansible ブームなので一昨日見つけた Ansible の WebUI ツール AWX を試してみました。www.ansibleworks.com/ansibleworks-awx/ （もう存在しない）から awx-setup-1.2.2.tar.gz をダウンロードします。 まずは中身の確認。``` [vagrant@localhost ~]$ tar ztf awx-setup-1.2.2.tar.gz awx-setup-1.2.2/ awx-setup-1.2.2/README.md awx-setup-1.2.2/group_vars/ awx-setup-1.2.2/group_vars/all awx-setup-1.2.2/site.yml awx-setup-1.2.2/myhosts awx-setup-1.2.2/setup.sh awx-setup-1.2.2/roles/ awx-setup-1.2.2/roles/packages_fedora/ awx-setup-1.2.2/roles/packages_fedora/templates/ awx-setup-1.2.2/roles/packages_fedora/templates/yum_repo.j2 awx-setup-1.2.2/roles/packages_fedora/tasks/ awx-setup-1.2.2/roles/packages_fedora/tasks/main.yml awx-setup-1.2.2/roles/packages_ubuntu/ awx-setup-1.2.2/roles/packages_ubuntu/templates/ awx-setup-1.2.2/roles/packages_ubuntu/templates/awx_repo.j2 awx-setup-1.2.2/roles/packages_ubuntu/tasks/ awx-setup-1.2.2/roles/packages_ubuntu/tasks/main.yml awx-setup-1.2.2/roles/iptables/ awx-setup-1.2.2/roles/iptables/tasks/ awx-setup-1.2.2/roles/iptables/tasks/main.yml awx-setup-1.2.2/roles/iptables/handlers/ awx-setup-1.2.2/roles/iptables/handlers/main.yml awx-setup-1.2.2/roles/misc/ awx-setup-1.2.2/roles/misc/files/ awx-setup-1.2.2/roles/misc/files/51-awx.conf awx-setup-1.2.2/roles/misc/tasks/ awx-setup-1.2.2/roles/misc/tasks/main.yml awx-setup-1.2.2/roles/misc/handlers/ awx-setup-1.2.2/roles/misc/handlers/main.yml awx-setup-1.2.2/roles/httpd/ awx-setup-1.2.2/roles/httpd/tasks/ awx-setup-1.2.2/roles/httpd/tasks/main.yml awx-setup-1.2.2/roles/httpd/handlers/ awx-setup-1.2.2/roles/httpd/handlers/main.yml awx-setup-1.2.2/roles/supervisor/ awx-setup-1.2.2/roles/supervisor/tasks/ awx-setup-1.2.2/roles/supervisor/tasks/main.yml awx-setup-1.2.2/roles/supervisor/handlers/ awx-setup-1.2.2/roles/supervisor/handlers/main.yml awx-setup-1.2.2/roles/postgres/ awx-setup-1.2.2/roles/postgres/templates/ awx-setup-1.2.2/roles/postgres/templates/pg_hba.conf.j2 awx-setup-1.2.2/roles/postgres/tasks/ awx-setup-1.2.2/roles/postgres/tasks/main.yml awx-setup-1.2.2/roles/postgres/handlers/ awx-setup-1.2.2/roles/postgres/handlers/main.yml awx-setup-1.2.2/roles/awx_install/ awx-setup-1.2.2/roles/awx_install/templates/ awx-setup-1.2.2/roles/awx_install/templates/settings.py.j2 awx-setup-1.2.2/roles/awx_install/tasks/ awx-setup-1.2.2/roles/awx_install/tasks/main.yml awx-setup-1.2.2/roles/packages_el6/ awx-setup-1.2.2/roles/packages_el6/templates/ awx-setup-1.2.2/roles/packages_el6/templates/yum_repo.j2 awx-setup-1.2.2/roles/packages_el6/tasks/ awx-setup-1.2.2/roles/packages_el6/tasks/main.yml\nおや？これは！ Playbook ですね。Best Practice なディレクトリ構成です。Playbook の勉強にもなりますね。 [AnsibleWorks AWX User Guide](http://www.ansibleworks.com/releases/awx/docs/awx_user_guide.pdf) という PDF ドキュメントがあります。これに沿ってインストールしてみます。 まず DB (PostgreSQL) のパスワードを `group_vars/all` の `pg_password` 変数で設定します。 この他にもこのファイルには DB 名や DB のユーザー名、アプリのユーザー名、パスワードの定義もありますね。Django + PostgreSQL という構成みたいです。Ansible が Python だからやっぱりこのツールも Python 製ですね。 変数を設定したら後は setup.sh を実行するだけです。Ansible の威力ですね。 [vagrant@localhost awx-setup-1.2.2]$ ./setup.sh\nPLAY [all] ********************************************************************\nGATHERING FACTS *************************************************************** ok: [127.0.0.1]\nTASK: [group hosts by distribution] ******************************************* changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;groups\u0026rdquo;: {\u0026ldquo;CentOS-6.4\u0026rdquo;: [\u0026ldquo;127.0.0.1\u0026rdquo;]}}\nPLAY [RedHat-6*:CentOS-6*:SL-6*] **********************************************\nTASK: [install epel6 repository] ********************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: \u0026ldquo;rpm -q epel-release || rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm \u0026ldquo;, \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:00.045414\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:09:31.568672\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:09:31.523258\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;epel-release-6-8.noarch\u0026rdquo;}\nTASK: [install EL6 awx yum repository] **************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/yum.repos.d/awx.repo\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;md5sum\u0026rdquo;: \u0026ldquo;98239f7b2f42100e29efde40377f15e1\u0026rdquo;, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 163, \u0026ldquo;src\u0026rdquo;: \u0026ldquo;/root/.ansible/tmp/ansible-1376057371.64-145330162957648/source\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0}\nTASK: [yum clean cached awx repository information] *************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: [\u0026ldquo;yum\u0026rdquo;, \u0026ldquo;clean\u0026rdquo;, \u0026ldquo;all\u0026rdquo;, \u0026ldquo;\u0026ndash;disablerepo=*\u0026rdquo;, \u0026ldquo;\u0026ndash;enablerepo=ansibleworks-awx\u0026rdquo;], \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:00.441243\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:09:32.364621\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:09:31.923378\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;Loaded plugins: fastestmirror\\nCleaning repos: ansibleworks-awx\\nCleaning up Everything\\nCleaning up list of fastest mirrors\u0026rdquo;}\nTASK: [install required packages for EL6] ************************************* changed: [127.0.0.1] =\u0026gt; (item=Django14,httpd,mod_wsgi,libselinux-python,policycoreutils-python,postgresql-server,python-psycopg2,python-setuptools,setools-libs-python,supervisor) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;Django14,httpd,mod_wsgi,libselinux-python,policycoreutils-python,postgresql-server,python-psycopg2,python-setuptools,setools-libs-python,supervisor\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;results\u0026rdquo;: [\u0026quot;\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n Django14 noarch 1.4.5-1.el6 epel 4.1 M\\nInstalling for dependencies:\\n python-simplejson x86_64 2.0.9-3.1.el6 base 126 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 2 Package(s)\\n\\nTotal download size: 4.2 M\\nInstalled size: 17 M\\n\\nInstalled:\\n Django14.noarch 0:1.4.5-1.el6 \\n\\nDependency Installed:\\n python-simplejson.x86_64 0:2.0.9-3.1.el6 \\n\\n\u0026rdquo;, \u0026ldquo;\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n httpd x86_64 2.2.15-28.el6.centos updates 821 k\\nInstalling for dependencies:\\n apr x86_64 1.3.9-5.el6_2 base 123 k\\n apr-util x86_64 1.3.9-3.el6_0.1 base 87 k\\n apr-util-ldap x86_64 1.3.9-3.el6_0.1 base 15 k\\n httpd-tools x86_64 2.2.15-28.el6.centos updates 73 k\\n mailcap noarch 2.1.31-2.el6 base 27 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 6 Package(s)\\n\\nTotal download size: 1.1 M\\nInstalled size: 3.6 M\\n\\nInstalled:\\n httpd.x86_64 0:2.2.15-28.el6.centos \\n\\nDependency Installed:\\n apr.x86_64 0:1.3.9-5.el6_2 \\n apr-util.x86_64 0:1.3.9-3.el6_0.1 \\n apr-util-ldap.x86_64 0:1.3.9-3.el6_0.1 \\n httpd-tools.x86_64 0:2.2.15-28.el6.centos \\n mailcap.noarch 0:2.1.31-2.el6 \\n\\n\u0026rdquo;, \u0026ldquo;\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n mod_wsgi x86_64 3.2-3.el6 base 66 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 1 Package(s)\\n\\nTotal download size: 66 k\\nInstalled size: 177 k\\n\\nInstalled:\\n mod_wsgi.x86_64 0:3.2-3.el6 \\n\\n\u0026rdquo;, \u0026ldquo;\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n libselinux-python x86_64 2.0.94-5.3.el6_4.1 updates 202 k\\nUpdating for dependencies:\\n libselinux x86_64 2.0.94-5.3.el6_4.1 updates 108 k\\n libselinux-devel x86_64 2.0.94-5.3.el6_4.1 updates 136 k\\n libselinux-ruby x86_64 2.0.94-5.3.el6_4.1 updates 99 k\\n libselinux-utils x86_64 2.0.94-5.3.el6_4.1 updates 81 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 1 Package(s)\\nUpgrade 4 Package(s)\\n\\nTotal download size: 625 k\\n\\nInstalled:\\n libselinux-python.x86_64 0:2.0.94-5.3.el6_4.1 \\n\\nDependency Updated:\\n libselinux.x86_64 0:2.0.94-5.3.el6_4.1 \\n libselinux-devel.x86_64 0:2.0.94-5.3.el6_4.1 \\n libselinux-ruby.x86_64 0:2.0.94-5.3.el6_4.1 \\n libselinux-utils.x86_64 0:2.0.94-5.3.el6_4.1 \\n\\n\u0026rdquo;, \u0026ldquo;\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n policycoreutils-python x86_64 2.0.83-19.30.el6 base 342 k\\nInstalling for dependencies:\\n audit-libs-python x86_64 2.2-2.el6 base 59 k\\n libcgroup x86_64 0.37-7.2.el6_4 updates 111 k\\n libsemanage-python x86_64 2.0.43-4.2.el6 base 81 k\\n setools-libs x86_64 3.3.7-4.el6 base 400 k\\n setools-libs-python x86_64 3.3.7-4.el6 base 222 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 6 Package(s)\\n\\nTotal download size: 1.2 M\\nInstalled size: 4.4 M\\n\\nInstalled:\\n policycoreutils-python.x86_64 0:2.0.83-19.30.el6 \\n\\nDependency Installed:\\n audit-libs-python.x86_64 0:2.2-2.el6 libcgroup.x86_64 0:0.37-7.2.el6_4 \\n libsemanage-python.x86_64 0:2.0.43-4.2.el6 setools-libs.x86_64 0:3.3.7-4.el6 \\n setools-libs-python.x86_64 0:3.3.7-4.el6 \\n\\n\u0026rdquo;, \u0026ldquo;\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n postgresql-server x86_64 8.4.13-1.el6_3 base 3.4 M\\nInstalling for dependencies:\\n postgresql x86_64 8.4.13-1.el6_3 base 2.8 M\\n postgresql-libs x86_64 8.4.13-1.el6_3 base 200 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 3 Package(s)\\n\\nTotal download size: 6.4 M\\nInstalled size: 29 M\\n\\nInstalled:\\n postgresql-server.x86_64 0:8.4.13-1.el6_3 \\n\\nDependency Installed:\\n postgresql.x86_64 0:8.4.13-1.el6_3 postgresql-libs.x86_64 0:8.4.13-1.el6_3 \\n\\n\u0026rdquo;, \u0026ldquo;\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n python-psycopg2 x86_64 2.0.14-2.el6 base 100 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 1 Package(s)\\n\\nTotal download size: 100 k\\nInstalled size: 318 k\\n\\nInstalled:\\n python-psycopg2.x86_64 0:2.0.14-2.el6 \\n\\n\u0026rdquo;, \u0026ldquo;\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n python-setuptools noarch 0.6.10-3.el6 base 336 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 1 Package(s)\\n\\nTotal download size: 336 k\\nInstalled size: 1.5 M\\n\\nInstalled:\\n python-setuptools.noarch 0:0.6.10-3.el6 \\n\\n\u0026rdquo;, \u0026ldquo;setools-libs-python-3.3.7-4.el6.x86_64 providing setools-libs-python is already installed\u0026rdquo;, \u0026ldquo;\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n supervisor noarch 2.1-8.el6 epel 292 k\\nInstalling for dependencies:\\n python-meld3 x86_64 0.6.7-1.el6 epel 71 k\\n\\nTransaction Summary\\n================================================================================\\nInstall 2 Package(s)\\n\\nTotal download size: 363 k\\nInstalled size: 1.4 M\\n\\nInstalled:\\n supervisor.noarch 0:2.1-8.el6 \\n\\nDependency Installed:\\n python-meld3.x86_64 0:0.6.7-1.el6 \\n\\n\u0026rdquo;]}\nTASK: [install awx RPM for EL6] *********************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;results\u0026rdquo;: [\u0026quot;\\n================================================================================\\n Package Arch Version Repository Size\\n================================================================================\\nInstalling:\\n awx noarch 1.2.2-0.el6 ansibleworks-awx 4.7 M\\n\\nTransaction Summary\\n================================================================================\\nInstall 1 Package(s)\\n\\nTotal download size: 4.7 M\\nInstalled size: 19 M\\n\\nInstalled:\\n awx.noarch 0:1.2.2-0.el6 \\n\\n\u0026rdquo;]}\nTASK: [init postgresql] ******************************************************* changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: [\u0026ldquo;service\u0026rdquo;, \u0026ldquo;postgresql\u0026rdquo;, \u0026ldquo;initdb\u0026rdquo;], \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:10.226856\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:10:51.195623\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:10:40.968767\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;Initializing database: [ OK ]\u0026quot;}\nTASK: [update postgresql authentication settings] ***************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/var/lib/pgsql/data/pg_hba.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 26, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;postgres\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;md5sum\u0026rdquo;: \u0026ldquo;20c708b0d8e275e6b3163c02ee82aaab\u0026rdquo;, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0600\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;postgres\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 620, \u0026ldquo;src\u0026rdquo;: \u0026ldquo;/root/.ansible/tmp/ansible-1376057451.29-79460499167534/source\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 26}\nTASK: [restart postgresql and configure to startup automatically] ************* changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;enabled\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;postgresql\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;started\u0026rdquo;}\nTASK: [wait for postgresql restart] ******************************************* changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: [\u0026ldquo;sleep\u0026rdquo;, \u0026ldquo;10\u0026rdquo;], \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:10.003686\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:04.001508\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:10:53.997822\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;\u0026quot;}\nTASK: [create the postgresql user for awx] ************************************ changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;user\u0026rdquo;: \u0026ldquo;awx\u0026rdquo;}\nTASK: [create the postgresql database for awx] ******************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;db\u0026rdquo;: \u0026ldquo;awx\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026quot;}\nTASK: [install awx package and dependencies via pip] ************************** skipping: [127.0.0.1]\nTASK: [configure awx user] **************************************************** skipping: [127.0.0.1]\nTASK: [configure awx user home directory] ************************************* ok: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;gid\u0026rdquo;: 497, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;awx\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0755\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;awx\u0026rdquo;, \u0026ldquo;path\u0026rdquo;: \u0026ldquo;/var/lib/awx\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 4096, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;directory\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 497}\nTASK: [configure awx projects directory] ************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;gid\u0026rdquo;: 497, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;awx\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0750\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;awx\u0026rdquo;, \u0026ldquo;path\u0026rdquo;: \u0026ldquo;/var/lib/awx/projects\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 4096, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;directory\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 497}\nTASK: [configure awx settings directory] ************************************** skipping: [127.0.0.1]\nTASK: [setup secret key for settings] ***************************************** skipping: [127.0.0.1]\nTASK: [configure awx settings] ************************************************ changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/awx/settings.py\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 497, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;awx\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;md5sum\u0026rdquo;: \u0026ldquo;c08fb3b628837d8670cb64980537bee6\u0026rdquo;, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;awx\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 1615, \u0026ldquo;src\u0026rdquo;: \u0026ldquo;/root/.ansible/tmp/ansible-1376057465.74-151147353765701/source\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 497}\nTASK: [create awx database schema] ******************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: [\u0026ldquo;awx-manage\u0026rdquo;, \u0026ldquo;syncdb\u0026rdquo;, \u0026ldquo;\u0026ndash;noinput\u0026rdquo;], \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:01.131647\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:07.184323\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:06.052676\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;Syncing\u0026hellip;\\nCreating tables \u0026hellip;\\nCreating table django_admin_log\\nCreating table auth_permission\\nCreating table auth_group_permissions\\nCreating table auth_group\\nCreating table auth_user_user_permissions\\nCreating table auth_user_groups\\nCreating table auth_user\\nCreating table django_content_type\\nCreating table django_session\\nCreating table django_site\\nCreating table south_migrationhistory\\nInstalling custom SQL \u0026hellip;\\nInstalling indexes \u0026hellip;\\nInstalled 0 object(s) from 0 fixture(s)\\n\\nSynced:\\n \u0026gt; django.contrib.admin\\n \u0026gt; django.contrib.auth\\n \u0026gt; django.contrib.contenttypes\\n \u0026gt; django.contrib.messages\\n \u0026gt; django.contrib.sessions\\n \u0026gt; django.contrib.sites\\n \u0026gt; django.contrib.staticfiles\\n \u0026gt; south\\n \u0026gt; rest_framework\\n \u0026gt; awx.ui\\n\\nNot synced (use migrations):\\n - rest_framework.authtoken\\n - django_extensions\\n - djcelery\\n - kombu.transport.django\\n - taggit\\n - awx.main\\n(use ./manage.py migrate to migrate these)\u0026quot;}\nTASK: [migrate awx database schema] ******************************************* changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: [\u0026ldquo;awx-manage\u0026rdquo;, \u0026ldquo;migrate\u0026rdquo;, \u0026ldquo;\u0026ndash;noinput\u0026rdquo;], \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:05.480693\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:12.849291\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:07.368598\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;Running migrations for authtoken:\\n - Migrating forwards to 0001_initial.\\n \u0026gt; authtoken:0001_initial\\n - Loading initial data for authtoken.\\nInstalled 0 object(s) from 0 fixture(s)\\nRunning migrations for django_extensions:\\n - Migrating forwards to 0001_empty.\\n \u0026gt; django_extensions:0001_empty\\n - Loading initial data for django_extensions.\\nInstalled 0 object(s) from 0 fixture(s)\\nRunning migrations for djcelery:\\n - Migrating forwards to 0004_v30_changes.\\n \u0026gt; djcelery:0001_initial\\n \u0026gt; djcelery:0002_v25_changes\\n \u0026gt; djcelery:0003_v26_changes\\n \u0026gt; djcelery:0004_v30_changes\\n - Loading initial data for djcelery.\\nInstalled 0 object(s) from 0 fixture(s)\\nRunning migrations for django:\\n - Migrating forwards to 0001_initial.\\n \u0026gt; django:0001_initial\\n - Loading initial data for django.\\nInstalled 0 object(s) from 0 fixture(s)\\nRunning migrations for taggit:\\n - Migrating forwards to 0002_unique_tagnames.\\n \u0026gt; taggit:0001_initial\\n \u0026gt; taggit:0002_unique_tagnames\\n - Loading initial data for taggit.\\nInstalled 0 object(s) from 0 fixture(s)\\nRunning migrations for main:\\n - Migrating forwards to 0008_v12changes.\\n \u0026gt; main:0001_v12b1_initial\\n \u0026gt; main:0002_v12b2_changes\\n \u0026gt; main:0003_v12b2_changes\\n \u0026gt; main:0004_v12b2_changes\\n \u0026gt; main:0005_v12b2_changes\\n \u0026gt; main:0006_v12b2_changes\\n \u0026gt; main:0007_v12b2_changes\\n \u0026gt; main:0008_v12changes\\n - Loading initial data for main.\\nInstalled 0 object(s) from 0 fixture(s)\u0026quot;}\nTASK: [collect awx static files] ********************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: [\u0026ldquo;awx-manage\u0026rdquo;, \u0026ldquo;collectstatic\u0026rdquo;, \u0026ldquo;\u0026ndash;noinput\u0026rdquo;], \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:00.632565\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:13.677810\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:13.045245\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;\\n0 static files copied, 213 unmodified.\u0026quot;}\nTASK: [create django super user] ********************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: \u0026ldquo;echo \\\u0026ldquo;from django.contrib.auth.models import User; User.objects.filter(username=\u0026lsquo;admin\u0026rsquo;).count() or User.objects.create_superuser(\u0026lsquo;admin\u0026rsquo;, \u0026lsquo;admin@example.com\u0026rsquo;, \u0026lsquo;password\u0026rsquo;)\\\u0026rdquo; | awx-manage shell \u0026ldquo;, \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:00.985082\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:14.887458\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:13.902376\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;Python 2.6.6 (r266:84292, Feb 22 2013, 00:00:18) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-3)] on linux2\\nType \\\u0026ldquo;help\\\u0026rdquo;, \\\u0026ldquo;copyright\\\u0026rdquo;, \\\u0026ldquo;credits\\\u0026rdquo; or \\\u0026ldquo;license\\\u0026rdquo; for more information.\\n(InteractiveConsole)\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;\\u001b[?1034h\u0026raquo;\u0026gt; \\n\u0026raquo;\u0026gt; \u0026ldquo;}\nTASK: [update supervisor config] ********************************************** ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;command\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: \u0026lsquo;/usr/bin/awx-manage celeryd -B -l info \u0026ndash;autoscale=20,2\u0026rsquo;}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;command\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;/usr/bin/awx-manage celeryd -B -l info \u0026ndash;autoscale=20,2\u0026rdquo;}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0} ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;directory\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: u'/var/lib/awx'}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;directory\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;/var/lib/awx\u0026rdquo;}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0} ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;user\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: u\u0026rsquo;awx'}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;user\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;awx\u0026rdquo;}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0} ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;autostart\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: \u0026lsquo;true\u0026rsquo;}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;autostart\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;true\u0026rdquo;}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0} ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;autorestart\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: \u0026lsquo;true\u0026rsquo;}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;autorestart\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;true\u0026rdquo;}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0} ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;stopwaitsecs\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: 600}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;stopwaitsecs\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: 600}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0} ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;log_stdout\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: \u0026lsquo;true\u0026rsquo;}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;log_stdout\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;true\u0026rdquo;}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0} ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;log_stderr\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: \u0026lsquo;true\u0026rsquo;}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;log_stderr\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;true\u0026rdquo;}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0} ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;logfile\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: \u0026lsquo;/var/log/supervisor/awx-celeryd.log\u0026rsquo;}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;logfile\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;/var/log/supervisor/awx-celeryd.log\u0026rdquo;}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0} ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;logfile_maxbytes\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: \u0026lsquo;50MB\u0026rsquo;}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;logfile_maxbytes\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: \u0026ldquo;50MB\u0026rdquo;}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0} ok: [127.0.0.1] =\u0026gt; (item={\u0026lsquo;option\u0026rsquo;: \u0026lsquo;logfile_backups\u0026rsquo;, \u0026lsquo;value\u0026rsquo;: 999}) =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/supervisord.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: {\u0026ldquo;option\u0026rdquo;: \u0026ldquo;logfile_backups\u0026rdquo;, \u0026ldquo;value\u0026rdquo;: 999}, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;OK\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 3662, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0}\nTASK: [stop supervisor] ******************************************************* ok: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: false, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;supervisord\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;stopped\u0026rdquo;}\nTASK: [start supervisor] ****************************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;enabled\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;supervisord\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;started\u0026rdquo;}\nTASK: [determine if selinux is installed] ************************************* changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: \u0026ldquo;which getenforce || exit 0 \u0026ldquo;, \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:00.016455\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:17.251571\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:17.235116\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;/usr/sbin/getenforce\u0026rdquo;}\nTASK: [determine if selinux is enabled] *************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: \u0026ldquo;getenforce | grep -q Disabled || echo yes \u0026amp;\u0026amp; echo no \u0026ldquo;, \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:00.010524\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:17.579428\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:17.568904\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;no\u0026rdquo;}\nTASK: [allow Apache network connections] ************************************** skipping: [127.0.0.1]\nTASK: [create awx wsgi application] ******************************************* skipping: [127.0.0.1]\nTASK: [create httpd virtualhost for awx] ************************************** skipping: [127.0.0.1]\nTASK: [start httpd and configure to startup automatically] ******************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;enabled\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;httpd\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;started\u0026rdquo;}\nTASK: [determine if iptables is installed] ************************************ changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;cmd\u0026rdquo;: [\u0026ldquo;iptables\u0026rdquo;, \u0026ldquo;\u0026ndash;version\u0026rdquo;], \u0026ldquo;delta\u0026rdquo;: \u0026ldquo;0:00:00.011655\u0026rdquo;, \u0026ldquo;end\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:19.375786\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;rc\u0026rdquo;: 0, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;2013-08-09 14:11:19.364131\u0026rdquo;, \u0026ldquo;stderr\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;stdout\u0026rdquo;: \u0026ldquo;iptables v1.4.7\u0026rdquo;}\nTASK: [insert iptables rule for httpd] **************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;msg\u0026rdquo;: \u0026ldquo;line added\u0026rdquo;}\nTASK: [deploy config file for rsyslogd] *************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;dest\u0026rdquo;: \u0026ldquo;/etc/rsyslog.d/51-awx.conf\u0026rdquo;, \u0026ldquo;gid\u0026rdquo;: 0, \u0026ldquo;group\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;md5sum\u0026rdquo;: \u0026ldquo;f732a302fd0c57e322fccc081ccf5407\u0026rdquo;, \u0026ldquo;mode\u0026rdquo;: \u0026ldquo;0644\u0026rdquo;, \u0026ldquo;owner\u0026rdquo;: \u0026ldquo;root\u0026rdquo;, \u0026ldquo;size\u0026rdquo;: 20, \u0026ldquo;src\u0026rdquo;: \u0026ldquo;/root/.ansible/tmp/ansible-1376057479.67-227872901679182/source\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;file\u0026rdquo;, \u0026ldquo;uid\u0026rdquo;: 0}\nNOTIFIED: [restart postgresql] ************************************************ changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;postgresql\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;started\u0026rdquo;}\nNOTIFIED: [restart supervisor] ************************************************ changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;supervisord\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;started\u0026rdquo;}\nNOTIFIED: [restart httpd] ***************************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;httpd\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;started\u0026rdquo;}\nNOTIFIED: [restart apache2] *************************************************** skipping: [127.0.0.1]\nNOTIFIED: [restart iptables] ************************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;iptables\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;started\u0026rdquo;}\nNOTIFIED: [restart rsyslogd] ************************************************** changed: [127.0.0.1] =\u0026gt; {\u0026ldquo;changed\u0026rdquo;: true, \u0026ldquo;item\u0026rdquo;: \u0026ldquo;\u0026rdquo;, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;rsyslog\u0026rdquo;, \u0026ldquo;state\u0026rdquo;: \u0026ldquo;started\u0026rdquo;}\nPLAY [Fedora-*] *************************************************************** skipping: no hosts matched\nPLAY [Ubuntu-12*:Ubuntu-13*] ************************************************** skipping: no hosts matched\nPLAY RECAP ******************************************************************** 127.0.0.1 : ok=34 changed=30 unreachable=0 failed=0 ```インストール完了。group_vars/all の admin_username と admin_password の値でログインします。 ログインすると次のように表示されます。Webサイトに書いてあったことですね。無料で使えるのは管理対象サーバー10台までだよ。それ以上はライセンス買ってねと。\n Thank you for trying AnsibleWorks AWX. You can use this edition to manage up to 10 hosts free. Should you wish to acquire a license for additional servers, please visit the AnsibleWorks online store, or contact info@ansibleworks.com for assistance.\n AWX ヒエラルキー\n Organizations  Inventories  Groups  Hosts     Teams  Credentials Permissions Users  Credentials Permissions       Projects  Playbooks Job Templates   Jobs   まずは Organization を任意の組織名で作成する その組織にユーザーを作成する 任意の名前でインベントリを作る ansible の inventory ファイルのファイル名相当 サーバーのグループを作成する inventory ファイルで設定するグループ グループのサーバーを追加する。グループの下にしか作れない ユーザーに Credential を作成する。SSH のキーファイルとかパスワードとか そして、いよいよ Job を作るのだが、そのためには AWX サーバーのファイルシステム上にプロジェクト用のディレクトリを作成する必要がある。デフォルトでは /var/lib/awx/projects/ の下に作成する。 で、そのディレクトリに playbook を作成する Projects タブで Create New をクリックし、Playbook Directory を選択する Job Templates タブでテンプレートを作成する 1. 任意の名称を入力 2. Run か Check のタイプを選択 3. Inventory を作成したものから選択 4. Projects も先ほど作成したものを選択 5. Projects ディレクトリに作成した Playbook (yml) ファイルを選択 6. Credential を選択これでやっと作成 作成された Job Template の Launch ボタンをクリックすれば Job が実行され、Job タブで状況・結果が確認できる  今日はひとまずここまで。 後でインストーラーの Playbook を読んでみよう。 2013年8月11日 追記 続きを書きました。\n","date":"2013年8月9日","permalink":"/2013/08/ansible-awx-part1/","section":"Posts","summary":"このところ Ansible Tutorial を書いたりして Ansible ブームなので一昨日見つけた Ansible の WebUI ツール AWX を試してみました。www.ansibleworks.com/ansib","title":"Ansible AWX を試す その1 #ansible"},{"content":"ただそれだけ。 ダッシュボードからワンクリックであっけなく終わりました。\n","date":"2013年8月5日","permalink":"/2013/08/wordpress-3-6/","section":"Posts","summary":"ただそれだけ。 ダッシュボードからワンクリックであっけなく終わりました。","title":"WordPress 3.6 に更新したよ"},{"content":"","date":"2013年7月26日","permalink":"/tags/nexus7/","section":"Tags","summary":"","title":"nexus7"},{"content":"5月に Nexus7 が壊れて修理してもらったのだけれどもまた同じ症状が出た。この症状というのは電源ボタンを押してもうんともすんとも言わなくなること。同梱の純正充電器＆ケーブルでもダメだった。でもほとんど触ってないのにさすがにこれは何かおかしいと思って、USBケーブルを交換したり、充電器を交換したり、外部バッテリーにつないだりといろいろやってたら、ちょっと変化があったので適当にいじってたらなんとか起動＆充電できるようになった。 前回の修理報告にはバッテリーの接触に問題があったから付け直したって書いてあったけど、きっと前回も同じ症状だったはず。Nexus7 は完全に放電してしまった場合の起動に難があるみたいです。 そしてググってみたらこんなのが見つかりました。 ヘルプ：Nexus 7 が正常に充電されない https://support.google.com/nexus/7/answer/2668668?hl=ja これはヒドい。この説明は目立つように同梱して販売するべきだよなぁ。 Nexus7 2013 を買うために今のはもう売ることにして初期化しました。\n","date":"2013年7月26日","permalink":"/2013/07/nexus7-power-trouble/","section":"Posts","summary":"5月に Nexus7 が壊れて修理してもらったのだけれどもまた同じ症状が出た。この症状というのは電源ボタンを押してもうんともすんとも言わなくなること。同梱","title":"Nexus7 また壊れた？"},{"content":"","date":"2013年7月23日","permalink":"/tags/firefox/","section":"Tags","summary":"","title":"firefox"},{"content":"どういう場合に発生するのかは不明だが、Firefox で自己署名証明書の使われた SSL サイトにアクセスした際になぜか例外承認で「セキュリティ例外を承認」というボタンがグレーアウトして承認出来ず、そのサイトにアクセスできないという現象に遭遇することがあります。\nセキュリティ例外を承認」ボタンがグレーアウト  有効な証明書だと言うくせに「セキュリティ例外を承認」ボタンがグレーアウトしていてアクセスさせてくれません\u0026hellip;[/caption]\n有効な証明書だって言ってるくせにぃ\u0026hellip; ムキーッ\nこれまで、IE や Chrome で逃げて来ましたが、ちょっと調べてみることにしました。\n簡単に結論を書くと、「キャッシュ」を消すことで承認できるようになります。\nブラウザ終了時に全部消してしまうようにするのが良いかもしれません。 「オプション」→「プライバシー」→「履歴」で『Firefox に「記憶させる履歴を詳細設定をする」』を選択し、「Firefox 終了時に履歴を消去する」にチェックを入れ、右の「設定」ボタンから消去する項目を選択します。消すことのできる履歴には幾つも項目がありますが、この問題では「キャッシュ」だけで良いです。\nプライバシー設定から終了時にキャッシュを消す設定 プライバシー設定  キャッシュを消すと承認できるようになりました 例外承認できる場合  承認時に「次回以降もこの例外を有効にする」のチェックを外してしまうと、次からグレーアウトしてしまうようです。昔のバージョンはそうでは無かったと思うのですが、いつからか変わってしまったようです。「次回以降もこの例外を有効にする」にチェックが入っていれば、「オプション」→「詳細」→「暗号化」→「証明書を表示」で、「サーバー証明書」の中にサーバー名、ポート番号とともに登録されます。消したくなったらここから消しましょう。\n承認情報がキャッシュとして残ってしまうようになったために、同じ証明書をつかっているが別のホスト名（IPアドレス）という場合に、別のタブで同時にアクセスしたりすることができなくて不便になってしまいました。冗長化構成の機器で別々のIPアドレスだけど同じ証明書を使ってる管理画面が\u0026hellip;\n あなたのサーバ証明書は認証局によって発行された他の証明書と同じシリアル番号を持っています。一意なシリアル番号を持つ新しい証明書を取得してください。 (エラーコード: sec_error_reused_issuer_and_serial)\n これはもう IE か Chrome で回避せざるを得ないようです。 ところで、ググってるときに↓こんなページを見つけたのですが https://support.mozillamessaging.com/ja/kb/add-security-exception\n 代替手段として、セキュリティ例外を追加することができます。ただしこれは、Thunderbird とメールサーバとの通信が暗号化されないため、第三者に通信内容を盗み見られたり改ざんされたりする可能性があることに注意してください。\n えーっ、うっそ～？暗号化されてるけど、相手が何者かわからないだけだよねぇ？？？ 英語のページを見ても \u0026ldquo;not encrypted\u0026rdquo; って書いてあるんだよね。ウムム\u0026hellip; https://support.mozillamessaging.com/en-US/kb/add-security-exception\n","date":"2013年7月23日","permalink":"/2013/07/firefox-security-exception/","section":"Posts","summary":"どういう場合に発生するのかは不明だが、Firefox で自己署名証明書の使われた SSL サイトにアクセスした際になぜか例外承認で「セキュリティ例外を","title":"Firefox でセキュリティ例外の承認ができない場合の対応方法"},{"content":"","date":"2013年7月19日","permalink":"/tags/chef/","section":"Tags","summary":"","title":"chef"},{"content":"July Tech Festa の Chef ハンズオン資料が公開されていたので家で試してみた。公開ありがとうございます。 Vagrant でテスト用環境を立ち上げる\n$ mkdir chef $ cd chef $ vagrant init centos6 http://developer.nrel.gov/downloads/vagrant-boxes/CentOS-6.4-x86_64-v20130427.box $ vagrant up $ vagrant ssh 1.2. Chef Solo のインストール # インストール\n$ curl -L https://www.opscode.com/chef/install.sh | sudo bash 試しに実行してみる\n$ sudo chef-solo [2013-07-19T12:54:08+00:00] WARN: ***************************************** [2013-07-19T12:54:08+00:00] WARN: Did not find config file: /etc/chef/solo.rb, using command line options. [2013-07-19T12:54:08+00:00] WARN: ***************************************** Starting Chef Client, version 11.4.4 Compiling Cookbooks... [2013-07-19T12:54:08+00:00] FATAL: No cookbook found in [\u0026quot;/var/chef/cookbooks\u0026quot;, \u0026quot;/var/chef/site-cookbooks\u0026quot;], make sure cookbook_path is set correctly. [2013-07-19T12:54:08+00:00] FATAL: No cookbook found in [\u0026quot;/var/chef/cookbooks\u0026quot;, \u0026quot;/var/chef/site-cookbooks\u0026quot;], make sure cookbook_path is set correctly. [2013-07-19T12:54:08+00:00] ERROR: Running exception handlers [2013-07-19T12:54:08+00:00] ERROR: Exception handlers complete Chef Client failed. 0 resources updated [2013-07-19T12:54:08+00:00] FATAL: Stacktrace dumped to /var/chef/cache/chef-stacktrace.out [2013-07-19T12:54:08+00:00] FATAL: Chef::Exceptions::CookbookNotFound: No cookbook found in [\u0026quot;/var/chef/cookbooks\u0026quot;, \u0026quot;/var/chef/site-cookbooks\u0026quot;], make sure cookbook_path is set correctly. 2. Chef Apply を試す # 2.2. レシピの作成と実行 # 1つファイルを作るだけの簡単なレシピを書いてみる\n$ cat \u0026lt;\u0026lt;_EOD_ \u0026gt; recipe.rb file '/var/tmp/test.txt' do content \u0026quot;hello, world\\n\u0026quot; end _EOD_ 実行\n$ sudo chef-apply recipe.rb Recipe: (chef-apply cookbook)::(chef-apply recipe) * file[/var/tmp/test.txt] action create - create new file /var/tmp/test.txt with content checksum 853ff9 --- /tmp/chef-tempfile20130719-1884-npeegm\t2013-07-19 13:01:09.688309813 +0000 +++ /tmp/chef-diff20130719-1884-uxn8m3\t2013-07-19 13:01:09.688309813 +0000 @@ -0,0 +1 @@ +hello, world 作成されたファイルを確認\n$ ls -l /var/tmp/test.txt -rw-r--r-- 1 root root 13 Jul 19 13:01 /var/tmp/test.txt $ cat /var/tmp/test.txt hello, world 2.3. べき等性の確認 # 再実行してみる\n$ sudo chef-apply recipe.rb Recipe: (chef-apply cookbook)::(chef-apply recipe) * file[/var/tmp/test.txt] action create (up to date) ファイルに変化なし\n$ ls -l /var/tmp/test.txt -rw-r--r-- 1 root root 13 Jul 19 13:01 /var/tmp/test.txt ファイルの内容を書き換えてから再実行してみる\n$ sudo bash -c \u0026quot;echo hello nifty cloud \u0026gt; /var/tmp/test.txt\u0026quot; $ cat /var/tmp/test.txt hello nifty cloud $ sudo chef-apply recipe.rb Recipe: (chef-apply cookbook)::(chef-apply recipe) * file[/var/tmp/test.txt] action create - update content in file /var/tmp/test.txt from bac308 to 853ff9 --- /var/tmp/test.txt\t2013-07-19 13:07:41.971353051 +0000 +++ /tmp/chef-diff20130719-2119-b2zqpu\t2013-07-19 13:09:10.647668845 +0000 @@ -1 +1 @@ -hello nifty cloud +hello, world $ cat /var/tmp/test.txt hello, world レシピを書き換えてみる\n$ cat recipe.rb file '/var/tmp/test.txt' do content \u0026quot;hello, world\\n\u0026quot; end $ sed -i -e 's/world/chef/' recipe.rb $ cat recipe.rb file '/var/tmp/test.txt' do content \u0026quot;hello, chef\\n\u0026quot; end $ sudo chef-apply recipe.rb Recipe: (chef-apply cookbook)::(chef-apply recipe) * file[/var/tmp/test.txt] action create - update content in file /var/tmp/test.txt from 853ff9 to 5b8079 --- /var/tmp/test.txt\t2013-07-19 13:14:11.097818719 +0000 +++ /tmp/chef-diff20130719-2251-ptr0j5\t2013-07-19 13:16:06.729605440 +0000 @@ -1 +1 @@ -hello, world +hello, chef 3. Chef Solo で WordPress レシピ開発 # 3.1. Chef Solo 用の設定ファイル配置 # $ sudo mkdir -p /var/chef/cookbooks 後で knife cookbook site install を実行するために git 管理にしておく\n$ sudo yum -y install git $ cd /var/chef/cookbooks $ sudo git init . $ sudo touch README $ sudo git add README $ sudo git commit -m \u0026quot;add readme.\u0026quot; 3.2. WordPress レシピのダウンロード # Opscode のコミュニティサイトから knife で wordpress cookbook をダウンロード\n$ sudo knife cookbook site install wordpress なにやら沢山ダウンロードされました。\n$ ls /var/chef/cookbooks/ apache2 build-essential mysql openssl php README wordpress xml ブランチが切られている\n$ cd /var/chef/cookbooks/ $ git branch chef-vendor-apache2 chef-vendor-build-essential chef-vendor-mysql chef-vendor-openssl chef-vendor-php chef-vendor-wordpress chef-vendor-xml * master 3.3. レシピ実行 # $ cat \u0026lt;\u0026lt;_EOD_ \u0026gt; dna.json { \u0026quot;run_list\u0026quot;: [\u0026quot;recipe[wordpress]\u0026quot;], \u0026quot;mysql\u0026quot;: { \u0026quot;server_root_password\u0026quot;: \u0026quot;password\u0026quot;, \u0026quot;server_debian_password\u0026quot;: \u0026quot;password\u0026quot;, \u0026quot;server_repl_password\u0026quot;: \u0026quot;password\u0026quot; } } _EOD_ $ sudo chef-solo -j dna.json {snip} Chef Client finished, 106 resources updated 確認\n$ curl -s -v http://localhost/ * About to connect() to localhost port 80 (#0) * Trying ::1... connected * Connected to localhost (::1) port 80 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.13.6.0 zlib/1.2.3 libidn/1.18 libssh2/1.4.2 \u0026gt; Host: localhost \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 302 Found \u0026lt; Date: Fri, 19 Jul 2013 13:35:54 GMT \u0026lt; Server: Apache \u0026lt; X-Powered-By: PHP/5.3.3 \u0026lt; Location: http://localhost/wp-admin/install.php \u0026lt; Vary: Accept-Encoding \u0026lt; Content-Length: 0 \u0026lt; Content-Type: text/html \u0026lt; * Connection #0 to host localhost left intact * Closing connection #0 ふむふむ、インストールできているようだ\n4. レシピのテストを書く # 4.1. serverspec のインストール # $ sudo /opt/chef/embedded/bin/gem install serverspec Fetching: rspec-core-2.13.1.gem (100%) Fetching: diff-lcs-1.2.4.gem (100%) Fetching: rspec-expectations-2.13.0.gem (100%) Fetching: rspec-mocks-2.13.1.gem (100%) Fetching: rspec-2.13.0.gem (100%) Fetching: serverspec-0.7.0.gem (100%) Successfully installed rspec-core-2.13.1 Successfully installed diff-lcs-1.2.4 Successfully installed rspec-expectations-2.13.0 Successfully installed rspec-mocks-2.13.1 Successfully installed rspec-2.13.0 Successfully installed serverspec-0.7.0 6 gems installed Installing ri documentation for rspec-core-2.13.1... Installing ri documentation for diff-lcs-1.2.4... Installing ri documentation for rspec-expectations-2.13.0... Installing ri documentation for rspec-mocks-2.13.1... Installing ri documentation for rspec-2.13.0... Installing ri documentation for serverspec-0.7.0... Installing RDoc documentation for rspec-core-2.13.1... Installing RDoc documentation for diff-lcs-1.2.4... Installing RDoc documentation for rspec-expectations-2.13.0... Installing RDoc documentation for rspec-mocks-2.13.1... Installing RDoc documentation for rspec-2.13.0... Installing RDoc documentation for serverspec-0.7.0... $ ls /opt/chef/embedded/bin/serverspec-init /opt/chef/embedded/bin/serverspec-init $ /opt/chef/embedded/bin/serverspec-init Select a backend type: 1) SSH 2) Exec (local) Select number: 2 + spec/ + spec/localhost/ + spec/localhost/httpd_spec.rb + spec/spec_helper.rb + Rakefile 4.2. httpd のテストを修正 # そのままでは rake spec に失敗する\n$ /opt/chef/embedded/bin/rake spec /opt/chef/embedded/bin/ruby -S rspec spec/localhost/httpd_spec.rb ..F..F Failures: 1) Service \u0026quot;httpd\u0026quot; Failure/Error: it { should be_running } service httpd status httpd dead but subsys locked expected Service \u0026quot;httpd\u0026quot; to be running # ./spec/localhost/httpd_spec.rb:9:in `block (2 levels) in ' 2) File \u0026quot;/etc/httpd/conf/httpd.conf\u0026quot; Failure/Error: it { should contain \u0026quot;ServerName localhost\u0026quot; } grep -q -- ServerName\\ localhost /etc/httpd/conf/httpd.conf expected File \u0026quot;/etc/httpd/conf/httpd.conf\u0026quot; to contain \u0026quot;ServerName localhost\u0026quot; # ./spec/localhost/httpd_spec.rb:18:in `block (2 levels) in ' Finished in 0.10016 seconds 6 examples, 2 failures Failed examples: rspec ./spec/localhost/httpd_spec.rb:9 # Service \u0026quot;httpd\u0026quot; rspec ./spec/localhost/httpd_spec.rb:18 # File \u0026quot;/etc/httpd/conf/httpd.conf\u0026quot; rake aborted! /opt/chef/embedded/bin/ruby -S rspec spec/localhost/httpd_spec.rb failed Tasks: TOP =\u0026gt; spec (See full trace by running task with --trace) ので、とりあえずエラーにならないようにテストの方を書き換える 1つ目のエラーは service コマンドは root で実行する必要があったため。\n$ sed -i -e '/ServerName localhost/d' spec/localhost/httpd_spec.rb $ sudo /opt/chef/embedded/bin/rake spec /opt/chef/embedded/bin/ruby -S rspec spec/localhost/httpd_spec.rb ..... Finished in 0.08218 seconds 5 examples, 0 failures 4.3. mysqld のテストを作成 #  mysql-server パッケージがインストールされていること mysqld デーモンが有効化されていること (chkconfig mysqld on されていること) mysqld デーモンが起動していること 3306 ポートが LISTEN していること  httpd_spec.rb をコピーしてちょっと書き換えればできた。\ncp spec/localhost/{httpd,mysqld}_spec.rb vi spec/localhost/mysqld_spec.rb 次は serverspec のドキュメント を参照しながら\n http://localhost/wp-admin/install.php にアクセスすると \u0026ldquo;Welcome to the famous five minute WordPress installation process!\u0026rdquo; という文字列が表示されること  を確認するテストを書きます\n$ vi spec/localhost/wordpress_spec.rb $ sudo /opt/chef/embedded/bin/rake spec /opt/chef/embedded/bin/ruby -S rspec spec/localhost/httpd_spec.rb spec/localhost/mysqld_spec.rb spec/localhost/wordpress_spec.rb .......... Finished in 0.47343 seconds 10 examples, 0 failures でけた。\n5. CloudAutomation β で自動化！ # これは省略。 さて次はこれと同じ事を Ansible で行うハンズオン資料を書いてみよう。\n","date":"2013年7月19日","permalink":"/2013/07/july-tech-festa-chef-serverspec-hands-on/","section":"Posts","summary":"July Tech Festa の Chef ハンズオン資料が公開されていたので家で試してみた。公開ありがとうございます。 Vagrant でテスト用環境を立ち上げる $ mkdir chef $ cd chef $ vagrant init centos6 http://developer.nrel.gov/downloads/vagrant-boxes/CentOS-6.4-x86_64-v20130427.box $ vagrant","title":"July Tech Festa の Chef / serverspec ハンズオンを家で試した #techfesta"},{"content":"","date":"2013年7月19日","permalink":"/tags/%E3%83%8F%E3%83%B3%E3%82%BA%E3%82%AA%E3%83%B3/","section":"Tags","summary":"","title":"ハンズオン"},{"content":"【AWS発表】AWS上でPostgreSQLを実行する - 新しいホワイトペーパーを公開 にあった RDBMS in the Cloud: PostgreSQL on AWS (PDF) を読んだので個人的にフムフムと思ったところをまとめておく。\nTemporary Data and SSD Instance Storage # 一時データ用のテーブルを SSD に置く方法。 SSD はインスタンスストレージなので OS 再起動でデータは消えてしまう。 まず、テーブルを作成したらデータを入れる前に関連データファイルを cp コマンドなどでバックアップし、OS　再起動時には PostgreSQL を起動する前にバックアップしたファイルを戻すことで、そのまま利用することができる。もちろん当該テーブルのデータは空っぽだが一時データだからそれを念頭に置いた使い方をする。 Replication を行なっている場合には通常テーブルこれを行うと不整合が発生するため、UNLOGGED テーブルとすることで回避できる。UNLOGGED テーブルは replication の対象外となるため。\nPerformance Suggestions # パフォーマンスを上げるには性能の良いインスタンスに切り替えることと、EBS を RAID0 で束ねること。EBS の数を増やすことでパフォーマンスを上げることが可能。Provisioned IOPS を使うこと。ただし、PIOS には制限があって 4,000 IOPS が必要な場合には最低でも 400GB のボリュームが必要（お金が必要） effective_io_concurrency の値は EBS のボリューム (RAID0 の stripe) 数に合わせる。 SSD 上で稼働するレプリカの場合は fsync と full_page_writes を無効にする。(SSD のデータはクラッシュ時には消えてしまうので)\nMaintenance and Vacuuming # auto vacuum はデフォルトで有効だが、同時実行数、実行間隔、実行タイミングをチューニングすべし。\nRead-Only Servers # メンテナンスのためなどに一時的に DB を Read-Only にするためには\nransaction_read_only=on default_transaction_read_only=on と設定して、pg_ctl reload する。\nStoring Backups and WAL Files # S3 を使ったバックアップ/アーカイブツールの紹介 https://github.com/wal-e/wal-e 次のコマンドがある\n backup-push - フルバックアップを S3 に保存する backup-fetch - フルバックアップを S3 から取得する wal-push - archive_command で使って WAL ファイルを S3 に保存する wal-fetch - restore_command で使って WAL ファイルを S3 から取得する backup-list - バックアップリストを取得する delete - 指定した base backup より前のファイルを削除する  圧縮と暗号化(GPG)もサポートする。AWS で使う場合、これは便利そう。\nTunables #  不要な swap を抑えるために vm.swappiness は 5 以下にする ファイルシステムは xfs でマウントオプションに nobarrier,noatime,noexec,nodiratime を指定する pg_xlog (WAL) ディレクトリはデータとは別ボリュームにするべし、そして fsync の効率から xfs にするべし。 postgresql.conf の設定には pgTune (https://github.com/gregs1104/pgtune/) が参考になる。ただし、サポートされているバージョンに注意  ext3 を使うなというメッセージが強い。\n","date":"2013年7月12日","permalink":"/2013/07/rdbms-in-the-cloud-postgresql-on-aws/","section":"Posts","summary":"【AWS発表】AWS上でPostgreSQLを実行する - 新しいホワイトペーパーを公開 にあった RDBMS in the Cloud: PostgreSQL on AWS (PDF) を読んだので個人的にフムフムと思","title":"RDBMS in the Cloud: PostgreSQL on AWS を読んで"},{"content":"","date":"2013年7月10日","permalink":"/tags/riak/","section":"Tags","summary":"","title":"Riak"},{"content":"2013/07/10 Yahoo! JAPAN にて開催された Riak Meetup Tokyo #2 に参加してきたのでメモ\nセッション1 FreakOut 久森さん 「Riak環境をプロダクションで構築＆運用してみた（仮）」 # RTB (こっちじゃない) という「50ms or die」な環境で Riak を導入してみて\u0026hellip;というお話。 SSP からのリクエストに 100ms 以内にリクエストを返さないと、リクエストすら来なくなるというハードな世界。ネットワークの TTL が 10ms 程度で、アプリ側の処理は 50ms に抑えたいという。 このような環境でデータストアとして Kyoto なんとか Tokyo なんとかを使ってきたがアプリ側での計算による分散であるため、スケールアウトが容易ではないという問題を解決するために Riak の導入にチャレンジしている。 構成はアプリとの間に HA Proxy を挟む Engine Yard 方式 (前回の Meetup で紹介されていました) を採用。 で、どうだったかというと HA Proxy が RoundRobin などで振り分けてしまうため、実際に必要なデータを持っていない node に振られることが多く、その場合、そこから更にデータを持っている node から取ってきて返すという処理 (Redirect) が発生し、そのレスポンス待ちでアプリの worker が詰まってしまい 100ms を超えてしまう状況が発生してしまった。 今後は HA Proxy の層を撤廃し、どの node がデータを持っているかを bucket と key から計算してダイレクトに node に問い合わせる方法を検討されているとのこと。 FreakOut のアプリは Perl がメインで今回の Riak クライアントも Perl (XS) で実装。 既存の Perl Module は遅かったので自作されたとのこと。Protocol は PBC https://github.com/myfinder/p5-riak-lite パフォーマンス比較 https://gist.github.com/myfinder/5232845 Riak を空きポートで起動させてテストするためのコードも https://github.com/myfinder/p5-riak-lite-pbc @johtani さんの Riak Meetup Tokyo #2に参加しました。 を見て思い出したので追記 (2013/7/11)\n HA Proxy 構成では厳しいので今はホットなデータを memcached にキャッシュしている Riak の backend は Bitcask (とりあえずデフォルトで評価というのと expire 機能に期待) Bucket の r, w は最初デフォルトの r:2, w:3 で、r:1 に変えてみたけど求める速度には至らなかった  LT IIJ 曽我部さん、田中さん 「Yokozuna 日本語検索性能を評価しました」 # Riak に Solr を組み合わせた Yokozuna というものが日本語に対応したということで、その検証レポート。まだまだ検証途中のようです。 Solr だからスキーマは定義して上げる必要がある。Bucket 単位で Solr の core を作成する。データは yz_extractor が plain text / JSON / XML を parse してくれる。 タイトルだけ見た段階では、「それなら Elastic Search じゃない？」って思いましたが Riak へのデータ登録、削除、node の増減にも追随するということなら良いかもと思いました。が、そのあたりはまだテストできてないとのことでしたので要望として上げておこう。\n すでにデータの存在する Bucket に設定すると index してくれるのか Re-index する機能があるのか スキーマを変えたい時は別のスキーマを指定した index を追加して切り替えられるのか データの更新・削除は index に反映されるのか node の追加、削除時に hand off に合わせて index も移動するのか Solr (Java) だけが一時的に落ちてしまった場合の対応方法 (Java のチューニングで再起動とかはありそう) Solr への登録は同期？非同期？  とか？ Solr の distributed search って全部からの応答を待つし、一つでもエラーがあったらエラーになってしまう記憶があるので、ちょっとでも遅いサーバーがあったりすると全体に影響するのかな？ しかし、テストのために32台もの結構良いスペックのサーバーが使えるなんて IIJ さんうらやましい。\nDrinkup # Riak Drinkup Tokyo #2 こちらも参加させていただきました。ごちそうさまでした。\nおまけ # Bucket と Key から Hash 値を取得する方法とか、Hash 値から node を取得する方法とか書いてあるサイト見つけました。参考になる。 第２回　NOSQL実機ハンズオン（Riak、Hibari） https://github.com/ossforum-jp-nosql/hands-on\n","date":"2013年7月10日","permalink":"/2013/07/riak-meetup-tokyo-2/","section":"Posts","summary":"2013/07/10 Yahoo! JAPAN にて開催された Riak Meetup Tokyo #2 に参加してきたのでメモ セッション1 FreakOut 久森さん 「Riak環境をプロダクションで構築＆運用してみた（仮）」 # RTB (こっ","title":"Riak Meetup Tokyo 2 に参加してきた #riakjp"},{"content":"2013-07-09 追記\n2013-07-04 に VirtualBox 4.2.16 がリリースされていて、vagrant up に失敗する問題も修正されています。 ノートPCの OS を入れ直したので再度 Vagrant 環境を構築してみたら vagrant up で\nNS_ERROR_ABORT となって起動しませんでした。 調べてみたら GitHub に issue がありました。 The latest upgrade to Virtualbox on Arch Linux breaks vagrant boxes 2013年6月21日リリースの VirtualBox 4.2.14 にバグがあるようです。 VirtualBox を一つ前の 4.2.12 に入れ替えたら起動しました。 今回はこんなマルチホームな vm を起動してみた。\nconfig.vm.define :node1 do |node| node.vm.box = \u0026#34;centos6\u0026#34; node.vm.network :forwarded_port, guest: 22, host: 2001, id: \u0026#34;ssh\u0026#34; node.vm.network :public_network, :bridge =\u0026gt; \u0026#34;wlan0\u0026#34;, ip: \u0026#34;192.168.0.101\u0026#34; node.vm.network :private_network, ip: \u0026#34;192.168.33.11\u0026#34; end config.vm.define :node2 do |node| node.vm.box = \u0026#34;centos6\u0026#34; node.vm.network :forwarded_port, guest: 22, host: 2002, id: \u0026#34;ssh\u0026#34; node.vm.network :public_network, :bridge =\u0026gt; \u0026#34;wlan0\u0026#34;, ip: \u0026#34;192.168.0.102\u0026#34; node.vm.network :private_network, ip: \u0026#34;192.168.33.12\u0026#34; end 前に試したときは Multi-Machine 設定はなぜかエラー（名前を指定して個別に起動すれば2つ起動した）になってたのが直ってる。 （気がする、前の設定に問題があったのかどうかはもはや不明）\n Vagrant メモ (1) Vagrant メモ (2)  ","date":"2013年6月27日","permalink":"/2013/06/vagrant-up-ns_error_abort/","section":"Posts","summary":"2013-07-09 追記 2013-07-04 に VirtualBox 4.2.16 がリリースされていて、vagrant up に失敗する問題も修正されています。 ノートPCの OS を入れ直したので再度 Vagrant 環境を構築してみた","title":"VirtualBox 4.2.14 で Vagrant が NS_ERROR_ABORT で up にコケる"},{"content":"OS (Linux Mint) を入れ直してまたググったのでメモ。ホームディレクトリにある日本語ディレクトリを英語に変更するには\n$ LANG=C xdg-user-dirs-update --force とすることで、英語のディレクトリが作成される。日本語ディレクトリはそのまま残るので不要なら削除する。 設定は .config/user-dirs.dirse と .config/user-dirs.locale に保存される。 xdg-user-dirs-update (1) GUI 版の xdg-user-dirs-gtk-update というコマンドもある。\n   日本語 English     デスクトップ Desktop   ダウンロード Download   テンプレート Templates   公開 Public   ドキュメント Documents   音楽 Music   画像 Pictures   ビデオ Videos    ","date":"2013年6月25日","permalink":"/2013/06/xdg-user-dirs-update/","section":"Posts","summary":"OS (Linux Mint) を入れ直してまたググったのでメモ。ホームディレクトリにある日本語ディレクトリを英語に変更するには $ LANG=C xdg-user-dirs-update --force とすることで、英語のディレクト","title":"デスクトップディレクトリを英語に変更"},{"content":"Linux Mint 15 を入れてる Note PC は SSD なのだけれど、Linux で Trim コマンドってどうなってるんだろうと調べてみたら\n Trim コマンドを使うためには discard オプションをつけてマウントする\nただし、使えるのは ext4 だけ SSD なら I/O スケジューラーを noop にするのが良い  という事だったので設定してみました。 まずは現在の I/O scheduler を確認してみる\n$ cat /sys/block/sda/queue/scheduler noop [deadline] cfq Linux Mint 15 は Ubuntu 13.04 ベースなので default の I/O scheduler を変更するためには /etc/default/grub に elevator=noop の設定を入れる。\n$ echo 'GRUB_CMDLINE_LINUX=\u0026quot;elevator=noop\u0026quot;' | sudo bash -c \u0026quot;cat \u0026gt;\u0026gt; /etc/default/grub\u0026quot; $ sudo update-grub で再起動後に確認。\n$ cat /sys/block/sda/queue/scheduler [noop] deadline cfq 外付け HDD とか繋げたら\n$ sudo bash -c \u0026quot;echo deadline \u0026gt; /sys/block/sd?/queue/scheduler\u0026quot; とかした方が良いのかな？ 次に Trim コマンドを有効にするため、fstab のマウントオプションに discard を追加する。 再起動無しで変更するには\n$ sudo mount -o remount,discard / $ mount | grep ' on / ' /dev/sda2 on / type ext4 (rw,errors=remount-ro,discard) パフォーマンスの計測などは行なっていない。\n","date":"2013年6月25日","permalink":"/2013/06/ssd-io-scheduler-and-trim/","section":"Posts","summary":"Linux Mint 15 を入れてる Note PC は SSD なのだけれど、Linux で Trim コマンドってどうなってるんだろうと調べてみたら Trim コマンドを使うためには discard オプションをつけ","title":"SSD だから I/O Scheduler の変更と Trim を有効にする"},{"content":"","date":"2013年6月5日","permalink":"/tags/backup/","section":"Tags","summary":"","title":"backup"},{"content":"","date":"2013年6月5日","permalink":"/tags/gitlab/","section":"Tags","summary":"","title":"gitlab"},{"content":"（2013/06/07追記あり） GitLab 環境のバックアップをどうしようかなぁ、KDE の repository が壊れた時の記事で git clone --mirror なんてのを見かけたなぁと思ってたら GitLab には gitlab:backup:create という rake task があるのを見つけたのでとりあえずこれを設定することにしました。 GitLab のバージョンは 5.2 です。 バックアップファイルの出力先は config/gitlab.yml の gitlab.backup.path で指定します。デフォルトは相対パスで tmp/backups になってます。/ で始めれば絶対パスになります。 実行時にこのディレクトリに repositories, db, uploads ディレクトリが作成されそれぞれが書きだされます。backup_information.yml というファイルにバージョンやバックアップ日時などが書かれます。そして {unix_timestamp}__gitlab_backup.tar という tar ファイルにまとめられます。3つのディレクトリと backup_information.yml は削除されます。 世代管理は config/gitlab.yml の gitlab.backup.keep_time に残しておきたい期間を秒で設定します。デフォルトは 0 で無期限です。 Git repository のバックアップは``` git bundle create #{path_to_bundle(project)} \u0026ndash;all\n```で行われています。git bundle はバックアップに適しているみたいです。速いです。 git bundle についてはこちらのスライドが大変参考になります。 「Gitによるバージョン管理」の執筆者によるGit勉強会か講演会 / git bundle https://speakerdeck.com/iwamatsu/git-bundle でも、やっぱりリアルタイムでバックアップした方が良いよなぁ gitのbareリポジトリのバックアップをとる - 馬鹿と天才は紙一重 あたりを参考にそのうち考えよう。\n※ 2013/06/07 追記 # public/uploads ディレクトリは何もファイルをアップロードしてない状態では作成されていないので、task がエラーで終了します。\n","date":"2013年6月5日","permalink":"/2013/06/backup-gitlab-data/","section":"Posts","summary":"（2013/06/07追記あり） GitLab 環境のバックアップをどうしようかなぁ、KDE の repository が壊れた時の記事で git clone --mirror なんてのを見かけたなぁと思ってたら","title":"GitLab のバックアップ"},{"content":"最近セットアップするのは仮想のゲストOSばっかりだったから気づかなかったけど、CentOS 6.4 (RHEL 6.4) では DELL の ServerAdministrator (OpenManage) が動かなくなっていた。 調べてみると IPMI 周りに問題があるようで、6.3 までだと kernel module に ipmi_si.ko が存在したのに、6.4 では存在しなくなっていることから起こっている問題っぽい。``` # find /lib/modules/`uname -r`/ -name \u0026lsquo;*ipmi*\u0026rsquo; /lib/modules/2.6.32-358.6.2.el6.x86_64/kernel/drivers/char/ipmi /lib/modules/2.6.32-358.6.2.el6.x86_64/kernel/drivers/char/ipmi/ipmi_devintf.ko /lib/modules/2.6.32-358.6.2.el6.x86_64/kernel/drivers/char/ipmi/ipmi_poweroff.ko /lib/modules/2.6.32-358.6.2.el6.x86_64/kernel/drivers/char/ipmi/ipmi_watchdog.ko\n古い kernel には ipmi\\_si.ko がある # modinfo ipmi_si | grep description description: Interface to the IPMI driver for the KCS, SMIC, and BT system interfaces.\nで、「ipmi\\_si.ko」をキーワードにググると出てきますね [Red Hat Enterprise Linux 6.4 breaks Dell Openmanage | robklg](http://robklg.wordpress.com/2013/02/27/red-hat-enterprise-linux-6-4-breaks-dell-openmanage/) ここの本文にある対策でも動くようになりますが、コメントにある「OpenIPMI」をインストールする方法で対応しておきましょう。 それから、srvadmin の最新版(7.2)を試してみたらこれまで通りに $ sudo yum install srvadmin-storage\nを実行しただけでは omreport コマンドがインストールされませんでした。 細かく別れたみたいです。そして依存関係情報に問題があるっぽく、別途 $ sudo yum install srvadmin-omacore srvadmin-storage-cli\n","date":"2013年6月5日","permalink":"/2013/06/dell-server-administrator-does-not-work-on-centos64/","section":"Posts","summary":"最近セットアップするのは仮想のゲストOSばっかりだったから気づかなかったけど、CentOS 6.4 (RHEL 6.4) では DELL の ServerAdministrator (OpenManage) が動かなくなっていた。 調べてみる","title":"CentOS 6.4 で DELL の ServerAdministrator が動かなかった件"},{"content":"","date":"2013年6月5日","permalink":"/tags/ipmi/","section":"Tags","summary":"","title":"ipmi"},{"content":"（2013/06/07追記あり） GitLabがgitoliteからgitlab-shellに切り替えてからそこそこ時間も経ったし、5.2でforkがサポートされということなのでアップグレードしてみました。 Gitリポジトリ管理ツール「GitLab 5.2」リリース、フォーク機能などを追加 いくつかハマりどころがあったのでメモっておきます。 全体的な流れは https://github.com/gitlabhq/gitlabhq/tree/master/doc/update にあるように順番にひとつずつアップグレードしていきます。\nひとつめのハマりどころ、「Wiki の切り替え」 # 4.2 から 5.0 への変更点に Wiki の管理が RDB (MySQL) から Git に変更となるということで\nbundle exec rake gitlab:wiki:migrate RAILS_ENV=production という処理があるのですが\nrake aborted! incompatible character encodings: UTF-8 and ASCII-8BIT となって止まってしまいます。ググってみると同じ問題にぶつかった人が沢山いました。そして\nbundle exec rake gitlab:wiki:migrate safe_migrate=true RAILS_ENV=production と、safe_migrate=true をつけることで回避できることが判りました。\n Task gitlab:wiki:migrate fails Add a safe migration mode to the wiki migrator.  がぁぁぁ、Wikiの問題はこれだけではありませんでした。 RDB の時にはページ間のリンクは title とは別に slug という column の値を使っていましたが Git 管理へ移す処理で title を使う様に変わっていました。かつ、アルファベットと数字(だけ?)がページのファイル名となるようになることで、「あいうえお」というマルチバイトだけの title の場合はページのファイル名が空っぽとなりエラー。「あああABCいいい」は「Abc」がファイル名となりリンク切れとなっていました。(slug じゃなくなってる時点でリンク切れですけど) そして、Web UI ではページのファイル名を変更できないので git で clone \u0026amp; push する必要がありました。\nふたつめの問題「Gitのバージョン」 # 5.0 まで来たので次は 5.1 に挑戦。そして、アクセスしてみると「ゲゲゲゲッ」リポジトリ内のファイルを見ようとするとエラー\nCompleted 500 Internal Server Error in 170ms ActionView::Template::Error (undefined method `committed_date' for nil:NilClass): 4: 5: :plain 6: var row = $(\u0026quot;table.table_#{@hex_path} tr.file_#{hexdigest(file_name)}\u0026quot;); 7: row.find(\u0026quot;td.tree_time_ago\u0026quot;).html('#{escape_javascript time_ago_in_words(commit.committed_date)} ago'); 8: row.find(\u0026quot;td.tree_commit\u0026quot;).html('#{escape_javascript render(\u0026quot;tree/tree_commit_column\u0026quot;, commit: commit)}'); app/views/refs/logs_tree.js.haml:7:in `block in _app_views_refs_logs_tree_js_haml___423991908861600613_57326520' app/views/refs/logs_tree.js.haml:1:in `each' app/views/refs/logs_tree.js.haml:1:in `_app_views_refs_logs_tree_js_haml___423991908861600613_57326520' そして、こんなわかりやすいエラーは「きっと 5.2 にすれば直ってるはずだ」と思い 5.2 にしてみたが同じエラーが出る\u0026hellip; またまたググってみたらやっぱり同じ問題に遭遇しいている人が沢山いました。\n Can not browse project files  git のバージョンは 1.7.10 以降が必要らしいです。しかし、CentOS 6 の yum で入るのは\ngit-1.7.1-3.el6_4.1.x86_64 でダメなので source をダウンロードしていれて解決。ちょうど 1.8.3 が出たところだったのでこれを入れました。 分散バージョン管理システム「Git 1.8.3」がリリース はー、長かった。\n※ 2013/06/07 追記 # GitLab のバックアップ を書いていて気づいたのだが upgrade を毎回別ディレクトリに clone や zip の展開で行なっていたら public/uploads のファイルが置き去りになっていました。 git fetch \u0026amp; git checkout でない方法で更新してる場合はお気をつけください。\n","date":"2013年5月29日","permalink":"/2013/05/gitlab-upgrade-40-to-52/","section":"Posts","summary":"（2013/06/07追記あり） GitLabがgitoliteからgitlab-shellに切り替えてからそこそこ時間も経ったし、5.2でf","title":"GitLabを4.0から5.2にアップグレードしたメモ"},{"content":"2013/07/27 追記 こっちも読んでね「Nexus7 また壊れた？」\n Nexus 7 がうんともすんとも言わなくなったので Nexus7が故障した時の修理方法と交換方法 を見て ASUS に連絡し修理を依頼した。 症状を話したら着払いですぐに送ってという事だったので19日に送付。 28日に修理されて戻って来ました。\n バッテリーの組み立て不良のため、再組み立て致しました。検証の為、OSのりカバーを行いました。作業後検証を行い動作に異常がないことを確認しました。\n とう報告書でした。送付は着払いで送料はかからなかったし、保証期間内で修理も無料で助かりました。送ってから返ってくるまで何の連絡もなかったのでどうなってるんだろーーって思ってたけど。 ただ、スマホやタブレットが起動しなくなったらデータをこちらで消して送るという対応ができないのがちょいと困りもの。GoogleとかTwitter、Facebookその他のパスワードは変更しましたが、ちょっと不安ですね。MACアドレスも変わってなかったから本当にバッテリーを組み立てなおして初期化されて返ってきたのだろうけども。 そう言えば、iPhoneのボタンが壊れて Apple Store で交換してもらったときはパスワード変更とか忘れてたな。危険だ。\n","date":"2013年5月29日","permalink":"/2013/05/nexus7-repair/","section":"Posts","summary":"2013/07/27 追記 こっちも読んでね「Nexus7 また壊れた？」 Nexus 7 がうんともすんとも言わなくなったので Nexus7が故障した時の修理方法と交換方法 を見て","title":"Nexus7 が修理から返ってきた"},{"content":"","date":"2013年5月29日","permalink":"/tags/%E4%BF%AE%E7%90%86/","section":"Tags","summary":"","title":"修理"},{"content":"","date":"2013年5月22日","permalink":"/tags/anyevent/","section":"Tags","summary":"","title":"anyevent"},{"content":"","date":"2013年5月22日","permalink":"/tags/cinnamon/","section":"Tags","summary":"","title":"Cinnamon"},{"content":"（2013/05/24追記あり） デプロイツールとして Cinnamon がカジュアルで良いかなぁと思って使い始めているのですが、remote で実行するコマンドが \\r を \\r\\n 以外で出力するとその後の出力がうまく拾えないという問題が発生しました。 \\r がどんな場合に出力されるかというと、progress メーターみたいに % 表示する場合ですね。``` for my $i (0 .. 10) { my $percent = sprintf \u0026ldquo;\\r%3d%%\u0026rdquo;, $i * 10; syswrite(STDOUT, $percent, length($percent)); sleep 1; } print \u0026ldquo;\\n\u0026rdquo;;\nmaven で依存ライブラリをダウンロードするところで引っかかりました。これは `mvn -B` と batch mode にすることで回避することができます。curl で -s を付けない場合にも download の進捗が表示されますね。 そんで、コードを追っかけてみようかなと。Cinnamon の当該部分は [HandleManager.pm](https://github.com/kentaro/cinnamon/blob/master/lib/Cinnamon/HandleManager.pm) の start\\_async\\_read() の中 on_read =\u0026gt; sub { $handle-\u0026gt;push_read(line =\u0026gt; sub { my $line = $_[1]; push @{$info-\u0026gt;{output_lines}}, $line; log info =\u0026gt; sprintf \u0026ldquo;[%s :: %s] %s\u0026rdquo;, $self-\u0026gt;{host}, $name, $line; }); },\nで push\\_read の type が line なのは AnyEvent::Handle の↓この部分かな。 register_read_type line =\u0026gt; sub { my ($self, $cb, $eol) = @_;\nif (@_ \u0026lt; 3) { # this is more than twice as fast as the generic code below sub { $_[0]{rbuf} =~ s/^([^\\015\\012]*)(\\015?\\012)// or return;\n $cb-\u0026gt;($\\_\\[0\\], \u0026quot;$1\u0026quot;, \u0026quot;$2\u0026quot;); 1 }  } else { $eol = quotemeta $eol unless ref $eol; $eol = qr|^(.*?)($eol)|s;\n sub { $\\_\\[0\\]{rbuf} =~ s/$eol// or return; $cb-\u0026gt;($\\_\\[0\\], \u0026quot;$1\u0026quot;, \u0026quot;$2\u0026quot;); 1 }  } };\nふむふむ Cinnamon::HandleManager をこう書き換えると期待通りかな？ $handle-\u0026gt;push_read(line =\u0026gt; sub { ↓ $handle-\u0026gt;push_read(line =\u0026gt; qr|\\r?\\n|, sub {\n### ※ 2013/05/24 追記 Cinnamon への Pull Request を merge してもらいました。 [https://github.com/kentaro/cinnamon/pull/23](https://github.com/kentaro/cinnamon/pull/23)","date":"2013年5月22日","permalink":"/2013/05/cinnamon-remote-cr/","section":"Posts","summary":"（2013/05/24追記あり） デプロイツールとして Cinnamon がカジュアルで良いかなぁと思って使い始めているのですが、remote で実行するコマンド","title":"Cinnamon の remote で carriage return が..."},{"content":"","date":"2013年5月22日","permalink":"/tags/perl/","section":"Tags","summary":"","title":"perl"},{"content":"cronolog は時間ベースの便利なログローテーションツールですが、オリジナルのサイト http://cronolog.org/ は随分昔から更新されておらず、バグも PATCH へのリンクがあるだけで修正されていません。そのため、自前で patch を当てて使っていました。 が、最近「あれ？EPELにパッケージがあるじゃん！！いまさら patch が当たってないなんてことは無いだろう。これ使えばいいや」と調べもせずに使い始めてました。そしたら「あれ？やっぱバグってね？」と気付き、SRPM を取ってきて調べてみたら largefile 対応の patch しか当たってませんでした。もうひとつ symbolic link がズレて直らない問題があるのにぃぃ。 この patch どうやって EPEL に入れてもらえばいいんだべか？ まぁ、ログのローテーションができないわけじゃなくて最新のログファイルや一つ前のログファイルへの Symbolic link が更新されないし、しばらくログが出ないなんてことが無い場合には発生しない。\n","date":"2013年5月21日","permalink":"/2013/05/cronolog-bug/","section":"Posts","summary":"cronolog は時間ベースの便利なログローテーションツールですが、オリジナルのサイト http://cronolog.org/ は随分昔から更新されておらず、バグも PATCH へのリンクがあるだけで修正さ","title":"EPEL の cronolog バグってた"},{"content":"","date":"2013年5月21日","permalink":"/tags/log/","section":"Tags","summary":"","title":"log"},{"content":"","date":"2013年5月2日","permalink":"/tags/kindle/","section":"Tags","summary":"","title":"kindle"},{"content":"","date":"2013年5月2日","permalink":"/tags/puboo/","section":"Tags","summary":"","title":"puboo"},{"content":"Puboo で 入門Puppet - Automate Your Infrastructure を買いました。 パブーは PDF、ePub、MOBI のどれでもダウンロードできるんですね。便利！！ そして、サイトから直接 Kindle に送る機能もあります。で、送ってみました。 送信先は Amazon のアカウントサービスから My Kindle の「Send-to-Kindle Eメールアドレス」を確認します。Kindle にはメールで送れるんですね。このブログ書く直前に知りました。（知らずにクラウドストレージ経由でスマホのアプリから送ってた） しかし、届かないのです・・・うむむ。 で、翌日ふとメールをチェックしてたら Amazon から「未承認アドレスからのパーソナル・ドキュメントについて」という件名のメールが届いてました。 メールで Kindle に送る場合は送信元アドレスを登録する必要があったみたいです。（他人から勝手に本を送ってもらうことはできないんですね :)）\n Eメールアドレスを承認するための手続きは以下の通りです。 1. My Kindleページを開く。 2. アカウントにサインインする。 3. 「Kindle アカウント」内の「パーソナル・ドキュメント設定」を開く。 4. 「承認済みEメールアドレス」内の「Eメールアドレスを追加」をクリック。 5. 承認したいEメールアドレスを入力し「追加」を選択。 6. 追加されたアドレスからドキュメントを送信する。 こちらのお知らせは1回しか送信されませんのでご注意ください。今後も引き続き、承認済みアドレスから送信されたドキュメントのみご利用いただけます。 パーソナル･ドキュメントの転送についての詳細は、ヘルプページをご覧ください: http://www.amazon.co.jp/kindlepersonaldocuments/ Amazon Kindle カスタマーサービス\n パブーから送る場合の送信元アドレスはこちらに書いて有ります。 Kindle送信設定\n","date":"2013年5月2日","permalink":"/2013/05/send-to-kindle/","section":"Posts","summary":"Puboo で 入門Puppet - Automate Your Infrastructure を買いました。 パブーは PDF、ePub、MOBI のどれでもダウンロードできるんですね。便利！！ そして、サイトから","title":"Puboo（パブー）からKindleに届かない場合に確認すること"},{"content":"","date":"2013年4月20日","permalink":"/tags/dokuwiki/","section":"Tags","summary":"","title":"dokuwiki"},{"content":"","date":"2013年4月20日","permalink":"/tags/pukiwiki/","section":"Tags","summary":"","title":"pukiwiki"},{"content":"先日、「PukiWiki から DokuWiki にデータ移行」を書いた後にもいろいろ改善をすすめたので改めて整理しておく。添付ファイルに対応しました。さらにその後、UTF-8対応の PullRequest をもらってマージしました。 データ移行スクリプトは Github https://github.com/yteraoka/puki2doku に。 PukiWki が /var/www/html/pukiwiki に、DokuWiki が /var/www/html/dokuwiki にある前提とする。 各データもそれぞれのデフォルトのフォルダを使用している前提。\n添付ファイルのコピー # 添付ファイルは PukiWiki では attach/ フォルダに、DokuWiki では data/media/ フォルダに保存される。 PukiWiki は各ページに対して添付するという考え方だが、DokuWiki はページとは独立した名前空間にファイルをアップロードする。データコピーの方法として、ページ名と同じ階層でディレクトリを作成し、そこへコピーすることとした。PukiWiki では attach フォルダに {ページ名}_{ファイル名} というファイル名で作成されている。これを {ページ名}/{ファイル名} とする。{ページ名} にも / が含まれる。それぞれ、ファイルシステム上はページと同様のエンコーディングが採用されている。\n$ puki2doku.pl -v -A \\ -s /var/www/html/pukiwiki/attach \\ -d /var/www/html/dokuwiki/data/media 課題 PukiWiki はページに結びついているので、ページ内にリンクが書かれていなくても添付ファルへアクセスできるのだが、DokuWiki ではページ内にリンクがないと迷子になってしまう。ページに添付ファルのリストを追記すべきかもしれない。\nwiki ページの変換 # $ puki2doku.pl \\ -v \\ --font-color \\ --indexmenu \\ --ignore-unknown-macro \\ -s /var/www/html/pukiwiki/wiki \\ -d /var/www/html/dokuwiki/data/pages 前回は --font-size で \u0026amp;size(N){str}; を fontsize plugin を利用して引き継ぐようにしていたが、このプラグインは他のテキスト装飾を入れ子にできないという問題があったため無効とした。\n検索インデックス # インデックスを作成しないと検索できないので次の手順で作成します。\n$ cd /var/www/html/dokuwiki/bin $ php indexer.pp この作業やデータ移行を DokuWiki の実行ユーザー以外で実行した場合は /var/www/html/dokuwiki/data 配下を全部 DokuWiki 実行ユーザーで書き換え可能にしましょう。\nキャッシュとかメタデータ # DokuWiki は data/cache/, data/meta/ などにキャッシュを作成するので、移行手順をやり直す場合などはこのあたりのファイルを消してやる必要があります。\nLDAP認証 # DokuWiki は標準で LDAP 認証をサポートしている。 LDAP サーバーとして以前取り上げた（その1、その2） OpenDJ を使っている場合は dokuwiki/conf/local.php に次のように書くことで\n$conf['useacl'] = 1; $conf['authtype'] = 'ldap'; $conf['openregister'] = '0'; $conf['auth']['ldap']['server'] = 'ldap://ldap.example.com:1389'; $conf['auth']['ldap']['usertree'] = 'ou=People,dc=example,dc=com'; $conf['auth']['ldap']['grouptree'] = 'ou=Group,dc=example,dc=com'; $conf['auth']['ldap']['userfilter'] = '(\u0026amp;(uid=%{user})(objectClass=inetorgperson))'; $conf['auth']['ldap']['groupfilter'] = '(\u0026amp;(objectClass=groupOfUniqueNames)(uniqueMember=%{dn}))'; dokuwiki/conf/acl.auth.php に @グループ名 としてグループでの権限設定も可能となる。\n矢印とかの自動置換 # dokuwiki/conf/entities.conf に\n\u0026lt;-\u0026gt; ↔ -\u0026gt; → \u0026lt;- ← \u0026lt;=\u0026gt; ⇔ =\u0026gt; ⇒ \u0026lt;= ⇐ \u0026gt;\u0026gt; » \u0026lt;\u0026lt; « --- — -- – (c) © (tm) ™ (r) ® ... … というページ表示時に自動的に置換して表示する機能がありますが、技術系の文書を書いてる場合「\u0026ndash;」とか「\u0026laquo;」などを置換されるとウザイので全部コメントアウトしてしまいましょう。\n諸設定 # スーパーユーザーに設定されたアカウントであれば管理ページにアクセスしてページ名やデザインやプラグインの有効化・無効化などいろいろな設定ができます。 スーパーユーザー設定は conf/local.php で\n$conf['superuser'] = 'user1,user2'; と設定します。複数人指定する場合はカンマ区切りで指定。 前回書いたファイル名のエンコーディングはここで変更可能でした。UTF-8でそのままファイルを作成することも出来るようですが、プラグインなどの互換性の問題で推奨されてません。 トップページを start から PukiWiki と同じ FrontPage にすることもできますが、こちらも互換性の問題で変更は推奨されていませんでした。 これで一段落かな。（もうちょっとかっこいい Template ないかなぁ\u0026hellip;）\n","date":"2013年4月20日","permalink":"/2013/04/migrating-from-pukiwiki-to-dokuwiki-2/","section":"Posts","summary":"先日、「PukiWiki から DokuWiki にデータ移行」を書いた後にもいろいろ改善をすすめたので改めて整理しておく。添付ファイルに対応しました。さらにそ","title":"（続）PukiWiki から DokuWiki にデータ移行"},{"content":"","date":"2013年4月20日","permalink":"/tags/dropbox/","section":"Tags","summary":"","title":"Dropbox"},{"content":"","date":"2013年4月20日","permalink":"/tags/owncloud/","section":"Tags","summary":"","title":"ownCloud"},{"content":"  ownCloud  Dropbox は便利ですが、データを外部に預けてしまうのは不安だとか、料金が気に入らないという人には ownCloud という Dropbox クローンを使うという選択肢があるようです。ということで試してみました。 日本で ownCloud を使ったサービスを提供されてるところもあるようです。(http://owncloud.jp/) サーバーは PHP で書かれていて Web Server は Apache でも Nginx でも Lighttpd でも IIS でも OK (Installation).IIS が使えることからわかるように Windows をサーバーに使うこともできます。 DB は SQLite か MySQL. ユーザー認証に LDAP や IMAP、SMB に FTP なんてのも使えるようです。 サーバー側のストレージには local disk、SMB mount、WebDAV に OpenStack の Swift が使えるようです。 インストールもとっても簡単、主要な Linux distrbution であれば package が存在します。 試した環境はさくらのVPSで CentOS 6 + Apache + MySQL + ローカルストレージ。 クライアントソフトも Windows、Mac OS X、Linux 向けに無料のフォルダ同期クライアントが提供されており、iOS、Android 向けのアプリ（iTunes、Google Play）も有料ですが最低価格程度で提供されています。 Webブラウザでもアクセス可能です。 フォルダ同期ではサーバー側、クライアント側それぞれで任意のフォルダ選択して同期させます。同期のペアは複数設定できます。アカウントは一つのみです。 グループでの共有、アクセス権管理もできます。参照だけさせる共有や編集も可能にする共有ができます。共有された側の Shared というフォルダに下に見えるようになります。ブラウザからの操作で任意のファイル用のURLを作成し不特定多数の人と共有することもできますし、そこにパスワードをつけることもできます。 iOS、Android のクライアントでは複数のアカウントを切り替えながら使う機能がありました。 Linux Mint 14、Windows 8、iOS (iPhone 5)、Android (Nexus 7) でクライアントを試してみました。 ↓これは Linux クライアント  Linux Client  Web サーバーで SSL を設定すれば通信は暗号化されます。自己署名の証明書でも使えました。 Linux クライアントは現状だと、サーバーに接続できなかった場合、エラーで終了してしまいます。無線LANが不安定だったりするといつのまにかいなくなってます\u0026hellip; Dropbox の場合、Windows でも Linux でもクライアント側のファイルを右クリックで過去のバージョンへアクセスできますが、現状の ownCloud ではブラウザでログインしてからそのファイルのメニューからアクセスする必要があります。  ファイル操作メニュー  削除したファイルを取り出すにはブラウザで画面右上の「Deleted files」からWindows のゴミ箱にアクセスする感じです。  Deleted files  戻したいファイルで「Restore」をクリックします。「Download」の方は機能しませんでした。なぜだろ？  Restore  まだ使ったことがありませんが、ownCloud にはアドレス帳、カレンダー、音楽プレーヤー、画像ビューワーなんて機能もあるようです。   Features | ownCloud.org Lucene ベースの検索とかドキュメントビューワーとかまだまだ機能はあります。 セットアップは簡単なので興味を持ったら是非お試しを。 今後に期待のソフトウェアですが、個人利用だったらまだまだ Dropbox が便利ですかね。100GBで年間$99だからさくらのVPSで100GBのプランを借りるのと変わらない。\n ownCloud  \nownCloud\nownCloud, Inc.\n価格： 85円\n  \nposted with sticky on 2013.4.20\n  ownCloud\nownCloud, Inc.\n価格：99円　平均評価：3.2（556）\n","date":"2013年4月20日","permalink":"/2013/04/owncloud-1/","section":"Posts","summary":"ownCloud Dropbox は便利ですが、データを外部に預けてしまうのは不安だとか、料金が気に入らないという人には ownCloud という Dropbox クローンを使うという選択肢があるようです","title":"ownCloud でオンプレ Dropbox"},{"content":"PukiWiki から DokuWiki にデータ移行するメモです。 移行されるかたは是非続編もご覧ください。\nDokuWiki Plugiin # DokuWiki は plugin なしでは PukiWiki よりも表現力が劣るので、次の Plugin を導入しました。\n definitions plugin（,  対応） indexmemu plugin（ls(), ls2() 対応） fontsize plugin（\u0026amp;size(n){str}; 対応） color plugin（\u0026amp;color(xxx){str}; 対応）  ファイル名の命名規則 # PukiWiki と DokuWiki ではファイル名の命名規則が異なります。\n PukiWiki はページ名を EUC-JP で、アルファベットも記号も込で全部を16進のコードにしてしまいます 「/」も16進にするため、wiki ディレクトリにフラットに全てのファイルが配置されます DokuWiki ネームスペースを「/」で区切るため、ネームスペースがディレクトリになります 文字コードは UTF-8 で、記号（マルチバイトのものも一部含む）は「_」に置き換え、アルファベットは小文字に統一します 連続する「_」はひとつにまとめ、ディレクトリ、ファイル名の末尾の「_」は削ります マルチバイト文字は URL エンコードされます。EUC-JP から UTF-8 への変更でほぼ2バイトだったものが3バイト以上になり、ただの HEX だったものが「%」+ HEX となり50%増量で、日本語ページ名のファイル名は結構な長さになります 256バイトを超えて、そのままでは移行できないページが発生しました アルファベットばかりのページ名であれば短くなります  PukiWiki も DokuWiki も拡張子「.txt」が付きます。 PHP で dokuwiki のファイル名生成関数を使うのが素直だと思いますが、PHP得意じゃないのでデータの変換処理書くのに時間がかかりそうだったから Perl で書きました。\nscript # ./puki2doku.pl -C -S -I \\ -s pukiwiki/wiki -d dokuwiki/data/pages 綺麗なコードではないですが、公開しておきます。 puki2doku.pl 2013/04/19 いくつか Bug を修正して、Git に移しました。\n Table Cell の色付けには対応してません FrontPage を start に置換します 「- - - -\\n#contents\\n- - - -\\n」という私がよく使っていた TOC のためのコードを消す特殊処理が入ってます touch コマンドで元のファイルの timestamp をコピーします  添付ファイル # 対応してません。（やりかけた形跡が残ってますけど\u0026hellip;） 後に対応しました。「（続）PukiWiki から DokuWiki にデータ移行」\n検索インデックス # データ移行後に\ncd dokuwiki/bin php indexer.php で検索インデックスを作成してください。 Web からではなくファイルを直接変更した場合はこの処理が必要です。 今日はこれまで。 2013/04/20 追記 fontsize plugin は color や bold 装飾と入れ子で使う場合一番内側に置かないと他の装飾が効かなし「**」とかがそのまま表示されてしまう・・・これは厄介。fontsize は無効にするかな。\n","date":"2013年4月15日","permalink":"/2013/04/migrating-from-pukiwiki-to-dokuwiki/","section":"Posts","summary":"PukiWiki から DokuWiki にデータ移行するメモです。 移行されるかたは是非続編もご覧ください。 DokuWiki Plugiin # DokuWiki は plugin なしでは PukiWiki よりも表現力が劣るので、次の Plugin を導入しました","title":"PukiWiki から DokuWiki にデータ移行"},{"content":"最近、他所で漏れたID、パスワードの組みで不正アクセスが試みられるという事象が散見されるとうことで おそらくはそれさえも平凡な日々: パスワードはサーバー側で生成したほうが良いんじゃないかという話 なんてポストがあったので、以前同僚が話してたアイデアを書いてみる。\nそもそも、パスワードなんて覚える気がない # 端から覚える（書き留める）気がない人がいるらしい。そしてブラウザに覚えさせたり、セッションが長かったりして、最初に設定して以降入力する機会がないので忘れる。\nメールでリセットできる # でも、メールアドレスとか入力してリセットすれば問題ない。忘れてもまたリセットすれば良いのです。\nじゃあ全部ワンタイムパスワードで # 「忘れたらリセット」を繰り返すのであれば、それをワンタイムパスワードにすれば良いのではないか。 ということで、ほぼ毎日アクセスするような状況であれば切れない程度にセッションを長めに設定して cookie に持たせ、切れたら一度だけ使えるパスワードをメールで送るようにすればパスワードをなくせそうです。もちろん Gmail は2要素認証にしてますよね？ 私は、前職がISPでメールサービスも提供していたのでメールのパスワード忘れたのをメールでリセットなんてできないじゃんっていうのがあって、こういう発想はなかったけど、軽いサービスだったらありなんじゃないの？と思ったのでした。 # あー、Ultrabook のキーボードの \u0026ldquo;i\u0026rdquo; が入力しづらくてイライラする・・・ # アイソレーションキーボードって何も手出しができない・・・\n","date":"2013年4月10日","permalink":"/2013/04/forget-password/","section":"Posts","summary":"最近、他所で漏れたID、パスワードの組みで不正アクセスが試みられるという事象が散見されるとうことで おそらくはそれさえも平凡な日々: パスワード","title":"覚えなきゃいけないパスワードはなくても良いんじゃないかという話"},{"content":"","date":"2013年4月9日","permalink":"/tags/ldap/","section":"Tags","summary":"","title":"LDAP"},{"content":"","date":"2013年4月9日","permalink":"/tags/opendj/","section":"Tags","summary":"","title":"OpenDJ"},{"content":"前回 OpenDJ - LDAP Server (1) の続き 今回はデータの export, import, backup, restore あたりを紹介。 control-panel は GUI での操作と等価なコマンド(CLI)も表示してくれるので便利。\nexport # Export はこんな感じでまるっと export できます。\nbin/export-ldif \\ --backendID userRoot \\ --hostname 192.168.0.12 \\ --port 4444 \\ --bindDN \u0026quot;cn=Directory Manager\u0026quot; \\ --bindPassword ******** \\ --trustAll \\ --ldifFile /tmp/export.ldif --excludeBranch ou=Group,dc=example,dc=com をつければグループ情報を除外することができます。 --excludeAttribute ds-sync-generation-id をつければその項目を除外できます。 これらは import 時にも指定可能です。 Replication 構成のサーバーから export すると ds-sync-* という attribute が export されますが、それをそのまま import しようとすると意図した通りにならなかったりするので注意。ただ export したものを import したいだけの場合は ds-sync-* は除外しておくのが良いでしょう。\nimport # export とよく似てます。\nbin/import-ldif \\ --backendID userRoot \\ --hostname 192.168.0.12 \\ --port 4444 \\ --bindDN cn=Directory\\ Manager \\ --bindPassword ******** \\ --trustAll \\ --excludeAttribute ds-sync-generation-id \\ --excludeBranch ou=Group,dc=example,dc=com \\ --append \\ --ldifFile /tmp/users.ldif 通常 import はデータの追加登録の用途で利用すると思います、この時 --append を忘れると全部消えてしまうという悲惨な目にある可能性が高いので要注意です。また、デフォルトではすでに存在するエントリーは上書きしないので、上書きしたい場合は --replaceExisting をつけます。export 同様 --excludeAttribute や --excludeBranch は必要に応じて指定。exclulde じゃなくて include という指定方法もあります。 1ユーザー追加しようと思って uid=user,ou=People,dc=example,dc=com の1エントリーだけの ldif を作って --append を忘れると dc=example,dc=com も ou=People も消えてしまって空っぽになってしまいます。要注意。 import で replication が崩れることがありますが、そんな場合は忘れずに dsreplication initialize してください。\nBackup # バックアップは保存先ディレクリと任意の名前(bakupID)を指定します。 control-panel からバックアップを行うと backupID は日時がセットされます。 圧縮、暗号化、増分バックアップも指定できます（今回は省略）。\nbin/backup \\ --backupDirectory /opt/OpenDJ-2.5.0-Xpress1/bak \\ --backupID 20130409232226 \\ --backendID userRoot \\ --hostName 192.168.0.12 \\ --port 4444 \\ --bindDN cn=Directory\\ Manager \\ --bindPassword ******** \\ --trustAll \\ --noPropertiesFile Restore # リストアは Backup の逆ですね。\nbin/restore \\ --backupID 20130409232226 \\ --backupDirectory /opt/OpenDJ-2.5.0-Xpress1/bak \\ --bindPassword ******** \\ --trustAll --dry-run をつけると実際には restore せずにチェックを行なってくれます。 でわでわ、また何かあったら続きを書くかも。\n","date":"2013年4月9日","permalink":"/2013/04/opendj-ldap-server-2/","section":"Posts","summary":"前回 OpenDJ - LDAP Server (1) の続き 今回はデータの export, import, backup, restore あたりを紹介。 control-panel は GUI での操作と等価なコマンド(CLI)も表示してくれるので便利。 export # Export はこんな感じ","title":"OpenDJ - LDAP Server (2)"},{"content":"","date":"2013年4月5日","permalink":"/tags/activedirectory/","section":"Tags","summary":"","title":"ActiveDirectory"},{"content":"メジャーな UTM である FortiGate で VPN などのユーザー認証に LDAP / Active Directory を使う方法を紹介。LDAP サーバーの構築方法は OpenDJ – LDAP Server (1) で。FortiGate の OS は Version 4.0 MR3 で確認。\nLDAP の場合 # これは簡単 User \u0026gt; Remote \u0026gt; LDAP \u0026gt; Create New で\nName\nldap.example.com （任意の名前）\nServer Name/IP\nldap.example.com （LDAPサーバー）\nServer Port\n1389 （IANA の wellknown port は 389）\nCommon Name Identifier\nuid\nDestinguished Name\nou=People,dc=example,dc=com\nBind Type\nSimple\nActive Directory の場合 # AD を使う場合は FortiGate からアクセスするためのユーザーが必要。Domain Users に属していれば OK ここでは fortigate@example.com というユーザーを作成する。表示名は「FortiGate VPN」とする。 User \u0026gt; Remote \u0026gt; LDAP \u0026gt; Create New で\nName\nActiveDirectory （任意の名前）\nServer Name/IP\n192.168.xx.xx （AD Server）\nServer Port\n389\nCommon Name Identifier\nsAMAccountName\nDistinguished Name\nOU=Users,DC=example,DC=com\nBind Type\nRegular\nUser DN\nCN=FortiGate VPN,CN=Users,DC=example,DC=com （CNはADの表示名の値）\nPassword\nパスワード （fortigate@example.comユーザーのパスワード）\nユーザーの ou は部署ごとに分かれてても ok でした。\n","date":"2013年4月5日","permalink":"/2013/04/fortigate-ldap-auth/","section":"Posts","summary":"メジャーな UTM である FortiGate で VPN などのユーザー認証に LDAP / Active Directory を使う方法を紹介。LDAP サーバーの構築方法は OpenDJ – LDAP Server (1) で。FortiGate の OS は Version 4.0","title":"FortiGate で VPN 認証に LDAP / Active Directory を使う"},{"content":"OpenDJ とは Sun Microsystems が OSS として開発していた OpenDS という LDAP サーバーを Oracle が買収後に「OSS やーめたっ」と発表したために ForgeRock が fork して開発を継続している Java 製の LDAP サーバーです。ForgeRock は OpenSSO についても OpenAM として開発を行なっています。 Sun が開発を行なっていたためか、日本語ローカライズがしっかりされています。LANG=ja_JP.UTF8 で作業すればほとんど日本語で表示されます。 OpenDJ は Multi-Master Replication の構成を組むことが可能です。 そして、LDAP エントリを操作するための GUI もついています。 ApacheDS も Multi-Master Replication 対応で Eclipse plugin の エントリ操作ツールApache Directory Studio が提供されてて便利ですね。\n ※2013-04-05追記 Apache Directory Studio は Eclipse プラグインじゃなくて単体アプリになってました。これだけでも使えます、便利です。Active Directory につないだりもできます。\n Multi-Master なんていう甘い言葉は危険なかおリもしますが、LDAP はもともと更新頻度の低いデータベースなので危険度は低いのではないかと。実際には、ホットスタンバイとして使ってるわけですけど。 それでは、OpenDJ のセットアップを。\nInstall # まずは1台目のセットアップ (192.168.0.12) 最新版の OpenDJ-2.5.0-Xpress1.zip をダウンロードし、任意の場所で展開します。 そこで setup コマンドを実行すれば GUI のインストーラーが起動しますが、サーバーだとコマンドラインですね。--cli をつけましょう。--no-prompt を付けなければ指定しなかった項目は入力を求められます。\n$ ./setup \\ --cli \\ --baseDN dc=example,dc=com \\ --sampleData 10 \\ --ldapPort 1389 \\ --adminConnectorPort 4444 \\ --rootUserDN cn=Directory\\ Manager \\ --rootUserPassword password \\ --ldapsPort 1636 \\ --generateSelfSignedCertificate \\ --hostName ldap.example.com \\ --no-prompt \\ --noPropertiesFile これで example.com ドメインの LDAP サーバーが構築できました。--sampleData 10 の指定により10個のサンプルユーザーデータが作成されています。root で実行しないので1000番台の port となっています。自己署名の SSL 付き。 bin/control-panel で接続してみるとこんな感じ。\n  次に2台目のセットアップ (192.168.0.11)\n$ ./setup \\ --cli \\ --baseDN dc=example,dc=com \\ --ldapPort 1389 \\ --adminConnectorPort 4444 \\ --rootUserDN cn=Directory\\ Manager \\ --rootUserPassword password \\ --ldapsPort 1636 \\ --generateSelfSignedCertificate \\ --hostName ldap.example.com \\ --no-prompt \\ --noPropertiesFile 今度はサンプルデータはなしでセットアップして、Replication で同期されることを確認します。\nReplication Setup # 1台目(192.168.0.12)と2台目(192.168.0.11)で Replication 構成を組むためのコマンドです。\n$ bin/dsreplication \\ enable \\ --adminUID admin \\ --adminPassword password \\ --baseDN dc=example,dc=com \\ --host1 192.168.0.12 \\ --port1 4444 \\ --bindDN1 \u0026quot;cn=Directory Manager\u0026quot; \\ --bindPassword1 password \\ --replicationPort1 8989 \\ --host2 192.168.0.11 \\ --port2 4444 \\ --bindDN2 \u0026quot;cn=Directory Manager\u0026quot; \\ --bindPassword2 password \\ --replicationPort2 8989 \\ --trustAll \\ --no-prompt これで 8989 ポートを使って Replication する設定が行われました。が、この状態ではまだデータは同期されていません。それぞれに control-panel で接続してみるとわかります。\nReplication Initialize # 1台目のデータを2台目にコピーすることでデータを初期化します。 --hostSource と --hostDestination を反対にしてしまわないように注意。\n$ bin/dsreplication \\ initialize \\ --baseDN \u0026quot;dc=example,dc=com\u0026quot; \\ --adminUID admin \\ --adminPassword password \\ --baseDN dc=example,dc=com \\ --hostSource 192.168.0.12 \\ --portSource 4444 \\ --hostDestination 192.168.0.11 \\ --portDestination 4444 \\ --trustAll \\ --no-prompt これでおしまい。とっても簡単。それぞれ status コマンドで確認してみましょう。\n$ bin/dsreplication status \\ --adminUID admin \\ --adminPassword password \\ --trustAll \\ --hostname 192.168.0.12 \\ --port 4444 $ bin/dsreplication status \\ --adminUID admin \\ --adminPassword password \\ --trustAll \\ --hostname 192.168.0.12 \\ --port 4444 かれこれ2年ほどトラブルフリーで動作してます。バージョンはちょいと古いですが。 1台停止中にもう1台で更新されたデータは復帰後に自動で反映されます。\nReplication をやめる # サーバーが壊れてしまって切り離したい場合とかで Replication をやめるためにはそれぞれのサーバーに接続して disable にします。\n$ bin/dsreplication \\ disable \\ --disableAll \\ --port 4444 \\ --hostname 192.168.0.12 \\ --bindDN \u0026quot;cn=Directory Manager\u0026quot; \\ --adminPassword password \\ --trustAll \\ --no-prompt $ bin/dsreplication \\ disable \\ --disableAll \\ --port 4444 \\ --hostname 192.168.0.11 \\ --bindDN \u0026quot;cn=Directory Manager\u0026quot; \\ --adminPassword password \\ --trustAll \\ --no-prompt いじょー。 データのバックアップとかリストアとかの手順はまたそのうち。\nつづきを書きました。\n","date":"2013年3月27日","permalink":"/2013/03/opendj-ldap-server-1/","section":"Posts","summary":"OpenDJ とは Sun Microsystems が OSS として開発していた OpenDS という LDAP サーバーを Oracle が買収後に「OSS やーめたっ」と発表したために ForgeRock が fork して開発を継続している Java 製の LDAP サー","title":"OpenDJ - LDAP Server (1)"},{"content":"Vagrant メモ (1) の続き。\nVagrant コマンドの概要 # init # 実行したディレクトリに Vagrantfile を作成することで初期化します。\nvagrant init BOX-NAME BOX-URL と box を指定することで同時に box add することができます。box を同時についかすれば Vagrantfile もそれに合わせて作成されます。そうでなかった場合は vagrant add box した後に Vagrantfile の config.vm.box をそれに合わせて書き換える必要があります。Vagrantfile と machine の情報（.vagrant ディレクトリ）は init を実行したディレクトリに作成されますが、box ファイルは ~/.vagrant.d にまとめて保存されます。\nbox # box の管理を行います。add, list, remove, repackage というサブコマンドがあります。box = machine ではありません、1つの box から複数の machine（サーバー）を構築できます。\nvagrant box add BOX-NAME BOX-URL で box を追加します。ダウンロード可能な BOX-URL のリストは Vagrantbox.es にあります。\nup # vagrant up [MACHINE-NAME] で指定の machine を起動させます。MACHINE-NAME を省略すると全てのマシンを起動させます。ただし、エラーが発生するとそれ以降のサーバーは起動されません。\nhalt # vagrant halt [MACHINE-NAME] で指定の machine を shutdown します。MACHINE-NAME を省略すると全てのマシンを shutdown します。\nssh # vagrant ssh [MACHINE-NAME] ssh でマシンにログインします。マルチマシンモードでは MACHINE-NAME が必須です。\nstatus # マシンの状態（起動してるかどうか）を確認します。\nreload # reboot # (halt \u0026amp; up) します。(再起動されるけどSSH関連のエラーが出る)\npackage # 現在のマシンの状態を再利用できるように .box ファイルを作成します。\nsuspend # サスペンドします。（メモリの状態を書き出すはずなのに .vagrant も .vagrant.d も増えないなぁと思ったら、仮想マシンは ~/VirtualBox VMs/ なのでした） この状態から halt すると state ファイルの削除だけとなります。 マシンを指定しないと全てのマシンを suspend します。\nresume # suspend 状態から復帰させます。マシンを指定しないと全てのマシンを復帰させます。 複数のマシンを稼働させる場合 ssh の port forwarding でポート番号の衝突を自動で回避する仕組みがありますが resume の時に問題があるらしく、別のポートを指定しろと言われます。 それには次のように :forwarded_port を設定します。id: \u0026quot;ssh\u0026quot; が無いと default の port forward である 2222 -\u0026gt; 22 も残った上で追加で設定されてしまいます。\nconfig.vm.define :node1 do |node| node.vm.box = \u0026#34;centos6\u0026#34; node.vm.network :forwarded_port, guest: 22, host: 2001, id: \u0026#34;ssh\u0026#34; end config.vm.define :node2 do |node| node.vm.box = \u0026#34;centos6\u0026#34; node.vm.network :forwarded_port, guest: 22, host: 2002, id: \u0026#34;ssh\u0026#34; end destroy # 仮想マシンを破棄します。Vagrantfile は変更されれないのでまた up すれば box から再作成される。\nssh-config # vagrant ssh-config [MACHINE-NAME] Vagrantfile のあるディレクトリで vagrant ssh しなくても ssh 接続できるように ~/.ssh/config に書くための情報を出力してくれるのでリダイレクトで追記すれば良い。 multi-VM の場合は MACHINE-NAME が必須。\nplugin # 未調査\nprovision # 未調査\nその他 # 既存の vmdk から box を作成する方法もあるようです。 Creating a Vagrant base box from an existing Vmdk\n","date":"2013年3月23日","permalink":"/2013/03/vagrant-2/","section":"Posts","summary":"Vagrant メモ (1) の続き。 Vagrant コマンドの概要 # init # 実行したディレクトリに Vagrantfile を作成することで初期化します。 vagrant init BOX-NAME BOX-URL と box を指定することで同時に box add することが","title":"Vagrant メモ (2)"},{"content":"Qiita の シェルスクリプトで便利な小技 で \u0026ldquo;set -u\u0026rdquo; の解説があり\n スクリプト中で値が設定されていない変数を参照した場合に エラーメッセージを表示してスクリプトを終了させる。シェル変数や環境変数を typo した場合など、 変数に値が設定されていない事で発生する問題が回避できる。\n おお、これは便利！！（root で動かす shell script で rm -fr 使うのってすごくコワイ）``` $ cat test.sh #!/bin/sh set -u work_dir=/tmp rm -fr ${work_dri}/*\n$ ./test.sh ./test.sh: 4: ./test.sh: work_dri: parameter not set\nで、Bash をもっと深く知ろうと思い [入門bash 第3版](http://www.amazon.co.jp/gp/product/4873112540/ref=as_li_qf_sp_asin_tl?ie=UTF8\u0026amp;camp=247\u0026amp;creative=1211\u0026amp;creativeASIN=4873112540\u0026amp;linkCode=as2\u0026amp;tag=ytera-22)![](https://www.assoc-amazon.jp/e/ir?t=ytera-22\u0026amp;l=as2\u0026amp;o=9\u0026amp;a=4873112540)（[電子版](http://www.oreilly.co.jp/books/4873112540/)）を買ってパラパラ見てたら **「4.3.1 文字列演算子の構文」**に ${variable:?message}\nというのが載ってました。 出来ることはほぼ \u0026quot;set -u\u0026quot; と変わらないのですが、 $ cat test.sh #!/bin/sh set -u work_dir=/tmp rm -fr ${work_dri:?}/*\n$ ./test.sh ./test.sh: 4: ./test.sh: work_dri: parameter not set or null\n「not set」だけじゃなくて「or null」と表示されてますね。そして、そこを任意のメッセージにすることもできます。 $ cat test.sh #!/bin/sh set -u work_dir=/tmp rm -fr ${work_dri:?\u0026ldquo;test message\u0026rdquo;}/*\n$ ./test.sh ./test.sh: 4: ./test.sh: work_dri: test message\n","date":"2013年3月22日","permalink":"/2013/03/bash-tips-1/","section":"Posts","summary":"Qiita の シェルスクリプトで便利な小技 で \u0026ldquo;set -u\u0026rdquo; の解説があり スクリプト中で値が設定されていない変数を参照した場合に エラーメッセージを表示してスクリプト","title":"Bash Tips （未定義変数）"},{"content":"","date":"2013年3月22日","permalink":"/tags/shell-script/","section":"Tags","summary":"","title":"shell script"},{"content":"流行りの Vagrant （ベイグラント）を試してみる http://downloads.vagrantup.com/tags/v1.1.2 から vagrant_x86_64.deb （手元のPCが Linux Mint なので）をダウンロードしてインストール。\n$ sudo dpkg -i vagrant_x86_64.deb /opt/vagrant にインストールされる。 1.0.x では gem でインストールするという方法もあったけど 1.1.x ではもう gem は提供されないとのこと。\n Gem Install? Vagrant 1.0.x had the option to be installed as a RubyGem. This installation method has been removed for installers and packages only.\n $ vagrant init centos6 \\ http://developer.nrel.gov/downloads/vagrant-boxes/CentOS-6.4-x86_64-v20130309.box A `Vagrantfile` has been placed in this directory. You are now ready to `vagrant up` your first virtual environment! Please read the comments in the Vagrantfile as well as documentation on `vagrantup.com` for more information on using Vagrant.  Vagrantfile がこのディレクトリに作成されました。vagrant up コマンドで最初の仮想環境を立ち上げる準備ができました。Vagrantfile のコメント行を読んでね。Vagrant についてもっと知りたかったら vagrantup.com にアクセスしてね。\n とのことなので\n$ vagrant up Bringing machine 'default' up with 'virtualbox' provider... [default] Box 'centos6' was not found. Fetching box from specified URL for the provider 'virtualbox'. Note that if the URL does not have a box for this provider, you should interrupt Vagrant now and add the box yourself. Otherwise Vagrant will attempt to download the full box prior to discovering this error. Downloading with Vagrant::Downloaders::HTTP... Downloading box: http://developer.nrel.gov/downloads/vagrant-boxes/CentOS-6.4-x86_64-v20130309.box Progress: 27% (134439529 / 491722240) Extracting box... Cleaning up downloaded box... Successfully added box 'centos6' with provider 'virtualbox'! [default] Importing base box 'centos6'... [default] No guest additions were detected on the base box for this VM! Guest additions are required for forwarded ports, shared folders, host only networking, and more. If SSH fails on this machine, please install the guest additions and repackage the box to continue. This is not an error message; everything may continue to work properly, in which case you may ignore this message. [default] Matching MAC address for NAT networking... [default] Setting the name of the VM... [default] Clearing any previously set forwarded ports... [default] Creating shared folders metadata... [default] Clearing any previously set network interfaces... [default] Preparing network interfaces based on configuration... [default] Forwarding ports... [default] -- 22 =\u0026gt; 2222 (adapter 1) [default] Booting VM... [default] Waiting for VM to boot. This can take a few minutes. [default] VM booted and ready for use! [default] Configuring and enabling network interfaces... [default] Mounting shared folders... [default] -- /vagrant 起動したので ssh でログイン\n$ vagrant ssh Welcome to your Vagrant-built virtual machine. [vagrant@localhost ~]$ ps で見てみるとこんなコマンドで ssh してました。なるほど。\nssh vagrant@127.0.0.1 \\ -p 2222 \\ -o LogLevel=FATAL \\ -o StrictHostKeyChecking=no \\ -o UserKnownHostsFile=/dev/null \\ -o IdentitiesOnly=yes \\ -i /home/ytera/.vagrant.d/insecure_private_key Bridgeインターフェースを使う # Vagrantfile を編集して\nconfig.vm.network :public_network をアンコメントして起動すると\n[default] Available bridged network interfaces: 1) wlan0 2) eth0 What interface should the network bridge to? と、どのインターフェースのBridgeとするかの選択肢が出る。 そして NAT のインターフェース(eth0)に加え、Bridge のインターフェース(eth1)が作成される。 （2013-03-23追記） 毎回どのインターフェースを使うか尋ねられると困るので Vagrantfile で指定する\nconfig.vm.network :public_network, :bridge =\u0026gt; \u0026#34;wlan0\u0026#34; ゲストOS起動時の出力を見たい # Vagrantfile の\nconfig.vm.provider :virtualbox do |vb| vb.gui = true end をアンコメントして起動させるとVirtualBoxの仮想ターミナルが起動する。\n CentOS のこの画面だと起動時のメッセージは見れないので F1 を押して表示させる。 この他に Vagrantfile ではメモリサイズの変更やホスト側のディレクトリをゲスト側でマウントする設定や Port Forwarding の追加、Puppet や Chef の設定が可能なようです。というか、Puppet や Chef 使うのが目的ですかね。\n","date":"2013年3月22日","permalink":"/2013/03/vagrant-1/","section":"Posts","summary":"流行りの Vagrant （ベイグラント）を試してみる http://downloads.vagrantup.com/tags/v1.1.2 から vagrant_x86_64.deb （手元のPCが Linux Mint なので）をダウンロードしてインストール。 $ sudo dpkg -i vagrant_x86_64.deb /opt/vagrant にインストールされる。 1.0.x","title":"Vagrant メモ (1)"},{"content":"雪で延期となっていた Riak Meetup Tokyo に参加してきました。 @kuenishi さんから Riak の概要と RICON 2012 (Riakに限らず分散ストレージ関連のカンファレンス、Basho主催なのでRiakの話題は多い) の紹介がありました。\nみんな MonboDB、CouchDB、MySQL から Riak に移行してますよと。後で動画や資料を見てみよう。\nRiak については「7つのデータベース7つの世界」の3章を読むのが良いと思います。@kuenishi さんもそう言われてましたし、私も今日読みました。日本語での一番詳しいドキュメントじゃないかなと。\n株式会社IDCフロンティアさんからは CloudStack Collaboration Conference 2012 の紹介がありました、CloudStack は ASF に移されて主流になりつつあるらしい。\nそして Riak を利用した AWS S3 と高い互換性をもつ分散ストレージの紹介、福島の白河と北九州とでの分散ということなので災害対策として悪くないんじゃないでしょうか。\nEngineYard さんのサービスを使えば、ブラウザからポチポチすれば簡単に Riak クラスターが作成できますよと。10台でも20台でもポチッとなと。EngineYard では EC2 で Gentoo を使っているそうです。自由度が高いから boot 時間を短くしたりできるんだと。EC2の起動って結構待たされるんですよねぇと。\nEngineYardパンダをもらいました。ありがとうございます。\nEngineYardパンダもらった＼(^o^)／ #riakjp pic.twitter.com/DMUEt6Jzi0\n\u0026mdash; yteraoka (@yteraoka) March 12, 2013  Beerup にも参加させていただきました。ごちそうさまでした。\n次はシリアスなドロドロしたやつを。\n","date":"2013年3月12日","permalink":"/2013/03/riak-meetup/","section":"Posts","summary":"雪で延期となっていた Riak Meetup Tokyo に参加してきました。 @kuenishi さんから Riak の概要と RICON 2012 (Riakに限らず分散ストレージ関連のカンファレンス、Basho主催な","title":"Riak Meetup に参加してきた"},{"content":"","date":"2013年3月12日","permalink":"/tags/%E5%8B%89%E5%BC%B7%E4%BC%9A/","section":"Tags","summary":"","title":"勉強会"},{"content":"","date":"2013年3月11日","permalink":"/tags/rails/","section":"Tags","summary":"","title":"Rails"},{"content":"Rails 3 アプリを各Webサーバー (Thin, Unicorn, Passenger) で Sub-URI で使う方法 （Qiita に書いたけど反応なくてさみしいので転載）\nThin # thin --prefix /subdir start とすると /subdir で動作はするが css や js のリンクが変わらない prefix 指定では RAILS_RELATIVE_URL_ROOT はセットされないので、起動時に環境変数としてセットし、config.ru で\nmap ActionController::Base.config.relative\\_url\\_root || \u0026#34;/\u0026#34; do run Test::Application end （ENV['RAILS_RELATIVE_URL_ROOT'] の値が ActionController::Base.config.relative_url_root にセットされます。） とすることで css や js のリンク先が変わる、ただし、prefix と併用するとアプリは /subdir/subidr/ での動作となり、css や js のリンク先は /subdir/ になってしまう。 期待通りに動作させるためには prefix 指定無しで RAILS_RELATIVE_URL_ROOT をセットし、config.ru で RAILS_RELATIVE_URL_ROOT を扱う必要がある。 thin --prefix /subdir と\nconfig.assets.prefix = \u0026#39;/subdir/assets\u0026#39; の組み合わせでも動作した。RAILS_RELATIVE_URL_ROOT は使わない。\nUnicorn # unicorn の場合は --path /subdir を指定して起動させると RAILS_RELATIVE_URL_ROOT がセットされるので config.ru で RAILS_RELATIVE_URL_ROOT を使ってやると動作する\nPassenger (Rack) # DocumentRoot にアプリの public ディレクトリを subdir という名前でシンボリックリンクをはり、\nRackBaseURI /subdir とすることで\nRAILS_RELATIVE_URL_ROOT=/subdir RACK_BASE_URI=/subdir がアプリに渡される。config.ru で RAILS_RELATIVE_URL_ROOT を扱うと Thin で両方指定した場合と同じように /subdir/subdir/ となってしまうため、config.ru で RAILS_RELATIVE_URL_ROOT を全く扱わないか RACK_BASE_URI に何かセットされていたら RAILS_RELATIVE_URL_ROOT を無視するという設定にする必要がある。\nif ! ENV[\u0026#39;RACK_BASE_URI\u0026#39;] \u0026amp;\u0026amp; ENV[\u0026#39;RAILS_RELATIVE_URL_ROOT\u0026#39;] map ENV[\u0026#39;RAILS_RELATIVE_URL_ROOT\u0026#39;] || \u0026#34;/\u0026#34; do run Test::Application end else run Test::Application end みたいな。\nPassenger (非Rack) # RackBaseURI の代わりに RailsBaseURI を使う\nRailsBaseURI /subdir こうすると config.ru は使われなくなり、RAILS_RELATIVE_URL_ROOT だけがセットされる。そして、これで期待の動作をする。\nまとめ # 実は Ruby ドシロウトです。 Rails (Rack) ってこういうものなんでしょうか？ わざわざ --prefix とか --path なんてオプションがあるのにそれだけではうまくいかない。アプリの書き方で対応可能？ どの Web サーバーでも書き換えたりしないで動かせればいいのにと思いました。 ツッコミお待ちしております。\n","date":"2013年3月11日","permalink":"/2013/03/rails-sub-uri/","section":"Posts","summary":"Rails 3 アプリを各Webサーバー (Thin, Unicorn, Passenger) で Sub-URI で使う方法 （Qiita に書いたけど反応なくてさみしいので転載） Thin # thin --prefix /subdir start とすると /subdir で動作はするが css や","title":"RailsアプリをSub-URIで動かす"},{"content":"3月8日(金) Monitoring Casual Talk #3 に参加してきました。 参加者全員が発表者というイベントです。ビールを飲みながらのゆるいイベントです。 今回は paperboy\u0026amp;co. さんの主催でした。会場の準備、二次会の準備などありがとうございました。 GMOグループの Friday Night Party がうらやましかった。きれいなお姉さんがお酒を配ってくれるなんて。 私の発表は以前このBlogにも書いた Apache で Response Header を消しつつその値をログに書き出す について。\nアプリからの情報を秘密裏にApacheのログに書き出す方法 from Yoshinori Teraoka\nでも、@kazeburo さんから次のようなツッコミがっ！！\n それ mod_copy_headers #monitoringcasual\n— masahiro naganoさん (@kazeburo) 2013年3月8日\n  s/headers/header/\n— masahiro naganoさん (@kazeburo) 2013年3月8日\n ん? mod_copy_header? ぐぐったら出てきました。 mod_copy_header ってのを書いた話 Re: Apache上のPerl FastCGIはCustomLogにデータを書くことができるか？ おぉ、こんなものが。これで良いですね。私もひとつのモジュールとして独立化させようかと思いましたが面倒なので諦めてました。 今回のモニカジの中心的な話題はサーバー台数が増えると監視の設定が大変なのでそれをどうやってツールで自動化するかというものでした。 モニカジ参加者のほとんどやTwitterでフォローしてる人とかが年下とわかり少しショックを受けたりもしましたが、また次回もよろしくお願いします。京都へ行きたいところですがちょっと無理ですね。\n","date":"2013年3月11日","permalink":"/2013/03/monitoring-casual-3/","section":"Posts","summary":"3月8日(金) Monitoring Casual Talk #3 に参加してきました。 参加者全員が発表者というイベントです。ビールを飲みながらのゆるいイベントです。 今回は paperboy\u0026amp;co. さんの主催で","title":"モニカジ#3に参加してきた"},{"content":"The source :rubygems is deprecated because HTTP requests are insecure. Please change your source to 'https://rubygems.org' if possible, or 'http://rubygems.org' if not. って表示されたら Gemfile の\nsource :rubygems を\nsource 'https://rubygems.org' に書き換えましょう。 以上\n","date":"2013年2月28日","permalink":"/2013/02/the-source-rubygems-is-deprecated/","section":"Posts","summary":"The source :rubygems is deprecated because HTTP requests are insecure. Please change your source to 'https://rubygems.org' if possible, or 'http://rubygems.org' if not. って表示されたら Gemfile の source :rubygems を source 'https://rubygems.org' に書き換えましょう。 以上","title":"The source :rubygems is deprecated"},{"content":"","date":"2013年2月17日","permalink":"/tags/dbi/","section":"Tags","summary":"","title":"dbi"},{"content":"fluentd で DB に書き込むために DBI を使って PostgreSQL でも MySQL にでも入れられるようにしてみました。PostgreSQL なら dbd-pg を、MySQL なら dbd-mysql が必要です。 https://github.com/yteraoka/fluent-plugin-dbi\n type dbi #dsn DBI:Pg:dbname:dbhost dsn DBI:Mysql:dbname:dbhost db_user username db_pass password keys host,time_m,method,uri,protocol,status query insert into access_log (host, time, method, uri, protocol, status) values (?, ?, ?, ?, ?, ?) Query は自動生成ではないので、任意の処理を実行できます。 keys のカンマ区切りの順に「?」のプレースホルダに入れます。 time_m っていうのは Apache 2.4 だと %{msec_frac}t という LogFormat マクロでミリ秒まで出せるので、次のように指定して DB にミリ秒精度で入れられます。\ntime_m:%{%Y-%m-%d %H:%M:%S}t.%{msec_frac}t %{usec_frac}t だとマイクロ秒でも出せますが、DB にマイクロ秒精度で入らないので msec で。 mod_log_config - Apache HTTP Server\nfluent-gem fluent-plugin-dbi で。\n","date":"2013年2月17日","permalink":"/2013/02/fluent-plugin-dbi/","section":"Posts","summary":"fluentd で DB に書き込むために DBI を使って PostgreSQL でも MySQL にでも入れられるようにしてみました。PostgreSQL なら dbd-pg を、MySQL なら dbd-mysql が必要です。 https://github.com/yteraoka/fluent-plugin-dbi type dbi","title":"fluent-plugin-dbi 書いた"},{"content":"Riak の bucket 設定とか、オブジェクトの管理ツールを Ruby on Rails の勉強がてら作ってみようかなぁと思ってたら\u0026hellip; ひとりでやるRiak Advent Calendar 2012 day1 - 入門 - kuenishi\u0026rsquo;s blog ん？！\n しかし私は軟弱なのでGUIを使う。riak_controlというイカしたWeb UIがあるのだ。\n なんですとっ！！標準添付されてた\u0026hellip; では早速試してみよう。 etc/app.config を書き換えて stop / start http と https を両方有効にしてどちらも 0.0.0.0 で listen させて、http から admin_gui にアクセスしようとしたら https://0.0.0.0:xxx/ というリンクになってたからここは 0.0.0.0 は使わないほうが良さそう。 そしてアクセスしてみた。かっちょいい！！       あれれ？？ Bucket の設定とか Object の操作するインターフェースがないなぁ。上西さんの blog の画像にはそんな機能が見えるのにな（グレーアウトしてる感じだけど） 1.2.1 だけどどうやってインストールしたんだっけなぁ Installing Riak from source package これか。git clone してるな。\n","date":"2013年2月13日","permalink":"/2013/02/riak-admin-gui/","section":"Posts","summary":"Riak の bucket 設定とか、オブジェクトの管理ツールを Ruby on Rails の勉強がてら作ってみようかなぁと思ってたら\u0026hellip; ひとりでやるRiak Advent Calendar 2012 day1 - 入門","title":"Riak Admin GUI"},{"content":"Ruby 素人が作ってみました。 https://github.com/yteraoka/fluent-plugin-tail-asis rubygems にも UP したった。\n fluent-agent-lite と in_tail in_tail_asis というのを書いた  あたりでやってたやつ。 ネーミングルールとか良くわからないけどまぁとりあえず動きましたよと。\nfluent-gem install fluent-plugin-tail-asis で\n type tail_asis asis_key message path /path/to/input_log_file pos_file /var/run/input_log_file.pos tag asis.test と指定して fluent-plugin-file-alternative に渡してやればそのままログの収集ができます。 正規表現を使わない分、素の tail より軽いはずじゃないかなと。\n","date":"2013年2月5日","permalink":"/2013/02/fluent-plugin-tail-asis/","section":"Posts","summary":"Ruby 素人が作ってみました。 https://github.com/yteraoka/fluent-plugin-tail-asis rubygems にも UP したった。 fluent-agent-lite と in_tail in_tail_asis というのを書いた あたりでやってたやつ。 ネーミングルールとか良くわからないけどまぁとりあえ","title":"fluent-plugin-tail-asis できた"},{"content":"logrotateがプロセスにHUP送る理由を調べてみた - カイワレの大冒険 へのコメントです。\n   HUPは設定ファイルを読み直すというような単純なものじゃない   rsyslog についての話かな、だとすると最新の rsyslog は SIGHUP では設定を読み直さないらしい。だからログファイルを開き直すだけかな。rsyslog や syslog-ng なら設定次第で書きだすファイル名に日付を入れられたりするけど。rsyslog に限定しない話だと、signal を受けた時の処理は SIGKILL 以外は自由に書けます。通常、Ctrl-C で SIGINT が送られ、process は中断されますが中断で不整合が起きないように綺麗に終了させたりします。\n   プロセスが生きている限りは、開いたままのファイルをmvしたりしても、それはmvした先に書かれる。要はファイル名なんて関係ない   ファイルシステムが違うところ(別パーティション)へ mv してしまうと、それは mv 先には書かれないので注意。開いた時の inode に書き続けるということですね。誤って削除してしまったファイルもどれかの process が開いていたら /proc/{PID}/fd/ の中から読み出せます。\n   ログローテートが走ったときにsyslogとかrsyslogにHUP送るけど、これはプロセスを殺さず、ログファイルのinodeを更新するため。 inode更新されるので、ファイルを読みなおしたのと同じ挙動となる   なんか表現に違和感がありますね。開いていたファイルを mv (rename) してなかったら同じ inode のファイルに追記しますよ。ファイルを読み直す？\n  そのうち、open(2)させたのちに無限ループさせたあたりの挙動追ってみると面白いのかもなぁと思ったり、誰か書いてくれると期待したり。\n これはどういうことだろう？\n ここからは追加の話 SIGHUP でログファイルを開き直してくれないプログラムの場合は logrotate で copytruncate を指定する必要があります。これはそれまでのファイルの中身を別ファイルにコピーした後に、ファイルサイズを0にします、inode は変えられません。でもログファイルが大きいとこれはかなりの負荷がかかります。rename だと負荷はほとんどないのに。そんな場合は pipe で cronolog とか rotatelogs をかましてあげるのが良いです。copytruncate でハマるのはログファイルが追記モードで開かれていない場合、java の \u0026ldquo;-Xloggc\u0026rdquo; とか。truncate 後も元のファイルの位置から書き続けます\u0026hellip; truncate ついでに書いておくと vi での編集・上書きや cp コマンドでの上書きは truncate して書きなおすので誰かが読込中だったりすると思わぬエラーが発生するかもしれません。shell script は読みながら順番に実行するため影響を受けやすいです。これを回避するためにはコピーして新しいファイルを編集した後に mv で戻すなどの対応が必要です。rsync コマンドは一時ファイルを作ってそれを rename してくれます。svn update もそうですね。 昔、symantec の virus 定義ファイルのダウンロード元が truncate して上書きしていたみたいでしょっちゅう中途半端なファイルをダウンロードさせられてました。\n","date":"2013年2月4日","permalink":"/2013/02/log-rotation-%E3%81%BE%E3%82%8F%E3%82%8A%E3%81%AE%E8%A9%B1/","section":"Posts","summary":"logrotateがプロセスにHUP送る理由を調べてみた - カイワレの大冒険 へのコメントです。 HUPは設定ファイルを読み直すというような単純な","title":"log rotation まわりの話"},{"content":"【試してみた】Amazon Route 53にドメインを移動してみた。 | Pocketstudio.jp log3 を見て、お、私も試してみようと。 簡単すぎて上記のブログ以上に書くことがない\u0026hellip; 個人のドメインはお名前.comのサービス使ってたし、めったにいじることもないから運用面で特に変わるところはないんだけど、費用がどの程度かかるのかは様子見かな。 さて、お仕事のドメインを Route 53 に移すかどうかだな。 BIND とかの運用なくせるなら良いけど内部用 DNS サーバーは必要だし、AWS のサービスとて 100% 信頼できるわけじゃないからなぁ。 でもこれらが全部落ちることはないから大丈夫か。\n ns-1464.awsdns-55.org. ns-1541.awsdns-00.co.uk. ns-315.awsdns-39.com. ns-754.awsdns-30.net.  ついでに AWS 用アカウントを Google Authenticator で2要素認証にしてみた。\n","date":"2013年2月3日","permalink":"/2013/02/route-53-%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%9F/","section":"Posts","summary":"【試してみた】Amazon Route 53にドメインを移動してみた。 | Pocketstudio.jp log3 を見て、お、私も試してみようと。 簡単すぎて上記のブログ以上に書くことがない\u0026","title":"Route 53 を使ってみた"},{"content":"先日、「fluent-agent-lite と in_tail」というエントリーで fluentd 本体への patch を書いて Pull Request までしたと書いたのだが、Ruby 全然わかんないし、数行足すだけだからそれが楽でそうしたのだが、plugin でも簡単に書けそうだと気づいたので別ファイルとして書いてみた。in_tail.rb と同じディレクトリに置いたら動いたけど正しくはどうすれば良いのだろうか？テストの書き方とか gem にする方法は\u0026hellip;先は長いな。\n","date":"2013年2月2日","permalink":"/2013/02/in_tail_asis-%E3%81%A8%E3%81%84%E3%81%86%E3%81%AE%E3%82%92%E6%9B%B8%E3%81%84%E3%81%9F/","section":"Posts","summary":"先日、「fluent-agent-lite と in_tail」というエントリーで fluentd 本体への patch を書いて Pull Request までしたと書いたのだが、Ruby 全然わ","title":"in_tail_asis というのを書いた"},{"content":"Apache のログにアプリから返された Response Header の情報を書きたいが、クライアントには送りたくないというものがあった場合、mod_headers で unset してしまうと %{HeaderName}o では書き出せなくなってしまいます。そこで、Apache 2.4 むけに patch を書きました。 ログにユーザーを特定する内部IDを書き出したいがクライアントには返したくないという場合に便利ではないでしょうか。\nHeader toenv {HeaderName} {EnvName} で、HeaderName というヘッダーの値を subprocess_env の EnvName にセットし、ヘッダーを消します。これで %{EnvName}e でログに書き出せます。\ndiff -uNr httpd-2.4.3.orig/modules/metadata/mod_headers.c httpd-2.4.3/modules/metadata/mod_headers.c --- httpd-2.4.3.orig/modules/metadata/mod_headers.c\t2011-12-05 09:08:01.000000000 +0900 +++ httpd-2.4.3/modules/metadata/mod_headers.c\t2013-02-01 16:25:33.279766574 +0900 @@ -96,7 +96,8 @@  hdr_unset = \u0026#39;u\u0026#39;, /* unset header */ hdr_echo = \u0026#39;e\u0026#39;, /* echo headers from request to response */ hdr_edit = \u0026#39;r\u0026#39;, /* change value by regexp, match once */ - hdr_edit_r = \u0026#39;R\u0026#39; /* change value by regexp, everymatch */ + hdr_edit_r = \u0026#39;R\u0026#39;, /* change value by regexp, everymatch */ + hdr_toenv = \u0026#39;E\u0026#39; /* copy to subprocess_env and unset */  } hdr_actions; /* @@ -417,6 +418,8 @@  new-\u0026gt;action = hdr_merge; else if (!strcasecmp(action, \u0026#34;unset\u0026#34;)) new-\u0026gt;action = hdr_unset; + else if (!strcasecmp(action, \u0026#34;toenv\u0026#34;)) + new-\u0026gt;action = hdr_toenv;  else if (!strcasecmp(action, \u0026#34;echo\u0026#34;)) new-\u0026gt;action = hdr_echo; else if (!strcasecmp(action, \u0026#34;edit\u0026#34;)) @@ -425,7 +428,7 @@  new-\u0026gt;action = hdr_edit_r; else return \u0026#34;first argument must be \u0026#39;add\u0026#39;, \u0026#39;set\u0026#39;, \u0026#39;append\u0026#39;, \u0026#39;merge\u0026#39;, \u0026#34; - \u0026#34;\u0026#39;unset\u0026#39;, \u0026#39;echo\u0026#39;, \u0026#39;edit\u0026#39;, or \u0026#39;edit*\u0026#39;.\u0026#34;; + \u0026#34;\u0026#39;unset\u0026#39;, \u0026#39;toenv\u0026#39;, \u0026#39;echo\u0026#39;, \u0026#39;edit\u0026#39;, or \u0026#39;edit*\u0026#39;.\u0026#34;;  if (new-\u0026gt;action == hdr_edit || new-\u0026gt;action == hdr_edit_r) { if (subs == NULL) { @@ -736,6 +739,11 @@  case hdr_unset: apr_table_unset(headers, hdr-\u0026gt;header); break; + case hdr_toenv: + apr_table_add(r-\u0026gt;subprocess_env, process_tags(hdr, r), + apr_table_get(headers, hdr-\u0026gt;header)); + apr_table_unset(headers, hdr-\u0026gt;header); + break;  case hdr_echo: v.r = r; v.hdr = hdr; 誰かの役に立つかな？\n","date":"2013年2月1日","permalink":"/2013/02/mod_headers-toenv/","section":"Posts","summary":"Apache のログにアプリから返された Response Header の情報を書きたいが、クライアントには送りたくないというものがあった場合、mod_headers で unset してしまう","title":"Apache で Response Header を消しつつその値をログに書き出す"},{"content":"","date":"2013年2月1日","permalink":"/tags/libc/","section":"Tags","summary":"","title":"libc"},{"content":"libc の buffer と perl の buffer は違うんだよという話を聞いたのでちょい調べてみた。 libc の buffer は grep を pipe で複数つなぐとなかなか表示されないやつ\n$ while : ; do echo hoge; sleep 1; done | grep hoge ↑これはすぐに hoge が表示されますが\n$ while : ; do echo hoge; sleep 1; done | grep hoge | grep hoge ↑こっちはずーっと待ってないと出力されません。出力先が端末でない場合、fwrite とかは buffering されるんですね。すぐに出力したい場合は次のように\n$ while : ; do echo hoge; sleep 1; done | grep --line-buffered hoge | grep hoge \u0026ldquo;\u0026ndash;line-buffered\u0026rdquo; オプションをつけることで、毎行 fflush() が実行されてすぐさま出力されます。もうひとつ\n$ while : ; do echo hoge; sleep 1; done | stdbuf -o0 grep hoge | grep hoge と、stdbuf で LD_PRELOAD を使って buffer をコントロールするという方法もあるようです。(How to fix stdio buffering) で、Perl もこんな仕組みで buffering されてるんだろうと思ってたら違うんですね。 7.12. Flushing Output\n$ perl -e 'while (1) { print \u0026quot;hoge\\n\u0026quot;; sleep 1; }' | grep hoge $ stdbuf -o0 perl -e 'while (1) { print \u0026quot;hoge\\n\u0026quot;; sleep 1; }' | grep hoge このどちらもすぐには出力されません。 次のようにするしかないようです。(IO::Handle 使っても良い)\n$ perl -e '$|=1; while (1) { print \u0026quot;hoge\\n\u0026quot;; sleep 1; }' | grep hoge なるほどねぇ。\nawk には fflush() っていう関数があって、sed には \u0026ndash;unbuffered というオプションがあるらしい。\ngrep, awk, sed でバッファしない方法\n","date":"2013年2月1日","permalink":"/2013/02/libc-buffer-perl-buffer/","section":"Posts","summary":"libc の buffer と perl の buffer は違うんだよという話を聞いたのでちょい調べてみた。 libc の buffer は grep を pipe で複数つなぐとなかなか表示されないやつ $ while : ; do echo hoge; sleep 1; done | grep","title":"libc の buffer と perl の buffer"},{"content":"ログの収集にそろそろ fluentd を使おうかなと思って、fluent-agent-lite だとフォーマットを変えずに集められそうだという事で見てみたのだが、tail -F で読み出してるだけだから再起動すると停止中のログを取りこぼすとか、ログがあまり出ない場合は最後の10行が再度送られるとか、そういうのに寛容なデータだったら良いのだけどそうではない場合ちょっとつらいかなと思った。in_tail.rb はどこまで処理したかを position ファイルに保存できるので続きから始められるとか、そうでなくてもファイルの末尾から読み出されるとか気を使ってあるので出来ればこっちかなと。ただし、次のように正規表現で全行 parse するってのは確かに辛いかな。\n みんな大好きfluentdはたいへん便利ですが、ログの収集＆集約だけをしたい、というときにちょっとオーバースペック気味のところがあります。特に in_tail はログの読み込みと同時に parse をする仕組みになっており、まあログが書かれた場所ならparseのルールもわかってるでしょ、というところは合理的なものでもあるのですが、loadavgが高いサーバでそういうことをするのは正直にいってなかなか厳しいです。 #fluentd 用ログ収集専用のエージェント fluent-agent-lite 書いた\n ということで\ntype tail format /^(?.*)$/ がどれくらいの負荷なのかを次のコードで試してみた\nrequire \u0026#39;benchmark\u0026#39; lines = [] f = open(ARGV[0]) while line = f.gets lines.push line.chomp end f.close regexp = Regexp.new(\u0026#39;^(?.*)$\u0026#39;) n = 1000 Benchmark.bm do |x| x.report(\u0026#34;asis: \u0026#34;) { (1..n).each { lines.each { |line| val = line } } } x.report(\u0026#34;regexp:\u0026#34;) { (1..n).each { lines.each { |line| m = regexp.match(line) val = m[\u0026#39;message\u0026#39;] } } } end 1,000行のログを1,000回ループさせた場合\n user system total real asis: 0.080000 0.000000 0.080000 ( 0.081076) regexp: 7.590000 0.000000 7.590000 ( 7.592521) という結果、正規表現使わずにただ読んだ行をそのまま変数に入れてあげれば結構負荷を抑えられそうです。そこで parse.rb に patch を当ててみたのです。\n--- parser.rb.orig\t2012-12-07 09:13:42.000000000 +0900 +++ parser.rb\t2013-02-01 16:48:47.435270316 +0900 @@ -147,6 +147,18 @@  end end + class AsisParser + include Configurable + + config_param :key, :string, :default =\u0026gt; \u0026#34;message\u0026#34; + + def call(text) + record = {} + record[@key] = text + return Engine.now, record + end + end +  class ApacheParser include Configurable @@ -205,6 +217,7 @@  \u0026#39;json\u0026#39; =\u0026gt; Proc.new { JSONParser.new }, \u0026#39;tsv\u0026#39; =\u0026gt; Proc.new { TSVParser.new }, \u0026#39;csv\u0026#39; =\u0026gt; Proc.new { CSVParser.new }, + \u0026#39;asis\u0026#39; =\u0026gt; Proc.new { AsisParser.new },  \u0026#39;nginx\u0026#39; =\u0026gt; Proc.new { RegexpParser.new(/^(?[^ ]*) (?[^ ]*) (?[^ ]*) \\[(?[^\\]]*)\\] \u0026#34;(?\\S+)(?: +(?[^ ]*) +\\S*)?\u0026#34; (?`[^ ]*) (?[^ ]*)(?: \u0026#34;(?[^\u0026#34;]*)\u0026#34; \u0026#34;(?[^\u0026#34;]*)\u0026#34;)?$/, {\u0026#39;time_format\u0026#39;=\u0026gt;\u0026#34;%d/%b/%Y:%H:%M:%S %z\u0026#34;}) }, }` これで\ntype tail format asis key message として fluent-plugin-file-alternative で保存してやれば元ファイルのフォーマットのままログの収集ができます。 初 PullRequest してみた。\u0026lsquo;message\u0026rsquo; 固定のところは keys で指定できるようにしたほうが良かったかな。 2013-02-01 更新 key で json の key を指定できるようにしてみました。\n","date":"2013年1月31日","permalink":"/2013/01/fluent-agent-lite-%E3%81%A8-in_tail/","section":"Posts","summary":"ログの収集にそろそろ fluentd を使おうかなと思って、fluent-agent-lite だとフォーマットを変えずに集められそうだという事で見てみたのだ","title":"fluent-agent-lite と in_tail"},{"content":"前回 (Installing Riak from source package) Riak を source からインストールして 3 node の cluster をセットアップしたので、これを使って cluster の操作をテストしてみる。 まず、cluster 操作は node 追加と削除などを同時に行えるように riak-admin で join や leave を実行した後に commit コマンドで反映させる仕様となっている。 まずは、3台で cluster を組んでいる状態\n$ rel/riak1/bin/riak-admin member-status ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 34.4% -- 'riak1@127.0.0.1' valid 32.8% -- 'riak2@127.0.0.1' valid 32.8% -- 'riak3@127.0.0.1' ------------------------------------------------------------------------------- Valid:3 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 ここに node4 を追加してみる\n$ rel/riak4/bin/riak start $ rel/riak4/bin/riak-admin cluster join riak1@127.0.0.1 Success: staged join request for 'riak4@127.0.0.1' to 'riak1@127.0.0.1' $ rel/riak4/bin/riak-admin cluster plan =============================== Staged Changes ================================ Action Nodes(s) ------------------------------------------------------------------------------- join 'riak4@127.0.0.1' ------------------------------------------------------------------------------- NOTE: Applying these changes will result in 1 cluster transition ############################################################################### After cluster transition 1/1 ############################################################################### ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 34.4% 25.0% 'riak1@127.0.0.1' valid 32.8% 25.0% 'riak2@127.0.0.1' valid 32.8% 25.0% 'riak3@127.0.0.1' valid 0.0% 25.0% 'riak4@127.0.0.1' ------------------------------------------------------------------------------- Valid:4 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 Transfers resulting from cluster changes: 47 5 transfers from 'riak2@127.0.0.1' to 'riak3@127.0.0.1' 10 transfers from 'riak3@127.0.0.1' to 'riak1@127.0.0.1' 16 transfers from 'riak2@127.0.0.1' to 'riak4@127.0.0.1' 16 transfers from 'riak1@127.0.0.1' to 'riak2@127.0.0.1' $ rel/riak4/bin/riak-admin cluster commit Cluster changes committed 追加されて、既存データのリバランスが行われる\n$ rel/riak4/bin/riak-admin member-status ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 32.8% 25.0% 'riak1@127.0.0.1' valid 34.4% 25.0% 'riak2@127.0.0.1' valid 28.1% 25.0% 'riak3@127.0.0.1' valid 4.7% 25.0% 'riak4@127.0.0.1' ------------------------------------------------------------------------------- Valid:4 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 $ rel/riak4/bin/riak-admin transfers 'riak4@127.0.0.1' waiting to handoff 59 partitions 'riak3@127.0.0.1' waiting to handoff 3 partitions 'riak2@127.0.0.1' waiting to handoff 8 partitions 'riak1@127.0.0.1' waiting to handoff 6 partitions Active Transfers: $ rel/riak4/bin/riak-admin member-status ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 32.8% 25.0% 'riak1@127.0.0.1' valid 32.8% 25.0% 'riak2@127.0.0.1' valid 23.4% 25.0% 'riak3@127.0.0.1' valid 10.9% 25.0% 'riak4@127.0.0.1' ------------------------------------------------------------------------------- Valid:4 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 リバランス (handoff) 完了\n$ rel/riak4/bin/riak-admin member-status ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 25.0% -- 'riak1@127.0.0.1' valid 25.0% -- 'riak2@127.0.0.1' valid 25.0% -- 'riak3@127.0.0.1' valid 25.0% -- 'riak4@127.0.0.1' ------------------------------------------------------------------------------- Valid:4 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 さらに追加して5 nodeにしてみる (20% x5 とはならなかった)\n$ rel/riak5/bin/riak-admin member-status ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 18.8% -- 'riak1@127.0.0.1' valid 18.8% -- 'riak2@127.0.0.1' valid 18.8% -- 'riak3@127.0.0.1' valid 25.0% -- 'riak4@127.0.0.1' valid 18.8% -- 'riak5@127.0.0.1' ------------------------------------------------------------------------------- Valid:5 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 そしてデータを足してみた\n$ du -sh rel/riak?/data/bitcask 129M\trel/riak1/data/bitcask 121M\trel/riak2/data/bitcask 126M\trel/riak3/data/bitcask 168M\trel/riak4/data/bitcask 131M\trel/riak5/data/bitcask node5 を SIGKILL で停止させてみる\n$ rel/riak1/bin/riak-admin ring-status ================================== Claimant =================================== Claimant: 'riak2@127.0.0.1' Status: up Ring Ready: true ============================== Ownership Handoff ============================== No pending changes. ============================== Unreachable Nodes ============================== The following nodes are unreachable: ['riak5@127.0.0.1'] WARNING: The cluster state will not converge until all nodes are up. Once the above nodes come back online, convergence will continue. If the outages are long-term or permanent, you can either mark the nodes as down (riak-admin down NODE) or forcibly remove the nodes from the cluster (riak-admin force-remove NODE) to allow the remaining nodes to settle. この状態で先ほど登録したデータは全て無事に取得できました、 ではもう一台強制停止してみよう\n$ rel/riak1/bin/riak-admin ring-status ================================== Claimant =================================== Claimant: 'riak2@127.0.0.1' Status: up Ring Ready: true ============================== Ownership Handoff ============================== No pending changes. ============================== Unreachable Nodes ============================== The following nodes are unreachable: ['riak4@127.0.0.1','riak5@127.0.0.1'] WARNING: The cluster state will not converge until all nodes are up. Once the above nodes come back online, convergence will continue. If the outages are long-term or permanent, you can either mark the nodes as down (riak-admin down NODE) or forcibly remove the nodes from the cluster (riak-admin force-remove NODE) to allow the remaining nodes to settle. 2 node 止まってしまうと取得できなくなってしまったオブジェクトがでてきました\nでも bucket の設定で v_val は 3 ですから 3 つのコピーがあるはずなので 2 台が停止してしまっても取得できることが期待されます。そこで r = quorum を r = 1 にしてしまいましょう。 r の default は quorum でこれは n_val/2+1 です。n_val の過半数が正常でないとオブジェクトが取得できません。(HTTP Bucket Properties)\n{ props: { name: \u0026#34;test\u0026#34;, allow_mult: false, basic_quorum: false, big_vclock: 50, chash_keyfun: { mod: \u0026#34;riak_core_util\u0026#34;, fun: \u0026#34;chash_std_keyfun\u0026#34; }, dw: \u0026#34;quorum\u0026#34;, last_write_wins: false, linkfun: { mod: \u0026#34;riak_kv_wm_link_walker\u0026#34;, fun: \u0026#34;mapreduce_linkfun\u0026#34; }, n_val: 3, notfound_ok: true, old_vclock: 86400, postcommit: [ ], pr: 0, precommit: [ ], pw: 0, r: \u0026#34;quorum\u0026#34;, rw: \u0026#34;quorum\u0026#34;, small_vclock: 50, w: \u0026#34;quorum\u0026#34;, young_vclock: 20 } } $ curl -X PUT \\ \u0026gt; -H \u0026quot;Content-Type: application/json\u0026quot; \\ \u0026gt; -d '{\u0026quot;props\u0026quot;:{ \u0026quot;r\u0026quot;: 1}}' \\ \u0026gt; http://localhost:8298/buckets/test/props 無事に全てのオブジェクトを取得できました。やったね！ 死んでしまった node4, node5 をきちんと消してしまいましょう。\n$ rel/riak1/bin/riak-admin cluster force-remove riak5@127.0.0.1 Success: staged remove request for 'riak5@127.0.0.1' $ rel/riak1/bin/riak-admin cluster force-remove riak4@127.0.0.1 Success: staged remove request for 'riak4@127.0.0.1' $ rel/riak1/bin/riak-admin cluster plan Cannot plan until cluster state has converged. Check 'Ring Ready' in 'riak-admin ring_status' おっと、みんな集まらないと plan は実行できないよと。node4, node5 はもういないから集まれないんだよと教えてあげる必要があるみたいです。\n$ rel/riak1/bin/riak-admin down riak5@127.0.0.1 Success: \u0026quot;riak5@127.0.0.1\u0026quot; marked as down $ rel/riak1/bin/riak-admin down riak4@127.0.0.1 Success: \u0026quot;riak4@127.0.0.1\u0026quot; marked as down $ rel/riak1/bin/riak-admin cluster plan =============================== Staged Changes ================================ Action Nodes(s) ------------------------------------------------------------------------------- force-remove 'riak4@127.0.0.1' force-remove 'riak5@127.0.0.1' ------------------------------------------------------------------------------- WARNING: All of 'riak4@127.0.0.1' replicas will be lost WARNING: All of 'riak5@127.0.0.1' replicas will be lost NOTE: Applying these changes will result in 1 cluster transition ############################################################################### After cluster transition 1/1 ############################################################################### ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 34.4% 34.4% 'riak1@127.0.0.1' valid 32.8% 32.8% 'riak2@127.0.0.1' valid 32.8% 32.8% 'riak3@127.0.0.1' ------------------------------------------------------------------------------- Valid:3 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 WARNING: Not all replicas will be on distinct nodes Partitions reassigned from cluster changes: 28 5 reassigned from 'riak4@127.0.0.1' to 'riak3@127.0.0.1' 4 reassigned from 'riak5@127.0.0.1' to 'riak3@127.0.0.1' 5 reassigned from 'riak4@127.0.0.1' to 'riak2@127.0.0.1' 4 reassigned from 'riak5@127.0.0.1' to 'riak2@127.0.0.1' 6 reassigned from 'riak4@127.0.0.1' to 'riak1@127.0.0.1' 4 reassigned from 'riak5@127.0.0.1' to 'riak1@127.0.0.1' Transfers resulting from cluster changes: 34 4 transfers from 'riak1@127.0.0.1' to 'riak3@127.0.0.1' 4 transfers from 'riak1@127.0.0.1' to 'riak2@127.0.0.1' 3 transfers from 'riak3@127.0.0.1' to 'riak2@127.0.0.1' 4 transfers from 'riak2@127.0.0.1' to 'riak1@127.0.0.1' 4 transfers from 'riak5@127.0.0.1' to 'riak3@127.0.0.1' 4 transfers from 'riak3@127.0.0.1' to 'riak1@127.0.0.1' 3 transfers from 'riak2@127.0.0.1' to 'riak3@127.0.0.1' 4 transfers from 'riak5@127.0.0.1' to 'riak2@127.0.0.1' 4 transfers from 'riak5@127.0.0.1' to 'riak1@127.0.0.1' $ rel/riak1/bin/riak-admin cluster commit Cluster changes committed $ rel/riak1/bin/riak-admin member-status ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 31.3% 34.4% 'riak1@127.0.0.1' valid 34.4% 32.8% 'riak2@127.0.0.1' valid 34.4% 32.8% 'riak3@127.0.0.1' ------------------------------------------------------------------------------- Valid:3 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 掃除終了\n$ rel/riak1/bin/riak-admin member-status ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 34.4% -- 'riak1@127.0.0.1' valid 32.8% -- 'riak2@127.0.0.1' valid 32.8% -- 'riak3@127.0.0.1' ------------------------------------------------------------------------------- Valid:3 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 r を quorum に戻して全てのオブジェクトが取得できるかテスト\n$ curl -X PUT \\ \u0026gt; -H \u0026quot;Content-Type: application/json\u0026quot; \\ \u0026gt; -d '{\u0026quot;props\u0026quot;:{ \u0026quot;r\u0026quot;: \u0026quot;quorum\u0026quot; }}' \\ \u0026gt; http://localhost:8298/buckets/test/props 無事全てのオブジェクトが取得できました。 node4, node5 としてはまだ cluster に参加しているつもりなので、再度起動してきたらどうなるのだろうか？と気になるところですが、きちんと起動時に確認して、もうメンバーでないことを認識して自ら自分の片付けをして shutdown します。\nでは、最後に通常の node 削除を試してみる\n$ rel/riak1/bin/riak-admin cluster leave riak3@127.0.0.1 Success: staged leave request for 'riak3@127.0.0.1' $ rel/riak1/bin/riak-admin cluster plan =============================== Staged Changes ================================ Action Nodes(s) ------------------------------------------------------------------------------- leave 'riak3@127.0.0.1' ------------------------------------------------------------------------------- NOTE: Applying these changes will result in 2 cluster transitions ############################################################################### After cluster transition 1/2 ############################################################################### ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- leaving 32.8% 0.0% 'riak3@127.0.0.1' valid 34.4% 50.0% 'riak1@127.0.0.1' valid 32.8% 50.0% 'riak2@127.0.0.1' ------------------------------------------------------------------------------- Valid:2 / Leaving:1 / Exiting:0 / Joining:0 / Down:0 WARNING: Not all replicas will be on distinct nodes Transfers resulting from cluster changes: 42 10 transfers from 'riak3@127.0.0.1' to 'riak2@127.0.0.1' 10 transfers from 'riak2@127.0.0.1' to 'riak1@127.0.0.1' 11 transfers from 'riak1@127.0.0.1' to 'riak2@127.0.0.1' 11 transfers from 'riak3@127.0.0.1' to 'riak1@127.0.0.1' ############################################################################### After cluster transition 2/2 ############################################################################### ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 50.0% -- 'riak1@127.0.0.1' valid 50.0% -- 'riak2@127.0.0.1' ------------------------------------------------------------------------------- Valid:2 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 WARNING: Not all replicas will be on distinct nodes $ rel/riak1/bin/riak-admin cluster commit Cluster changes committed データは node1, node2 に寄せられています。node3 は leave コマンドで node1,2 に handoff することで空っぽになってます。\n$ du -sh rel/riak?/data/bitcask 316M\trel/riak1/data/bitcask 279M\trel/riak2/data/bitcask 12K\trel/riak3/data/bitcask 198M\trel/riak4/data/bitcask 131M\trel/riak5/data/bitcask ざっと簡単なパターンのクラスターオペレーションを試してみました。簡単ですね。 Riak CS でないと認証機能がないのがちょっとつらい。認証つけた Proxy を前に置くのかなぁ。 次回はもっと深いところを調べてみよう。\n","date":"2013年1月28日","permalink":"/2013/01/riak-cluster/","section":"Posts","summary":"前回 (Installing Riak from source package) Riak を source からインストールして 3 node の cluster をセットアップしたので、これを使って cluster の操作をテストしてみる。 まず、cluster 操作は node 追","title":"Riak cluster を試してみる"},{"content":"","date":"2013年1月13日","permalink":"/tags/erlang/","section":"Tags","summary":"","title":"Erlang"},{"content":"運用が楽な KVS ということで Riak をテストしてみる 1サーバーに複数 node を起動させてクラスタのテストをする ( Running-Multiple-Nodes-on-One-Host ) ために source から入れてみる ( bin/riak は shell script でこいつを書き換えれば各種ディレクトリを指定できるので package で Riak を入れても1サーバーで複数起動させられそう )\nまずは Erlang のインストール # Installing Erlang を参考にインストールする 安易に最新版を使ってはいけない。Riak 1.2, 1.2.1 の場合は R15B01 をインストールする。 Ruby の rvm みたいな kerl を使う\n$ curl -o ~/bin/kerl https://raw.github.com/spawngrid/kerl/master/kerl $ chmod a+x ~/bin/kerl $ sudo yum install gcc glibc-devel make ncurses-devel openssl-devel autoconf $ kerl build R15B01 r15b01 Getting the available releases from erlang.org... Downloading otp_src_R15B01.tar.gz to /home/ytera/.kerl/archives Getting the checksum file from erlang.org... Verifying archive checksum... Checksum verified (f12d00f6e62b36ad027d6c0c08905fad) Extracting source code Building Erlang/OTP R15B01 (r15b01), please wait... Erlang/OTP R15B01 (r15b01) has been successfully built $ kerl list builds R15B01,r15b01 $ kerl install r15b01 ~/erlang/r15b01 Installing Erlang/OTP R15B01 (r15b01) in /home/ytera/erlang/r15b01... You can activate this installation running the following command: . /home/ytera/erlang/r15b01/activate Later on, you can leave the installation typing: kerl_deactivate $ kerl list installations r15b01 /home/ytera/erlang/r15b01 $ kerl active No Erlang/OTP kerl installation is currently active $ . /home/ytera/erlang/r15b01/activate $ kerl active The current active installation is: /home/ytera/erlang/r15b01 $ erl -version Erlang (SMP,ASYNC_THREADS,HIPE) (BEAM) emulator version 5.9.1 次は Riak を GitHub から Download してインストール # Installing Riak from Source を参考にインストール\n$ git clone git://github.com/basho/riak.git Initialized empty Git repository in /home/ytera/riak/.git/ remote: Counting objects: 13705, done. remote: Compressing objects: 100% (4461/4461), done. remote: Total 13705 (delta 8951), reused 13573 (delta 8837) Receiving objects: 100% (13705/13705), 10.16 MiB | 3.49 MiB/s, done. Resolving deltas: 100% (8951/8951), done. $ cd riak $ make rel クラスタを構成する3つの node を起動するための準備 # まずはコピー # $ for i in 1 2 3; do cp -a rel/riak rel/riak$i; done port を変更する # Riak は Protocol Buffer (pb_port) と HTTP (http) をサポートしている pb_port: 8087 -\u0026gt; 8187,8287,8387 (Protocol Buffer) http: 8098 -\u0026gt; 8198, 8298, 8398 (HTTP or HTTPS) handoff_port: 8099 -\u0026gt; 8199,9299,8399 (cluster 制御用) Protocol Buffer と HTTP では他のサーバーからのアクセスできるように bind address を 0.0.0.0 に変更している\n$ vi rel/riak1/etc/app.config $ diff -u rel/riak{,1}/etc/app.config --- rel/riak/etc/app.config\t2013-01-12 23:54:38.000000000 +0900 +++ rel/riak1/etc/app.config\t2013-01-13 14:18:45.052914322 +0900 @@ -12,11 +12,11 @@ %% pb_ip is the IP address that the Riak Protocol Buffers interface %% will bind to. If this is undefined, the interface will not run. - {pb_ip, \u0026quot;127.0.0.1\u0026quot; }, + {pb_ip, \u0026quot;0.0.0.0\u0026quot; }, %% pb_port is the TCP port that the Riak Protocol Buffers interface %% will bind to - {pb_port, 8087 } + {pb_port, 8187 } ]}, %% Riak Core config @@ -30,7 +30,7 @@ %% http is a list of IP addresses and TCP ports that the Riak %% HTTP interface will bind. - {http, [ {\u0026quot;127.0.0.1\u0026quot;, 8098 } ]}, + {http, [ {\u0026quot;0.0.0.0\u0026quot;, 8198 } ]}, %% https is a list of IP addresses and TCP ports that the Riak %% HTTPS interface will bind. @@ -45,7 +45,7 @@ %% riak_handoff_port is the TCP port that Riak uses for %% intra-cluster data handoff. - {handoff_port, 8099 }, + {handoff_port, 8199 }, %% To encrypt riak_core intra-cluster data handoff traffic, %% uncomment the following line and edit its path to an $ vi rel/riak1/etc/vm.args $ diff -u rel/riak{,1}/etc/vm.args --- rel/riak/etc/vm.args\t2013-01-12 23:54:38.000000000 +0900 +++ rel/riak1/etc/vm.args\t2013-01-13 14:18:53.802141554 +0900 @@ -1,5 +1,5 @@ ## Name of the riak node --name riak@127.0.0.1 +-name riak1@127.0.0.1 ## Cookie for distributed erlang. All nodes in the same cluster ## should use the same cookie or they will not be able to communicate. $ vi rel/riak2/etc/vm.args $ vi rel/riak3/etc/vm.args Riak を起動させて、クラスターを組む # 一つ目の Riak を起動させると epmd という process も起動する、これは Riak の node がお互いに探しあうために使われているようだ (Riak Users - epmd daemon runs after riak stops) そして、Riak を停止しても epmd は停止しないが、これは放置で問題ないとのこと\n$ rel/riak1/bin/riak start $ rel/riak2/bin/riak start $ rel/riak3/bin/riak start $ rel/riak2/bin/riak-admin cluster join riak1@127.0.0.1 Success: staged join request for 'riak2@127.0.0.1' to 'riak1@127.0.0.1' $ rel/riak3/bin/riak-admin cluster join riak1@127.0.0.1 Success: staged join request for 'riak3@127.0.0.1' to 'riak1@127.0.0.1' $ rel/riak2/bin/riak-admin cluster plan =============================== Staged Changes ================================ Action Nodes(s) ------------------------------------------------------------------------------- join 'riak2@127.0.0.1' join 'riak3@127.0.0.1' ------------------------------------------------------------------------------- NOTE: Applying these changes will result in 1 cluster transition ############################################################################### After cluster transition 1/1 ############################################################################### ================================= Membership ================================== Status Ring Pending Node ------------------------------------------------------------------------------- valid 100.0% 34.4% 'riak1@127.0.0.1' valid 0.0% 32.8% 'riak2@127.0.0.1' valid 0.0% 32.8% 'riak3@127.0.0.1' ------------------------------------------------------------------------------- Valid:3 / Leaving:0 / Exiting:0 / Joining:0 / Down:0 WARNING: Not all replicas will be on distinct nodes Transfers resulting from cluster changes: 42 21 transfers from 'riak1@127.0.0.1' to 'riak3@127.0.0.1' 21 transfers from 'riak1@127.0.0.1' to 'riak2@127.0.0.1' $ rel/riak2/bin/riak-admin cluster commit Cluster changes committed これで 8198,8298,8398 port にブラウザでアクセス可能となる。\nテスト # データを登録してみる (キーの値を指定しない場合は自動採番される)\n$ curl -v -d 'this is a test' -H \u0026quot;Content-Type: text/plain\u0026quot; http://127.0.0.1:8198/riak/test * About to connect() to 127.0.0.1 port 8198 (#0) * Trying 127.0.0.1... connected * Connected to 127.0.0.1 (127.0.0.1) port 8198 (#0) \u0026gt; POST /riak/test HTTP/1.1 \u0026gt; User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.13.1.0 zlib/1.2.3 libidn/1.18 libssh2/1.2.2 \u0026gt; Host: 127.0.0.1:8198 \u0026gt; Accept: */* \u0026gt; Content-Type: text/plain \u0026gt; Content-Length: 14 \u0026gt; \u0026lt; HTTP/1.1 201 Created \u0026lt; Vary: Accept-Encoding \u0026lt; Server: MochiWeb/1.1 WebMachine/1.9.2 (someone had painted it blue) \u0026lt; Location: /riak/test/8ug82sCaLj0HjrnKh8RxhyNVTW9 \u0026lt; Date: Sun, 13 Jan 2013 05:49:38 GMT \u0026lt; Content-Type: text/plain \u0026lt; Content-Length: 0 \u0026lt; * Connection #0 to host 127.0.0.1 left intact * Closing connection #0 登録したデータの確認 3つのどの node にアクセスしても取得可能\n$ curl http://127.0.0.1:8198/riak/test/8ug82sCaLj0HjrnKh8RxhyNVTW9 this is a test $ curl http://127.0.0.1:8298/riak/test/8ug82sCaLj0HjrnKh8RxhyNVTW9 this is a test $ curl http://127.0.0.1:8398/riak/test/8ug82sCaLj0HjrnKh8RxhyNVTW9 this is a test クラスタのオペレーションやもっと高度なテストはまた今度。\n","date":"2013年1月13日","permalink":"/2013/01/installing-riak-from-source-package/","section":"Posts","summary":"運用が楽な KVS ということで Riak をテストしてみる 1サーバーに複数 node を起動させてクラスタのテストをする ( Running-Multiple-Nodes-on-One-Host ) ために source から入れてみる ( bin/riak は shell script でこいつ","title":"Installing Riak from source package"},{"content":"その昔、SoftwareDesign 2010年11月号 で見たメールのエラーリターンを管理する仕組み BounceHammer を2年の年月を隔てて試してみた。 オープンソースのバウンスメール解析システム BounceHammer：メール｜gihyo.jp … 技術評論社 各社それぞれの思惑で実装されたエラーメールはそれはそれはバリエーションに富んでおり、これの Parser を書くのは大変骨の折れる作業だから、これはとてもありがたい。 これまで試さなかったのは、Perl のモジュール沢山いれるの嫌だなというのが原因なのと、一応それらしく動いているっぽい独自の仕組みが存在したから。でも誰もメンテしないのでなんか怪しくなってきた。そこで過去の記憶をたどり試してみることにした。 cpanm を覚えた今 ( いまさら cpanm ) となっては、インストールはとっても簡単。 CentOS 6 の /opt/bouncehammer にインストールする方法 Perl の Module 読み込みのための環境変数設定``` $ echo \u0026ldquo;export PERL5OPT=\\\u0026quot;-I~/bouncehammer/lib/perl5 -I~/bouncehammer/lib/perl5/$(perl -MConfig -e \u0026lsquo;print $Config{archname}')\\\u0026rdquo;\u0026quot; \u0026raquo; .bashrc\n$ wget http://dist.bouncehammer.jp/bouncehammer-2.7.9.tar.gz $ tar zxvf bouncehammer-2.7.9.tar.gz $ cd bouncehammer-2.7.9 $ mkdir ~/bouncehammer $ perl Modules.PL missing | awk '{print $4}' | cpanm -l ~/bouncehammer $ ./configure --prefix=~/bouncehammer $ make $ make test $ make install ```DB は PostgreSQL にしておく (標準の yum のはいまだに 8.4 だけど)、MySQL でも OK``` $ sudo yum install postgresql-devel postgresql-server $ cpanm -l ~/bouncehammer DBD::Pg $ sudo -u postgres initdb -E utf8 --locale=ja\\_JP.utf8 -D /var/lib/pgsql/data $ sudo /sbin/chkconfig postgresql on $ sudo /sbin/service postgresql start $ sudo -u postgres createuser bouncehammer $ sudo -u postgres createdb -E utf8 -O bouncehammer bouncehammer $ cat ~/bouncehammer/share/script/PostgreSQL\\*.sql | psql -U bouncehammer bouncehammer $ cat ~/bouncehammer/share/script/mastertable-\\*.sql | psql -U bouncehammer bouncehammer ```BounceHammer 設定``` $ cd ~/bouncehammer/etc $ cp available-countries{-example,} $ cp bouncehammer.cf{-example,} $ touch neighbor-domains $ cp webui.cf{-example,} $ vi bouncehammer.cf $ vi webui.cf ```送信元メールアドレスの domain 登録``` $ ~/bouncehammer/bin/tablectl --insert -ts --name example.com ```確認``` $ ~/bouncehammer/bin/tablectl --list -ts -Fa .----------------------------------------------. | SenderDomains | +-----+---------------+-------------+----------+ | #ID | domainname | description | disabled | +-----+---------------+-------------+----------+ | 1 | m3.com | | 0 | | 2 | askdoctors.jp | | 0 | '-----+---------------+-------------+----------' ```メールの取り込みは fetchmail \u0026amp; procmail``` $ sudo yum install fetchmail procmail $ cat \u0026gt; ~/.fetchmailrc \u0026lt;\u0026lt;\\_EOD\\_ set no bouncemail defaults uidl no mimedecode keep poll メールサーバー名 protocol pop3 user ユーザー名 password パスワード smtphost localhost mda /usr/bin/procmail \\_EOD\\_ $ cat \u0026gt; ~/.procmailrc \u0026lt;\u0026lt;\\_EOD\\_ MAILDIR=$HOME/Maildir DEFAULT=$MAILDIR/ LOCKFILE=$HOME/procmail.lock $LOGFILE=$HOME/procmail.log \\_EOD\\_ ```メール受信``` $ fetchmail --ssl ```これでメールが ~/Maildir/new/ に溜まる (.fetchmailrc に keep と書いているため、メールサーバーからは削除しないので ~/.fetchids に受信済み UIDL が保存される) Web Interface は Apache + CGI (mod\\_perl も可能)``` $ sudo yum install httpd $ sudo cp ~/bouncehammer/share/script/bouncehammer.cgi /var/www/cgi-bin/ $ sudo cp ~/bouncehammer/share/script/api.cgi /var/www/cgi-bin/ $ sudo chmod 755 /var/www/cgi-bin/{bouncehammer,api}.cgi $ sudo vi /var/www/cgi-bin/{bouncehammer,api}.cgi use lib '/home/xxx/bouncehammer/lib'; ↓ use lib qw(/home/xxx/bouncehammer/lib /home/xxx/bouncehammer/lib/perl5); ```リターンメールを BounceHammer に登録``` $ ~/bouncehammer/bin/mailboxparser -g --log ~/Maildir/new --remove (~/bouncehammer/var/spool/ にデータが溜まる) $ ~/bouncehammer/bin/logger -c --remove (spool のデータから var/log/hammer.YYYY-MM-DD.log を生成) $ ~/bouncehammer/bin/databasectl --update --today (log/hammer.YYYY-MM-DD.log の当日分をDBに登録) $ ~/bouncehammer/bin/databasectl --update --yesterday (log/hammer.YYYY-MM-DD.log の前日分をDBに登録) ```初回は全部登録させる``` $ for f in ~/bouncehammer/var/log/\\*.log do ~/bouncehammer/bin/databasectl --update $f done ```海外のMTAからのメールとか、ひどい実装のMTAが変なリターンメールを送ってくるから UTF-8 じゃねーよと言われてDBに登録できないことがあるのにちょっと対応が必要かな","date":"2013年1月10日","permalink":"/2013/01/bouncehammer%E3%82%92%E8%A9%A6%E3%81%97%E3%81%A6%E3%81%BF%E3%81%9F_%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB%E7%B7%A8/","section":"Posts","summary":"その昔、SoftwareDesign 2010年11月号 で見たメールのエラーリターンを管理する仕組み BounceHammer を2年の年月を隔てて試してみた。 オープン","title":"BounceHammerを試してみた (インストール編)"},{"content":"","date":"2013年1月10日","permalink":"/tags/mail/","section":"Tags","summary":"","title":"mail"},{"content":"は参考になった。SSL の仕組みからするとそういう通信は発生するはずだよなぁとは思いつつも調べて見ることはしてこなかった。(まぁ、今回もちょっと調べるだけだけど) しかし、『安い証明書だと中間証明書というものが入っており』は間違いでしょ。GlobalSignだって3階層、EVは4階層で、VeriSignも同様。 GlobalSign 以外はこれを速くするってことにあまり積極的じゃないのかな？ OCSP and CRL Performance Report を見ると GlobalSign が圧倒的!! VeriSign 傘下以外はアクセスが少ないから速いのかなぁ? 200ms 以上の差って大きいな。\nGLobalSign: 76ms VeriSign / Thawte: 299ms GeoTrust / RapidSSL: 295ms あのスピード狂の Google が使ってるのは Thawte だけど、一度アクセスすればしばらくキャッシュされるからそれほど重要じゃなかったのだろうか。False Start, OCSP preloading, Snap Start なんかで対応してるんですね。\nワイルドカード証明書にしておけばキャッシュの範囲が広くて有利? OCSP verification with OpenSSL に OpenSSL を使って OCSP 検証する方法が載ってる。試してみたけどへーって感じで\u0026hellip; トレンドマイクロの年間固定で同一組織なら無制限でEVでも取得し放題っていうのが気になってる。\nまぁ、普通の人は RapidSSL とかの安さに負けるよねぇ。\n追記\nこのサイト超便利 WebPagetest - Website Performance and Optimization Test ここも参考になる SSL Performance Case Study | Insouciant Google って最近のは Google Internet Authority が発行してるんですね   ","date":"2013年1月8日","permalink":"/2013/01/ocsp-and-crl-performance-report/","section":"Posts","summary":"は参考になった。SSL の仕組みからするとそういう通信は発生するはずだよなぁとは思いつつも調べて見ることはしてこなかった。(まぁ、今回もちょっ","title":"OCSP and CRL Performance Report"},{"content":"","date":"2013年1月7日","permalink":"/tags/vim/","section":"Tags","summary":"","title":"vim"},{"content":"自作 RPM を作ろうかと思って\nvim xxx.spec って実行したら\nName: Version: Release: 1%{?dist} Summary: Group: License: URL: Source0: BuildRoot: %(mktemp -ud %{_tmppath}/%{name}-%{version}-%{release}-XXXXXX) BuildRequires: Requires: %description %prep %setup -q %build %configure \u0026quot;/usr/share/vim/vimfiles/template.spec\u0026quot; 42L, 405C 1,1 Top って表示されて、「えっ！新規ファイルだったはずなのに！？なんで？」と驚いてしまいました。template 機能があるんですね。 /etc/vimrc に\nautocmd BufNewFile *.spec 0r /usr/share/vim/vimfiles/template.spec って書いてありました。\n:h template で help が出てきます。\n Big Sky :: ファイルタイプ別にテンプレートを選べるsonictemplate-vim書いた blog.paz-para.com » Blog Archive » Vimでテンプレートを挿入する  ","date":"2013年1月7日","permalink":"/2013/01/vim-template/","section":"Posts","summary":"自作 RPM を作ろうかと思って vim xxx.spec って実行したら Name: Version: Release: 1%{?dist} Summary: Group: License: URL: Source0: BuildRoot: %(mktemp -ud %{_tmppath}/%{name}-%{version}-%{release}-XXXXXX) BuildRequires: Requires: %description %prep %setup -q %build %configure \u0026quot;/usr/share/vim/vimfiles/template.spec\u0026quot; 42L, 405C 1,1 Top って表示されて、「えっ！新規ファイルだったはず","title":"Vim の template"},{"content":"もともと Perl メインでプログラミングしてたのですが、CPAN が肥大化(ただメール送るためだけに Email::Sender::Simple を入れようとしたらすげー依存で大量の CPAN モジュールがインストールされて Enter 打つだけで疲れた、あと yum で入る奴とそうでないのが混ざって気持ち悪い)してたのでなんか面倒で避けてたのですが、リターンメールの処理に BounceHammer を使ってみようかなと思って重い腰を上げてみた。 自分でコード書くときはできるだけ依存させないように、標準モジュールでおさまるようにしてた。大したコードではないけれど。もちろん車輪の再発明もある。 cpanm すら避けていたが、Rails が bundler でかなりイケてる感じなので Perl にも似たようなのないかなと思ったら、やっぱりあるんですね Carton !! あれ、でもα版で止まってる？？？ Carton 使ってたけど cpanm に戻したっていう blog もあったので cpanm だけで行く事に。``` $ cpanm -l /path/to/install ModuleName\nって超便利!! なぜ今まで試さなかったんでしょうか。 こんなに簡単にインストールできるのに $ cd ~/bin $ curl -LO http://xrl.us/cpanm $ chmod +x cpanm\nあと、これ $ export PERL5OPT=-I$HOME/perl5/lib/perl5\n","date":"2013年1月7日","permalink":"/2013/01/cpanm/","section":"Posts","summary":"もともと Perl メインでプログラミングしてたのですが、CPAN が肥大化(ただメール送るためだけに Email::Sender::Simple を入れようとしたらすげー依存で大量の CPAN モジュール","title":"いまさら cpanm"},{"content":"そうかぁ、Z3からはDTCP-IPサーバー機能があるのかぁ、我が家のはZ2だったよ。 DiXiM Digital TV plus で見れないわけだ。ソフトウェアアップデートで対応して欲しいよなぁ。nasne 欲しいな。買っちゃおうかな。 東芝、AVC長時間録画/DLNAサーバー搭載「REGZA Z3」 - AV Watch DiXiM BD Burner 2013 で録画番組の move は可能っぽいな。(●東芝REGZA Z2レグザ42Z2買って使ったレビュー 旧機種比較 最安値 - レグザREGZA研究)\n","date":"2013年1月6日","permalink":"/2013/01/regza-dtcp-ip-server/","section":"Posts","summary":"そうかぁ、Z3からはDTCP-IPサーバー機能があるのかぁ、我が家のはZ2だったよ。 DiXiM Digital TV plus で見れないわけだ。ソフトウェアアップデートで対応","title":"REGZAのDTCP-IPサーバー"},{"content":"Redis の内部を探ってみる (save) の続き、今回は appendonlyfile について見てみよう。 と思ってたらRedis Persistence (redis.io) に全部書いてあるじゃない\u0026hellip; 更新系コマンドについて、レスポンスを返す前にずっとログファイルに追記して、最起動時にはそれを順番にリプレイするんですね。 save はある瞬間の dump でしかないので、電源障害などの時に最後の save 以降のデータを失うが appendonlyfile を有効にすることで回避できる、その分レスポンスタイムは悪くなる。 ファイルへの書き込みの sync タイミングを\n 常に sync 1秒おきに sync (default) 明示的な sync を行わない  から選べる。(上側がより遅い) 同じキーの更新を続けると無駄にファイルが肥大化する(counterのincrementとか)ので save と同様の仕組みで fork してその時点の dump からの追記へと作りなおすことができる。この処理中の変更はメモリに溜めておいて処理後に追加するが、処理中も古いファイルへ書き込みを続けているので安全。 ちなみに shutdown 時には sync される、aof が無効で save が有効な場合は shutdown 時に save されるので、正常は Redis の再起動ではデータは失われない。 ドットインストールに Redisの基礎 (全14回) ができてみたいです。 『Redisの基礎 (全14回)』をドットインストールに追加しました #dotinstall\n","date":"2013年1月4日","permalink":"/2013/01/redis-inside-aof/","section":"Posts","summary":"Redis の内部を探ってみる (save) の続き、今回は appendonlyfile について見てみよう。 と思ってたらRedis Persistence (redis.io) に全部書いてあるじゃない\u0026hellip; 更新系コマンドに","title":"Redis の内部を探ってみる (aof)"},{"content":"カフェインなしでは眠くて仕事できない\u0026amp;なにか口にしてないと仕事できないので1杯20円のブルックスを買って毎日飲んでましたが、ある時ブルックスを切らしてしまいスターバックス オリガミ® パーソナルドリップ™ コーヒー エスプレッソ ローストを買って飲んだら美味かった。その後ブルックスを飲んだら不味くてもう飲むのが嫌になってしまった。しかし、毎日3〜4杯飲むのに1杯100円のスタバのは辛いので探してみたところ辻本珈琲が良さそうだったので現在お試し中。ブルックスよりは美味しい。1杯あたり45.6円。\n 辻本珈琲 ブルックス  ","date":"2012年12月29日","permalink":"/2012/12/tsujimoto-coffee/","section":"Posts","summary":"カフェインなしでは眠くて仕事できない\u0026amp;なにか口にしてないと仕事できないので1杯20円のブルックスを買って毎日飲んでましたが、ある時ブ","title":"辻本珈琲"},{"content":"Redis の versin up ついでに内部を軽く探ってみる 対象の version は 2.6.7 Redis を起動すると WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. と表示されることから、save って fork した子供がコピーされたメモリを dump してるのかぁと想像し、実際にコードを見てみることにした。 rdb.c の rdbSaveBackground() の中で fork して書き出している。 fork した子供を看取るのは redis.c の serverCron() がずっと looop で動いていて、いろんな処理をしてる中の一つの仕事で rdb.c の backgroundSaveDoneHandler()。rdbSaveBackground() を実行するのも serverCron() の中。 ということで、save (dump) 処理でレスポンスが落ちるとしたらそれは save 中に更新が多くてメモリの Copy on write が沢山発生する場合なんだろうな。 次回は aof (AppendOnlyFile) を調べてみる。\n","date":"2012年12月27日","permalink":"/2012/12/redis-inside-save/","section":"Posts","summary":"Redis の versin up ついでに内部を軽く探ってみる 対象の version は 2.6.7 Redis を起動すると WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. と","title":"Redis の内部を探ってみる (save)"},{"content":"日本語の方が気軽に読めて便利なこともあるので\nsudo apt-get install manpages-ja ","date":"2012年12月22日","permalink":"/2012/12/manpages-ja/","section":"Posts","summary":"日本語の方が気軽に読めて便利なこともあるので sudo apt-get install manpages-ja","title":"Linux Mint (Ubuntu) に日本語 manpage をインストール"},{"content":"","date":"2012年12月19日","permalink":"/tags/mosh/","section":"Tags","summary":"","title":"mosh"},{"content":"iPhone アプリの iSSH が mosh をサポートしたというのでさくらインターネットで借りてる VPS に mosh でログインできるようにしてみました。 VPS は CentOS 6 で epel リポジトリに mosh があったので``` sudo yum install mosh\nVPS で UDP 60000 - 61000 ポートを開放 (ポートは変更可能) sudo iptables -A INPUT -m udp -p udp \u0026ndash;dport 60000:61000 -j ACCEPT (たとえば)\nまずは手元の Linux Mint 14.1 からログインできるように sudo apt-get install mosh\nSSH の設定は ~/.ssh/config で Host vps として設定済み mosh vps\n次に iSSH からの接続テスト、でも LANG が C だとダメらしく /etc/sysconfig/i18n で LANG=en\\_US.UTF=8 にして再起動したらつながりました。 Linux から接続すると mosh-server の引数で LANG が設定されてるけど、iSSH だと設定されてないのが原因っぽい $ ps -ef | grep mosh ytera 1720 1 0 22:18 ? 00:00:01 mosh-server new -s -c 8 -l LANG=ja_JP.UTF-8 ytera 2033 1 0 22:45 ? 00:00:00 mosh-server new\n [![iSSH - SSH / VNC Console](http://a1927.phobos.apple.com/us/r1000/119/Purple/v4/0a/ff/07/0aff072e-5425-7033-24c1-7e026f5c9079/icon.png \u0026quot;iSSH - SSH / VNC Console\u0026quot;)](https://itunes.apple.com/jp/app/issh-ssh-vnc-console/id287765826?mt=8\u0026amp;uo=4) [iSSH - SSH / VNC Console](https://itunes.apple.com/jp/app/issh-ssh-vnc-console/id287765826?mt=8\u0026amp;uo=4) Zinger-Soft 価格： 850円 [![iTunesで見る](http://ax.phobos.apple.com.edgesuite.net/ja_jp/images/web/linkmaker/badge_appstore-sm.gif)](https://itunes.apple.com/jp/app/issh-ssh-vnc-console/id287765826?mt=8\u0026amp;uo=4) posted with [sticky](http://sticky.linclip.com/linkmaker/) on 2012.12.19 iSSH は SSH Tunnel + RemoteDesktop も簡単にできるし、Bluetooth keyboard で Ctrl key も使えてとっても便利。 アプリとしてはちょっと高めだけど価格なりの価値はあります。安いのを沢山試すくらいならこれでOK * [Jun Mukai's blog: mosh (mobile shell)の論文を読んでみた](http://blog.jmuk.org/2012/04/mosh-mobile-shell.html) * [#16 「moshは確かにイノベーション」 tech.kayac.com Advent Calendar 2012 | tech.kayac.com - KAYAC engineers' blog](http://tech.kayac.com/archive/16_advent_calender_2012.html)","date":"2012年12月19日","permalink":"/2012/12/connect-to-sakura-vps-with-mosh/","section":"Posts","summary":"iPhone アプリの iSSH が mosh をサポートしたというのでさくらインターネットで借りてる VPS に mosh でログインできるようにしてみました。 VPS は CentOS 6 で epel リポジトリに mosh が","title":"さくらのVPSに mosh で接続"},{"content":"Linux Mint 14.1 から Windows へ RemoteDesktop 接続するのには Remmina を使うのかなと思って試してみたが、キーマップが合わなかったので設定を見てみて RDP 欄の「クライアントのキーボードマッピングを使用する」にチェックを入れたら CapsLock を Ctrl に入れ替えてるのも反映されたけど、「| (縦棒)」だけが入力できなかった\u0026hellip; これでは pipe での処理 (ps -ef | grep xxx とか) が出来なくて致命的なので馴染みの rdesktop をインストールした。\nrdesktop -f -a 16 -k ja -z -u {ユーザー名} -d {ADドメイン} {接続先ホスト} -k でキーマップを指定している。使えるマップは /usr/share/rdesktop/keymaps/ にあるもの。\nUbuntu 12.04のリモートデスクトップ・クライアントはRemminaに変更 - 情報技術の四方山話\n","date":"2012年12月16日","permalink":"/2012/12/remotedesktop-from-linux-mint/","section":"Posts","summary":"Linux Mint 14.1 から Windows へ RemoteDesktop 接続するのには Remmina を使うのかなと思って試してみたが、キーマップが合わなかったので設定を見てみて RDP 欄の「クライアントのキーボード","title":"Linux Mint から Windows へのリモートデスクトップ"},{"content":"ネットワークの設定から無線LANを無効にしてみたら、元に戻せなくなった\u0026hellip; 再起動してもダメだった。\nifconfig wlan0 up してみたら\nSIOCSIFFLAGS: Operation not possible due to RF-kill と表示されたので RF-kill でググってみたところ rfkill っていうコマンドがあったので\nrfkill unblock wifi で戻せました。\n$ sudo rfkill list 0: sony-wifi: Wireless LAN Soft blocked: no Hard blocked: no 1: sony-bluetooth: Bluetooth Soft blocked: yes Hard blocked: no 2: phy0: Wireless LAN Soft blocked: no Hard blocked: no Bluetooth はタスクバー(?)の Bluetooth アイコンから Off / On できた。\nVAIO T11 には無線LANとかを On / Off するハードウェアスイッチがない。\n","date":"2012年12月16日","permalink":"/2012/12/tool-for-enabling-and-disabling-wireless-devices/","section":"Posts","summary":"ネットワークの設定から無線LANを無効にしてみたら、元に戻せなくなった\u0026hellip; 再起動してもダメだった。 ifconfig wlan0 up してみたら SIOCSIFFLAGS: Operation not possible due to RF-kill","title":"Linux Mint で無線LANが無効から戻らない"},{"content":"Ubuntuでデスクトップのディレクトリ名を「Desktop」にする - ぬいぐるみライフ(仮)``` LANG=C xdg-user-dirs-gtk-update\n","date":"2012年12月12日","permalink":"/2012/12/change-directory-name-language/","section":"Posts","summary":"Ubuntuでデスクトップのディレクトリ名を「Desktop」にする - ぬいぐるみライフ(仮)``` LANG=C xdg-user-dirs-gtk-update","title":"ホームディレクトリの「デスクトップ」とかを英語に変更"},{"content":"VAIO T11 の Pad が大きくて、ホームポジションのちょい右寄りにあるので右手のハラが触れて誤操作してしまうので無効にする方法を調べてみた。``` synclient MaxTapTime=0\n","date":"2012年12月12日","permalink":"/2012/12/disabling-touchpad-tap/","section":"Posts","summary":"VAIO T11 の Pad が大きくて、ホームポジションのちょい右寄りにあるので右手のハラが触れて誤操作してしまうので無効にする方法を調べてみた。``` synclient MaxTapTime=0","title":"TouchPad の Tap を無効にする"},{"content":"※ 2014/03/29 on と off が逆だったので訂正 Linux でバッテリー駆動時の無線LANが異常に遅いなあと思って調べてみた。 省電力のためにバッテリー駆動時には省エネモードに切り替えられていた。 iwconfig で確認すると Bit Rate=65 Mb/s → 1 Mb/s ACアダプターを挿したり抜いたりすると /var/log/pm-powersave.log に何をやってるかが出力される。(Linux Mint 14.1 で確認) あまりに遅いので、バッテリー駆動時に一時的に速度を上げたい場合には\nsudo iwconfig wlan0 power off を実行し、Power Management を無効にすることで速くできる。用が終わったら\nsudo iwconfig wlan0 power on で元に戻すことを忘れずに。 ACアダプター接続時\nmypc log # iwconfig wlan0 wlan0 IEEE 802.11bgn ESSID:\u0026quot;********\u0026quot; Mode:Managed Frequency:2.442 GHz Access Point: XX:XX:XX:XX:XX:XX Bit Rate=65 Mb/s Tx-Power=16 dBm Retry long limit:7 RTS thr:off Fragment thr:off Encryption key:off Power Management:off Link Quality=70/70 Signal level=-33 dBm Rx invalid nwid:0 Rx invalid crypt:0 Rx invalid frag:0 Tx excessive retries:29 Invalid misc:7578 Missed beacon:0 バッテリー駆動時\nmypc log # iwconfig wlan0 wlan0 IEEE 802.11bgn ESSID:\u0026quot;********\u0026quot; Mode:Managed Frequency:2.442 GHz Access Point: XX:XX:XX:XX:XX:XX Bit Rate=1 Mb/s Tx-Power=16 dBm Retry long limit:7 RTS thr:off Fragment thr:off Encryption key:off Power Management:on Link Quality=69/70 Signal level=-41 dBm Rx invalid nwid:0 Rx invalid crypt:0 Rx invalid frag:0 Tx excessive retries:29 Invalid misc:7584 Missed beacon:0 ","date":"2012年12月9日","permalink":"/2012/12/wireless-lan-power-saving/","section":"Posts","summary":"※ 2014/03/29 on と off が逆だったので訂正 Linux でバッテリー駆動時の無線LANが異常に遅いなあと思って調べてみた。 省電力のためにバッテリー駆動時には省エネモー","title":"Battey駆動時の無線LANが遅い"},{"content":"Cinnamon で使いそうなキーボードショートカットの一覧 Super は Windows キー\nワークスペース一覧\nCtrl + Alt + Up\nWindowの最大化\nSuper + Up\nWindowの最大化解除\nSuper + Down\nWindowを閉じる\nAlt + F4\nデスクトップを表示\nCtrl + Alt + d\nWindow切り替え\nAlt + Tab\n右のワークスペースに切り替え\nCtrl + Alt + Right\n左のワークスペースに切り替え\nCtrl + Alt + Left\nカスタムショートカット\nTerminal起動\nCtrl + Alt + t\n","date":"2012年12月9日","permalink":"/2012/12/cinnamon-keyboard-shortcut/","section":"Posts","summary":"Cinnamon で使いそうなキーボードショートカットの一覧 Super は Windows キー ワークスペース一覧 Ctrl + Alt + Up Windowの最大化 Super + Up Windowの最大化解除 Super + Down W","title":"Cinnamon のキーボードショートカット"},{"content":"Linux Mint を使い始めたが、小さなモニターでの作業は辛いのでワークスペースを切り替えながら使おうと思ったらどこで切り替えるのかわからず・・・ Cinnamon の設定からできるらしいと分かったのだが、はて、どこにその設定画面が・・・ と探してたら右下に上矢印っぽいアイコンがあってそこから行けました。 「すべての設定」から「ホットコーナー」で設定。 しかし、わざわざ角までポインタを移動させるのは大変だ。キーボードショートカットはなんだろう？ Alt+F1 で切り替え画面になるので、その後ワークスペースの数字を入力して切り替えるか左右の矢印キーで切り替えられることが分かった。 さらに、キーボードの設定からショートカットで設定可能だった。 Ctrl+Alt+Right, Ctrl+Alt+Left で左右に切り替え可能。\n","date":"2012年12月8日","permalink":"/2012/12/cinnamon-workspace/","section":"Posts","summary":"Linux Mint を使い始めたが、小さなモニターでの作業は辛いのでワークスペースを切り替えながら使おうと思ったらどこで切り替えるのかわからず・・・ Cinnamon の設定","title":"Cinnamon でのワークスペース切り替え"},{"content":"","date":"2012年12月7日","permalink":"/tags/iphone/","section":"Tags","summary":"","title":"iPhone"},{"content":"iPhone でニュース記事などを読むのに Site Viewer の頃から長らく ニュースボード を愛用していました。オフラインで読めるようにキャッシュされるし、任意の RSS から本文全体をスマホ用に変換してくれるので全文が読める。さらに、2ページ、3ページに渡る記事もまとめてくれるので大変重宝していました。 しかし、iPhone5 になってからたまに記事取得時に落ちたり、ずっと iPhone5 のディスプレイサイズに対応してもらえなかったり、一部のサイトで装飾された文字が消えてしまうという問題が改善されないので他のに乗り換えたいと何度か考えてました。が、なかなか私の希望を満たしてくれるものが見つからないでいました。 そして最近ついに見つけました。Newsify RSS Reader (Google Reader Client) です。 まず、何より見た目がカッコイイ！！ でもただの Google Reader では記事本文が入ってなかったりするので物足りません。 そこで Full Text Feed を組み合わせることにしました。この組み合わせなかなかイケてます。\n Newsify RSS Reader (Google Reader Client)  \nNewsify RSS Reader (Google Reader Client)\nBen Alexander\n価格： 85円   \nposted with sticky on 2012.12.19\n ニュースボード  \nニュースボード\nMobilus Corporation\n価格： 0円   \nposted with sticky on 2012.12.19\n","date":"2012年12月7日","permalink":"/2012/12/iphone-news-reader-appli/","section":"Posts","summary":"iPhone でニュース記事などを読むのに Site Viewer の頃から長らく ニュースボード を愛用していました。オフラインで読めるようにキャッシュされるし、任意の RSS から本","title":"iPhone のニュースリーダーアプリ"},{"content":"","date":"2012年12月7日","permalink":"/tags/%E3%82%A2%E3%83%97%E3%83%AA/","section":"Tags","summary":"","title":"アプリ"},{"content":"","date":"2012年12月7日","permalink":"/tags/vaio/","section":"Tags","summary":"","title":"VAIO"},{"content":"Mac Book Air に惹かれながらも、Mac OS X が好きになれないので Sony の Ultrabook VAIO T11 を購入し、Linux Mint をインストールしました。 持ち運び重視なら注目！ 11.6型Ultrabook「VAIO T 11」   この VAIO Tシリーズは Ultrabook でありながら、メモリ、HDD/SSD、バッテリーが自分で交換できるというスグレモノ。 Sony Store でメモリを Max の 8GB にし、他は一番廉価かモデルを選択。 購入後に HDD は SSD M4-CT128M4SSD1 に交換し、Linux Mint をインストールしました。 光学ドライブは無いため Universal USB Installer で USB のインストーラーを準備しましたが、HDD 以外から起動する方法が最初わからなくて悩みました。 きっと起動時に Function キーのどれかを押すんだろうと思って試そうとしたが、UEFI ブートが高速すぎて押す隙がありません\u0026hellip; 付属のドキュメントを読んだら「ASSISTキー」という専用の起動ボタンがありました。 ここから起動するとリカバリしたり他のメディアから軌道させることができます。 インストールが完了し、再起動したらやはり「Secure Boot」の制限にひっかかりましたので「Secure Boot」を無効にしました。 タッチパッドもカメラも有線・無線LANも問題なく動作してます。\n","date":"2012年12月7日","permalink":"/2012/12/install-linux-mint-14-1-on-vaio-t11/","section":"Posts","summary":"Mac Book Air に惹かれながらも、Mac OS X が好きになれないので Sony の Ultrabook VAIO T11 を購入し、Linux Mint をインストールしました。 持ち運び重視なら注目！ 11.6","title":"VAIO T11 に Linux Mint 14.1 をインストール"},{"content":"dmidecode コマンドを使ってハードウエアの情報が取得できます。 \u0026ldquo;-t memory\u0026rdquo; と指定すればメモリの情報だけを取得できます。\n$ sudo dmidecode -t memory # dmidecode 2.11 # SMBIOS entry point at 0xaaebdf18 SMBIOS 2.7 present. Handle 0x000B, DMI type 16, 23 bytes Physical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 2 GB Error Information Handle: Not Provided Number Of Devices: 2 Handle 0x000C, DMI type 17, 34 bytes Memory Device Array Handle: 0x000B Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 4096 MB Form Factor: SODIMM Set: None Locator: SODIMM1 Bank Locator: Bank 0 Type: DDR3 Type Detail: Unknown Speed: 1333 MHz Manufacturer: Unknown Serial Number: 00000000 Asset Tag: Unknown Part Number: Not Specified Rank: 1 Configured Clock Speed: 1333 MHz Handle 0x000D, DMI type 17, 34 bytes Memory Device Array Handle: 0x000B Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 4096 MB Form Factor: SODIMM Set: None Locator: SODIMM2 Bank Locator: Bank 1 Type: DDR3 Type Detail: Unknown Speed: 1333 MHz Manufacturer: Samsung Serial Number: 00ACDF4C Asset Tag: Unknown Part Number: M471B5273DH0-YH9 Rank: 2 Configured Clock Speed: 1333 MHz ","date":"2012年12月7日","permalink":"/2012/12/how-to-check-memory-pn-on-linux/","section":"Posts","summary":"dmidecode コマンドを使ってハードウエアの情報が取得できます。 \u0026ldquo;-t memory\u0026rdquo; と指定すればメモリの情報だけを取得できます。 $ sudo dmidecode -t memory # dmidecode 2.11 # SMBIOS entry point at 0xaaebdf18 SMBIOS 2.7 present. Handle 0x000B, DMI","title":"Linux でメモリの型番を調べる"},{"content":"ls コマンドでファイルの timesamp (mtime) を秒(もしくはより詳細)まで確認する方法です。 ls -l では分までです\n$ date Sat Sep 8 08:37:25 JST 2012 $ touch test $ ls -l test -rw-r--r-- 1 ytera users 0 Sep 8 08:37 test そして、古いファイルだと日までしか表示されません。 2年前に変更して確認します\n$ touch -t $(date -d \u0026quot;2 year ago\u0026quot; +%y%m%d%H%M) test $ ls -l test -rw-r--r-- 1 ytera users 0 Sep 8 2010 test そこで --full-time オプションです。\n$ ls -l --full-time test -rw-r--r-- 1 ytera users 0 2010-09-08 08:39:00.000000000 +0900 test touch -t で mtime を更新したため秒以下が 0 になっているので 再度更新して確認\n$ touch test $ ls -l --full-time test -rw-r--r-- 1 ytera users 0 2012-09-08 08:43:01.410675831 +0900 test atime も ctime も確認したい場合は stat コマンドです\n$ stat test File: `test' Size: 0 Blocks: 0 IO Block: 4096 regular empty file Device: fc03h/64515d Inode: 2753833 Links: 1 Access: (0644/-rw-r--r--) Uid: ( 1000/ ytera) Gid: ( 100/ users) Access: 2012-09-08 08:43:01.410675831 +0900 Modify: 2012-09-08 08:43:01.410675831 +0900 Change: 2012-09-08 08:43:01.410675831 +0900 ext4 や ZFS などでは秒より細かな制度でタイムスタンプが保存されてます、ext2, ext3, ufs などでは秒までです。\n","date":"2012年12月7日","permalink":"/2012/12/ls-command-full-time-option/","section":"Posts","summary":"ls コマンドでファイルの timesamp (mtime) を秒(もしくはより詳細)まで確認する方法です。 ls -l では分までです $ date Sat Sep 8 08:37:25 JST 2012 $ touch test $ ls -l test -rw-r--r-- 1 ytera users 0 Sep 8 08:37 test そ","title":"ls コマンドで秒まで表示"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. 🚀\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","date":"1年1月1日","permalink":"/tags/advanced/","section":"Tags","summary":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. 🚀\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.","title":"advanced"}]